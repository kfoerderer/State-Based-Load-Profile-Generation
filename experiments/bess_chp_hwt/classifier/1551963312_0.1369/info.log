2019-03-07 13:55:12,643 [INFO] learning_rate_initialization: 0.001000, learning_rate_loss_factor: 0.010000, learning_rate_decay_after: 30000, learning_rate_decay_at: 10000, learning_rate_decay_factor: 0.250000
2019-03-07 13:55:12,861 [INFO] Training model #0: (9, 64, 402) @ 2
2019-03-07 13:55:32,365 [INFO] ---------------------------------
2019-03-07 13:55:32,367 [INFO] Summary:
2019-03-07 13:55:32,367 [INFO] Batch 1000, worst loss 1940.467407 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:55:32,368 [INFO] Regularization: 3290.090820 * 0.0001000000 = 0.3290090859
2019-03-07 13:55:32,369 [INFO] Sum of grad norms: 5.723672
2019-03-07 13:55:32,370 [INFO] ---------------------------------
2019-03-07 13:55:51,819 [INFO] ---------------------------------
2019-03-07 13:55:51,820 [INFO] Summary:
2019-03-07 13:55:51,821 [INFO] Batch 2000, worst loss 0.792152 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:55:51,822 [INFO] Regularization: 3022.980469 * 0.0001000000 = 0.3022980392
2019-03-07 13:55:51,823 [INFO] reducing reg_loss_factor
2019-03-07 13:55:51,823 [INFO] Sum of grad norms: 7.647015
2019-03-07 13:55:51,824 [INFO] ---------------------------------
2019-03-07 13:56:04,900 [INFO] ---------------------------------
2019-03-07 13:56:04,901 [INFO] Evaluation:
2019-03-07 13:56:04,902 [INFO] Batch 2000, worst loss 0.443220 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:56:04,903 [INFO] New best loss 0.443220, saved to file classifier/1551963312/1551963364_0_classifier_2000.pth
2019-03-07 13:56:04,918 [INFO] Target
2019-03-07 13:56:04,919 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 ...
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]]
2019-03-07 13:56:04,922 [INFO] Classifier output
2019-03-07 13:56:04,922 [INFO] [[0.984771 0.984708 0.984767 ... 0.005427 0.005461 0.005425]
 [0.928056 0.92738  0.927373 ... 0.916236 0.916392 0.916353]
 [0.910419 0.909126 0.909206 ... 0.976121 0.976185 0.976135]
 ...
 [0.001425 0.001442 0.001441 ... 0.986511 0.986528 0.986443]
 [0.986991 0.987086 0.987122 ... 0.000012 0.000012 0.000012]
 [0.880072 0.880652 0.880449 ... 0.011278 0.011355 0.011283]]
2019-03-07 13:56:04,924 [INFO] ---------------------------------
2019-03-07 13:56:24,604 [INFO] ---------------------------------
2019-03-07 13:56:24,606 [INFO] Summary:
2019-03-07 13:56:24,606 [INFO] Batch 3000, worst loss 0.448231 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:56:24,607 [INFO] Regularization: 3166.104492 * 0.0000100000 = 0.0316610448
2019-03-07 13:56:24,608 [INFO] Sum of grad norms: 7.837489
2019-03-07 13:56:24,609 [INFO] ---------------------------------
2019-03-07 13:56:44,289 [INFO] ---------------------------------
2019-03-07 13:56:44,290 [INFO] Summary:
2019-03-07 13:56:44,290 [INFO] Batch 4000, worst loss 0.333207 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:56:44,291 [INFO] Regularization: 3211.087158 * 0.0000100000 = 0.0321108699
2019-03-07 13:56:44,292 [INFO] Sum of grad norms: 3.941230
2019-03-07 13:56:44,293 [INFO] ---------------------------------
2019-03-07 13:56:57,410 [INFO] ---------------------------------
2019-03-07 13:56:57,412 [INFO] Evaluation:
2019-03-07 13:56:57,413 [INFO] Batch 4000, worst loss 0.222340 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:56:57,414 [INFO] New best loss 0.222340, saved to file classifier/1551963312/1551963417_0_classifier_4000.pth
2019-03-07 13:56:57,428 [INFO] Target
2019-03-07 13:56:57,429 [INFO] [[0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 ...
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-07 13:56:57,431 [INFO] Classifier output
2019-03-07 13:56:57,432 [INFO] [[0.87354  0.873743 0.873679 ... 0.905585 0.905806 0.905685]
 [0.021428 0.021685 0.021669 ... 0.998429 0.998429 0.998429]
 [0.99716  0.997162 0.997162 ... 0.317006 0.316642 0.316805]
 ...
 [0.781978 0.78276  0.782869 ... 0.997238 0.997241 0.99724 ]
 [0.120461 0.120554 0.120444 ... 0.000456 0.000457 0.000457]
 [0.985945 0.985938 0.985933 ... 0.993617 0.993622 0.99362 ]]
2019-03-07 13:56:57,434 [INFO] ---------------------------------
2019-03-07 13:57:17,151 [INFO] ---------------------------------
2019-03-07 13:57:17,151 [INFO] Summary:
2019-03-07 13:57:17,152 [INFO] Batch 5000, worst loss 0.370696 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:57:17,153 [INFO] Regularization: 3188.975586 * 0.0000100000 = 0.0318897553
2019-03-07 13:57:17,154 [INFO] Sum of grad norms: 2.723719
2019-03-07 13:57:17,156 [INFO] ---------------------------------
2019-03-07 13:57:36,865 [INFO] ---------------------------------
2019-03-07 13:57:36,866 [INFO] Summary:
2019-03-07 13:57:36,867 [INFO] Batch 6000, worst loss 0.376860 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:57:36,867 [INFO] Regularization: 3199.572998 * 0.0000100000 = 0.0319957286
2019-03-07 13:57:36,868 [INFO] Sum of grad norms: 4.237652
2019-03-07 13:57:36,869 [INFO] ---------------------------------
2019-03-07 13:57:56,372 [INFO] ---------------------------------
2019-03-07 13:57:56,373 [INFO] Summary:
2019-03-07 13:57:56,374 [INFO] Batch 7000, worst loss 0.314990 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:57:56,375 [INFO] Regularization: 3144.727051 * 0.0000100000 = 0.0314472690
2019-03-07 13:57:56,376 [INFO] Sum of grad norms: 7.162265
2019-03-07 13:57:56,376 [INFO] ---------------------------------
2019-03-07 13:58:15,995 [INFO] ---------------------------------
2019-03-07 13:58:15,996 [INFO] Summary:
2019-03-07 13:58:15,997 [INFO] Batch 8000, worst loss 0.366736 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:58:15,997 [INFO] Regularization: 3087.481689 * 0.0000100000 = 0.0308748167
2019-03-07 13:58:15,999 [INFO] Sum of grad norms: 7.748698
2019-03-07 13:58:15,999 [INFO] ---------------------------------
2019-03-07 13:58:35,413 [INFO] ---------------------------------
2019-03-07 13:58:35,414 [INFO] Summary:
2019-03-07 13:58:35,415 [INFO] Batch 9000, worst loss 0.351557 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:58:35,416 [INFO] Regularization: 3009.075439 * 0.0000100000 = 0.0300907530
2019-03-07 13:58:35,417 [INFO] Sum of grad norms: 7.773643
2019-03-07 13:58:35,417 [INFO] ---------------------------------
2019-03-07 13:58:54,924 [INFO] ---------------------------------
2019-03-07 13:58:54,925 [INFO] Summary:
2019-03-07 13:58:54,925 [INFO] Batch 10000, worst loss 0.332980 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:58:54,926 [INFO] Regularization: 2955.387695 * 0.0000100000 = 0.0295538753
2019-03-07 13:58:54,927 [INFO] Sum of grad norms: 3.354713
2019-03-07 13:58:54,928 [INFO] ---------------------------------
2019-03-07 13:59:07,944 [INFO] ---------------------------------
2019-03-07 13:59:07,945 [INFO] Evaluation:
2019-03-07 13:59:07,946 [INFO] Batch 10000, worst loss 0.202247 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:59:07,948 [INFO] New best loss 0.202247, saved to file classifier/1551963312/1551963547_0_classifier_10000.pth
2019-03-07 13:59:07,962 [INFO] Target
2019-03-07 13:59:07,963 [INFO] [[0.01 0.01 0.01 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.01 0.01 0.01]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.01 0.01 0.01]]
2019-03-07 13:59:07,965 [INFO] Classifier output
2019-03-07 13:59:07,966 [INFO] [[0.000671 0.000671 0.000672 ... 0.501002 0.50099  0.500971]
 [0.988552 0.988562 0.988556 ... 0.000187 0.000187 0.000187]
 [0.002263 0.002268 0.002268 ... 0.001278 0.001278 0.00128 ]
 ...
 [0.987975 0.987978 0.98798  ... 0.392504 0.392497 0.392417]
 [0.000086 0.000086 0.000086 ... 0.635856 0.635873 0.635861]
 [0.000307 0.000307 0.000307 ... 0.003404 0.003404 0.003403]]
2019-03-07 13:59:07,969 [INFO] ---------------------------------
2019-03-07 13:59:27,613 [INFO] ---------------------------------
2019-03-07 13:59:27,614 [INFO] Summary:
2019-03-07 13:59:27,615 [INFO] Batch 11000, worst loss 0.446442 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:59:27,615 [INFO] Regularization: 2951.455078 * 0.0000100000 = 0.0295145493
2019-03-07 13:59:27,616 [INFO] Sum of grad norms: 6.366774
2019-03-07 13:59:27,617 [INFO] ---------------------------------
2019-03-07 13:59:46,772 [INFO] ---------------------------------
2019-03-07 13:59:46,773 [INFO] Summary:
2019-03-07 13:59:46,774 [INFO] Batch 12000, worst loss 0.307850 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 13:59:46,774 [INFO] Regularization: 2923.386475 * 0.0000100000 = 0.0292338636
2019-03-07 13:59:46,775 [INFO] Sum of grad norms: 2.020597
2019-03-07 13:59:46,776 [INFO] ---------------------------------
2019-03-07 14:00:06,284 [INFO] ---------------------------------
2019-03-07 14:00:06,285 [INFO] Summary:
2019-03-07 14:00:06,285 [INFO] Batch 13000, worst loss 0.295811 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:00:06,286 [INFO] Regularization: 2884.294189 * 0.0000100000 = 0.0288429409
2019-03-07 14:00:06,287 [INFO] Sum of grad norms: 6.569697
2019-03-07 14:00:06,288 [INFO] ---------------------------------
2019-03-07 14:00:25,735 [INFO] ---------------------------------
2019-03-07 14:00:25,736 [INFO] Summary:
2019-03-07 14:00:25,737 [INFO] Batch 14000, worst loss 0.371756 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:00:25,738 [INFO] Regularization: 2860.220459 * 0.0000100000 = 0.0286022034
2019-03-07 14:00:25,739 [INFO] Sum of grad norms: 3.258980
2019-03-07 14:00:25,739 [INFO] ---------------------------------
2019-03-07 14:00:45,492 [INFO] ---------------------------------
2019-03-07 14:00:45,493 [INFO] Summary:
2019-03-07 14:00:45,493 [INFO] Batch 15000, worst loss 0.298413 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:00:45,494 [INFO] Regularization: 2999.643799 * 0.0000100000 = 0.0299964380
2019-03-07 14:00:45,495 [INFO] reducing reg_loss_factor
2019-03-07 14:00:45,495 [INFO] Sum of grad norms: 1.466763
2019-03-07 14:00:45,496 [INFO] ---------------------------------
2019-03-07 14:01:04,693 [INFO] ---------------------------------
2019-03-07 14:01:04,694 [INFO] Summary:
2019-03-07 14:01:04,694 [INFO] Batch 16000, worst loss 0.297310 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:01:04,695 [INFO] Regularization: 2948.574951 * 0.0000010000 = 0.0029485750
2019-03-07 14:01:04,695 [INFO] Sum of grad norms: 5.398850
2019-03-07 14:01:04,696 [INFO] ---------------------------------
2019-03-07 14:01:24,189 [INFO] ---------------------------------
2019-03-07 14:01:24,190 [INFO] Summary:
2019-03-07 14:01:24,190 [INFO] Batch 17000, worst loss 0.358465 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:01:24,191 [INFO] Regularization: 3022.806885 * 0.0000010000 = 0.0030228070
2019-03-07 14:01:24,192 [INFO] Sum of grad norms: 3.516171
2019-03-07 14:01:24,193 [INFO] ---------------------------------
2019-03-07 14:01:43,291 [INFO] ---------------------------------
2019-03-07 14:01:43,292 [INFO] Summary:
2019-03-07 14:01:43,293 [INFO] Batch 18000, worst loss 0.309747 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:01:43,294 [INFO] Regularization: 3068.639893 * 0.0000010000 = 0.0030686399
2019-03-07 14:01:43,295 [INFO] Sum of grad norms: 2.706704
2019-03-07 14:01:43,295 [INFO] ---------------------------------
2019-03-07 14:02:03,218 [INFO] ---------------------------------
2019-03-07 14:02:03,219 [INFO] Summary:
2019-03-07 14:02:03,219 [INFO] Batch 19000, worst loss 0.324422 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:02:03,220 [INFO] Regularization: 3183.108643 * 0.0000010000 = 0.0031831085
2019-03-07 14:02:03,221 [INFO] Sum of grad norms: 2.694801
2019-03-07 14:02:03,221 [INFO] ---------------------------------
2019-03-07 14:02:22,586 [INFO] ---------------------------------
2019-03-07 14:02:22,587 [INFO] Summary:
2019-03-07 14:02:22,588 [INFO] Batch 20000, worst loss 0.308577 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:02:22,589 [INFO] Regularization: 3211.967285 * 0.0000010000 = 0.0032119672
2019-03-07 14:02:22,590 [INFO] Sum of grad norms: 6.849062
2019-03-07 14:02:22,591 [INFO] ---------------------------------
2019-03-07 14:02:35,716 [INFO] ---------------------------------
2019-03-07 14:02:35,718 [INFO] Evaluation:
2019-03-07 14:02:35,718 [INFO] Batch 20000, worst loss 0.195099 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:02:35,719 [INFO] New best loss 0.195099, saved to file classifier/1551963312/1551963755_0_classifier_20000.pth
2019-03-07 14:02:35,733 [INFO] Target
2019-03-07 14:02:35,734 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 ...
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]]
2019-03-07 14:02:35,736 [INFO] Classifier output
2019-03-07 14:02:35,737 [INFO] [[0.0291   0.029101 0.0291   ... 0.961299 0.961301 0.9613  ]
 [0.3801   0.380088 0.380064 ... 0.966598 0.966598 0.966595]
 [0.981317 0.981317 0.981312 ... 0.001072 0.001072 0.001072]
 ...
 [0.004853 0.004853 0.004853 ... 0.929134 0.929131 0.929125]
 [0.000032 0.000032 0.000032 ... 0.992204 0.992204 0.992204]
 [0.008479 0.008478 0.008477 ... 0.992525 0.992526 0.992526]]
2019-03-07 14:02:35,739 [INFO] ---------------------------------
2019-03-07 14:02:55,522 [INFO] ---------------------------------
2019-03-07 14:02:55,524 [INFO] Summary:
2019-03-07 14:02:55,524 [INFO] Batch 21000, worst loss 0.320211 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:02:55,525 [INFO] Regularization: 3227.020996 * 0.0000010000 = 0.0032270211
2019-03-07 14:02:55,526 [INFO] Sum of grad norms: 11.981325
2019-03-07 14:02:55,527 [INFO] ---------------------------------
2019-03-07 14:03:14,916 [INFO] ---------------------------------
2019-03-07 14:03:14,917 [INFO] Summary:
2019-03-07 14:03:14,917 [INFO] Batch 22000, worst loss 0.278371 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:03:14,918 [INFO] Regularization: 3232.822998 * 0.0000010000 = 0.0032328230
2019-03-07 14:03:14,919 [INFO] Sum of grad norms: 3.475341
2019-03-07 14:03:14,920 [INFO] ---------------------------------
2019-03-07 14:03:34,346 [INFO] ---------------------------------
2019-03-07 14:03:34,347 [INFO] Summary:
2019-03-07 14:03:34,347 [INFO] Batch 23000, worst loss 0.306718 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:03:34,348 [INFO] Regularization: 3206.064941 * 0.0000010000 = 0.0032060649
2019-03-07 14:03:34,349 [INFO] Sum of grad norms: 4.459101
2019-03-07 14:03:34,349 [INFO] ---------------------------------
2019-03-07 14:03:53,612 [INFO] ---------------------------------
2019-03-07 14:03:53,613 [INFO] Summary:
2019-03-07 14:03:53,613 [INFO] Batch 24000, worst loss 0.275176 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:03:53,614 [INFO] Regularization: 3251.139893 * 0.0000010000 = 0.0032511398
2019-03-07 14:03:53,615 [INFO] Sum of grad norms: 5.724394
2019-03-07 14:03:53,616 [INFO] ---------------------------------
2019-03-07 14:04:13,204 [INFO] ---------------------------------
2019-03-07 14:04:13,205 [INFO] Summary:
2019-03-07 14:04:13,205 [INFO] Batch 25000, worst loss 0.374962 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:04:13,206 [INFO] Regularization: 3273.781738 * 0.0000010000 = 0.0032737816
2019-03-07 14:04:13,206 [INFO] Sum of grad norms: 7.749752
2019-03-07 14:04:13,207 [INFO] ---------------------------------
2019-03-07 14:04:32,283 [INFO] ---------------------------------
2019-03-07 14:04:32,284 [INFO] Summary:
2019-03-07 14:04:32,285 [INFO] Batch 26000, worst loss 0.263485 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:04:32,286 [INFO] Regularization: 3256.343262 * 0.0000010000 = 0.0032563433
2019-03-07 14:04:32,287 [INFO] Sum of grad norms: 8.995340
2019-03-07 14:04:32,288 [INFO] ---------------------------------
2019-03-07 14:04:51,520 [INFO] ---------------------------------
2019-03-07 14:04:51,521 [INFO] Summary:
2019-03-07 14:04:51,522 [INFO] Batch 27000, worst loss 0.368685 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:04:51,523 [INFO] Regularization: 3337.351318 * 0.0000010000 = 0.0033373514
2019-03-07 14:04:51,523 [INFO] Sum of grad norms: 9.386040
2019-03-07 14:04:51,524 [INFO] ---------------------------------
2019-03-07 14:05:10,622 [INFO] ---------------------------------
2019-03-07 14:05:10,623 [INFO] Summary:
2019-03-07 14:05:10,624 [INFO] Batch 28000, worst loss 0.290692 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:05:10,626 [INFO] Regularization: 3410.812012 * 0.0000010000 = 0.0034108120
2019-03-07 14:05:10,628 [INFO] Sum of grad norms: 6.876300
2019-03-07 14:05:10,629 [INFO] ---------------------------------
2019-03-07 14:05:30,324 [INFO] ---------------------------------
2019-03-07 14:05:30,325 [INFO] Summary:
2019-03-07 14:05:30,326 [INFO] Batch 29000, worst loss 0.320907 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:05:30,327 [INFO] Regularization: 3284.275879 * 0.0000010000 = 0.0032842758
2019-03-07 14:05:30,328 [INFO] Sum of grad norms: 4.706286
2019-03-07 14:05:30,329 [INFO] ---------------------------------
2019-03-07 14:05:49,716 [INFO] ---------------------------------
2019-03-07 14:05:49,717 [INFO] Summary:
2019-03-07 14:05:49,718 [INFO] Batch 30000, worst loss 0.249739 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:05:49,718 [INFO] Regularization: 3246.382812 * 0.0000010000 = 0.0032463828
2019-03-07 14:05:49,719 [INFO] Sum of grad norms: 3.370595
2019-03-07 14:05:49,722 [INFO] ---------------------------------
2019-03-07 14:06:02,801 [INFO] ---------------------------------
2019-03-07 14:06:02,802 [INFO] Evaluation:
2019-03-07 14:06:02,803 [INFO] Batch 30000, worst loss 0.432854 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:06:02,803 [INFO] ---------------------------------
2019-03-07 14:06:22,250 [INFO] ---------------------------------
2019-03-07 14:06:22,251 [INFO] Summary:
2019-03-07 14:06:22,251 [INFO] Batch 31000, worst loss 0.287977 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:06:22,252 [INFO] Regularization: 3564.873047 * 0.0000010000 = 0.0035648730
2019-03-07 14:06:22,253 [INFO] Sum of grad norms: 5.342897
2019-03-07 14:06:22,254 [INFO] ---------------------------------
2019-03-07 14:06:41,727 [INFO] ---------------------------------
2019-03-07 14:06:41,728 [INFO] Summary:
2019-03-07 14:06:41,728 [INFO] Batch 32000, worst loss 0.335941 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:06:41,729 [INFO] Regularization: 3313.784424 * 0.0000010000 = 0.0033137845
2019-03-07 14:06:41,730 [INFO] Sum of grad norms: 5.338553
2019-03-07 14:06:41,732 [INFO] ---------------------------------
2019-03-07 14:07:01,442 [INFO] ---------------------------------
2019-03-07 14:07:01,444 [INFO] Summary:
2019-03-07 14:07:01,444 [INFO] Batch 33000, worst loss 0.273905 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:07:01,445 [INFO] Regularization: 3335.816895 * 0.0000010000 = 0.0033358168
2019-03-07 14:07:01,446 [INFO] Sum of grad norms: 7.560487
2019-03-07 14:07:01,447 [INFO] ---------------------------------
2019-03-07 14:07:20,550 [INFO] ---------------------------------
2019-03-07 14:07:20,551 [INFO] Summary:
2019-03-07 14:07:20,554 [INFO] Batch 34000, worst loss 0.271590 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:07:20,554 [INFO] Regularization: 3307.435791 * 0.0000010000 = 0.0033074359
2019-03-07 14:07:20,555 [INFO] Sum of grad norms: 9.285279
2019-03-07 14:07:20,556 [INFO] ---------------------------------
2019-03-07 14:07:40,024 [INFO] ---------------------------------
2019-03-07 14:07:40,025 [INFO] Summary:
2019-03-07 14:07:40,026 [INFO] Batch 35000, worst loss 0.265734 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:07:40,027 [INFO] Regularization: 3444.247314 * 0.0000010000 = 0.0034442474
2019-03-07 14:07:40,028 [INFO] Sum of grad norms: 2.802402
2019-03-07 14:07:40,029 [INFO] ---------------------------------
2019-03-07 14:07:59,598 [INFO] ---------------------------------
2019-03-07 14:07:59,599 [INFO] Summary:
2019-03-07 14:07:59,599 [INFO] Batch 36000, worst loss 0.332351 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:07:59,600 [INFO] Regularization: 3303.680176 * 0.0000010000 = 0.0033036801
2019-03-07 14:07:59,601 [INFO] Sum of grad norms: 2.101321
2019-03-07 14:07:59,602 [INFO] ---------------------------------
2019-03-07 14:08:19,227 [INFO] ---------------------------------
2019-03-07 14:08:19,228 [INFO] Summary:
2019-03-07 14:08:19,228 [INFO] Batch 37000, worst loss 0.262382 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:08:19,229 [INFO] Regularization: 3286.917969 * 0.0000010000 = 0.0032869179
2019-03-07 14:08:19,230 [INFO] Sum of grad norms: 4.814493
2019-03-07 14:08:19,230 [INFO] ---------------------------------
2019-03-07 14:08:38,427 [INFO] ---------------------------------
2019-03-07 14:08:38,428 [INFO] Summary:
2019-03-07 14:08:38,429 [INFO] Batch 38000, worst loss 0.316617 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:08:38,430 [INFO] Regularization: 3375.542725 * 0.0000010000 = 0.0033755428
2019-03-07 14:08:38,430 [INFO] Sum of grad norms: 3.999802
2019-03-07 14:08:38,431 [INFO] ---------------------------------
2019-03-07 14:08:57,945 [INFO] ---------------------------------
2019-03-07 14:08:57,946 [INFO] Summary:
2019-03-07 14:08:57,947 [INFO] Batch 39000, worst loss 0.288443 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:08:57,949 [INFO] Regularization: 3360.074463 * 0.0000010000 = 0.0033600745
2019-03-07 14:08:57,950 [INFO] Sum of grad norms: 7.975912
2019-03-07 14:08:57,951 [INFO] ---------------------------------
2019-03-07 14:09:17,732 [INFO] ---------------------------------
2019-03-07 14:09:17,733 [INFO] Summary:
2019-03-07 14:09:17,734 [INFO] Batch 40000, worst loss 0.278190 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:09:17,735 [INFO] Regularization: 3341.661133 * 0.0000010000 = 0.0033416611
2019-03-07 14:09:17,735 [INFO] Sum of grad norms: 4.655342
2019-03-07 14:09:17,736 [INFO] ---------------------------------
2019-03-07 14:09:30,886 [INFO] ---------------------------------
2019-03-07 14:09:30,887 [INFO] Evaluation:
2019-03-07 14:09:30,888 [INFO] Batch 40000, worst loss 0.280258 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 14:09:30,890 [INFO] ---------------------------------
2019-03-07 14:09:50,308 [INFO] ---------------------------------
2019-03-07 14:09:50,309 [INFO] Summary:
2019-03-07 14:09:50,310 [INFO] Batch 41000, worst loss 0.252429 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:09:50,311 [INFO] Regularization: 3263.365234 * 0.0000010000 = 0.0032633652
2019-03-07 14:09:50,312 [INFO] Sum of grad norms: 2.170945
2019-03-07 14:09:50,312 [INFO] ---------------------------------
2019-03-07 14:10:09,517 [INFO] ---------------------------------
2019-03-07 14:10:09,518 [INFO] Summary:
2019-03-07 14:10:09,518 [INFO] Batch 42000, worst loss 0.218733 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:10:09,519 [INFO] Regularization: 3235.054932 * 0.0000010000 = 0.0032350549
2019-03-07 14:10:09,520 [INFO] Sum of grad norms: 0.259710
2019-03-07 14:10:09,521 [INFO] ---------------------------------
2019-03-07 14:10:28,548 [INFO] ---------------------------------
2019-03-07 14:10:28,549 [INFO] Summary:
2019-03-07 14:10:28,550 [INFO] Batch 43000, worst loss 0.244935 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:10:28,550 [INFO] Regularization: 3197.430176 * 0.0000010000 = 0.0031974302
2019-03-07 14:10:28,551 [INFO] Sum of grad norms: 5.929501
2019-03-07 14:10:28,552 [INFO] ---------------------------------
2019-03-07 14:10:47,967 [INFO] ---------------------------------
2019-03-07 14:10:47,968 [INFO] Summary:
2019-03-07 14:10:47,969 [INFO] Batch 44000, worst loss 0.173828 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:10:47,969 [INFO] Regularization: 3189.083984 * 0.0000010000 = 0.0031890839
2019-03-07 14:10:47,970 [INFO] Sum of grad norms: 0.903903
2019-03-07 14:10:47,970 [INFO] ---------------------------------
2019-03-07 14:11:01,043 [INFO] ---------------------------------
2019-03-07 14:11:01,044 [INFO] Evaluation:
2019-03-07 14:11:01,044 [INFO] Batch 44000, worst loss 0.217985 (without reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:11:01,045 [INFO] ---------------------------------
2019-03-07 14:11:20,463 [INFO] ---------------------------------
2019-03-07 14:11:20,464 [INFO] Summary:
2019-03-07 14:11:20,465 [INFO] Batch 45000, worst loss 0.186842 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:11:20,466 [INFO] Regularization: 3185.351562 * 0.0000010000 = 0.0031853516
2019-03-07 14:11:20,466 [INFO] Sum of grad norms: 0.436479
2019-03-07 14:11:20,467 [INFO] ---------------------------------
2019-03-07 14:11:33,561 [INFO] ---------------------------------
2019-03-07 14:11:33,562 [INFO] Evaluation:
2019-03-07 14:11:33,563 [INFO] Batch 45000, worst loss 0.238841 (without reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:11:33,564 [INFO] ---------------------------------
2019-03-07 14:11:53,043 [INFO] ---------------------------------
2019-03-07 14:11:53,045 [INFO] Summary:
2019-03-07 14:11:53,045 [INFO] Batch 46000, worst loss 0.286111 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:11:53,047 [INFO] Regularization: 3175.916748 * 0.0000010000 = 0.0031759168
2019-03-07 14:11:53,048 [INFO] Sum of grad norms: 7.353101
2019-03-07 14:11:53,049 [INFO] ---------------------------------
2019-03-07 14:12:12,485 [INFO] ---------------------------------
2019-03-07 14:12:12,486 [INFO] Summary:
2019-03-07 14:12:12,487 [INFO] Batch 47000, worst loss 0.232638 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:12:12,488 [INFO] Regularization: 3164.527344 * 0.0000010000 = 0.0031645272
2019-03-07 14:12:12,489 [INFO] Sum of grad norms: 6.728886
2019-03-07 14:12:12,489 [INFO] ---------------------------------
2019-03-07 14:12:32,066 [INFO] ---------------------------------
2019-03-07 14:12:32,067 [INFO] Summary:
2019-03-07 14:12:32,068 [INFO] Batch 48000, worst loss 0.223111 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:12:32,069 [INFO] Regularization: 3156.212402 * 0.0000010000 = 0.0031562124
2019-03-07 14:12:32,070 [INFO] Sum of grad norms: 1.892920
2019-03-07 14:12:32,070 [INFO] ---------------------------------
2019-03-07 14:12:51,113 [INFO] ---------------------------------
2019-03-07 14:12:51,114 [INFO] Summary:
2019-03-07 14:12:51,115 [INFO] Batch 49000, worst loss 0.216868 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:12:51,115 [INFO] Regularization: 3184.998779 * 0.0000010000 = 0.0031849989
2019-03-07 14:12:51,116 [INFO] Sum of grad norms: 1.671377
2019-03-07 14:12:51,117 [INFO] ---------------------------------
2019-03-07 14:13:10,988 [INFO] ---------------------------------
2019-03-07 14:13:10,989 [INFO] Summary:
2019-03-07 14:13:10,989 [INFO] Batch 50000, worst loss 0.243778 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:13:10,990 [INFO] Regularization: 3143.868896 * 0.0000010000 = 0.0031438689
2019-03-07 14:13:10,991 [INFO] Sum of grad norms: 0.386523
2019-03-07 14:13:10,991 [INFO] ---------------------------------
2019-03-07 14:13:24,102 [INFO] ---------------------------------
2019-03-07 14:13:24,104 [INFO] Evaluation:
2019-03-07 14:13:24,105 [INFO] Batch 50000, worst loss 0.215453 (without reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 14:13:24,106 [INFO] ---------------------------------
2019-03-07 14:13:43,181 [INFO] ---------------------------------
2019-03-07 14:13:43,182 [INFO] Summary:
2019-03-07 14:13:43,183 [INFO] Batch 51000, worst loss 0.218713 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:13:43,184 [INFO] Regularization: 3140.453369 * 0.0000010000 = 0.0031404532
2019-03-07 14:13:43,186 [INFO] Sum of grad norms: 12.314382
2019-03-07 14:13:43,187 [INFO] ---------------------------------
2019-03-07 14:14:02,352 [INFO] ---------------------------------
2019-03-07 14:14:02,353 [INFO] Summary:
2019-03-07 14:14:02,353 [INFO] Batch 52000, worst loss 0.190255 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:14:02,355 [INFO] Regularization: 3132.634033 * 0.0000010000 = 0.0031326341
2019-03-07 14:14:02,356 [INFO] Sum of grad norms: 5.631161
2019-03-07 14:14:02,356 [INFO] ---------------------------------
2019-03-07 14:14:15,462 [INFO] ---------------------------------
2019-03-07 14:14:15,465 [INFO] Evaluation:
2019-03-07 14:14:15,468 [INFO] Batch 52000, worst loss 0.176308 (without reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:14:15,468 [INFO] New best loss 0.176308, saved to file classifier/1551963312/1551964455_0_classifier_52000.pth
2019-03-07 14:14:15,483 [INFO] Target
2019-03-07 14:14:15,484 [INFO] [[0.01 0.01 0.01 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.01 0.01 0.01]
 ...
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-07 14:14:15,486 [INFO] Classifier output
2019-03-07 14:14:15,488 [INFO] [[0.012951 0.012951 0.012955 ... 0.029028 0.029031 0.02903 ]
 [0.980255 0.980255 0.980255 ... 0.982456 0.982456 0.982456]
 [0.       0.       0.       ... 0.009008 0.009007 0.009007]
 ...
 [0.005885 0.005885 0.005886 ... 0.991876 0.991876 0.991876]
 [0.982283 0.982283 0.982283 ... 0.985103 0.985103 0.985103]
 [0.982552 0.982552 0.982552 ... 0.986344 0.986344 0.986344]]
2019-03-07 14:14:15,490 [INFO] ---------------------------------
2019-03-07 14:14:35,184 [INFO] ---------------------------------
2019-03-07 14:14:35,185 [INFO] Summary:
2019-03-07 14:14:35,186 [INFO] Batch 53000, worst loss 0.203773 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:14:35,186 [INFO] Regularization: 3136.638916 * 0.0000010000 = 0.0031366390
2019-03-07 14:14:35,187 [INFO] Sum of grad norms: 9.735119
2019-03-07 14:14:35,188 [INFO] ---------------------------------
2019-03-07 14:14:54,590 [INFO] ---------------------------------
2019-03-07 14:14:54,591 [INFO] Summary:
2019-03-07 14:14:54,594 [INFO] Batch 54000, worst loss 0.228804 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:14:54,594 [INFO] Regularization: 3131.978271 * 0.0000010000 = 0.0031319782
2019-03-07 14:14:54,595 [INFO] Sum of grad norms: 0.188865
2019-03-07 14:14:54,596 [INFO] ---------------------------------
2019-03-07 14:15:13,921 [INFO] ---------------------------------
2019-03-07 14:15:13,922 [INFO] Summary:
2019-03-07 14:15:13,922 [INFO] Batch 55000, worst loss 0.192801 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:15:13,923 [INFO] Regularization: 3131.553711 * 0.0000010000 = 0.0031315538
2019-03-07 14:15:13,924 [INFO] Sum of grad norms: 9.981627
2019-03-07 14:15:13,925 [INFO] ---------------------------------
2019-03-07 14:15:33,520 [INFO] ---------------------------------
2019-03-07 14:15:33,521 [INFO] Summary:
2019-03-07 14:15:33,521 [INFO] Batch 56000, worst loss 0.174058 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:15:33,522 [INFO] Regularization: 3128.909424 * 0.0000010000 = 0.0031289095
2019-03-07 14:15:33,523 [INFO] Sum of grad norms: 1.492301
2019-03-07 14:15:33,524 [INFO] ---------------------------------
2019-03-07 14:15:46,755 [INFO] ---------------------------------
2019-03-07 14:15:46,759 [INFO] Evaluation:
2019-03-07 14:15:46,760 [INFO] Batch 56000, worst loss 0.197060 (without reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:15:46,761 [INFO] ---------------------------------
2019-03-07 14:16:06,364 [INFO] ---------------------------------
2019-03-07 14:16:06,365 [INFO] Summary:
2019-03-07 14:16:06,366 [INFO] Batch 57000, worst loss 0.294720 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:16:06,366 [INFO] Regularization: 3136.925781 * 0.0000010000 = 0.0031369259
2019-03-07 14:16:06,367 [INFO] Sum of grad norms: 0.671898
2019-03-07 14:16:06,368 [INFO] ---------------------------------
2019-03-07 14:16:25,960 [INFO] ---------------------------------
2019-03-07 14:16:25,962 [INFO] Summary:
2019-03-07 14:16:25,962 [INFO] Batch 58000, worst loss 0.233714 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:16:25,963 [INFO] Regularization: 3137.153809 * 0.0000010000 = 0.0031371538
2019-03-07 14:16:25,964 [INFO] Sum of grad norms: 0.157371
2019-03-07 14:16:25,965 [INFO] ---------------------------------
2019-03-07 14:16:45,396 [INFO] ---------------------------------
2019-03-07 14:16:45,397 [INFO] Summary:
2019-03-07 14:16:45,397 [INFO] Batch 59000, worst loss 0.199826 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:16:45,399 [INFO] Regularization: 3142.354492 * 0.0000010000 = 0.0031423545
2019-03-07 14:16:45,399 [INFO] Sum of grad norms: 1.289081
2019-03-07 14:16:45,400 [INFO] ---------------------------------
2019-03-07 14:17:04,874 [INFO] ---------------------------------
2019-03-07 14:17:04,875 [INFO] Summary:
2019-03-07 14:17:04,876 [INFO] Batch 60000, worst loss 0.215929 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:17:04,876 [INFO] Regularization: 3137.439941 * 0.0000010000 = 0.0031374400
2019-03-07 14:17:04,877 [INFO] Sum of grad norms: 0.456942
2019-03-07 14:17:04,878 [INFO] ---------------------------------
2019-03-07 14:17:17,895 [INFO] ---------------------------------
2019-03-07 14:17:17,896 [INFO] Evaluation:
2019-03-07 14:17:17,896 [INFO] Batch 60000, worst loss 0.228441 (without reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 14:17:17,899 [INFO] ---------------------------------
2019-03-07 14:17:37,527 [INFO] ---------------------------------
2019-03-07 14:17:37,528 [INFO] Summary:
2019-03-07 14:17:37,529 [INFO] Batch 61000, worst loss 0.191095 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:17:37,529 [INFO] Regularization: 3136.304443 * 0.0000010000 = 0.0031363044
2019-03-07 14:17:37,530 [INFO] Sum of grad norms: 10.507710
2019-03-07 14:17:37,531 [INFO] ---------------------------------
2019-03-07 14:17:56,425 [INFO] ---------------------------------
2019-03-07 14:17:56,427 [INFO] Summary:
2019-03-07 14:17:56,427 [INFO] Batch 62000, worst loss 0.180418 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:17:56,428 [INFO] Regularization: 3132.667725 * 0.0000010000 = 0.0031326676
2019-03-07 14:17:56,430 [INFO] Sum of grad norms: 6.113873
2019-03-07 14:17:56,431 [INFO] ---------------------------------
2019-03-07 14:18:15,779 [INFO] ---------------------------------
2019-03-07 14:18:15,780 [INFO] Summary:
2019-03-07 14:18:15,781 [INFO] Batch 63000, worst loss 0.189986 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:18:15,782 [INFO] Regularization: 3131.223145 * 0.0000010000 = 0.0031312231
2019-03-07 14:18:15,783 [INFO] Sum of grad norms: 6.789022
2019-03-07 14:18:15,783 [INFO] ---------------------------------
2019-03-07 14:18:35,314 [INFO] ---------------------------------
2019-03-07 14:18:35,315 [INFO] Summary:
2019-03-07 14:18:35,316 [INFO] Batch 64000, worst loss 0.165350 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:18:35,316 [INFO] Regularization: 3129.977295 * 0.0000010000 = 0.0031299773
2019-03-07 14:18:35,317 [INFO] Sum of grad norms: 0.234094
2019-03-07 14:18:35,318 [INFO] ---------------------------------
2019-03-07 14:18:48,483 [INFO] ---------------------------------
2019-03-07 14:18:48,484 [INFO] Evaluation:
2019-03-07 14:18:48,485 [INFO] Batch 64000, worst loss 0.182303 (without reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:18:48,486 [INFO] ---------------------------------
2019-03-07 14:19:07,476 [INFO] ---------------------------------
2019-03-07 14:19:07,477 [INFO] Summary:
2019-03-07 14:19:07,478 [INFO] Batch 65000, worst loss 0.182395 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:19:07,479 [INFO] Regularization: 3130.538330 * 0.0000010000 = 0.0031305384
2019-03-07 14:19:07,479 [INFO] Sum of grad norms: 5.771624
2019-03-07 14:19:07,480 [INFO] ---------------------------------
2019-03-07 14:19:27,130 [INFO] ---------------------------------
2019-03-07 14:19:27,132 [INFO] Summary:
2019-03-07 14:19:27,132 [INFO] Batch 66000, worst loss 0.216938 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:19:27,133 [INFO] Regularization: 3130.761963 * 0.0000010000 = 0.0031307619
2019-03-07 14:19:27,133 [INFO] Sum of grad norms: 0.183349
2019-03-07 14:19:27,134 [INFO] ---------------------------------
2019-03-07 14:19:46,504 [INFO] ---------------------------------
2019-03-07 14:19:46,504 [INFO] Summary:
2019-03-07 14:19:46,505 [INFO] Batch 67000, worst loss 0.185329 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:19:46,506 [INFO] Regularization: 3129.590088 * 0.0000010000 = 0.0031295901
2019-03-07 14:19:46,506 [INFO] Sum of grad norms: 0.359184
2019-03-07 14:19:46,507 [INFO] ---------------------------------
2019-03-07 14:20:05,647 [INFO] ---------------------------------
2019-03-07 14:20:05,648 [INFO] Summary:
2019-03-07 14:20:05,648 [INFO] Batch 68000, worst loss 0.175103 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:20:05,649 [INFO] Regularization: 3129.930420 * 0.0000010000 = 0.0031299305
2019-03-07 14:20:05,650 [INFO] Sum of grad norms: 2.753224
2019-03-07 14:20:05,651 [INFO] ---------------------------------
2019-03-07 14:20:18,720 [INFO] ---------------------------------
2019-03-07 14:20:18,721 [INFO] Evaluation:
2019-03-07 14:20:18,721 [INFO] Batch 68000, worst loss 0.149387 (without reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:20:18,722 [INFO] New best loss 0.149387, saved to file classifier/1551963312/1551964818_0_classifier_68000.pth
2019-03-07 14:20:18,736 [INFO] Target
2019-03-07 14:20:18,737 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 ...
 [0.01 0.01 0.01 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]]
2019-03-07 14:20:18,739 [INFO] Classifier output
2019-03-07 14:20:18,740 [INFO] [[0.986632 0.986633 0.986633 ... 0.       0.       0.      ]
 [0.958398 0.958402 0.958402 ... 0.000004 0.000004 0.000004]
 [0.430572 0.430557 0.430548 ... 0.991726 0.991726 0.991726]
 ...
 [0.       0.       0.       ... 0.151693 0.151697 0.1517  ]
 [0.006436 0.006435 0.006435 ... 0.989104 0.989104 0.989103]
 [0.       0.       0.       ... 0.989894 0.989894 0.989894]]
2019-03-07 14:20:18,742 [INFO] ---------------------------------
2019-03-07 14:20:38,254 [INFO] ---------------------------------
2019-03-07 14:20:38,255 [INFO] Summary:
2019-03-07 14:20:38,256 [INFO] Batch 69000, worst loss 0.167781 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:20:38,256 [INFO] Regularization: 3129.005127 * 0.0000010000 = 0.0031290052
2019-03-07 14:20:38,257 [INFO] Sum of grad norms: 3.076086
2019-03-07 14:20:38,258 [INFO] ---------------------------------
2019-03-07 14:20:57,427 [INFO] ---------------------------------
2019-03-07 14:20:57,428 [INFO] Summary:
2019-03-07 14:20:57,429 [INFO] Batch 70000, worst loss 0.192485 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:20:57,430 [INFO] Regularization: 3128.165283 * 0.0000010000 = 0.0031281654
2019-03-07 14:20:57,430 [INFO] Sum of grad norms: 0.326878
2019-03-07 14:20:57,431 [INFO] ---------------------------------
2019-03-07 14:21:10,416 [INFO] ---------------------------------
2019-03-07 14:21:10,417 [INFO] Evaluation:
2019-03-07 14:21:10,418 [INFO] Batch 70000, worst loss 0.158612 (without reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 14:21:10,418 [INFO] ---------------------------------
2019-03-07 14:21:29,980 [INFO] ---------------------------------
2019-03-07 14:21:29,981 [INFO] Summary:
2019-03-07 14:21:29,981 [INFO] Batch 71000, worst loss 0.204538 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:21:29,982 [INFO] Regularization: 3128.598877 * 0.0000010000 = 0.0031285989
2019-03-07 14:21:29,983 [INFO] Sum of grad norms: 10.696593
2019-03-07 14:21:29,984 [INFO] ---------------------------------
2019-03-07 14:21:49,267 [INFO] ---------------------------------
2019-03-07 14:21:49,268 [INFO] Summary:
2019-03-07 14:21:49,269 [INFO] Batch 72000, worst loss 0.206960 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:21:49,269 [INFO] Regularization: 3128.277100 * 0.0000010000 = 0.0031282771
2019-03-07 14:21:49,270 [INFO] Sum of grad norms: 4.498399
2019-03-07 14:21:49,271 [INFO] ---------------------------------
2019-03-07 14:22:08,869 [INFO] ---------------------------------
2019-03-07 14:22:08,870 [INFO] Summary:
2019-03-07 14:22:08,871 [INFO] Batch 73000, worst loss 0.146519 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:22:08,871 [INFO] Regularization: 3127.959717 * 0.0000010000 = 0.0031279598
2019-03-07 14:22:08,872 [INFO] Sum of grad norms: 5.388778
2019-03-07 14:22:08,873 [INFO] ---------------------------------
2019-03-07 14:22:21,920 [INFO] ---------------------------------
2019-03-07 14:22:21,921 [INFO] Evaluation:
2019-03-07 14:22:21,922 [INFO] Batch 73000, worst loss 0.136853 (without reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:22:21,923 [INFO] New best loss 0.136853, saved to file classifier/1551963312/1551964941_0_classifier_73000.pth
2019-03-07 14:22:21,937 [INFO] Target
2019-03-07 14:22:21,939 [INFO] [[0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.01 0.01 0.01]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]]
2019-03-07 14:22:21,941 [INFO] Classifier output
2019-03-07 14:22:21,942 [INFO] [[0.981817 0.981816 0.981815 ... 0.992031 0.992031 0.992031]
 [0.980975 0.980974 0.980975 ... 0.011578 0.011578 0.011578]
 [0.909218 0.909233 0.909236 ... 0.000002 0.000002 0.000002]
 ...
 [0.990158 0.990158 0.990158 ... 0.0327   0.032701 0.032701]
 [0.986134 0.986135 0.986136 ... 0.984633 0.984632 0.984632]
 [0.       0.       0.       ... 0.989407 0.989407 0.989407]]
2019-03-07 14:22:21,944 [INFO] ---------------------------------
2019-03-07 14:22:41,499 [INFO] ---------------------------------
2019-03-07 14:22:41,500 [INFO] Summary:
2019-03-07 14:22:41,500 [INFO] Batch 74000, worst loss 0.207950 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:22:41,501 [INFO] Regularization: 3128.273193 * 0.0000010000 = 0.0031282732
2019-03-07 14:22:41,502 [INFO] Sum of grad norms: 2.453282
2019-03-07 14:22:41,502 [INFO] ---------------------------------
2019-03-07 14:23:01,192 [INFO] ---------------------------------
2019-03-07 14:23:01,193 [INFO] Summary:
2019-03-07 14:23:01,194 [INFO] Batch 75000, worst loss 0.200464 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:23:01,195 [INFO] Regularization: 3127.857666 * 0.0000010000 = 0.0031278576
2019-03-07 14:23:01,196 [INFO] Sum of grad norms: 0.370287
2019-03-07 14:23:01,197 [INFO] ---------------------------------
2019-03-07 14:23:20,370 [INFO] ---------------------------------
2019-03-07 14:23:20,371 [INFO] Summary:
2019-03-07 14:23:20,372 [INFO] Batch 76000, worst loss 0.197533 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:23:20,372 [INFO] Regularization: 3127.557129 * 0.0000010000 = 0.0031275572
2019-03-07 14:23:20,373 [INFO] Sum of grad norms: 6.362347
2019-03-07 14:23:20,374 [INFO] ---------------------------------
2019-03-07 14:23:39,528 [INFO] ---------------------------------
2019-03-07 14:23:39,529 [INFO] Summary:
2019-03-07 14:23:39,529 [INFO] Batch 77000, worst loss 0.181691 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:23:39,530 [INFO] Regularization: 3128.018799 * 0.0000010000 = 0.0031280187
2019-03-07 14:23:39,531 [INFO] Sum of grad norms: 4.366855
2019-03-07 14:23:39,533 [INFO] ---------------------------------
2019-03-07 14:23:58,673 [INFO] ---------------------------------
2019-03-07 14:23:58,674 [INFO] Summary:
2019-03-07 14:23:58,674 [INFO] Batch 78000, worst loss 0.183774 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:23:58,675 [INFO] Regularization: 3127.601807 * 0.0000010000 = 0.0031276017
2019-03-07 14:23:58,676 [INFO] Sum of grad norms: 4.235021
2019-03-07 14:23:58,677 [INFO] ---------------------------------
2019-03-07 14:24:18,101 [INFO] ---------------------------------
2019-03-07 14:24:18,102 [INFO] Summary:
2019-03-07 14:24:18,103 [INFO] Batch 79000, worst loss 0.198891 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:24:18,103 [INFO] Regularization: 3127.770752 * 0.0000010000 = 0.0031277707
2019-03-07 14:24:18,104 [INFO] Sum of grad norms: 4.655934
2019-03-07 14:24:18,104 [INFO] ---------------------------------
2019-03-07 14:24:37,521 [INFO] ---------------------------------
2019-03-07 14:24:37,522 [INFO] Summary:
2019-03-07 14:24:37,523 [INFO] Batch 80000, worst loss 0.171859 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:24:37,524 [INFO] Regularization: 3127.865723 * 0.0000010000 = 0.0031278657
2019-03-07 14:24:37,525 [INFO] Sum of grad norms: 3.070478
2019-03-07 14:24:37,526 [INFO] ---------------------------------
2019-03-07 14:24:50,546 [INFO] ---------------------------------
2019-03-07 14:24:50,547 [INFO] Evaluation:
2019-03-07 14:24:50,548 [INFO] Batch 80000, worst loss 0.190862 (without reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 14:24:50,549 [INFO] ---------------------------------
2019-03-07 14:25:10,184 [INFO] ---------------------------------
2019-03-07 14:25:10,185 [INFO] Summary:
2019-03-07 14:25:10,185 [INFO] Batch 81000, worst loss 0.225588 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:25:10,186 [INFO] Regularization: 3127.421143 * 0.0000010000 = 0.0031274210
2019-03-07 14:25:10,186 [INFO] Sum of grad norms: 7.818027
2019-03-07 14:25:10,187 [INFO] ---------------------------------
2019-03-07 14:25:29,281 [INFO] ---------------------------------
2019-03-07 14:25:29,281 [INFO] Summary:
2019-03-07 14:25:29,282 [INFO] Batch 82000, worst loss 0.211297 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:25:29,283 [INFO] Regularization: 3127.197266 * 0.0000010000 = 0.0031271973
2019-03-07 14:25:29,284 [INFO] Sum of grad norms: 9.880123
2019-03-07 14:25:29,284 [INFO] ---------------------------------
2019-03-07 14:25:48,596 [INFO] ---------------------------------
2019-03-07 14:25:48,597 [INFO] Summary:
2019-03-07 14:25:48,597 [INFO] Batch 83000, worst loss 0.151238 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:25:48,598 [INFO] Regularization: 3127.192871 * 0.0000010000 = 0.0031271928
2019-03-07 14:25:48,599 [INFO] Sum of grad norms: 10.598637
2019-03-07 14:25:48,600 [INFO] ---------------------------------
2019-03-07 14:26:07,649 [INFO] ---------------------------------
2019-03-07 14:26:07,650 [INFO] Summary:
2019-03-07 14:26:07,651 [INFO] Batch 84000, worst loss 0.240861 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:26:07,652 [INFO] Regularization: 3127.165039 * 0.0000010000 = 0.0031271651
2019-03-07 14:26:07,652 [INFO] Sum of grad norms: 0.835367
2019-03-07 14:26:07,654 [INFO] ---------------------------------
2019-03-07 14:26:27,301 [INFO] ---------------------------------
2019-03-07 14:26:27,303 [INFO] Summary:
2019-03-07 14:26:27,303 [INFO] Batch 85000, worst loss 0.152611 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:26:27,304 [INFO] Regularization: 3127.105469 * 0.0000010000 = 0.0031271055
2019-03-07 14:26:27,304 [INFO] Sum of grad norms: 13.888168
2019-03-07 14:26:27,305 [INFO] ---------------------------------
2019-03-07 14:26:46,561 [INFO] ---------------------------------
2019-03-07 14:26:46,562 [INFO] Summary:
2019-03-07 14:26:46,563 [INFO] Batch 86000, worst loss 0.222074 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:26:46,563 [INFO] Regularization: 3127.034668 * 0.0000010000 = 0.0031270348
2019-03-07 14:26:46,564 [INFO] Sum of grad norms: 3.335898
2019-03-07 14:26:46,564 [INFO] ---------------------------------
2019-03-07 14:27:05,770 [INFO] ---------------------------------
2019-03-07 14:27:05,771 [INFO] Summary:
2019-03-07 14:27:05,772 [INFO] Batch 87000, worst loss 0.142171 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:27:05,773 [INFO] Regularization: 3126.954102 * 0.0000010000 = 0.0031269542
2019-03-07 14:27:05,773 [INFO] Sum of grad norms: 1.966096
2019-03-07 14:27:05,775 [INFO] ---------------------------------
2019-03-07 14:27:25,441 [INFO] ---------------------------------
2019-03-07 14:27:25,442 [INFO] Summary:
2019-03-07 14:27:25,443 [INFO] Batch 88000, worst loss 0.157533 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:27:25,443 [INFO] Regularization: 3127.037842 * 0.0000010000 = 0.0031270378
2019-03-07 14:27:25,445 [INFO] Sum of grad norms: 9.937969
2019-03-07 14:27:25,445 [INFO] ---------------------------------
2019-03-07 14:27:44,888 [INFO] ---------------------------------
2019-03-07 14:27:44,889 [INFO] Summary:
2019-03-07 14:27:44,889 [INFO] Batch 89000, worst loss 0.175369 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:27:44,890 [INFO] Regularization: 3127.045654 * 0.0000010000 = 0.0031270457
2019-03-07 14:27:44,891 [INFO] Sum of grad norms: 5.658177
2019-03-07 14:27:44,892 [INFO] ---------------------------------
2019-03-07 14:28:04,344 [INFO] ---------------------------------
2019-03-07 14:28:04,345 [INFO] Summary:
2019-03-07 14:28:04,346 [INFO] Batch 90000, worst loss 0.195545 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:28:04,346 [INFO] Regularization: 3127.054443 * 0.0000010000 = 0.0031270545
2019-03-07 14:28:04,347 [INFO] Sum of grad norms: 0.268101
2019-03-07 14:28:04,348 [INFO] ---------------------------------
2019-03-07 14:28:17,509 [INFO] ---------------------------------
2019-03-07 14:28:17,510 [INFO] Evaluation:
2019-03-07 14:28:17,511 [INFO] Batch 90000, worst loss 0.191938 (without reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 14:28:17,513 [INFO] ---------------------------------
2019-03-07 14:28:36,966 [INFO] ---------------------------------
2019-03-07 14:28:36,967 [INFO] Summary:
2019-03-07 14:28:36,968 [INFO] Batch 91000, worst loss 0.214068 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:28:36,969 [INFO] Regularization: 3126.922119 * 0.0000010000 = 0.0031269221
2019-03-07 14:28:36,969 [INFO] Sum of grad norms: 1.352245
2019-03-07 14:28:36,970 [INFO] ---------------------------------
2019-03-07 14:28:56,519 [INFO] ---------------------------------
2019-03-07 14:28:56,521 [INFO] Summary:
2019-03-07 14:28:56,521 [INFO] Batch 92000, worst loss 0.178858 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:28:56,522 [INFO] Regularization: 3126.879883 * 0.0000010000 = 0.0031268799
2019-03-07 14:28:56,523 [INFO] Sum of grad norms: 6.917863
2019-03-07 14:28:56,524 [INFO] ---------------------------------
2019-03-07 14:29:16,081 [INFO] ---------------------------------
2019-03-07 14:29:16,082 [INFO] Summary:
2019-03-07 14:29:16,082 [INFO] Batch 93000, worst loss 0.164591 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:29:16,083 [INFO] Regularization: 3126.876465 * 0.0000010000 = 0.0031268764
2019-03-07 14:29:16,084 [INFO] Sum of grad norms: 5.436392
2019-03-07 14:29:16,084 [INFO] ---------------------------------
2019-03-07 14:29:35,524 [INFO] ---------------------------------
2019-03-07 14:29:35,525 [INFO] Summary:
2019-03-07 14:29:35,525 [INFO] Batch 94000, worst loss 0.168945 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:29:35,526 [INFO] Regularization: 3126.861572 * 0.0000010000 = 0.0031268615
2019-03-07 14:29:35,527 [INFO] Sum of grad norms: 0.387176
2019-03-07 14:29:35,528 [INFO] ---------------------------------
2019-03-07 14:29:54,179 [INFO] ---------------------------------
2019-03-07 14:29:54,181 [INFO] Summary:
2019-03-07 14:29:54,181 [INFO] Batch 95000, worst loss 0.155245 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:29:54,182 [INFO] Regularization: 3126.858154 * 0.0000010000 = 0.0031268580
2019-03-07 14:29:54,183 [INFO] Sum of grad norms: 6.762355
2019-03-07 14:29:54,184 [INFO] ---------------------------------
2019-03-07 14:30:13,424 [INFO] ---------------------------------
2019-03-07 14:30:13,425 [INFO] Summary:
2019-03-07 14:30:13,425 [INFO] Batch 96000, worst loss 0.168830 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:30:13,427 [INFO] Regularization: 3126.824463 * 0.0000010000 = 0.0031268245
2019-03-07 14:30:13,427 [INFO] Sum of grad norms: 14.078678
2019-03-07 14:30:13,428 [INFO] ---------------------------------
2019-03-07 14:30:32,944 [INFO] ---------------------------------
2019-03-07 14:30:32,945 [INFO] Summary:
2019-03-07 14:30:32,946 [INFO] Batch 97000, worst loss 0.172675 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:30:32,947 [INFO] Regularization: 3126.828125 * 0.0000010000 = 0.0031268282
2019-03-07 14:30:32,948 [INFO] Sum of grad norms: 0.227686
2019-03-07 14:30:32,949 [INFO] ---------------------------------
2019-03-07 14:30:52,535 [INFO] ---------------------------------
2019-03-07 14:30:52,536 [INFO] Summary:
2019-03-07 14:30:52,537 [INFO] Batch 98000, worst loss 0.166775 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:30:52,537 [INFO] Regularization: 3126.838623 * 0.0000010000 = 0.0031268387
2019-03-07 14:30:52,538 [INFO] Sum of grad norms: 1.021120
2019-03-07 14:30:52,539 [INFO] ---------------------------------
2019-03-07 14:31:11,981 [INFO] ---------------------------------
2019-03-07 14:31:11,982 [INFO] Summary:
2019-03-07 14:31:11,982 [INFO] Batch 99000, worst loss 0.158688 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:31:11,984 [INFO] Regularization: 3126.796875 * 0.0000010000 = 0.0031267968
2019-03-07 14:31:11,984 [INFO] Sum of grad norms: 1.028132
2019-03-07 14:31:11,985 [INFO] ---------------------------------
2019-03-07 14:31:31,825 [INFO] ---------------------------------
2019-03-07 14:31:31,827 [INFO] Summary:
2019-03-07 14:31:31,827 [INFO] Batch 100000, worst loss 0.163850 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:31:31,828 [INFO] Regularization: 3126.804932 * 0.0000010000 = 0.0031268049
2019-03-07 14:31:31,829 [INFO] Sum of grad norms: 0.175787
2019-03-07 14:31:31,830 [INFO] ---------------------------------
2019-03-07 14:31:45,232 [INFO] ---------------------------------
2019-03-07 14:31:45,232 [INFO] Evaluation:
2019-03-07 14:31:45,233 [INFO] Batch 100000, worst loss 0.162581 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:31:45,236 [INFO] ---------------------------------
2019-03-07 14:32:04,972 [INFO] ---------------------------------
2019-03-07 14:32:04,973 [INFO] Summary:
2019-03-07 14:32:04,974 [INFO] Batch 101000, worst loss 0.160568 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:32:04,975 [INFO] Regularization: 3126.811523 * 0.0000010000 = 0.0031268115
2019-03-07 14:32:04,976 [INFO] Sum of grad norms: 0.429792
2019-03-07 14:32:04,977 [INFO] ---------------------------------
2019-03-07 14:32:23,853 [INFO] ---------------------------------
2019-03-07 14:32:23,854 [INFO] Summary:
2019-03-07 14:32:23,856 [INFO] Batch 102000, worst loss 0.160004 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:32:23,857 [INFO] Regularization: 3126.801514 * 0.0000010000 = 0.0031268015
2019-03-07 14:32:23,858 [INFO] Sum of grad norms: 6.731329
2019-03-07 14:32:23,860 [INFO] ---------------------------------
2019-03-07 14:32:43,248 [INFO] ---------------------------------
2019-03-07 14:32:43,249 [INFO] Summary:
2019-03-07 14:32:43,250 [INFO] Batch 103000, worst loss 0.157333 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:32:43,251 [INFO] Regularization: 3126.800293 * 0.0000010000 = 0.0031268003
2019-03-07 14:32:43,252 [INFO] Sum of grad norms: 1.712885
2019-03-07 14:32:43,252 [INFO] ---------------------------------
2019-03-07 14:33:02,569 [INFO] ---------------------------------
2019-03-07 14:33:02,570 [INFO] Summary:
2019-03-07 14:33:02,571 [INFO] Batch 104000, worst loss 0.171146 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:33:02,572 [INFO] Regularization: 3126.794434 * 0.0000010000 = 0.0031267945
2019-03-07 14:33:02,574 [INFO] Sum of grad norms: 3.899586
2019-03-07 14:33:02,575 [INFO] ---------------------------------
2019-03-07 14:33:21,809 [INFO] ---------------------------------
2019-03-07 14:33:21,810 [INFO] Summary:
2019-03-07 14:33:21,811 [INFO] Batch 105000, worst loss 0.150994 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:33:21,812 [INFO] Regularization: 3126.796631 * 0.0000010000 = 0.0031267966
2019-03-07 14:33:21,812 [INFO] Sum of grad norms: 0.274188
2019-03-07 14:33:21,813 [INFO] ---------------------------------
2019-03-07 14:33:41,225 [INFO] ---------------------------------
2019-03-07 14:33:41,227 [INFO] Summary:
2019-03-07 14:33:41,227 [INFO] Batch 106000, worst loss 0.169226 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:33:41,228 [INFO] Regularization: 3126.790283 * 0.0000010000 = 0.0031267903
2019-03-07 14:33:41,229 [INFO] Sum of grad norms: 6.322947
2019-03-07 14:33:41,229 [INFO] ---------------------------------
2019-03-07 14:34:00,654 [INFO] ---------------------------------
2019-03-07 14:34:00,655 [INFO] Summary:
2019-03-07 14:34:00,655 [INFO] Batch 107000, worst loss 0.161726 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:34:00,656 [INFO] Regularization: 3126.790527 * 0.0000010000 = 0.0031267905
2019-03-07 14:34:00,657 [INFO] Sum of grad norms: 1.060315
2019-03-07 14:34:00,658 [INFO] ---------------------------------
2019-03-07 14:34:20,319 [INFO] ---------------------------------
2019-03-07 14:34:20,320 [INFO] Summary:
2019-03-07 14:34:20,321 [INFO] Batch 108000, worst loss 0.185575 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:34:20,322 [INFO] Regularization: 3126.793213 * 0.0000010000 = 0.0031267933
2019-03-07 14:34:20,323 [INFO] Sum of grad norms: 7.364759
2019-03-07 14:34:20,323 [INFO] ---------------------------------
2019-03-07 14:34:39,760 [INFO] ---------------------------------
2019-03-07 14:34:39,760 [INFO] Summary:
2019-03-07 14:34:39,761 [INFO] Batch 109000, worst loss 0.177435 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:34:39,762 [INFO] Regularization: 3126.791992 * 0.0000010000 = 0.0031267919
2019-03-07 14:34:39,762 [INFO] Sum of grad norms: 0.595107
2019-03-07 14:34:39,763 [INFO] ---------------------------------
2019-03-07 14:34:59,223 [INFO] ---------------------------------
2019-03-07 14:34:59,224 [INFO] Summary:
2019-03-07 14:34:59,225 [INFO] Batch 110000, worst loss 0.187880 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:34:59,226 [INFO] Regularization: 3126.789551 * 0.0000010000 = 0.0031267896
2019-03-07 14:34:59,226 [INFO] Sum of grad norms: 0.124810
2019-03-07 14:34:59,227 [INFO] ---------------------------------
2019-03-07 14:35:12,597 [INFO] ---------------------------------
2019-03-07 14:35:12,598 [INFO] Evaluation:
2019-03-07 14:35:12,599 [INFO] Batch 110000, worst loss 0.193062 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:35:12,601 [INFO] ---------------------------------
2019-03-07 14:35:32,373 [INFO] ---------------------------------
2019-03-07 14:35:32,374 [INFO] Summary:
2019-03-07 14:35:32,374 [INFO] Batch 111000, worst loss 0.178452 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:35:32,375 [INFO] Regularization: 3126.784424 * 0.0000010000 = 0.0031267845
2019-03-07 14:35:32,376 [INFO] Sum of grad norms: 1.196151
2019-03-07 14:35:32,376 [INFO] ---------------------------------
2019-03-07 14:35:51,917 [INFO] ---------------------------------
2019-03-07 14:35:51,918 [INFO] Summary:
2019-03-07 14:35:51,918 [INFO] Batch 112000, worst loss 0.152004 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:35:51,919 [INFO] Regularization: 3126.782959 * 0.0000010000 = 0.0031267831
2019-03-07 14:35:51,920 [INFO] Sum of grad norms: 0.113901
2019-03-07 14:35:51,920 [INFO] ---------------------------------
2019-03-07 14:36:11,789 [INFO] ---------------------------------
2019-03-07 14:36:11,790 [INFO] Summary:
2019-03-07 14:36:11,790 [INFO] Batch 113000, worst loss 0.159042 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:36:11,791 [INFO] Regularization: 3126.782959 * 0.0000010000 = 0.0031267831
2019-03-07 14:36:11,792 [INFO] Sum of grad norms: 2.877493
2019-03-07 14:36:11,793 [INFO] ---------------------------------
2019-03-07 14:36:31,391 [INFO] ---------------------------------
2019-03-07 14:36:31,392 [INFO] Summary:
2019-03-07 14:36:31,393 [INFO] Batch 114000, worst loss 0.177013 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:36:31,393 [INFO] Regularization: 3126.780518 * 0.0000010000 = 0.0031267805
2019-03-07 14:36:31,395 [INFO] Sum of grad norms: 0.281954
2019-03-07 14:36:31,396 [INFO] ---------------------------------
2019-03-07 14:36:50,876 [INFO] ---------------------------------
2019-03-07 14:36:50,877 [INFO] Summary:
2019-03-07 14:36:50,877 [INFO] Batch 115000, worst loss 0.171153 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:36:50,878 [INFO] Regularization: 3126.780518 * 0.0000010000 = 0.0031267805
2019-03-07 14:36:50,879 [INFO] Sum of grad norms: 0.119904
2019-03-07 14:36:50,880 [INFO] ---------------------------------
2019-03-07 14:37:09,950 [INFO] ---------------------------------
2019-03-07 14:37:09,951 [INFO] Summary:
2019-03-07 14:37:09,952 [INFO] Batch 116000, worst loss 0.190851 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:37:09,953 [INFO] Regularization: 3126.781738 * 0.0000010000 = 0.0031267817
2019-03-07 14:37:09,954 [INFO] Sum of grad norms: 8.859054
2019-03-07 14:37:09,955 [INFO] ---------------------------------
2019-03-07 14:37:29,248 [INFO] ---------------------------------
2019-03-07 14:37:29,250 [INFO] Summary:
2019-03-07 14:37:29,250 [INFO] Batch 117000, worst loss 0.190825 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:37:29,251 [INFO] Regularization: 3126.780762 * 0.0000010000 = 0.0031267807
2019-03-07 14:37:29,252 [INFO] Sum of grad norms: 1.336082
2019-03-07 14:37:29,253 [INFO] ---------------------------------
2019-03-07 14:37:48,939 [INFO] ---------------------------------
2019-03-07 14:37:48,940 [INFO] Summary:
2019-03-07 14:37:48,941 [INFO] Batch 118000, worst loss 0.183310 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:37:48,941 [INFO] Regularization: 3126.781738 * 0.0000010000 = 0.0031267817
2019-03-07 14:37:48,942 [INFO] Sum of grad norms: 0.210216
2019-03-07 14:37:48,943 [INFO] ---------------------------------
2019-03-07 14:38:08,623 [INFO] ---------------------------------
2019-03-07 14:38:08,624 [INFO] Summary:
2019-03-07 14:38:08,624 [INFO] Batch 119000, worst loss 0.177854 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:38:08,625 [INFO] Regularization: 3126.778809 * 0.0000010000 = 0.0031267789
2019-03-07 14:38:08,626 [INFO] Sum of grad norms: 3.669749
2019-03-07 14:38:08,627 [INFO] ---------------------------------
2019-03-07 14:38:28,241 [INFO] ---------------------------------
2019-03-07 14:38:28,243 [INFO] Summary:
2019-03-07 14:38:28,243 [INFO] Batch 120000, worst loss 0.161415 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:38:28,244 [INFO] Regularization: 3126.779541 * 0.0000010000 = 0.0031267796
2019-03-07 14:38:28,245 [INFO] Sum of grad norms: 13.166910
2019-03-07 14:38:28,246 [INFO] ---------------------------------
2019-03-07 14:38:41,256 [INFO] ---------------------------------
2019-03-07 14:38:41,257 [INFO] Evaluation:
2019-03-07 14:38:41,258 [INFO] Batch 120000, worst loss 0.164175 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:38:41,260 [INFO] ---------------------------------
2019-03-07 14:39:00,922 [INFO] ---------------------------------
2019-03-07 14:39:00,923 [INFO] Summary:
2019-03-07 14:39:00,923 [INFO] Batch 121000, worst loss 0.167299 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:39:00,924 [INFO] Regularization: 3126.779053 * 0.0000010000 = 0.0031267791
2019-03-07 14:39:00,925 [INFO] Sum of grad norms: 6.465965
2019-03-07 14:39:00,926 [INFO] ---------------------------------
2019-03-07 14:39:20,515 [INFO] ---------------------------------
2019-03-07 14:39:20,516 [INFO] Summary:
2019-03-07 14:39:20,516 [INFO] Batch 122000, worst loss 0.165670 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:39:20,517 [INFO] Regularization: 3126.779297 * 0.0000010000 = 0.0031267793
2019-03-07 14:39:20,517 [INFO] Sum of grad norms: 16.248671
2019-03-07 14:39:20,518 [INFO] ---------------------------------
2019-03-07 14:39:39,880 [INFO] ---------------------------------
2019-03-07 14:39:39,881 [INFO] Summary:
2019-03-07 14:39:39,881 [INFO] Batch 123000, worst loss 0.166954 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:39:39,882 [INFO] Regularization: 3126.778809 * 0.0000010000 = 0.0031267789
2019-03-07 14:39:39,883 [INFO] Sum of grad norms: 0.890242
2019-03-07 14:39:39,884 [INFO] ---------------------------------
2019-03-07 14:39:59,506 [INFO] ---------------------------------
2019-03-07 14:39:59,507 [INFO] Summary:
2019-03-07 14:39:59,508 [INFO] Batch 124000, worst loss 0.198963 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:39:59,509 [INFO] Regularization: 3126.779053 * 0.0000010000 = 0.0031267791
2019-03-07 14:39:59,509 [INFO] Sum of grad norms: 7.349185
2019-03-07 14:39:59,510 [INFO] ---------------------------------
2019-03-07 14:40:19,052 [INFO] ---------------------------------
2019-03-07 14:40:19,053 [INFO] Summary:
2019-03-07 14:40:19,054 [INFO] Batch 125000, worst loss 0.168867 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:40:19,055 [INFO] Regularization: 3126.778809 * 0.0000010000 = 0.0031267789
2019-03-07 14:40:19,056 [INFO] Sum of grad norms: 2.400509
2019-03-07 14:40:19,057 [INFO] ---------------------------------
2019-03-07 14:40:38,663 [INFO] ---------------------------------
2019-03-07 14:40:38,664 [INFO] Summary:
2019-03-07 14:40:38,665 [INFO] Batch 126000, worst loss 0.142998 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:40:38,666 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:40:38,667 [INFO] Sum of grad norms: 0.165441
2019-03-07 14:40:38,667 [INFO] ---------------------------------
2019-03-07 14:40:58,238 [INFO] ---------------------------------
2019-03-07 14:40:58,239 [INFO] Summary:
2019-03-07 14:40:58,240 [INFO] Batch 127000, worst loss 0.142997 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:40:58,240 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:40:58,241 [INFO] Sum of grad norms: 4.465454
2019-03-07 14:40:58,242 [INFO] ---------------------------------
2019-03-07 14:41:17,956 [INFO] ---------------------------------
2019-03-07 14:41:17,957 [INFO] Summary:
2019-03-07 14:41:17,958 [INFO] Batch 128000, worst loss 0.161359 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:41:17,959 [INFO] Regularization: 3126.778809 * 0.0000010000 = 0.0031267789
2019-03-07 14:41:17,960 [INFO] Sum of grad norms: 4.838260
2019-03-07 14:41:17,961 [INFO] ---------------------------------
2019-03-07 14:41:37,601 [INFO] ---------------------------------
2019-03-07 14:41:37,602 [INFO] Summary:
2019-03-07 14:41:37,603 [INFO] Batch 129000, worst loss 0.161356 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:41:37,604 [INFO] Regularization: 3126.779053 * 0.0000010000 = 0.0031267791
2019-03-07 14:41:37,605 [INFO] Sum of grad norms: 10.525337
2019-03-07 14:41:37,606 [INFO] ---------------------------------
2019-03-07 14:41:57,248 [INFO] ---------------------------------
2019-03-07 14:41:57,249 [INFO] Summary:
2019-03-07 14:41:57,250 [INFO] Batch 130000, worst loss 0.144319 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:41:57,250 [INFO] Regularization: 3126.778809 * 0.0000010000 = 0.0031267789
2019-03-07 14:41:57,251 [INFO] Sum of grad norms: 1.015548
2019-03-07 14:41:57,252 [INFO] ---------------------------------
2019-03-07 14:42:10,523 [INFO] ---------------------------------
2019-03-07 14:42:10,524 [INFO] Evaluation:
2019-03-07 14:42:10,525 [INFO] Batch 130000, worst loss 0.199995 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:42:10,525 [INFO] ---------------------------------
2019-03-07 14:42:29,906 [INFO] ---------------------------------
2019-03-07 14:42:29,907 [INFO] Summary:
2019-03-07 14:42:29,908 [INFO] Batch 131000, worst loss 0.203119 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:42:29,910 [INFO] Regularization: 3126.778809 * 0.0000010000 = 0.0031267789
2019-03-07 14:42:29,911 [INFO] Sum of grad norms: 14.622920
2019-03-07 14:42:29,912 [INFO] ---------------------------------
2019-03-07 14:42:49,422 [INFO] ---------------------------------
2019-03-07 14:42:49,423 [INFO] Summary:
2019-03-07 14:42:49,426 [INFO] Batch 132000, worst loss 0.167461 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:42:49,427 [INFO] Regularization: 3126.778809 * 0.0000010000 = 0.0031267789
2019-03-07 14:42:49,428 [INFO] Sum of grad norms: 7.489566
2019-03-07 14:42:49,430 [INFO] ---------------------------------
2019-03-07 14:43:09,021 [INFO] ---------------------------------
2019-03-07 14:43:09,022 [INFO] Summary:
2019-03-07 14:43:09,022 [INFO] Batch 133000, worst loss 0.167461 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:43:09,024 [INFO] Regularization: 3126.778809 * 0.0000010000 = 0.0031267789
2019-03-07 14:43:09,025 [INFO] Sum of grad norms: 2.132650
2019-03-07 14:43:09,026 [INFO] ---------------------------------
2019-03-07 14:43:28,386 [INFO] ---------------------------------
2019-03-07 14:43:28,387 [INFO] Summary:
2019-03-07 14:43:28,388 [INFO] Batch 134000, worst loss 0.163298 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:43:28,389 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:43:28,390 [INFO] Sum of grad norms: 0.447502
2019-03-07 14:43:28,390 [INFO] ---------------------------------
2019-03-07 14:43:47,670 [INFO] ---------------------------------
2019-03-07 14:43:47,671 [INFO] Summary:
2019-03-07 14:43:47,672 [INFO] Batch 135000, worst loss 0.182789 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:43:47,673 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:43:47,674 [INFO] Sum of grad norms: 0.507399
2019-03-07 14:43:47,675 [INFO] ---------------------------------
2019-03-07 14:44:07,021 [INFO] ---------------------------------
2019-03-07 14:44:07,022 [INFO] Summary:
2019-03-07 14:44:07,023 [INFO] Batch 136000, worst loss 0.179662 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:44:07,023 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:44:07,024 [INFO] Sum of grad norms: 0.389674
2019-03-07 14:44:07,025 [INFO] ---------------------------------
2019-03-07 14:44:26,196 [INFO] ---------------------------------
2019-03-07 14:44:26,197 [INFO] Summary:
2019-03-07 14:44:26,197 [INFO] Batch 137000, worst loss 0.198473 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:44:26,198 [INFO] Regularization: 3126.778809 * 0.0000010000 = 0.0031267789
2019-03-07 14:44:26,199 [INFO] Sum of grad norms: 5.340464
2019-03-07 14:44:26,200 [INFO] ---------------------------------
2019-03-07 14:44:45,561 [INFO] ---------------------------------
2019-03-07 14:44:45,562 [INFO] Summary:
2019-03-07 14:44:45,562 [INFO] Batch 138000, worst loss 0.171710 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:44:45,563 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:44:45,564 [INFO] Sum of grad norms: 10.016426
2019-03-07 14:44:45,565 [INFO] ---------------------------------
2019-03-07 14:45:04,852 [INFO] ---------------------------------
2019-03-07 14:45:04,853 [INFO] Summary:
2019-03-07 14:45:04,854 [INFO] Batch 139000, worst loss 0.171710 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:45:04,854 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:45:04,855 [INFO] Sum of grad norms: 5.628522
2019-03-07 14:45:04,856 [INFO] ---------------------------------
2019-03-07 14:45:24,395 [INFO] ---------------------------------
2019-03-07 14:45:24,396 [INFO] Summary:
2019-03-07 14:45:24,397 [INFO] Batch 140000, worst loss 0.192651 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:45:24,398 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:45:24,398 [INFO] Sum of grad norms: 0.539249
2019-03-07 14:45:24,399 [INFO] ---------------------------------
2019-03-07 14:45:37,698 [INFO] ---------------------------------
2019-03-07 14:45:37,699 [INFO] Evaluation:
2019-03-07 14:45:37,699 [INFO] Batch 140000, worst loss 0.161267 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:45:37,700 [INFO] ---------------------------------
2019-03-07 14:45:57,393 [INFO] ---------------------------------
2019-03-07 14:45:57,394 [INFO] Summary:
2019-03-07 14:45:57,395 [INFO] Batch 141000, worst loss 0.161436 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:45:57,396 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:45:57,397 [INFO] Sum of grad norms: 0.136368
2019-03-07 14:45:57,397 [INFO] ---------------------------------
2019-03-07 14:46:16,734 [INFO] ---------------------------------
2019-03-07 14:46:16,735 [INFO] Summary:
2019-03-07 14:46:16,736 [INFO] Batch 142000, worst loss 0.162426 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:46:16,738 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:46:16,740 [INFO] Sum of grad norms: 17.216097
2019-03-07 14:46:16,741 [INFO] ---------------------------------
2019-03-07 14:46:36,014 [INFO] ---------------------------------
2019-03-07 14:46:36,015 [INFO] Summary:
2019-03-07 14:46:36,017 [INFO] Batch 143000, worst loss 0.186451 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:46:36,019 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:46:36,020 [INFO] Sum of grad norms: 6.710433
2019-03-07 14:46:36,020 [INFO] ---------------------------------
2019-03-07 14:46:55,803 [INFO] ---------------------------------
2019-03-07 14:46:55,804 [INFO] Summary:
2019-03-07 14:46:55,805 [INFO] Batch 144000, worst loss 0.164739 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:46:55,806 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:46:55,807 [INFO] Sum of grad norms: 5.981455
2019-03-07 14:46:55,807 [INFO] ---------------------------------
2019-03-07 14:47:15,306 [INFO] ---------------------------------
2019-03-07 14:47:15,307 [INFO] Summary:
2019-03-07 14:47:15,307 [INFO] Batch 145000, worst loss 0.159217 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:47:15,308 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:47:15,309 [INFO] Sum of grad norms: 2.344349
2019-03-07 14:47:15,310 [INFO] ---------------------------------
2019-03-07 14:47:34,156 [INFO] ---------------------------------
2019-03-07 14:47:34,157 [INFO] Summary:
2019-03-07 14:47:34,158 [INFO] Batch 146000, worst loss 0.173484 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:47:34,159 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:47:34,160 [INFO] Sum of grad norms: 1.013117
2019-03-07 14:47:34,161 [INFO] ---------------------------------
2019-03-07 14:47:53,724 [INFO] ---------------------------------
2019-03-07 14:47:53,725 [INFO] Summary:
2019-03-07 14:47:53,726 [INFO] Batch 147000, worst loss 0.173484 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:47:53,727 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:47:53,728 [INFO] Sum of grad norms: 5.650665
2019-03-07 14:47:53,729 [INFO] ---------------------------------
2019-03-07 14:48:12,935 [INFO] ---------------------------------
2019-03-07 14:48:12,936 [INFO] Summary:
2019-03-07 14:48:12,936 [INFO] Batch 148000, worst loss 0.144450 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:48:12,937 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:48:12,938 [INFO] Sum of grad norms: 1.738995
2019-03-07 14:48:12,939 [INFO] ---------------------------------
2019-03-07 14:48:32,421 [INFO] ---------------------------------
2019-03-07 14:48:32,422 [INFO] Summary:
2019-03-07 14:48:32,422 [INFO] Batch 149000, worst loss 0.177794 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:48:32,423 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:48:32,424 [INFO] Sum of grad norms: 1.371006
2019-03-07 14:48:32,425 [INFO] ---------------------------------
2019-03-07 14:48:51,615 [INFO] ---------------------------------
2019-03-07 14:48:51,616 [INFO] Summary:
2019-03-07 14:48:51,618 [INFO] Batch 150000, worst loss 0.177794 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:48:51,619 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:48:51,620 [INFO] Sum of grad norms: 2.273757
2019-03-07 14:48:51,621 [INFO] ---------------------------------
2019-03-07 14:49:04,705 [INFO] ---------------------------------
2019-03-07 14:49:04,706 [INFO] Evaluation:
2019-03-07 14:49:04,706 [INFO] Batch 150000, worst loss 0.170058 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:49:04,707 [INFO] ---------------------------------
2019-03-07 14:49:24,287 [INFO] ---------------------------------
2019-03-07 14:49:24,288 [INFO] Summary:
2019-03-07 14:49:24,288 [INFO] Batch 151000, worst loss 0.209263 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:49:24,289 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:49:24,290 [INFO] Sum of grad norms: 3.191687
2019-03-07 14:49:24,290 [INFO] ---------------------------------
2019-03-07 14:49:43,741 [INFO] ---------------------------------
2019-03-07 14:49:43,742 [INFO] Summary:
2019-03-07 14:49:43,742 [INFO] Batch 152000, worst loss 0.209263 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:49:43,743 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:49:43,744 [INFO] Sum of grad norms: 0.167928
2019-03-07 14:49:43,745 [INFO] ---------------------------------
2019-03-07 14:50:03,212 [INFO] ---------------------------------
2019-03-07 14:50:03,213 [INFO] Summary:
2019-03-07 14:50:03,214 [INFO] Batch 153000, worst loss 0.174537 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:50:03,214 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:50:03,215 [INFO] Sum of grad norms: 8.516314
2019-03-07 14:50:03,216 [INFO] ---------------------------------
2019-03-07 14:50:22,467 [INFO] ---------------------------------
2019-03-07 14:50:22,468 [INFO] Summary:
2019-03-07 14:50:22,469 [INFO] Batch 154000, worst loss 0.154824 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:50:22,470 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:50:22,471 [INFO] Sum of grad norms: 2.027260
2019-03-07 14:50:22,472 [INFO] ---------------------------------
2019-03-07 14:50:42,296 [INFO] ---------------------------------
2019-03-07 14:50:42,296 [INFO] Summary:
2019-03-07 14:50:42,297 [INFO] Batch 155000, worst loss 0.167949 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:50:42,298 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:50:42,298 [INFO] Sum of grad norms: 2.837141
2019-03-07 14:50:42,300 [INFO] ---------------------------------
2019-03-07 14:51:01,866 [INFO] ---------------------------------
2019-03-07 14:51:01,867 [INFO] Summary:
2019-03-07 14:51:01,867 [INFO] Batch 156000, worst loss 0.153615 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:51:01,868 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:51:01,869 [INFO] Sum of grad norms: 15.399500
2019-03-07 14:51:01,870 [INFO] ---------------------------------
2019-03-07 14:51:21,133 [INFO] ---------------------------------
2019-03-07 14:51:21,134 [INFO] Summary:
2019-03-07 14:51:21,134 [INFO] Batch 157000, worst loss 0.169549 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:51:21,135 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:51:21,136 [INFO] Sum of grad norms: 0.312135
2019-03-07 14:51:21,137 [INFO] ---------------------------------
2019-03-07 14:51:40,566 [INFO] ---------------------------------
2019-03-07 14:51:40,567 [INFO] Summary:
2019-03-07 14:51:40,567 [INFO] Batch 158000, worst loss 0.157493 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:51:40,569 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:51:40,569 [INFO] Sum of grad norms: 11.479709
2019-03-07 14:51:40,570 [INFO] ---------------------------------
2019-03-07 14:52:00,436 [INFO] ---------------------------------
2019-03-07 14:52:00,437 [INFO] Summary:
2019-03-07 14:52:00,437 [INFO] Batch 159000, worst loss 0.164187 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:52:00,439 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:52:00,443 [INFO] Sum of grad norms: 8.127169
2019-03-07 14:52:00,445 [INFO] ---------------------------------
2019-03-07 14:52:19,793 [INFO] ---------------------------------
2019-03-07 14:52:19,794 [INFO] Summary:
2019-03-07 14:52:19,795 [INFO] Batch 160000, worst loss 0.210878 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:52:19,796 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:52:19,797 [INFO] Sum of grad norms: 0.251921
2019-03-07 14:52:19,798 [INFO] ---------------------------------
2019-03-07 14:52:32,924 [INFO] ---------------------------------
2019-03-07 14:52:32,925 [INFO] Evaluation:
2019-03-07 14:52:32,926 [INFO] Batch 160000, worst loss 0.227679 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:52:32,927 [INFO] ---------------------------------
2019-03-07 14:52:52,484 [INFO] ---------------------------------
2019-03-07 14:52:52,486 [INFO] Summary:
2019-03-07 14:52:52,486 [INFO] Batch 161000, worst loss 0.230806 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:52:52,487 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:52:52,488 [INFO] Sum of grad norms: 0.251123
2019-03-07 14:52:52,489 [INFO] ---------------------------------
2019-03-07 14:53:11,920 [INFO] ---------------------------------
2019-03-07 14:53:11,921 [INFO] Summary:
2019-03-07 14:53:11,922 [INFO] Batch 162000, worst loss 0.189822 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:53:11,923 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:53:11,924 [INFO] Sum of grad norms: 0.210185
2019-03-07 14:53:11,924 [INFO] ---------------------------------
2019-03-07 14:53:31,166 [INFO] ---------------------------------
2019-03-07 14:53:31,168 [INFO] Summary:
2019-03-07 14:53:31,172 [INFO] Batch 163000, worst loss 0.174370 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:53:31,172 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:53:31,175 [INFO] Sum of grad norms: 2.286313
2019-03-07 14:53:31,177 [INFO] ---------------------------------
2019-03-07 14:53:50,499 [INFO] ---------------------------------
2019-03-07 14:53:50,501 [INFO] Summary:
2019-03-07 14:53:50,501 [INFO] Batch 164000, worst loss 0.164451 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:53:50,502 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:53:50,502 [INFO] Sum of grad norms: 0.943830
2019-03-07 14:53:50,503 [INFO] ---------------------------------
2019-03-07 14:54:09,857 [INFO] ---------------------------------
2019-03-07 14:54:09,858 [INFO] Summary:
2019-03-07 14:54:09,859 [INFO] Batch 165000, worst loss 0.164451 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:54:09,859 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:54:09,860 [INFO] Sum of grad norms: 6.399755
2019-03-07 14:54:09,861 [INFO] ---------------------------------
2019-03-07 14:54:29,343 [INFO] ---------------------------------
2019-03-07 14:54:29,344 [INFO] Summary:
2019-03-07 14:54:29,344 [INFO] Batch 166000, worst loss 0.158762 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:54:29,345 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:54:29,346 [INFO] Sum of grad norms: 2.014657
2019-03-07 14:54:29,347 [INFO] ---------------------------------
2019-03-07 14:54:48,796 [INFO] ---------------------------------
2019-03-07 14:54:48,797 [INFO] Summary:
2019-03-07 14:54:48,797 [INFO] Batch 167000, worst loss 0.157707 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:54:48,798 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:54:48,799 [INFO] Sum of grad norms: 8.735546
2019-03-07 14:54:48,799 [INFO] ---------------------------------
2019-03-07 14:55:08,426 [INFO] ---------------------------------
2019-03-07 14:55:08,428 [INFO] Summary:
2019-03-07 14:55:08,428 [INFO] Batch 168000, worst loss 0.156114 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:55:08,429 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:55:08,430 [INFO] Sum of grad norms: 4.373651
2019-03-07 14:55:08,431 [INFO] ---------------------------------
2019-03-07 14:55:28,020 [INFO] ---------------------------------
2019-03-07 14:55:28,021 [INFO] Summary:
2019-03-07 14:55:28,022 [INFO] Batch 169000, worst loss 0.176088 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:55:28,023 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:55:28,024 [INFO] Sum of grad norms: 0.323187
2019-03-07 14:55:28,025 [INFO] ---------------------------------
2019-03-07 14:55:47,559 [INFO] ---------------------------------
2019-03-07 14:55:47,561 [INFO] Summary:
2019-03-07 14:55:47,562 [INFO] Batch 170000, worst loss 0.176088 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:55:47,563 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:55:47,563 [INFO] Sum of grad norms: 0.146455
2019-03-07 14:55:47,564 [INFO] ---------------------------------
2019-03-07 14:56:00,857 [INFO] ---------------------------------
2019-03-07 14:56:00,858 [INFO] Evaluation:
2019-03-07 14:56:00,859 [INFO] Batch 170000, worst loss 0.143517 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:56:00,860 [INFO] ---------------------------------
2019-03-07 14:56:20,037 [INFO] ---------------------------------
2019-03-07 14:56:20,039 [INFO] Summary:
2019-03-07 14:56:20,039 [INFO] Batch 171000, worst loss 0.158427 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:56:20,040 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:56:20,042 [INFO] Sum of grad norms: 6.222581
2019-03-07 14:56:20,043 [INFO] ---------------------------------
2019-03-07 14:56:39,450 [INFO] ---------------------------------
2019-03-07 14:56:39,451 [INFO] Summary:
2019-03-07 14:56:39,451 [INFO] Batch 172000, worst loss 0.164025 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:56:39,452 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:56:39,452 [INFO] Sum of grad norms: 9.335126
2019-03-07 14:56:39,453 [INFO] ---------------------------------
2019-03-07 14:56:58,701 [INFO] ---------------------------------
2019-03-07 14:56:58,702 [INFO] Summary:
2019-03-07 14:56:58,702 [INFO] Batch 173000, worst loss 0.164661 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:56:58,703 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:56:58,704 [INFO] Sum of grad norms: 0.393207
2019-03-07 14:56:58,705 [INFO] ---------------------------------
2019-03-07 14:57:18,082 [INFO] ---------------------------------
2019-03-07 14:57:18,083 [INFO] Summary:
2019-03-07 14:57:18,083 [INFO] Batch 174000, worst loss 0.188207 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:57:18,084 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:57:18,085 [INFO] Sum of grad norms: 6.662012
2019-03-07 14:57:18,085 [INFO] ---------------------------------
2019-03-07 14:57:37,483 [INFO] ---------------------------------
2019-03-07 14:57:37,484 [INFO] Summary:
2019-03-07 14:57:37,484 [INFO] Batch 175000, worst loss 0.188207 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:57:37,485 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:57:37,486 [INFO] Sum of grad norms: 2.040206
2019-03-07 14:57:37,486 [INFO] ---------------------------------
2019-03-07 14:57:56,579 [INFO] ---------------------------------
2019-03-07 14:57:56,580 [INFO] Summary:
2019-03-07 14:57:56,580 [INFO] Batch 176000, worst loss 0.178121 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:57:56,581 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:57:56,582 [INFO] Sum of grad norms: 0.158336
2019-03-07 14:57:56,583 [INFO] ---------------------------------
2019-03-07 14:58:15,775 [INFO] ---------------------------------
2019-03-07 14:58:15,776 [INFO] Summary:
2019-03-07 14:58:15,777 [INFO] Batch 177000, worst loss 0.161194 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:58:15,777 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:58:15,778 [INFO] Sum of grad norms: 0.248650
2019-03-07 14:58:15,779 [INFO] ---------------------------------
2019-03-07 14:58:34,761 [INFO] ---------------------------------
2019-03-07 14:58:34,762 [INFO] Summary:
2019-03-07 14:58:34,764 [INFO] Batch 178000, worst loss 0.202133 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:58:34,767 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:58:34,768 [INFO] Sum of grad norms: 1.726990
2019-03-07 14:58:34,769 [INFO] ---------------------------------
2019-03-07 14:58:54,157 [INFO] ---------------------------------
2019-03-07 14:58:54,158 [INFO] Summary:
2019-03-07 14:58:54,159 [INFO] Batch 179000, worst loss 0.200206 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:58:54,159 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:58:54,160 [INFO] Sum of grad norms: 3.649052
2019-03-07 14:58:54,161 [INFO] ---------------------------------
2019-03-07 14:59:13,757 [INFO] ---------------------------------
2019-03-07 14:59:13,758 [INFO] Summary:
2019-03-07 14:59:13,758 [INFO] Batch 180000, worst loss 0.174492 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:59:13,759 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:59:13,760 [INFO] Sum of grad norms: 5.661694
2019-03-07 14:59:13,760 [INFO] ---------------------------------
2019-03-07 14:59:26,920 [INFO] ---------------------------------
2019-03-07 14:59:26,921 [INFO] Evaluation:
2019-03-07 14:59:26,922 [INFO] Batch 180000, worst loss 0.173809 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:59:26,923 [INFO] ---------------------------------
2019-03-07 14:59:46,574 [INFO] ---------------------------------
2019-03-07 14:59:46,575 [INFO] Summary:
2019-03-07 14:59:46,575 [INFO] Batch 181000, worst loss 0.188792 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 14:59:46,576 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 14:59:46,579 [INFO] Sum of grad norms: 0.128372
2019-03-07 14:59:46,580 [INFO] ---------------------------------
2019-03-07 15:00:06,019 [INFO] ---------------------------------
2019-03-07 15:00:06,020 [INFO] Summary:
2019-03-07 15:00:06,020 [INFO] Batch 182000, worst loss 0.188792 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:00:06,021 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:00:06,022 [INFO] Sum of grad norms: 0.246824
2019-03-07 15:00:06,022 [INFO] ---------------------------------
2019-03-07 15:00:25,586 [INFO] ---------------------------------
2019-03-07 15:00:25,587 [INFO] Summary:
2019-03-07 15:00:25,588 [INFO] Batch 183000, worst loss 0.141354 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:00:25,588 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:00:25,589 [INFO] Sum of grad norms: 0.232857
2019-03-07 15:00:25,590 [INFO] ---------------------------------
2019-03-07 15:00:45,194 [INFO] ---------------------------------
2019-03-07 15:00:45,195 [INFO] Summary:
2019-03-07 15:00:45,196 [INFO] Batch 184000, worst loss 0.157324 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:00:45,197 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:00:45,197 [INFO] Sum of grad norms: 1.799005
2019-03-07 15:00:45,198 [INFO] ---------------------------------
2019-03-07 15:01:04,761 [INFO] ---------------------------------
2019-03-07 15:01:04,762 [INFO] Summary:
2019-03-07 15:01:04,762 [INFO] Batch 185000, worst loss 0.202050 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:01:04,763 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:01:04,763 [INFO] Sum of grad norms: 5.155796
2019-03-07 15:01:04,764 [INFO] ---------------------------------
2019-03-07 15:01:24,302 [INFO] ---------------------------------
2019-03-07 15:01:24,304 [INFO] Summary:
2019-03-07 15:01:24,304 [INFO] Batch 186000, worst loss 0.202050 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:01:24,305 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:01:24,306 [INFO] Sum of grad norms: 7.198794
2019-03-07 15:01:24,307 [INFO] ---------------------------------
2019-03-07 15:01:43,925 [INFO] ---------------------------------
2019-03-07 15:01:43,926 [INFO] Summary:
2019-03-07 15:01:43,926 [INFO] Batch 187000, worst loss 0.149048 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:01:43,927 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:01:43,928 [INFO] Sum of grad norms: 3.211183
2019-03-07 15:01:43,929 [INFO] ---------------------------------
2019-03-07 15:02:03,262 [INFO] ---------------------------------
2019-03-07 15:02:03,262 [INFO] Summary:
2019-03-07 15:02:03,263 [INFO] Batch 188000, worst loss 0.186417 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:02:03,264 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:02:03,264 [INFO] Sum of grad norms: 2.714100
2019-03-07 15:02:03,265 [INFO] ---------------------------------
2019-03-07 15:02:23,182 [INFO] ---------------------------------
2019-03-07 15:02:23,183 [INFO] Summary:
2019-03-07 15:02:23,184 [INFO] Batch 189000, worst loss 0.186417 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:02:23,185 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:02:23,186 [INFO] Sum of grad norms: 7.996652
2019-03-07 15:02:23,187 [INFO] ---------------------------------
2019-03-07 15:02:42,552 [INFO] ---------------------------------
2019-03-07 15:02:42,553 [INFO] Summary:
2019-03-07 15:02:42,554 [INFO] Batch 190000, worst loss 0.167035 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:02:42,554 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:02:42,556 [INFO] Sum of grad norms: 0.329007
2019-03-07 15:02:42,556 [INFO] ---------------------------------
2019-03-07 15:02:55,595 [INFO] ---------------------------------
2019-03-07 15:02:55,596 [INFO] Evaluation:
2019-03-07 15:02:55,597 [INFO] Batch 190000, worst loss 0.154973 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:02:55,599 [INFO] ---------------------------------
2019-03-07 15:03:15,085 [INFO] ---------------------------------
2019-03-07 15:03:15,086 [INFO] Summary:
2019-03-07 15:03:15,087 [INFO] Batch 191000, worst loss 0.162959 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:03:15,087 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:03:15,088 [INFO] Sum of grad norms: 0.286351
2019-03-07 15:03:15,089 [INFO] ---------------------------------
2019-03-07 15:03:34,359 [INFO] ---------------------------------
2019-03-07 15:03:34,360 [INFO] Summary:
2019-03-07 15:03:34,361 [INFO] Batch 192000, worst loss 0.165793 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:03:34,361 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:03:34,362 [INFO] Sum of grad norms: 3.746209
2019-03-07 15:03:34,363 [INFO] ---------------------------------
2019-03-07 15:03:53,264 [INFO] ---------------------------------
2019-03-07 15:03:53,265 [INFO] Summary:
2019-03-07 15:03:53,268 [INFO] Batch 193000, worst loss 0.165793 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:03:53,268 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:03:53,270 [INFO] Sum of grad norms: 1.396208
2019-03-07 15:03:53,272 [INFO] ---------------------------------
2019-03-07 15:04:12,547 [INFO] ---------------------------------
2019-03-07 15:04:12,548 [INFO] Summary:
2019-03-07 15:04:12,549 [INFO] Batch 194000, worst loss 0.157921 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:04:12,551 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:04:12,554 [INFO] Sum of grad norms: 0.866796
2019-03-07 15:04:12,556 [INFO] ---------------------------------
2019-03-07 15:04:31,877 [INFO] ---------------------------------
2019-03-07 15:04:31,878 [INFO] Summary:
2019-03-07 15:04:31,879 [INFO] Batch 195000, worst loss 0.169283 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:04:31,880 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:04:31,881 [INFO] Sum of grad norms: 0.156552
2019-03-07 15:04:31,882 [INFO] ---------------------------------
2019-03-07 15:04:51,150 [INFO] ---------------------------------
2019-03-07 15:04:51,152 [INFO] Summary:
2019-03-07 15:04:51,152 [INFO] Batch 196000, worst loss 0.182321 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:04:51,154 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:04:51,158 [INFO] Sum of grad norms: 0.308795
2019-03-07 15:04:51,158 [INFO] ---------------------------------
2019-03-07 15:05:10,278 [INFO] ---------------------------------
2019-03-07 15:05:10,279 [INFO] Summary:
2019-03-07 15:05:10,279 [INFO] Batch 197000, worst loss 0.204417 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:05:10,280 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:05:10,281 [INFO] Sum of grad norms: 12.800255
2019-03-07 15:05:10,282 [INFO] ---------------------------------
2019-03-07 15:05:29,666 [INFO] ---------------------------------
2019-03-07 15:05:29,667 [INFO] Summary:
2019-03-07 15:05:29,668 [INFO] Batch 198000, worst loss 0.238565 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:05:29,668 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:05:29,669 [INFO] Sum of grad norms: 2.152663
2019-03-07 15:05:29,670 [INFO] ---------------------------------
2019-03-07 15:05:49,078 [INFO] ---------------------------------
2019-03-07 15:05:49,079 [INFO] Summary:
2019-03-07 15:05:49,079 [INFO] Batch 199000, worst loss 0.238565 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:05:49,080 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:05:49,081 [INFO] Sum of grad norms: 0.168397
2019-03-07 15:05:49,081 [INFO] ---------------------------------
2019-03-07 15:06:08,567 [INFO] ---------------------------------
2019-03-07 15:06:08,568 [INFO] Summary:
2019-03-07 15:06:08,568 [INFO] Batch 200000, worst loss 0.158838 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:06:08,569 [INFO] Regularization: 3126.778564 * 0.0000010000 = 0.0031267786
2019-03-07 15:06:08,570 [INFO] Sum of grad norms: 11.942440
2019-03-07 15:06:08,571 [INFO] ---------------------------------
2019-03-07 15:06:22,054 [INFO] ---------------------------------
2019-03-07 15:06:22,055 [INFO] Evaluation:
2019-03-07 15:06:22,056 [INFO] Batch 200000, worst loss 0.155711 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:06:22,058 [INFO] ---------------------------------
2019-03-07 15:06:22,060 [INFO] Finished training, saved to file classifier/1551963312/1551967582_0_classifier_final.pth
2019-03-07 15:06:22,323 [INFO] Training model #1: (9, 64, 402) @ 2
2019-03-07 15:06:41,757 [INFO] ---------------------------------
2019-03-07 15:06:41,758 [INFO] Summary:
2019-03-07 15:06:41,759 [INFO] Batch 1000, worst loss 4285.331543 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:06:41,760 [INFO] Regularization: 15831.426758 * 0.0000010000 = 0.0158314276
2019-03-07 15:06:41,761 [INFO] Sum of grad norms: 7.086388
2019-03-07 15:06:41,762 [INFO] ---------------------------------
2019-03-07 15:07:00,984 [INFO] ---------------------------------
2019-03-07 15:07:00,985 [INFO] Summary:
2019-03-07 15:07:00,985 [INFO] Batch 2000, worst loss 1.129971 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:07:00,986 [INFO] Regularization: 14803.454102 * 0.0000010000 = 0.0148034543
2019-03-07 15:07:00,987 [INFO] Sum of grad norms: 4.345046
2019-03-07 15:07:00,988 [INFO] ---------------------------------
2019-03-07 15:07:20,247 [INFO] ---------------------------------
2019-03-07 15:07:20,248 [INFO] Summary:
2019-03-07 15:07:20,248 [INFO] Batch 3000, worst loss 0.729118 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:07:20,249 [INFO] Regularization: 13075.064453 * 0.0000010000 = 0.0130750649
2019-03-07 15:07:20,250 [INFO] Sum of grad norms: 0.995155
2019-03-07 15:07:20,251 [INFO] ---------------------------------
2019-03-07 15:07:39,682 [INFO] ---------------------------------
2019-03-07 15:07:39,683 [INFO] Summary:
2019-03-07 15:07:39,683 [INFO] Batch 4000, worst loss 0.786632 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:07:39,684 [INFO] Regularization: 11849.270508 * 0.0000010000 = 0.0118492702
2019-03-07 15:07:39,685 [INFO] Sum of grad norms: 3.617981
2019-03-07 15:07:39,687 [INFO] ---------------------------------
2019-03-07 15:07:59,083 [INFO] ---------------------------------
2019-03-07 15:07:59,084 [INFO] Summary:
2019-03-07 15:07:59,085 [INFO] Batch 5000, worst loss 0.677672 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:07:59,085 [INFO] Regularization: 10864.019531 * 0.0000010000 = 0.0108640194
2019-03-07 15:07:59,087 [INFO] Sum of grad norms: 0.995148
2019-03-07 15:07:59,088 [INFO] ---------------------------------
2019-03-07 15:08:18,123 [INFO] ---------------------------------
2019-03-07 15:08:18,124 [INFO] Summary:
2019-03-07 15:08:18,125 [INFO] Batch 6000, worst loss 0.786819 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:08:18,126 [INFO] Regularization: 9918.875000 * 0.0000010000 = 0.0099188751
2019-03-07 15:08:18,127 [INFO] Sum of grad norms: 4.853523
2019-03-07 15:08:18,128 [INFO] ---------------------------------
2019-03-07 15:08:37,454 [INFO] ---------------------------------
2019-03-07 15:08:37,455 [INFO] Summary:
2019-03-07 15:08:37,456 [INFO] Batch 7000, worst loss 0.592245 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:08:37,457 [INFO] Regularization: 9018.687500 * 0.0000010000 = 0.0090186875
2019-03-07 15:08:37,457 [INFO] Sum of grad norms: 9.245169
2019-03-07 15:08:37,458 [INFO] ---------------------------------
2019-03-07 15:08:56,912 [INFO] ---------------------------------
2019-03-07 15:08:56,912 [INFO] Summary:
2019-03-07 15:08:56,913 [INFO] Batch 8000, worst loss 0.441107 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:08:56,914 [INFO] Regularization: 8452.169922 * 0.0000010000 = 0.0084521696
2019-03-07 15:08:56,915 [INFO] Sum of grad norms: 6.950913
2019-03-07 15:08:56,916 [INFO] ---------------------------------
2019-03-07 15:09:16,646 [INFO] ---------------------------------
2019-03-07 15:09:16,647 [INFO] Summary:
2019-03-07 15:09:16,647 [INFO] Batch 9000, worst loss 0.366625 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:09:16,648 [INFO] Regularization: 8013.969238 * 0.0000010000 = 0.0080139693
2019-03-07 15:09:16,649 [INFO] Sum of grad norms: 6.745238
2019-03-07 15:09:16,649 [INFO] ---------------------------------
2019-03-07 15:09:36,023 [INFO] ---------------------------------
2019-03-07 15:09:36,024 [INFO] Summary:
2019-03-07 15:09:36,025 [INFO] Batch 10000, worst loss 0.344666 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:09:36,025 [INFO] Regularization: 7766.178711 * 0.0000010000 = 0.0077661788
2019-03-07 15:09:36,026 [INFO] Sum of grad norms: 6.544208
2019-03-07 15:09:36,027 [INFO] ---------------------------------
2019-03-07 15:09:49,054 [INFO] ---------------------------------
2019-03-07 15:09:49,055 [INFO] Evaluation:
2019-03-07 15:09:49,056 [INFO] Batch 10000, worst loss 0.351072 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:09:49,056 [INFO] ---------------------------------
2019-03-07 15:10:08,059 [INFO] ---------------------------------
2019-03-07 15:10:08,060 [INFO] Summary:
2019-03-07 15:10:08,061 [INFO] Batch 11000, worst loss 0.364777 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:10:08,061 [INFO] Regularization: 7546.083008 * 0.0000010000 = 0.0075460831
2019-03-07 15:10:08,062 [INFO] Sum of grad norms: 7.900633
2019-03-07 15:10:08,063 [INFO] ---------------------------------
2019-03-07 15:10:27,525 [INFO] ---------------------------------
2019-03-07 15:10:27,526 [INFO] Summary:
2019-03-07 15:10:27,527 [INFO] Batch 12000, worst loss 0.341593 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:10:27,527 [INFO] Regularization: 7399.004883 * 0.0000010000 = 0.0073990049
2019-03-07 15:10:27,528 [INFO] Sum of grad norms: 2.622640
2019-03-07 15:10:27,529 [INFO] ---------------------------------
2019-03-07 15:10:47,069 [INFO] ---------------------------------
2019-03-07 15:10:47,071 [INFO] Summary:
2019-03-07 15:10:47,071 [INFO] Batch 13000, worst loss 0.403637 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:10:47,072 [INFO] Regularization: 7267.588379 * 0.0000010000 = 0.0072675883
2019-03-07 15:10:47,073 [INFO] Sum of grad norms: 4.597818
2019-03-07 15:10:47,074 [INFO] ---------------------------------
2019-03-07 15:11:06,510 [INFO] ---------------------------------
2019-03-07 15:11:06,511 [INFO] Summary:
2019-03-07 15:11:06,512 [INFO] Batch 14000, worst loss 0.367776 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:11:06,513 [INFO] Regularization: 7096.683105 * 0.0000010000 = 0.0070966831
2019-03-07 15:11:06,514 [INFO] Sum of grad norms: 4.739518
2019-03-07 15:11:06,514 [INFO] ---------------------------------
2019-03-07 15:11:26,000 [INFO] ---------------------------------
2019-03-07 15:11:26,002 [INFO] Summary:
2019-03-07 15:11:26,002 [INFO] Batch 15000, worst loss 0.287681 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:11:26,004 [INFO] Regularization: 7028.671387 * 0.0000010000 = 0.0070286714
2019-03-07 15:11:26,005 [INFO] Sum of grad norms: 2.493269
2019-03-07 15:11:26,006 [INFO] ---------------------------------
2019-03-07 15:11:45,569 [INFO] ---------------------------------
2019-03-07 15:11:45,570 [INFO] Summary:
2019-03-07 15:11:45,570 [INFO] Batch 16000, worst loss 0.320870 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:11:45,571 [INFO] Regularization: 6928.375000 * 0.0000010000 = 0.0069283750
2019-03-07 15:11:45,573 [INFO] Sum of grad norms: 3.610569
2019-03-07 15:11:45,574 [INFO] ---------------------------------
2019-03-07 15:12:05,014 [INFO] ---------------------------------
2019-03-07 15:12:05,015 [INFO] Summary:
2019-03-07 15:12:05,015 [INFO] Batch 17000, worst loss 0.322961 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:12:05,016 [INFO] Regularization: 6916.389648 * 0.0000010000 = 0.0069163898
2019-03-07 15:12:05,017 [INFO] Sum of grad norms: 6.547778
2019-03-07 15:12:05,017 [INFO] ---------------------------------
2019-03-07 15:12:24,342 [INFO] ---------------------------------
2019-03-07 15:12:24,343 [INFO] Summary:
2019-03-07 15:12:24,346 [INFO] Batch 18000, worst loss 0.290347 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:12:24,348 [INFO] Regularization: 6901.066406 * 0.0000010000 = 0.0069010663
2019-03-07 15:12:24,350 [INFO] Sum of grad norms: 2.968138
2019-03-07 15:12:24,352 [INFO] ---------------------------------
2019-03-07 15:12:43,781 [INFO] ---------------------------------
2019-03-07 15:12:43,782 [INFO] Summary:
2019-03-07 15:12:43,782 [INFO] Batch 19000, worst loss 0.405835 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:12:43,784 [INFO] Regularization: 6751.683594 * 0.0000010000 = 0.0067516835
2019-03-07 15:12:43,784 [INFO] Sum of grad norms: 4.556836
2019-03-07 15:12:43,785 [INFO] ---------------------------------
2019-03-07 15:13:03,239 [INFO] ---------------------------------
2019-03-07 15:13:03,240 [INFO] Summary:
2019-03-07 15:13:03,241 [INFO] Batch 20000, worst loss 0.324147 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:13:03,241 [INFO] Regularization: 6554.382324 * 0.0000010000 = 0.0065543824
2019-03-07 15:13:03,242 [INFO] Sum of grad norms: 5.724747
2019-03-07 15:13:03,243 [INFO] ---------------------------------
2019-03-07 15:13:16,400 [INFO] ---------------------------------
2019-03-07 15:13:16,402 [INFO] Evaluation:
2019-03-07 15:13:16,403 [INFO] Batch 20000, worst loss 0.419333 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:13:16,404 [INFO] ---------------------------------
2019-03-07 15:13:36,205 [INFO] ---------------------------------
2019-03-07 15:13:36,206 [INFO] Summary:
2019-03-07 15:13:36,207 [INFO] Batch 21000, worst loss 0.410382 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:13:36,207 [INFO] Regularization: 6499.251465 * 0.0000010000 = 0.0064992514
2019-03-07 15:13:36,208 [INFO] Sum of grad norms: 1.127659
2019-03-07 15:13:36,209 [INFO] ---------------------------------
2019-03-07 15:13:55,661 [INFO] ---------------------------------
2019-03-07 15:13:55,662 [INFO] Summary:
2019-03-07 15:13:55,663 [INFO] Batch 22000, worst loss 0.324404 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:13:55,663 [INFO] Regularization: 6509.924316 * 0.0000010000 = 0.0065099243
2019-03-07 15:13:55,664 [INFO] Sum of grad norms: 4.354227
2019-03-07 15:13:55,665 [INFO] ---------------------------------
2019-03-07 15:14:15,287 [INFO] ---------------------------------
2019-03-07 15:14:15,288 [INFO] Summary:
2019-03-07 15:14:15,288 [INFO] Batch 23000, worst loss 0.415409 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:14:15,289 [INFO] Regularization: 6363.156250 * 0.0000010000 = 0.0063631563
2019-03-07 15:14:15,289 [INFO] Sum of grad norms: 3.023151
2019-03-07 15:14:15,290 [INFO] ---------------------------------
2019-03-07 15:14:34,322 [INFO] ---------------------------------
2019-03-07 15:14:34,323 [INFO] Summary:
2019-03-07 15:14:34,323 [INFO] Batch 24000, worst loss 0.251316 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:14:34,324 [INFO] Regularization: 6292.383789 * 0.0000010000 = 0.0062923837
2019-03-07 15:14:34,325 [INFO] Sum of grad norms: 4.458967
2019-03-07 15:14:34,326 [INFO] ---------------------------------
2019-03-07 15:14:53,885 [INFO] ---------------------------------
2019-03-07 15:14:53,886 [INFO] Summary:
2019-03-07 15:14:53,887 [INFO] Batch 25000, worst loss 0.373639 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:14:53,888 [INFO] Regularization: 6242.838379 * 0.0000010000 = 0.0062428382
2019-03-07 15:14:53,889 [INFO] Sum of grad norms: 7.714880
2019-03-07 15:14:53,890 [INFO] ---------------------------------
2019-03-07 15:15:13,353 [INFO] ---------------------------------
2019-03-07 15:15:13,354 [INFO] Summary:
2019-03-07 15:15:13,355 [INFO] Batch 26000, worst loss 0.352093 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:15:13,356 [INFO] Regularization: 6382.509766 * 0.0000010000 = 0.0063825096
2019-03-07 15:15:13,357 [INFO] Sum of grad norms: 1.632072
2019-03-07 15:15:13,357 [INFO] ---------------------------------
2019-03-07 15:15:33,021 [INFO] ---------------------------------
2019-03-07 15:15:33,022 [INFO] Summary:
2019-03-07 15:15:33,023 [INFO] Batch 27000, worst loss 0.347853 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:15:33,023 [INFO] Regularization: 6213.838867 * 0.0000010000 = 0.0062138387
2019-03-07 15:15:33,024 [INFO] Sum of grad norms: 6.473311
2019-03-07 15:15:33,024 [INFO] ---------------------------------
2019-03-07 15:15:52,433 [INFO] ---------------------------------
2019-03-07 15:15:52,434 [INFO] Summary:
2019-03-07 15:15:52,434 [INFO] Batch 28000, worst loss 0.298985 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:15:52,435 [INFO] Regularization: 6114.661621 * 0.0000010000 = 0.0061146617
2019-03-07 15:15:52,436 [INFO] Sum of grad norms: 1.380997
2019-03-07 15:15:52,437 [INFO] ---------------------------------
2019-03-07 15:16:11,760 [INFO] ---------------------------------
2019-03-07 15:16:11,761 [INFO] Summary:
2019-03-07 15:16:11,762 [INFO] Batch 29000, worst loss 0.259577 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:16:11,771 [INFO] Regularization: 6062.814941 * 0.0000010000 = 0.0060628150
2019-03-07 15:16:11,772 [INFO] Sum of grad norms: 0.961277
2019-03-07 15:16:11,774 [INFO] ---------------------------------
2019-03-07 15:16:31,165 [INFO] ---------------------------------
2019-03-07 15:16:31,166 [INFO] Summary:
2019-03-07 15:16:31,167 [INFO] Batch 30000, worst loss 0.344170 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:16:31,167 [INFO] Regularization: 6001.484863 * 0.0000010000 = 0.0060014850
2019-03-07 15:16:31,168 [INFO] Sum of grad norms: 9.380944
2019-03-07 15:16:31,169 [INFO] ---------------------------------
2019-03-07 15:16:44,215 [INFO] ---------------------------------
2019-03-07 15:16:44,216 [INFO] Evaluation:
2019-03-07 15:16:44,216 [INFO] Batch 30000, worst loss 0.224252 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:16:44,217 [INFO] ---------------------------------
2019-03-07 15:17:03,887 [INFO] ---------------------------------
2019-03-07 15:17:03,888 [INFO] Summary:
2019-03-07 15:17:03,888 [INFO] Batch 31000, worst loss 0.335998 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:17:03,889 [INFO] Regularization: 5944.941895 * 0.0000010000 = 0.0059449417
2019-03-07 15:17:03,890 [INFO] Sum of grad norms: 4.105677
2019-03-07 15:17:03,891 [INFO] ---------------------------------
2019-03-07 15:17:23,716 [INFO] ---------------------------------
2019-03-07 15:17:23,717 [INFO] Summary:
2019-03-07 15:17:23,718 [INFO] Batch 32000, worst loss 0.447728 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:17:23,719 [INFO] Regularization: 5894.757812 * 0.0000010000 = 0.0058947578
2019-03-07 15:17:23,720 [INFO] Sum of grad norms: 5.091332
2019-03-07 15:17:23,720 [INFO] ---------------------------------
2019-03-07 15:17:43,063 [INFO] ---------------------------------
2019-03-07 15:17:43,064 [INFO] Summary:
2019-03-07 15:17:43,068 [INFO] Batch 33000, worst loss 0.272323 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:17:43,070 [INFO] Regularization: 5856.982422 * 0.0000010000 = 0.0058569824
2019-03-07 15:17:43,071 [INFO] Sum of grad norms: 2.983229
2019-03-07 15:17:43,072 [INFO] ---------------------------------
2019-03-07 15:18:02,571 [INFO] ---------------------------------
2019-03-07 15:18:02,572 [INFO] Summary:
2019-03-07 15:18:02,573 [INFO] Batch 34000, worst loss 0.396213 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:18:02,573 [INFO] Regularization: 5865.784180 * 0.0000010000 = 0.0058657844
2019-03-07 15:18:02,574 [INFO] Sum of grad norms: 1.663962
2019-03-07 15:18:02,575 [INFO] ---------------------------------
2019-03-07 15:18:21,939 [INFO] ---------------------------------
2019-03-07 15:18:21,940 [INFO] Summary:
2019-03-07 15:18:21,941 [INFO] Batch 35000, worst loss 0.291335 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:18:21,941 [INFO] Regularization: 5804.866211 * 0.0000010000 = 0.0058048661
2019-03-07 15:18:21,942 [INFO] Sum of grad norms: 7.376687
2019-03-07 15:18:21,943 [INFO] ---------------------------------
2019-03-07 15:18:41,429 [INFO] ---------------------------------
2019-03-07 15:18:41,430 [INFO] Summary:
2019-03-07 15:18:41,430 [INFO] Batch 36000, worst loss 0.291670 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:18:41,431 [INFO] Regularization: 5824.002930 * 0.0000010000 = 0.0058240029
2019-03-07 15:18:41,431 [INFO] Sum of grad norms: 8.715918
2019-03-07 15:18:41,432 [INFO] ---------------------------------
2019-03-07 15:19:00,531 [INFO] ---------------------------------
2019-03-07 15:19:00,532 [INFO] Summary:
2019-03-07 15:19:00,532 [INFO] Batch 37000, worst loss 0.278819 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:19:00,533 [INFO] Regularization: 5843.686523 * 0.0000010000 = 0.0058436864
2019-03-07 15:19:00,534 [INFO] Sum of grad norms: 1.979177
2019-03-07 15:19:00,534 [INFO] ---------------------------------
2019-03-07 15:19:19,893 [INFO] ---------------------------------
2019-03-07 15:19:19,894 [INFO] Summary:
2019-03-07 15:19:19,895 [INFO] Batch 38000, worst loss 0.321283 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:19:19,896 [INFO] Regularization: 5726.197266 * 0.0000010000 = 0.0057261973
2019-03-07 15:19:19,896 [INFO] Sum of grad norms: 2.583901
2019-03-07 15:19:19,897 [INFO] ---------------------------------
2019-03-07 15:19:39,372 [INFO] ---------------------------------
2019-03-07 15:19:39,373 [INFO] Summary:
2019-03-07 15:19:39,373 [INFO] Batch 39000, worst loss 0.382171 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:19:39,374 [INFO] Regularization: 5735.187988 * 0.0000010000 = 0.0057351878
2019-03-07 15:19:39,374 [INFO] Sum of grad norms: 3.796478
2019-03-07 15:19:39,375 [INFO] ---------------------------------
2019-03-07 15:19:58,834 [INFO] ---------------------------------
2019-03-07 15:19:58,835 [INFO] Summary:
2019-03-07 15:19:58,835 [INFO] Batch 40000, worst loss 0.308572 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:19:58,836 [INFO] Regularization: 5693.929688 * 0.0000010000 = 0.0056939297
2019-03-07 15:19:58,837 [INFO] Sum of grad norms: 6.857371
2019-03-07 15:19:58,837 [INFO] ---------------------------------
2019-03-07 15:20:11,904 [INFO] ---------------------------------
2019-03-07 15:20:11,905 [INFO] Evaluation:
2019-03-07 15:20:11,905 [INFO] Batch 40000, worst loss 0.213640 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 15:20:11,908 [INFO] ---------------------------------
2019-03-07 15:20:31,136 [INFO] ---------------------------------
2019-03-07 15:20:31,138 [INFO] Summary:
2019-03-07 15:20:31,138 [INFO] Batch 41000, worst loss 0.307631 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:20:31,139 [INFO] Regularization: 5639.186523 * 0.0000010000 = 0.0056391866
2019-03-07 15:20:31,140 [INFO] Sum of grad norms: 0.855487
2019-03-07 15:20:31,141 [INFO] ---------------------------------
2019-03-07 15:20:50,616 [INFO] ---------------------------------
2019-03-07 15:20:50,617 [INFO] Summary:
2019-03-07 15:20:50,617 [INFO] Batch 42000, worst loss 0.250058 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:20:50,618 [INFO] Regularization: 5566.660156 * 0.0000010000 = 0.0055666603
2019-03-07 15:20:50,618 [INFO] Sum of grad norms: 2.343553
2019-03-07 15:20:50,619 [INFO] ---------------------------------
2019-03-07 15:21:10,209 [INFO] ---------------------------------
2019-03-07 15:21:10,209 [INFO] Summary:
2019-03-07 15:21:10,210 [INFO] Batch 43000, worst loss 0.192421 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:21:10,211 [INFO] Regularization: 5552.250000 * 0.0000010000 = 0.0055522500
2019-03-07 15:21:10,211 [INFO] Sum of grad norms: 0.403519
2019-03-07 15:21:10,212 [INFO] ---------------------------------
2019-03-07 15:21:29,477 [INFO] ---------------------------------
2019-03-07 15:21:29,478 [INFO] Summary:
2019-03-07 15:21:29,479 [INFO] Batch 44000, worst loss 0.225653 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:21:29,479 [INFO] Regularization: 5568.896484 * 0.0000010000 = 0.0055688964
2019-03-07 15:21:29,480 [INFO] Sum of grad norms: 6.494428
2019-03-07 15:21:29,481 [INFO] ---------------------------------
2019-03-07 15:21:48,593 [INFO] ---------------------------------
2019-03-07 15:21:48,594 [INFO] Summary:
2019-03-07 15:21:48,594 [INFO] Batch 45000, worst loss 0.272314 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:21:48,595 [INFO] Regularization: 5543.041992 * 0.0000010000 = 0.0055430420
2019-03-07 15:21:48,596 [INFO] Sum of grad norms: 0.693629
2019-03-07 15:21:48,597 [INFO] ---------------------------------
2019-03-07 15:22:07,805 [INFO] ---------------------------------
2019-03-07 15:22:07,806 [INFO] Summary:
2019-03-07 15:22:07,807 [INFO] Batch 46000, worst loss 0.246562 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:22:07,808 [INFO] Regularization: 5528.152344 * 0.0000010000 = 0.0055281525
2019-03-07 15:22:07,808 [INFO] Sum of grad norms: 3.088400
2019-03-07 15:22:07,809 [INFO] ---------------------------------
2019-03-07 15:22:27,460 [INFO] ---------------------------------
2019-03-07 15:22:27,461 [INFO] Summary:
2019-03-07 15:22:27,461 [INFO] Batch 47000, worst loss 0.253567 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:22:27,462 [INFO] Regularization: 5536.482422 * 0.0000010000 = 0.0055364822
2019-03-07 15:22:27,463 [INFO] Sum of grad norms: 3.782728
2019-03-07 15:22:27,464 [INFO] ---------------------------------
2019-03-07 15:22:46,628 [INFO] ---------------------------------
2019-03-07 15:22:46,629 [INFO] Summary:
2019-03-07 15:22:46,630 [INFO] Batch 48000, worst loss 0.213338 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:22:46,630 [INFO] Regularization: 5543.182129 * 0.0000010000 = 0.0055431821
2019-03-07 15:22:46,631 [INFO] Sum of grad norms: 9.674359
2019-03-07 15:22:46,632 [INFO] ---------------------------------
2019-03-07 15:23:05,634 [INFO] ---------------------------------
2019-03-07 15:23:05,635 [INFO] Summary:
2019-03-07 15:23:05,636 [INFO] Batch 49000, worst loss 0.232069 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:23:05,637 [INFO] Regularization: 5517.588379 * 0.0000010000 = 0.0055175885
2019-03-07 15:23:05,637 [INFO] Sum of grad norms: 9.075434
2019-03-07 15:23:05,638 [INFO] ---------------------------------
2019-03-07 15:23:25,310 [INFO] ---------------------------------
2019-03-07 15:23:25,311 [INFO] Summary:
2019-03-07 15:23:25,311 [INFO] Batch 50000, worst loss 0.277680 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:23:25,312 [INFO] Regularization: 5532.440918 * 0.0000010000 = 0.0055324407
2019-03-07 15:23:25,313 [INFO] Sum of grad norms: 3.470219
2019-03-07 15:23:25,314 [INFO] ---------------------------------
2019-03-07 15:23:38,349 [INFO] ---------------------------------
2019-03-07 15:23:38,350 [INFO] Evaluation:
2019-03-07 15:23:38,351 [INFO] Batch 50000, worst loss 0.198947 (without reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 15:23:38,351 [INFO] ---------------------------------
2019-03-07 15:23:57,972 [INFO] ---------------------------------
2019-03-07 15:23:57,973 [INFO] Summary:
2019-03-07 15:23:57,974 [INFO] Batch 51000, worst loss 0.235867 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:23:57,974 [INFO] Regularization: 5512.899902 * 0.0000010000 = 0.0055128997
2019-03-07 15:23:57,975 [INFO] Sum of grad norms: 8.515316
2019-03-07 15:23:57,976 [INFO] ---------------------------------
2019-03-07 15:24:17,259 [INFO] ---------------------------------
2019-03-07 15:24:17,261 [INFO] Summary:
2019-03-07 15:24:17,261 [INFO] Batch 52000, worst loss 0.198647 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:24:17,262 [INFO] Regularization: 5482.695312 * 0.0000010000 = 0.0054826955
2019-03-07 15:24:17,263 [INFO] Sum of grad norms: 2.649615
2019-03-07 15:24:17,264 [INFO] ---------------------------------
2019-03-07 15:24:36,555 [INFO] ---------------------------------
2019-03-07 15:24:36,556 [INFO] Summary:
2019-03-07 15:24:36,556 [INFO] Batch 53000, worst loss 0.256043 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:24:36,557 [INFO] Regularization: 5482.583984 * 0.0000010000 = 0.0054825838
2019-03-07 15:24:36,557 [INFO] Sum of grad norms: 3.901890
2019-03-07 15:24:36,558 [INFO] ---------------------------------
2019-03-07 15:24:56,046 [INFO] ---------------------------------
2019-03-07 15:24:56,047 [INFO] Summary:
2019-03-07 15:24:56,047 [INFO] Batch 54000, worst loss 0.229009 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:24:56,048 [INFO] Regularization: 5488.368652 * 0.0000010000 = 0.0054883687
2019-03-07 15:24:56,049 [INFO] Sum of grad norms: 16.780193
2019-03-07 15:24:56,049 [INFO] ---------------------------------
2019-03-07 15:25:15,490 [INFO] ---------------------------------
2019-03-07 15:25:15,491 [INFO] Summary:
2019-03-07 15:25:15,491 [INFO] Batch 55000, worst loss 0.202736 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:25:15,492 [INFO] Regularization: 5476.497070 * 0.0000010000 = 0.0054764971
2019-03-07 15:25:15,493 [INFO] Sum of grad norms: 6.012243
2019-03-07 15:25:15,494 [INFO] ---------------------------------
2019-03-07 15:25:35,075 [INFO] ---------------------------------
2019-03-07 15:25:35,076 [INFO] Summary:
2019-03-07 15:25:35,077 [INFO] Batch 56000, worst loss 0.223771 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:25:35,078 [INFO] Regularization: 5472.935059 * 0.0000010000 = 0.0054729353
2019-03-07 15:25:35,079 [INFO] Sum of grad norms: 1.169675
2019-03-07 15:25:35,080 [INFO] ---------------------------------
2019-03-07 15:25:54,486 [INFO] ---------------------------------
2019-03-07 15:25:54,487 [INFO] Summary:
2019-03-07 15:25:54,488 [INFO] Batch 57000, worst loss 0.190545 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:25:54,489 [INFO] Regularization: 5475.281250 * 0.0000010000 = 0.0054752813
2019-03-07 15:25:54,489 [INFO] Sum of grad norms: 1.148649
2019-03-07 15:25:54,490 [INFO] ---------------------------------
2019-03-07 15:26:14,001 [INFO] ---------------------------------
2019-03-07 15:26:14,002 [INFO] Summary:
2019-03-07 15:26:14,003 [INFO] Batch 58000, worst loss 0.226123 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:26:14,003 [INFO] Regularization: 5474.874023 * 0.0000010000 = 0.0054748738
2019-03-07 15:26:14,004 [INFO] Sum of grad norms: 9.249681
2019-03-07 15:26:14,005 [INFO] ---------------------------------
2019-03-07 15:26:33,045 [INFO] ---------------------------------
2019-03-07 15:26:33,046 [INFO] Summary:
2019-03-07 15:26:33,047 [INFO] Batch 59000, worst loss 0.225506 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:26:33,048 [INFO] Regularization: 5475.189453 * 0.0000010000 = 0.0054751895
2019-03-07 15:26:33,049 [INFO] Sum of grad norms: 9.522811
2019-03-07 15:26:33,049 [INFO] ---------------------------------
2019-03-07 15:26:52,676 [INFO] ---------------------------------
2019-03-07 15:26:52,677 [INFO] Summary:
2019-03-07 15:26:52,678 [INFO] Batch 60000, worst loss 0.217391 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:26:52,678 [INFO] Regularization: 5469.769043 * 0.0000010000 = 0.0054697692
2019-03-07 15:26:52,679 [INFO] Sum of grad norms: 0.423886
2019-03-07 15:26:52,680 [INFO] ---------------------------------
2019-03-07 15:27:05,745 [INFO] ---------------------------------
2019-03-07 15:27:05,746 [INFO] Evaluation:
2019-03-07 15:27:05,747 [INFO] Batch 60000, worst loss 0.183892 (without reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 15:27:05,748 [INFO] ---------------------------------
2019-03-07 15:27:25,068 [INFO] ---------------------------------
2019-03-07 15:27:25,069 [INFO] Summary:
2019-03-07 15:27:25,070 [INFO] Batch 61000, worst loss 0.202244 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:27:25,071 [INFO] Regularization: 5474.978516 * 0.0000010000 = 0.0054749786
2019-03-07 15:27:25,072 [INFO] Sum of grad norms: 6.909431
2019-03-07 15:27:25,073 [INFO] ---------------------------------
2019-03-07 15:27:44,117 [INFO] ---------------------------------
2019-03-07 15:27:44,118 [INFO] Summary:
2019-03-07 15:27:44,119 [INFO] Batch 62000, worst loss 0.199801 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:27:44,120 [INFO] Regularization: 5460.970703 * 0.0000010000 = 0.0054609706
2019-03-07 15:27:44,122 [INFO] Sum of grad norms: 12.596704
2019-03-07 15:27:44,123 [INFO] ---------------------------------
2019-03-07 15:28:03,545 [INFO] ---------------------------------
2019-03-07 15:28:03,546 [INFO] Summary:
2019-03-07 15:28:03,547 [INFO] Batch 63000, worst loss 0.220471 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:28:03,547 [INFO] Regularization: 5460.909180 * 0.0000010000 = 0.0054609091
2019-03-07 15:28:03,548 [INFO] Sum of grad norms: 0.141093
2019-03-07 15:28:03,549 [INFO] ---------------------------------
2019-03-07 15:28:22,478 [INFO] ---------------------------------
2019-03-07 15:28:22,479 [INFO] Summary:
2019-03-07 15:28:22,480 [INFO] Batch 64000, worst loss 0.183971 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:28:22,480 [INFO] Regularization: 5458.297363 * 0.0000010000 = 0.0054582972
2019-03-07 15:28:22,481 [INFO] Sum of grad norms: 0.178746
2019-03-07 15:28:22,482 [INFO] ---------------------------------
2019-03-07 15:28:41,518 [INFO] ---------------------------------
2019-03-07 15:28:41,519 [INFO] Summary:
2019-03-07 15:28:41,520 [INFO] Batch 65000, worst loss 0.142151 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:28:41,521 [INFO] Regularization: 5459.345215 * 0.0000010000 = 0.0054593454
2019-03-07 15:28:41,522 [INFO] Sum of grad norms: 5.230776
2019-03-07 15:28:41,523 [INFO] ---------------------------------
2019-03-07 15:29:00,988 [INFO] ---------------------------------
2019-03-07 15:29:00,989 [INFO] Summary:
2019-03-07 15:29:00,990 [INFO] Batch 66000, worst loss 0.235845 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:29:00,990 [INFO] Regularization: 5457.713867 * 0.0000010000 = 0.0054577137
2019-03-07 15:29:00,991 [INFO] Sum of grad norms: 10.764049
2019-03-07 15:29:00,992 [INFO] ---------------------------------
2019-03-07 15:29:20,298 [INFO] ---------------------------------
2019-03-07 15:29:20,299 [INFO] Summary:
2019-03-07 15:29:20,299 [INFO] Batch 67000, worst loss 0.174581 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:29:20,300 [INFO] Regularization: 5456.393555 * 0.0000010000 = 0.0054563936
2019-03-07 15:29:20,300 [INFO] Sum of grad norms: 2.834612
2019-03-07 15:29:20,301 [INFO] ---------------------------------
2019-03-07 15:29:39,633 [INFO] ---------------------------------
2019-03-07 15:29:39,634 [INFO] Summary:
2019-03-07 15:29:39,634 [INFO] Batch 68000, worst loss 0.211648 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:29:39,636 [INFO] Regularization: 5453.834961 * 0.0000010000 = 0.0054538348
2019-03-07 15:29:39,636 [INFO] Sum of grad norms: 0.817902
2019-03-07 15:29:39,637 [INFO] ---------------------------------
2019-03-07 15:29:58,906 [INFO] ---------------------------------
2019-03-07 15:29:58,907 [INFO] Summary:
2019-03-07 15:29:58,908 [INFO] Batch 69000, worst loss 0.203471 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:29:58,909 [INFO] Regularization: 5454.365723 * 0.0000010000 = 0.0054543656
2019-03-07 15:29:58,910 [INFO] Sum of grad norms: 8.001717
2019-03-07 15:29:58,910 [INFO] ---------------------------------
2019-03-07 15:30:18,281 [INFO] ---------------------------------
2019-03-07 15:30:18,282 [INFO] Summary:
2019-03-07 15:30:18,282 [INFO] Batch 70000, worst loss 0.173978 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:30:18,283 [INFO] Regularization: 5454.332031 * 0.0000010000 = 0.0054543321
2019-03-07 15:30:18,284 [INFO] Sum of grad norms: 6.277133
2019-03-07 15:30:18,285 [INFO] ---------------------------------
2019-03-07 15:30:31,376 [INFO] ---------------------------------
2019-03-07 15:30:31,376 [INFO] Evaluation:
2019-03-07 15:30:31,377 [INFO] Batch 70000, worst loss 0.241762 (without reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 15:30:31,379 [INFO] ---------------------------------
2019-03-07 15:30:50,666 [INFO] ---------------------------------
2019-03-07 15:30:50,668 [INFO] Summary:
2019-03-07 15:30:50,668 [INFO] Batch 71000, worst loss 0.200254 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:30:50,669 [INFO] Regularization: 5455.166504 * 0.0000010000 = 0.0054551666
2019-03-07 15:30:50,670 [INFO] Sum of grad norms: 1.734876
2019-03-07 15:30:50,671 [INFO] ---------------------------------
2019-03-07 15:31:09,719 [INFO] ---------------------------------
2019-03-07 15:31:09,720 [INFO] Summary:
2019-03-07 15:31:09,722 [INFO] Batch 72000, worst loss 0.209335 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:31:09,723 [INFO] Regularization: 5451.984863 * 0.0000010000 = 0.0054519847
2019-03-07 15:31:09,724 [INFO] Sum of grad norms: 0.859323
2019-03-07 15:31:09,725 [INFO] ---------------------------------
2019-03-07 15:31:29,175 [INFO] ---------------------------------
2019-03-07 15:31:29,176 [INFO] Summary:
2019-03-07 15:31:29,177 [INFO] Batch 73000, worst loss 0.199359 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:31:29,177 [INFO] Regularization: 5451.550293 * 0.0000010000 = 0.0054515502
2019-03-07 15:31:29,178 [INFO] Sum of grad norms: 5.573720
2019-03-07 15:31:29,179 [INFO] ---------------------------------
2019-03-07 15:31:48,846 [INFO] ---------------------------------
2019-03-07 15:31:48,847 [INFO] Summary:
2019-03-07 15:31:48,847 [INFO] Batch 74000, worst loss 0.161967 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:31:48,848 [INFO] Regularization: 5451.008301 * 0.0000010000 = 0.0054510082
2019-03-07 15:31:48,849 [INFO] Sum of grad norms: 4.297958
2019-03-07 15:31:48,849 [INFO] ---------------------------------
2019-03-07 15:32:08,320 [INFO] ---------------------------------
2019-03-07 15:32:08,321 [INFO] Summary:
2019-03-07 15:32:08,321 [INFO] Batch 75000, worst loss 0.157655 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:32:08,322 [INFO] Regularization: 5450.879395 * 0.0000010000 = 0.0054508792
2019-03-07 15:32:08,323 [INFO] Sum of grad norms: 0.283468
2019-03-07 15:32:08,323 [INFO] ---------------------------------
2019-03-07 15:32:27,825 [INFO] ---------------------------------
2019-03-07 15:32:27,826 [INFO] Summary:
2019-03-07 15:32:27,827 [INFO] Batch 76000, worst loss 0.191375 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:32:27,828 [INFO] Regularization: 5450.539551 * 0.0000010000 = 0.0054505398
2019-03-07 15:32:27,829 [INFO] Sum of grad norms: 6.819873
2019-03-07 15:32:27,830 [INFO] ---------------------------------
2019-03-07 15:32:47,219 [INFO] ---------------------------------
2019-03-07 15:32:47,220 [INFO] Summary:
2019-03-07 15:32:47,221 [INFO] Batch 77000, worst loss 0.195113 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:32:47,221 [INFO] Regularization: 5450.223633 * 0.0000010000 = 0.0054502236
2019-03-07 15:32:47,222 [INFO] Sum of grad norms: 1.314583
2019-03-07 15:32:47,223 [INFO] ---------------------------------
2019-03-07 15:33:06,580 [INFO] ---------------------------------
2019-03-07 15:33:06,581 [INFO] Summary:
2019-03-07 15:33:06,582 [INFO] Batch 78000, worst loss 0.189107 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:33:06,583 [INFO] Regularization: 5449.639648 * 0.0000010000 = 0.0054496396
2019-03-07 15:33:06,584 [INFO] Sum of grad norms: 3.891174
2019-03-07 15:33:06,584 [INFO] ---------------------------------
2019-03-07 15:33:25,585 [INFO] ---------------------------------
2019-03-07 15:33:25,586 [INFO] Summary:
2019-03-07 15:33:25,587 [INFO] Batch 79000, worst loss 0.201849 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:33:25,588 [INFO] Regularization: 5449.790527 * 0.0000010000 = 0.0054497905
2019-03-07 15:33:25,589 [INFO] Sum of grad norms: 5.203301
2019-03-07 15:33:25,590 [INFO] ---------------------------------
2019-03-07 15:33:44,984 [INFO] ---------------------------------
2019-03-07 15:33:44,985 [INFO] Summary:
2019-03-07 15:33:44,986 [INFO] Batch 80000, worst loss 0.179739 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:33:44,990 [INFO] Regularization: 5449.411133 * 0.0000010000 = 0.0054494110
2019-03-07 15:33:44,991 [INFO] Sum of grad norms: 0.344665
2019-03-07 15:33:44,992 [INFO] ---------------------------------
2019-03-07 15:33:58,073 [INFO] ---------------------------------
2019-03-07 15:33:58,074 [INFO] Evaluation:
2019-03-07 15:33:58,075 [INFO] Batch 80000, worst loss 0.148429 (without reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 15:33:58,077 [INFO] ---------------------------------
2019-03-07 15:34:17,371 [INFO] ---------------------------------
2019-03-07 15:34:17,372 [INFO] Summary:
2019-03-07 15:34:17,373 [INFO] Batch 81000, worst loss 0.170909 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:34:17,373 [INFO] Regularization: 5448.947754 * 0.0000010000 = 0.0054489477
2019-03-07 15:34:17,374 [INFO] Sum of grad norms: 0.260572
2019-03-07 15:34:17,376 [INFO] ---------------------------------
2019-03-07 15:34:36,577 [INFO] ---------------------------------
2019-03-07 15:34:36,578 [INFO] Summary:
2019-03-07 15:34:36,581 [INFO] Batch 82000, worst loss 0.181127 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:34:36,582 [INFO] Regularization: 5448.738281 * 0.0000010000 = 0.0054487381
2019-03-07 15:34:36,583 [INFO] Sum of grad norms: 8.099627
2019-03-07 15:34:36,584 [INFO] ---------------------------------
2019-03-07 15:34:55,256 [INFO] ---------------------------------
2019-03-07 15:34:55,258 [INFO] Summary:
2019-03-07 15:34:55,260 [INFO] Batch 83000, worst loss 0.203219 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:34:55,261 [INFO] Regularization: 5448.544922 * 0.0000010000 = 0.0054485449
2019-03-07 15:34:55,263 [INFO] Sum of grad norms: 9.070232
2019-03-07 15:34:55,264 [INFO] ---------------------------------
2019-03-07 15:35:14,503 [INFO] ---------------------------------
2019-03-07 15:35:14,504 [INFO] Summary:
2019-03-07 15:35:14,505 [INFO] Batch 84000, worst loss 0.195117 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:35:14,506 [INFO] Regularization: 5448.457031 * 0.0000010000 = 0.0054484569
2019-03-07 15:35:14,507 [INFO] Sum of grad norms: 7.282367
2019-03-07 15:35:14,508 [INFO] ---------------------------------
2019-03-07 15:35:33,761 [INFO] ---------------------------------
2019-03-07 15:35:33,762 [INFO] Summary:
2019-03-07 15:35:33,762 [INFO] Batch 85000, worst loss 0.174832 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:35:33,763 [INFO] Regularization: 5448.246094 * 0.0000010000 = 0.0054482459
2019-03-07 15:35:33,764 [INFO] Sum of grad norms: 0.170752
2019-03-07 15:35:33,765 [INFO] ---------------------------------
2019-03-07 15:35:53,089 [INFO] ---------------------------------
2019-03-07 15:35:53,090 [INFO] Summary:
2019-03-07 15:35:53,090 [INFO] Batch 86000, worst loss 0.175694 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:35:53,091 [INFO] Regularization: 5448.117676 * 0.0000010000 = 0.0054481179
2019-03-07 15:35:53,092 [INFO] Sum of grad norms: 0.793885
2019-03-07 15:35:53,093 [INFO] ---------------------------------
2019-03-07 15:36:12,246 [INFO] ---------------------------------
2019-03-07 15:36:12,247 [INFO] Summary:
2019-03-07 15:36:12,247 [INFO] Batch 87000, worst loss 0.234292 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:36:12,248 [INFO] Regularization: 5447.996582 * 0.0000010000 = 0.0054479968
2019-03-07 15:36:12,249 [INFO] Sum of grad norms: 10.411602
2019-03-07 15:36:12,249 [INFO] ---------------------------------
2019-03-07 15:36:31,274 [INFO] ---------------------------------
2019-03-07 15:36:31,275 [INFO] Summary:
2019-03-07 15:36:31,275 [INFO] Batch 88000, worst loss 0.185471 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:36:31,276 [INFO] Regularization: 5447.932129 * 0.0000010000 = 0.0054479321
2019-03-07 15:36:31,279 [INFO] Sum of grad norms: 7.957074
2019-03-07 15:36:31,280 [INFO] ---------------------------------
2019-03-07 15:36:50,499 [INFO] ---------------------------------
2019-03-07 15:36:50,500 [INFO] Summary:
2019-03-07 15:36:50,500 [INFO] Batch 89000, worst loss 0.195691 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:36:50,501 [INFO] Regularization: 5447.772461 * 0.0000010000 = 0.0054477723
2019-03-07 15:36:50,502 [INFO] Sum of grad norms: 15.345996
2019-03-07 15:36:50,503 [INFO] ---------------------------------
2019-03-07 15:37:09,861 [INFO] ---------------------------------
2019-03-07 15:37:09,862 [INFO] Summary:
2019-03-07 15:37:09,863 [INFO] Batch 90000, worst loss 0.256359 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:37:09,864 [INFO] Regularization: 5447.783203 * 0.0000010000 = 0.0054477830
2019-03-07 15:37:09,865 [INFO] Sum of grad norms: 5.907465
2019-03-07 15:37:09,866 [INFO] ---------------------------------
2019-03-07 15:37:22,921 [INFO] ---------------------------------
2019-03-07 15:37:22,923 [INFO] Evaluation:
2019-03-07 15:37:22,926 [INFO] Batch 90000, worst loss 0.172917 (without reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 15:37:22,927 [INFO] ---------------------------------
2019-03-07 15:37:42,592 [INFO] ---------------------------------
2019-03-07 15:37:42,593 [INFO] Summary:
2019-03-07 15:37:42,594 [INFO] Batch 91000, worst loss 0.178181 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:37:42,595 [INFO] Regularization: 5447.697754 * 0.0000010000 = 0.0054476978
2019-03-07 15:37:42,596 [INFO] Sum of grad norms: 0.507352
2019-03-07 15:37:42,596 [INFO] ---------------------------------
2019-03-07 15:38:01,960 [INFO] ---------------------------------
2019-03-07 15:38:01,961 [INFO] Summary:
2019-03-07 15:38:01,961 [INFO] Batch 92000, worst loss 0.266229 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:38:01,962 [INFO] Regularization: 5447.626465 * 0.0000010000 = 0.0054476266
2019-03-07 15:38:01,963 [INFO] Sum of grad norms: 0.399084
2019-03-07 15:38:01,963 [INFO] ---------------------------------
2019-03-07 15:38:21,262 [INFO] ---------------------------------
2019-03-07 15:38:21,264 [INFO] Summary:
2019-03-07 15:38:21,264 [INFO] Batch 93000, worst loss 0.265971 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:38:21,265 [INFO] Regularization: 5447.586914 * 0.0000010000 = 0.0054475870
2019-03-07 15:38:21,267 [INFO] Sum of grad norms: 0.420354
2019-03-07 15:38:21,268 [INFO] ---------------------------------
2019-03-07 15:38:40,468 [INFO] ---------------------------------
2019-03-07 15:38:40,469 [INFO] Summary:
2019-03-07 15:38:40,470 [INFO] Batch 94000, worst loss 0.241998 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:38:40,471 [INFO] Regularization: 5447.523926 * 0.0000010000 = 0.0054475241
2019-03-07 15:38:40,473 [INFO] Sum of grad norms: 0.645225
2019-03-07 15:38:40,475 [INFO] ---------------------------------
2019-03-07 15:38:59,688 [INFO] ---------------------------------
2019-03-07 15:38:59,689 [INFO] Summary:
2019-03-07 15:38:59,690 [INFO] Batch 95000, worst loss 0.175661 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:38:59,691 [INFO] Regularization: 5447.498535 * 0.0000010000 = 0.0054474985
2019-03-07 15:38:59,692 [INFO] Sum of grad norms: 15.755782
2019-03-07 15:38:59,693 [INFO] ---------------------------------
2019-03-07 15:39:19,053 [INFO] ---------------------------------
2019-03-07 15:39:19,054 [INFO] Summary:
2019-03-07 15:39:19,055 [INFO] Batch 96000, worst loss 0.178443 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:39:19,056 [INFO] Regularization: 5447.464355 * 0.0000010000 = 0.0054474645
2019-03-07 15:39:19,057 [INFO] Sum of grad norms: 7.687186
2019-03-07 15:39:19,057 [INFO] ---------------------------------
2019-03-07 15:39:38,227 [INFO] ---------------------------------
2019-03-07 15:39:38,229 [INFO] Summary:
2019-03-07 15:39:38,229 [INFO] Batch 97000, worst loss 0.192077 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:39:38,230 [INFO] Regularization: 5447.453125 * 0.0000010000 = 0.0054474529
2019-03-07 15:39:38,230 [INFO] Sum of grad norms: 4.819087
2019-03-07 15:39:38,231 [INFO] ---------------------------------
2019-03-07 15:39:57,656 [INFO] ---------------------------------
2019-03-07 15:39:57,657 [INFO] Summary:
2019-03-07 15:39:57,658 [INFO] Batch 98000, worst loss 0.190794 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:39:57,659 [INFO] Regularization: 5447.424805 * 0.0000010000 = 0.0054474249
2019-03-07 15:39:57,660 [INFO] Sum of grad norms: 2.782460
2019-03-07 15:39:57,661 [INFO] ---------------------------------
2019-03-07 15:40:16,954 [INFO] ---------------------------------
2019-03-07 15:40:16,956 [INFO] Summary:
2019-03-07 15:40:16,957 [INFO] Batch 99000, worst loss 0.191273 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:40:16,957 [INFO] Regularization: 5447.403320 * 0.0000010000 = 0.0054474035
2019-03-07 15:40:16,958 [INFO] Sum of grad norms: 0.385712
2019-03-07 15:40:16,959 [INFO] ---------------------------------
2019-03-07 15:40:36,047 [INFO] ---------------------------------
2019-03-07 15:40:36,048 [INFO] Summary:
2019-03-07 15:40:36,048 [INFO] Batch 100000, worst loss 0.200297 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:40:36,049 [INFO] Regularization: 5447.362305 * 0.0000010000 = 0.0054473621
2019-03-07 15:40:36,049 [INFO] Sum of grad norms: 0.181149
2019-03-07 15:40:36,050 [INFO] ---------------------------------
2019-03-07 15:40:49,378 [INFO] ---------------------------------
2019-03-07 15:40:49,379 [INFO] Evaluation:
2019-03-07 15:40:49,380 [INFO] Batch 100000, worst loss 0.180645 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:40:49,381 [INFO] ---------------------------------
2019-03-07 15:41:08,826 [INFO] ---------------------------------
2019-03-07 15:41:08,827 [INFO] Summary:
2019-03-07 15:41:08,827 [INFO] Batch 101000, worst loss 0.175538 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:41:08,828 [INFO] Regularization: 5447.348145 * 0.0000010000 = 0.0054473481
2019-03-07 15:41:08,829 [INFO] Sum of grad norms: 2.847292
2019-03-07 15:41:08,830 [INFO] ---------------------------------
2019-03-07 15:41:28,333 [INFO] ---------------------------------
2019-03-07 15:41:28,334 [INFO] Summary:
2019-03-07 15:41:28,338 [INFO] Batch 102000, worst loss 0.184317 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:41:28,339 [INFO] Regularization: 5447.321777 * 0.0000010000 = 0.0054473216
2019-03-07 15:41:28,339 [INFO] Sum of grad norms: 11.595828
2019-03-07 15:41:28,340 [INFO] ---------------------------------
2019-03-07 15:41:47,654 [INFO] ---------------------------------
2019-03-07 15:41:47,655 [INFO] Summary:
2019-03-07 15:41:47,656 [INFO] Batch 103000, worst loss 0.177764 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:41:47,656 [INFO] Regularization: 5447.317383 * 0.0000010000 = 0.0054473174
2019-03-07 15:41:47,657 [INFO] Sum of grad norms: 0.185749
2019-03-07 15:41:47,657 [INFO] ---------------------------------
2019-03-07 15:42:06,951 [INFO] ---------------------------------
2019-03-07 15:42:06,952 [INFO] Summary:
2019-03-07 15:42:06,953 [INFO] Batch 104000, worst loss 0.187813 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:42:06,954 [INFO] Regularization: 5447.306641 * 0.0000010000 = 0.0054473067
2019-03-07 15:42:06,955 [INFO] Sum of grad norms: 6.249933
2019-03-07 15:42:06,957 [INFO] ---------------------------------
2019-03-07 15:42:26,225 [INFO] ---------------------------------
2019-03-07 15:42:26,226 [INFO] Summary:
2019-03-07 15:42:26,227 [INFO] Batch 105000, worst loss 0.222423 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:42:26,228 [INFO] Regularization: 5447.297852 * 0.0000010000 = 0.0054472978
2019-03-07 15:42:26,229 [INFO] Sum of grad norms: 0.204770
2019-03-07 15:42:26,230 [INFO] ---------------------------------
2019-03-07 15:42:45,723 [INFO] ---------------------------------
2019-03-07 15:42:45,724 [INFO] Summary:
2019-03-07 15:42:45,725 [INFO] Batch 106000, worst loss 0.204063 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:42:45,726 [INFO] Regularization: 5447.291992 * 0.0000010000 = 0.0054472918
2019-03-07 15:42:45,727 [INFO] Sum of grad norms: 0.215340
2019-03-07 15:42:45,727 [INFO] ---------------------------------
2019-03-07 15:43:05,287 [INFO] ---------------------------------
2019-03-07 15:43:05,288 [INFO] Summary:
2019-03-07 15:43:05,288 [INFO] Batch 107000, worst loss 0.203874 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:43:05,289 [INFO] Regularization: 5447.283203 * 0.0000010000 = 0.0054472834
2019-03-07 15:43:05,290 [INFO] Sum of grad norms: 8.753749
2019-03-07 15:43:05,291 [INFO] ---------------------------------
2019-03-07 15:43:24,525 [INFO] ---------------------------------
2019-03-07 15:43:24,526 [INFO] Summary:
2019-03-07 15:43:24,526 [INFO] Batch 108000, worst loss 0.182446 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:43:24,527 [INFO] Regularization: 5447.286621 * 0.0000010000 = 0.0054472866
2019-03-07 15:43:24,527 [INFO] Sum of grad norms: 10.087871
2019-03-07 15:43:24,528 [INFO] ---------------------------------
2019-03-07 15:43:43,683 [INFO] ---------------------------------
2019-03-07 15:43:43,684 [INFO] Summary:
2019-03-07 15:43:43,684 [INFO] Batch 109000, worst loss 0.183823 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:43:43,685 [INFO] Regularization: 5447.282227 * 0.0000010000 = 0.0054472820
2019-03-07 15:43:43,686 [INFO] Sum of grad norms: 9.600106
2019-03-07 15:43:43,687 [INFO] ---------------------------------
2019-03-07 15:44:02,894 [INFO] ---------------------------------
2019-03-07 15:44:02,895 [INFO] Summary:
2019-03-07 15:44:02,896 [INFO] Batch 110000, worst loss 0.183912 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:44:02,897 [INFO] Regularization: 5447.284180 * 0.0000010000 = 0.0054472843
2019-03-07 15:44:02,897 [INFO] Sum of grad norms: 0.259592
2019-03-07 15:44:02,899 [INFO] ---------------------------------
2019-03-07 15:44:16,136 [INFO] ---------------------------------
2019-03-07 15:44:16,137 [INFO] Evaluation:
2019-03-07 15:44:16,138 [INFO] Batch 110000, worst loss 0.190109 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:44:16,138 [INFO] ---------------------------------
2019-03-07 15:44:35,457 [INFO] ---------------------------------
2019-03-07 15:44:35,457 [INFO] Summary:
2019-03-07 15:44:35,458 [INFO] Batch 111000, worst loss 0.195558 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:44:35,458 [INFO] Regularization: 5447.274414 * 0.0000010000 = 0.0054472745
2019-03-07 15:44:35,459 [INFO] Sum of grad norms: 16.440634
2019-03-07 15:44:35,460 [INFO] ---------------------------------
2019-03-07 15:44:54,813 [INFO] ---------------------------------
2019-03-07 15:44:54,814 [INFO] Summary:
2019-03-07 15:44:54,814 [INFO] Batch 112000, worst loss 0.199674 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:44:54,815 [INFO] Regularization: 5447.266602 * 0.0000010000 = 0.0054472666
2019-03-07 15:44:54,816 [INFO] Sum of grad norms: 7.381777
2019-03-07 15:44:54,816 [INFO] ---------------------------------
2019-03-07 15:45:14,502 [INFO] ---------------------------------
2019-03-07 15:45:14,503 [INFO] Summary:
2019-03-07 15:45:14,504 [INFO] Batch 113000, worst loss 0.179458 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:45:14,504 [INFO] Regularization: 5447.266602 * 0.0000010000 = 0.0054472666
2019-03-07 15:45:14,505 [INFO] Sum of grad norms: 11.939110
2019-03-07 15:45:14,506 [INFO] ---------------------------------
2019-03-07 15:45:33,683 [INFO] ---------------------------------
2019-03-07 15:45:33,684 [INFO] Summary:
2019-03-07 15:45:33,684 [INFO] Batch 114000, worst loss 0.188397 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:45:33,685 [INFO] Regularization: 5447.268555 * 0.0000010000 = 0.0054472685
2019-03-07 15:45:33,686 [INFO] Sum of grad norms: 6.492592
2019-03-07 15:45:33,687 [INFO] ---------------------------------
2019-03-07 15:45:53,395 [INFO] ---------------------------------
2019-03-07 15:45:53,397 [INFO] Summary:
2019-03-07 15:45:53,397 [INFO] Batch 115000, worst loss 0.214916 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:45:53,398 [INFO] Regularization: 5447.265625 * 0.0000010000 = 0.0054472657
2019-03-07 15:45:53,398 [INFO] Sum of grad norms: 0.479435
2019-03-07 15:45:53,399 [INFO] ---------------------------------
2019-03-07 15:46:13,046 [INFO] ---------------------------------
2019-03-07 15:46:13,047 [INFO] Summary:
2019-03-07 15:46:13,048 [INFO] Batch 116000, worst loss 0.214915 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:46:13,049 [INFO] Regularization: 5447.265625 * 0.0000010000 = 0.0054472657
2019-03-07 15:46:13,050 [INFO] Sum of grad norms: 1.145208
2019-03-07 15:46:13,051 [INFO] ---------------------------------
2019-03-07 15:46:32,497 [INFO] ---------------------------------
2019-03-07 15:46:32,498 [INFO] Summary:
2019-03-07 15:46:32,499 [INFO] Batch 117000, worst loss 0.183363 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:46:32,500 [INFO] Regularization: 5447.267090 * 0.0000010000 = 0.0054472671
2019-03-07 15:46:32,501 [INFO] Sum of grad norms: 5.917709
2019-03-07 15:46:32,502 [INFO] ---------------------------------
2019-03-07 15:46:52,039 [INFO] ---------------------------------
2019-03-07 15:46:52,040 [INFO] Summary:
2019-03-07 15:46:52,041 [INFO] Batch 118000, worst loss 0.182746 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:46:52,041 [INFO] Regularization: 5447.261719 * 0.0000010000 = 0.0054472615
2019-03-07 15:46:52,042 [INFO] Sum of grad norms: 4.321430
2019-03-07 15:46:52,043 [INFO] ---------------------------------
2019-03-07 15:47:11,488 [INFO] ---------------------------------
2019-03-07 15:47:11,489 [INFO] Summary:
2019-03-07 15:47:11,490 [INFO] Batch 119000, worst loss 0.211950 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:47:11,491 [INFO] Regularization: 5447.260742 * 0.0000010000 = 0.0054472606
2019-03-07 15:47:11,492 [INFO] Sum of grad norms: 7.837830
2019-03-07 15:47:11,492 [INFO] ---------------------------------
2019-03-07 15:47:30,480 [INFO] ---------------------------------
2019-03-07 15:47:30,481 [INFO] Summary:
2019-03-07 15:47:30,482 [INFO] Batch 120000, worst loss 0.223277 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:47:30,482 [INFO] Regularization: 5447.259766 * 0.0000010000 = 0.0054472596
2019-03-07 15:47:30,483 [INFO] Sum of grad norms: 0.605978
2019-03-07 15:47:30,484 [INFO] ---------------------------------
2019-03-07 15:47:43,453 [INFO] ---------------------------------
2019-03-07 15:47:43,454 [INFO] Evaluation:
2019-03-07 15:47:43,455 [INFO] Batch 120000, worst loss 0.217809 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:47:43,457 [INFO] ---------------------------------
2019-03-07 15:48:02,909 [INFO] ---------------------------------
2019-03-07 15:48:02,910 [INFO] Summary:
2019-03-07 15:48:02,911 [INFO] Batch 121000, worst loss 0.187510 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:48:02,911 [INFO] Regularization: 5447.258301 * 0.0000010000 = 0.0054472582
2019-03-07 15:48:02,912 [INFO] Sum of grad norms: 2.945457
2019-03-07 15:48:02,913 [INFO] ---------------------------------
2019-03-07 15:48:22,068 [INFO] ---------------------------------
2019-03-07 15:48:22,069 [INFO] Summary:
2019-03-07 15:48:22,069 [INFO] Batch 122000, worst loss 0.183325 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:48:22,071 [INFO] Regularization: 5447.257812 * 0.0000010000 = 0.0054472578
2019-03-07 15:48:22,071 [INFO] Sum of grad norms: 0.259522
2019-03-07 15:48:22,072 [INFO] ---------------------------------
2019-03-07 15:48:41,314 [INFO] ---------------------------------
2019-03-07 15:48:41,315 [INFO] Summary:
2019-03-07 15:48:41,316 [INFO] Batch 123000, worst loss 0.204036 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:48:41,317 [INFO] Regularization: 5447.257812 * 0.0000010000 = 0.0054472578
2019-03-07 15:48:41,318 [INFO] Sum of grad norms: 0.401767
2019-03-07 15:48:41,318 [INFO] ---------------------------------
2019-03-07 15:49:00,765 [INFO] ---------------------------------
2019-03-07 15:49:00,766 [INFO] Summary:
2019-03-07 15:49:00,766 [INFO] Batch 124000, worst loss 0.228672 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:49:00,767 [INFO] Regularization: 5447.257324 * 0.0000010000 = 0.0054472573
2019-03-07 15:49:00,768 [INFO] Sum of grad norms: 0.387618
2019-03-07 15:49:00,769 [INFO] ---------------------------------
2019-03-07 15:49:20,013 [INFO] ---------------------------------
2019-03-07 15:49:20,014 [INFO] Summary:
2019-03-07 15:49:20,014 [INFO] Batch 125000, worst loss 0.228672 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:49:20,015 [INFO] Regularization: 5447.257324 * 0.0000010000 = 0.0054472573
2019-03-07 15:49:20,016 [INFO] Sum of grad norms: 10.385802
2019-03-07 15:49:20,016 [INFO] ---------------------------------
2019-03-07 15:49:38,983 [INFO] ---------------------------------
2019-03-07 15:49:38,984 [INFO] Summary:
2019-03-07 15:49:38,985 [INFO] Batch 126000, worst loss 0.180276 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:49:38,986 [INFO] Regularization: 5447.256836 * 0.0000010000 = 0.0054472568
2019-03-07 15:49:38,987 [INFO] Sum of grad norms: 11.449632
2019-03-07 15:49:38,988 [INFO] ---------------------------------
2019-03-07 15:49:57,937 [INFO] ---------------------------------
2019-03-07 15:49:57,938 [INFO] Summary:
2019-03-07 15:49:57,939 [INFO] Batch 127000, worst loss 0.188130 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:49:57,940 [INFO] Regularization: 5447.256836 * 0.0000010000 = 0.0054472568
2019-03-07 15:49:57,941 [INFO] Sum of grad norms: 4.179732
2019-03-07 15:49:57,942 [INFO] ---------------------------------
2019-03-07 15:50:17,103 [INFO] ---------------------------------
2019-03-07 15:50:17,104 [INFO] Summary:
2019-03-07 15:50:17,105 [INFO] Batch 128000, worst loss 0.180676 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:50:17,106 [INFO] Regularization: 5447.256836 * 0.0000010000 = 0.0054472568
2019-03-07 15:50:17,107 [INFO] Sum of grad norms: 0.172059
2019-03-07 15:50:17,108 [INFO] ---------------------------------
2019-03-07 15:50:36,640 [INFO] ---------------------------------
2019-03-07 15:50:36,641 [INFO] Summary:
2019-03-07 15:50:36,642 [INFO] Batch 129000, worst loss 0.240136 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:50:36,646 [INFO] Regularization: 5447.256348 * 0.0000010000 = 0.0054472564
2019-03-07 15:50:36,647 [INFO] Sum of grad norms: 0.837406
2019-03-07 15:50:36,648 [INFO] ---------------------------------
2019-03-07 15:50:56,274 [INFO] ---------------------------------
2019-03-07 15:50:56,275 [INFO] Summary:
2019-03-07 15:50:56,276 [INFO] Batch 130000, worst loss 0.240135 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:50:56,276 [INFO] Regularization: 5447.256348 * 0.0000010000 = 0.0054472564
2019-03-07 15:50:56,277 [INFO] Sum of grad norms: 1.404340
2019-03-07 15:50:56,278 [INFO] ---------------------------------
2019-03-07 15:51:09,679 [INFO] ---------------------------------
2019-03-07 15:51:09,682 [INFO] Evaluation:
2019-03-07 15:51:09,683 [INFO] Batch 130000, worst loss 0.213724 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:51:09,684 [INFO] ---------------------------------
2019-03-07 15:51:29,138 [INFO] ---------------------------------
2019-03-07 15:51:29,138 [INFO] Summary:
2019-03-07 15:51:29,139 [INFO] Batch 131000, worst loss 0.187756 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:51:29,140 [INFO] Regularization: 5447.256348 * 0.0000010000 = 0.0054472564
2019-03-07 15:51:29,141 [INFO] Sum of grad norms: 4.264670
2019-03-07 15:51:29,141 [INFO] ---------------------------------
2019-03-07 15:51:48,651 [INFO] ---------------------------------
2019-03-07 15:51:48,652 [INFO] Summary:
2019-03-07 15:51:48,652 [INFO] Batch 132000, worst loss 0.196893 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:51:48,653 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:51:48,654 [INFO] Sum of grad norms: 17.249701
2019-03-07 15:51:48,655 [INFO] ---------------------------------
2019-03-07 15:52:07,937 [INFO] ---------------------------------
2019-03-07 15:52:07,938 [INFO] Summary:
2019-03-07 15:52:07,938 [INFO] Batch 133000, worst loss 0.179242 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:52:07,939 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:52:07,939 [INFO] Sum of grad norms: 10.180065
2019-03-07 15:52:07,940 [INFO] ---------------------------------
2019-03-07 15:52:27,308 [INFO] ---------------------------------
2019-03-07 15:52:27,309 [INFO] Summary:
2019-03-07 15:52:27,309 [INFO] Batch 134000, worst loss 0.162322 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:52:27,310 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:52:27,311 [INFO] Sum of grad norms: 2.421324
2019-03-07 15:52:27,312 [INFO] ---------------------------------
2019-03-07 15:52:46,763 [INFO] ---------------------------------
2019-03-07 15:52:46,764 [INFO] Summary:
2019-03-07 15:52:46,765 [INFO] Batch 135000, worst loss 0.182665 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:52:46,766 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:52:46,767 [INFO] Sum of grad norms: 0.418338
2019-03-07 15:52:46,768 [INFO] ---------------------------------
2019-03-07 15:53:06,330 [INFO] ---------------------------------
2019-03-07 15:53:06,331 [INFO] Summary:
2019-03-07 15:53:06,331 [INFO] Batch 136000, worst loss 0.197452 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:53:06,332 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:53:06,333 [INFO] Sum of grad norms: 0.690750
2019-03-07 15:53:06,333 [INFO] ---------------------------------
2019-03-07 15:53:25,706 [INFO] ---------------------------------
2019-03-07 15:53:25,707 [INFO] Summary:
2019-03-07 15:53:25,708 [INFO] Batch 137000, worst loss 0.197452 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:53:25,709 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:53:25,709 [INFO] Sum of grad norms: 0.326093
2019-03-07 15:53:25,710 [INFO] ---------------------------------
2019-03-07 15:53:45,344 [INFO] ---------------------------------
2019-03-07 15:53:45,345 [INFO] Summary:
2019-03-07 15:53:45,346 [INFO] Batch 138000, worst loss 0.164977 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:53:45,346 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:53:45,347 [INFO] Sum of grad norms: 6.263101
2019-03-07 15:53:45,348 [INFO] ---------------------------------
2019-03-07 15:54:04,844 [INFO] ---------------------------------
2019-03-07 15:54:04,845 [INFO] Summary:
2019-03-07 15:54:04,846 [INFO] Batch 139000, worst loss 0.176110 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:54:04,847 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:54:04,847 [INFO] Sum of grad norms: 7.752517
2019-03-07 15:54:04,848 [INFO] ---------------------------------
2019-03-07 15:54:24,403 [INFO] ---------------------------------
2019-03-07 15:54:24,404 [INFO] Summary:
2019-03-07 15:54:24,405 [INFO] Batch 140000, worst loss 0.194751 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:54:24,405 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:54:24,407 [INFO] Sum of grad norms: 0.233460
2019-03-07 15:54:24,407 [INFO] ---------------------------------
2019-03-07 15:54:37,421 [INFO] ---------------------------------
2019-03-07 15:54:37,422 [INFO] Evaluation:
2019-03-07 15:54:37,423 [INFO] Batch 140000, worst loss 0.182186 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:54:37,424 [INFO] ---------------------------------
2019-03-07 15:54:56,551 [INFO] ---------------------------------
2019-03-07 15:54:56,552 [INFO] Summary:
2019-03-07 15:54:56,553 [INFO] Batch 141000, worst loss 0.187633 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:54:56,553 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:54:56,554 [INFO] Sum of grad norms: 0.270657
2019-03-07 15:54:56,555 [INFO] ---------------------------------
2019-03-07 15:55:15,808 [INFO] ---------------------------------
2019-03-07 15:55:15,809 [INFO] Summary:
2019-03-07 15:55:15,810 [INFO] Batch 142000, worst loss 0.182831 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:55:15,810 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:55:15,812 [INFO] Sum of grad norms: 4.070207
2019-03-07 15:55:15,813 [INFO] ---------------------------------
2019-03-07 15:55:35,589 [INFO] ---------------------------------
2019-03-07 15:55:35,590 [INFO] Summary:
2019-03-07 15:55:35,591 [INFO] Batch 143000, worst loss 0.212511 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:55:35,592 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:55:35,592 [INFO] Sum of grad norms: 10.128325
2019-03-07 15:55:35,593 [INFO] ---------------------------------
2019-03-07 15:55:55,166 [INFO] ---------------------------------
2019-03-07 15:55:55,167 [INFO] Summary:
2019-03-07 15:55:55,167 [INFO] Batch 144000, worst loss 0.212511 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:55:55,168 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:55:55,168 [INFO] Sum of grad norms: 1.396362
2019-03-07 15:55:55,169 [INFO] ---------------------------------
2019-03-07 15:56:14,934 [INFO] ---------------------------------
2019-03-07 15:56:14,935 [INFO] Summary:
2019-03-07 15:56:14,936 [INFO] Batch 145000, worst loss 0.196644 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:56:14,936 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:56:14,937 [INFO] Sum of grad norms: 8.645495
2019-03-07 15:56:14,938 [INFO] ---------------------------------
2019-03-07 15:56:34,565 [INFO] ---------------------------------
2019-03-07 15:56:34,566 [INFO] Summary:
2019-03-07 15:56:34,567 [INFO] Batch 146000, worst loss 0.176957 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:56:34,568 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:56:34,569 [INFO] Sum of grad norms: 0.154976
2019-03-07 15:56:34,570 [INFO] ---------------------------------
2019-03-07 15:56:54,042 [INFO] ---------------------------------
2019-03-07 15:56:54,043 [INFO] Summary:
2019-03-07 15:56:54,044 [INFO] Batch 147000, worst loss 0.182799 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:56:54,044 [INFO] Regularization: 5447.255859 * 0.0000010000 = 0.0054472559
2019-03-07 15:56:54,045 [INFO] Sum of grad norms: 0.220221
2019-03-07 15:56:54,046 [INFO] ---------------------------------
2019-03-07 15:57:13,609 [INFO] ---------------------------------
2019-03-07 15:57:13,610 [INFO] Summary:
2019-03-07 15:57:13,611 [INFO] Batch 148000, worst loss 0.220356 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:57:13,612 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 15:57:13,613 [INFO] Sum of grad norms: 1.143442
2019-03-07 15:57:13,614 [INFO] ---------------------------------
2019-03-07 15:57:33,063 [INFO] ---------------------------------
2019-03-07 15:57:33,064 [INFO] Summary:
2019-03-07 15:57:33,064 [INFO] Batch 149000, worst loss 0.220356 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:57:33,065 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 15:57:33,066 [INFO] Sum of grad norms: 3.814412
2019-03-07 15:57:33,067 [INFO] ---------------------------------
2019-03-07 15:57:52,327 [INFO] ---------------------------------
2019-03-07 15:57:52,328 [INFO] Summary:
2019-03-07 15:57:52,329 [INFO] Batch 150000, worst loss 0.207318 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:57:52,330 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 15:57:52,330 [INFO] Sum of grad norms: 8.551861
2019-03-07 15:57:52,331 [INFO] ---------------------------------
2019-03-07 15:58:05,291 [INFO] ---------------------------------
2019-03-07 15:58:05,292 [INFO] Evaluation:
2019-03-07 15:58:05,293 [INFO] Batch 150000, worst loss 0.201871 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:58:05,295 [INFO] ---------------------------------
2019-03-07 15:58:24,563 [INFO] ---------------------------------
2019-03-07 15:58:24,565 [INFO] Summary:
2019-03-07 15:58:24,565 [INFO] Batch 151000, worst loss 0.219946 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:58:24,566 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 15:58:24,567 [INFO] Sum of grad norms: 0.952494
2019-03-07 15:58:24,567 [INFO] ---------------------------------
2019-03-07 15:58:44,097 [INFO] ---------------------------------
2019-03-07 15:58:44,098 [INFO] Summary:
2019-03-07 15:58:44,099 [INFO] Batch 152000, worst loss 0.219946 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:58:44,100 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 15:58:44,101 [INFO] Sum of grad norms: 0.273177
2019-03-07 15:58:44,102 [INFO] ---------------------------------
2019-03-07 15:59:03,339 [INFO] ---------------------------------
2019-03-07 15:59:03,340 [INFO] Summary:
2019-03-07 15:59:03,341 [INFO] Batch 153000, worst loss 0.186512 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:59:03,342 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 15:59:03,343 [INFO] Sum of grad norms: 0.266855
2019-03-07 15:59:03,344 [INFO] ---------------------------------
2019-03-07 15:59:22,902 [INFO] ---------------------------------
2019-03-07 15:59:22,903 [INFO] Summary:
2019-03-07 15:59:22,904 [INFO] Batch 154000, worst loss 0.216096 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:59:22,905 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 15:59:22,906 [INFO] Sum of grad norms: 3.049980
2019-03-07 15:59:22,907 [INFO] ---------------------------------
2019-03-07 15:59:42,203 [INFO] ---------------------------------
2019-03-07 15:59:42,204 [INFO] Summary:
2019-03-07 15:59:42,205 [INFO] Batch 155000, worst loss 0.191696 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 15:59:42,205 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 15:59:42,206 [INFO] Sum of grad norms: 5.824128
2019-03-07 15:59:42,207 [INFO] ---------------------------------
2019-03-07 16:00:01,483 [INFO] ---------------------------------
2019-03-07 16:00:01,485 [INFO] Summary:
2019-03-07 16:00:01,485 [INFO] Batch 156000, worst loss 0.183496 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:00:01,486 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:00:01,486 [INFO] Sum of grad norms: 13.632387
2019-03-07 16:00:01,487 [INFO] ---------------------------------
2019-03-07 16:00:21,113 [INFO] ---------------------------------
2019-03-07 16:00:21,114 [INFO] Summary:
2019-03-07 16:00:21,115 [INFO] Batch 157000, worst loss 0.180116 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:00:21,116 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:00:21,117 [INFO] Sum of grad norms: 3.364707
2019-03-07 16:00:21,118 [INFO] ---------------------------------
2019-03-07 16:00:40,450 [INFO] ---------------------------------
2019-03-07 16:00:40,451 [INFO] Summary:
2019-03-07 16:00:40,451 [INFO] Batch 158000, worst loss 0.204452 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:00:40,452 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:00:40,454 [INFO] Sum of grad norms: 1.610410
2019-03-07 16:00:40,455 [INFO] ---------------------------------
2019-03-07 16:00:59,959 [INFO] ---------------------------------
2019-03-07 16:00:59,960 [INFO] Summary:
2019-03-07 16:00:59,960 [INFO] Batch 159000, worst loss 0.204452 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:00:59,961 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:00:59,962 [INFO] Sum of grad norms: 0.402141
2019-03-07 16:00:59,963 [INFO] ---------------------------------
2019-03-07 16:01:19,639 [INFO] ---------------------------------
2019-03-07 16:01:19,640 [INFO] Summary:
2019-03-07 16:01:19,641 [INFO] Batch 160000, worst loss 0.237128 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:01:19,641 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:01:19,642 [INFO] Sum of grad norms: 23.763004
2019-03-07 16:01:19,643 [INFO] ---------------------------------
2019-03-07 16:01:33,094 [INFO] ---------------------------------
2019-03-07 16:01:33,095 [INFO] Evaluation:
2019-03-07 16:01:33,096 [INFO] Batch 160000, worst loss 0.231681 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:01:33,098 [INFO] ---------------------------------
2019-03-07 16:01:52,874 [INFO] ---------------------------------
2019-03-07 16:01:52,875 [INFO] Summary:
2019-03-07 16:01:52,876 [INFO] Batch 161000, worst loss 0.189708 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:01:52,877 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:01:52,877 [INFO] Sum of grad norms: 3.967997
2019-03-07 16:01:52,878 [INFO] ---------------------------------
2019-03-07 16:02:12,542 [INFO] ---------------------------------
2019-03-07 16:02:12,543 [INFO] Summary:
2019-03-07 16:02:12,544 [INFO] Batch 162000, worst loss 0.189708 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:02:12,545 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:02:12,546 [INFO] Sum of grad norms: 0.548748
2019-03-07 16:02:12,546 [INFO] ---------------------------------
2019-03-07 16:02:31,663 [INFO] ---------------------------------
2019-03-07 16:02:31,664 [INFO] Summary:
2019-03-07 16:02:31,664 [INFO] Batch 163000, worst loss 0.213941 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:02:31,665 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:02:31,666 [INFO] Sum of grad norms: 4.265945
2019-03-07 16:02:31,667 [INFO] ---------------------------------
2019-03-07 16:02:51,132 [INFO] ---------------------------------
2019-03-07 16:02:51,133 [INFO] Summary:
2019-03-07 16:02:51,133 [INFO] Batch 164000, worst loss 0.204531 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:02:51,134 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:02:51,135 [INFO] Sum of grad norms: 10.208519
2019-03-07 16:02:51,136 [INFO] ---------------------------------
2019-03-07 16:03:10,133 [INFO] ---------------------------------
2019-03-07 16:03:10,134 [INFO] Summary:
2019-03-07 16:03:10,135 [INFO] Batch 165000, worst loss 0.191649 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:03:10,136 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:03:10,137 [INFO] Sum of grad norms: 20.484386
2019-03-07 16:03:10,138 [INFO] ---------------------------------
2019-03-07 16:03:29,469 [INFO] ---------------------------------
2019-03-07 16:03:29,470 [INFO] Summary:
2019-03-07 16:03:29,470 [INFO] Batch 166000, worst loss 0.184050 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:03:29,471 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:03:29,472 [INFO] Sum of grad norms: 0.321825
2019-03-07 16:03:29,473 [INFO] ---------------------------------
2019-03-07 16:03:49,008 [INFO] ---------------------------------
2019-03-07 16:03:49,009 [INFO] Summary:
2019-03-07 16:03:49,010 [INFO] Batch 167000, worst loss 0.165056 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:03:49,011 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:03:49,012 [INFO] Sum of grad norms: 0.208884
2019-03-07 16:03:49,013 [INFO] ---------------------------------
2019-03-07 16:04:08,604 [INFO] ---------------------------------
2019-03-07 16:04:08,605 [INFO] Summary:
2019-03-07 16:04:08,605 [INFO] Batch 168000, worst loss 0.181485 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:04:08,606 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:04:08,607 [INFO] Sum of grad norms: 0.277387
2019-03-07 16:04:08,608 [INFO] ---------------------------------
2019-03-07 16:04:27,620 [INFO] ---------------------------------
2019-03-07 16:04:27,621 [INFO] Summary:
2019-03-07 16:04:27,621 [INFO] Batch 169000, worst loss 0.238975 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:04:27,622 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:04:27,623 [INFO] Sum of grad norms: 5.183223
2019-03-07 16:04:27,624 [INFO] ---------------------------------
2019-03-07 16:04:46,906 [INFO] ---------------------------------
2019-03-07 16:04:46,907 [INFO] Summary:
2019-03-07 16:04:46,910 [INFO] Batch 170000, worst loss 0.238975 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:04:46,910 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:04:46,911 [INFO] Sum of grad norms: 1.030522
2019-03-07 16:04:46,913 [INFO] ---------------------------------
2019-03-07 16:04:59,988 [INFO] ---------------------------------
2019-03-07 16:04:59,989 [INFO] Evaluation:
2019-03-07 16:04:59,990 [INFO] Batch 170000, worst loss 0.202053 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:04:59,991 [INFO] ---------------------------------
2019-03-07 16:05:19,779 [INFO] ---------------------------------
2019-03-07 16:05:19,780 [INFO] Summary:
2019-03-07 16:05:19,780 [INFO] Batch 171000, worst loss 0.207500 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:05:19,781 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:05:19,782 [INFO] Sum of grad norms: 4.110985
2019-03-07 16:05:19,783 [INFO] ---------------------------------
2019-03-07 16:05:39,468 [INFO] ---------------------------------
2019-03-07 16:05:39,469 [INFO] Summary:
2019-03-07 16:05:39,470 [INFO] Batch 172000, worst loss 0.183052 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:05:39,471 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:05:39,471 [INFO] Sum of grad norms: 3.666906
2019-03-07 16:05:39,472 [INFO] ---------------------------------
2019-03-07 16:05:59,297 [INFO] ---------------------------------
2019-03-07 16:05:59,298 [INFO] Summary:
2019-03-07 16:05:59,298 [INFO] Batch 173000, worst loss 0.181693 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:05:59,299 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:05:59,299 [INFO] Sum of grad norms: 4.380060
2019-03-07 16:05:59,300 [INFO] ---------------------------------
2019-03-07 16:06:18,317 [INFO] ---------------------------------
2019-03-07 16:06:18,318 [INFO] Summary:
2019-03-07 16:06:18,319 [INFO] Batch 174000, worst loss 0.186004 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:06:18,320 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:06:18,321 [INFO] Sum of grad norms: 2.109800
2019-03-07 16:06:18,322 [INFO] ---------------------------------
2019-03-07 16:06:38,077 [INFO] ---------------------------------
2019-03-07 16:06:38,078 [INFO] Summary:
2019-03-07 16:06:38,079 [INFO] Batch 175000, worst loss 0.186004 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:06:38,081 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:06:38,083 [INFO] Sum of grad norms: 0.244275
2019-03-07 16:06:38,084 [INFO] ---------------------------------
2019-03-07 16:06:57,533 [INFO] ---------------------------------
2019-03-07 16:06:57,534 [INFO] Summary:
2019-03-07 16:06:57,534 [INFO] Batch 176000, worst loss 0.179587 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:06:57,535 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:06:57,536 [INFO] Sum of grad norms: 0.521790
2019-03-07 16:06:57,537 [INFO] ---------------------------------
2019-03-07 16:07:17,185 [INFO] ---------------------------------
2019-03-07 16:07:17,186 [INFO] Summary:
2019-03-07 16:07:17,186 [INFO] Batch 177000, worst loss 0.183856 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:07:17,187 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:07:17,189 [INFO] Sum of grad norms: 3.892343
2019-03-07 16:07:17,190 [INFO] ---------------------------------
2019-03-07 16:07:36,737 [INFO] ---------------------------------
2019-03-07 16:07:36,738 [INFO] Summary:
2019-03-07 16:07:36,739 [INFO] Batch 178000, worst loss 0.186838 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:07:36,740 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:07:36,741 [INFO] Sum of grad norms: 4.366082
2019-03-07 16:07:36,742 [INFO] ---------------------------------
2019-03-07 16:07:56,046 [INFO] ---------------------------------
2019-03-07 16:07:56,047 [INFO] Summary:
2019-03-07 16:07:56,048 [INFO] Batch 179000, worst loss 0.186838 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:07:56,048 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:07:56,049 [INFO] Sum of grad norms: 0.672562
2019-03-07 16:07:56,051 [INFO] ---------------------------------
2019-03-07 16:08:15,512 [INFO] ---------------------------------
2019-03-07 16:08:15,514 [INFO] Summary:
2019-03-07 16:08:15,515 [INFO] Batch 180000, worst loss 0.212621 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:08:15,517 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:08:15,519 [INFO] Sum of grad norms: 6.937375
2019-03-07 16:08:15,520 [INFO] ---------------------------------
2019-03-07 16:08:28,520 [INFO] ---------------------------------
2019-03-07 16:08:28,521 [INFO] Evaluation:
2019-03-07 16:08:28,522 [INFO] Batch 180000, worst loss 0.207174 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:08:28,524 [INFO] ---------------------------------
2019-03-07 16:08:47,980 [INFO] ---------------------------------
2019-03-07 16:08:47,982 [INFO] Summary:
2019-03-07 16:08:47,982 [INFO] Batch 181000, worst loss 0.174299 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:08:47,983 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:08:47,984 [INFO] Sum of grad norms: 1.856879
2019-03-07 16:08:47,985 [INFO] ---------------------------------
2019-03-07 16:09:07,228 [INFO] ---------------------------------
2019-03-07 16:09:07,230 [INFO] Summary:
2019-03-07 16:09:07,230 [INFO] Batch 182000, worst loss 0.210070 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:09:07,231 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:09:07,232 [INFO] Sum of grad norms: 0.529109
2019-03-07 16:09:07,233 [INFO] ---------------------------------
2019-03-07 16:09:26,689 [INFO] ---------------------------------
2019-03-07 16:09:26,690 [INFO] Summary:
2019-03-07 16:09:26,690 [INFO] Batch 183000, worst loss 0.210070 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:09:26,691 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:09:26,691 [INFO] Sum of grad norms: 7.102508
2019-03-07 16:09:26,692 [INFO] ---------------------------------
2019-03-07 16:09:46,125 [INFO] ---------------------------------
2019-03-07 16:09:46,126 [INFO] Summary:
2019-03-07 16:09:46,126 [INFO] Batch 184000, worst loss 0.191249 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:09:46,127 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:09:46,128 [INFO] Sum of grad norms: 0.458895
2019-03-07 16:09:46,128 [INFO] ---------------------------------
2019-03-07 16:10:05,510 [INFO] ---------------------------------
2019-03-07 16:10:05,511 [INFO] Summary:
2019-03-07 16:10:05,512 [INFO] Batch 185000, worst loss 0.191249 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:10:05,512 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:10:05,514 [INFO] Sum of grad norms: 8.439245
2019-03-07 16:10:05,515 [INFO] ---------------------------------
2019-03-07 16:10:24,861 [INFO] ---------------------------------
2019-03-07 16:10:24,862 [INFO] Summary:
2019-03-07 16:10:24,862 [INFO] Batch 186000, worst loss 0.267755 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:10:24,863 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:10:24,864 [INFO] Sum of grad norms: 2.777338
2019-03-07 16:10:24,865 [INFO] ---------------------------------
2019-03-07 16:10:43,955 [INFO] ---------------------------------
2019-03-07 16:10:43,956 [INFO] Summary:
2019-03-07 16:10:43,956 [INFO] Batch 187000, worst loss 0.185773 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:10:43,957 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:10:43,958 [INFO] Sum of grad norms: 0.523863
2019-03-07 16:10:43,959 [INFO] ---------------------------------
2019-03-07 16:11:03,370 [INFO] ---------------------------------
2019-03-07 16:11:03,371 [INFO] Summary:
2019-03-07 16:11:03,371 [INFO] Batch 188000, worst loss 0.193767 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:11:03,372 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:11:03,373 [INFO] Sum of grad norms: 11.935006
2019-03-07 16:11:03,374 [INFO] ---------------------------------
2019-03-07 16:11:22,991 [INFO] ---------------------------------
2019-03-07 16:11:22,992 [INFO] Summary:
2019-03-07 16:11:22,992 [INFO] Batch 189000, worst loss 0.219352 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:11:22,993 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:11:22,994 [INFO] Sum of grad norms: 6.608654
2019-03-07 16:11:22,995 [INFO] ---------------------------------
2019-03-07 16:11:42,508 [INFO] ---------------------------------
2019-03-07 16:11:42,509 [INFO] Summary:
2019-03-07 16:11:42,510 [INFO] Batch 190000, worst loss 0.219352 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:11:42,511 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:11:42,512 [INFO] Sum of grad norms: 1.036052
2019-03-07 16:11:42,512 [INFO] ---------------------------------
2019-03-07 16:11:55,986 [INFO] ---------------------------------
2019-03-07 16:11:55,987 [INFO] Evaluation:
2019-03-07 16:11:55,987 [INFO] Batch 190000, worst loss 0.177289 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:11:55,988 [INFO] ---------------------------------
2019-03-07 16:12:15,868 [INFO] ---------------------------------
2019-03-07 16:12:15,869 [INFO] Summary:
2019-03-07 16:12:15,870 [INFO] Batch 191000, worst loss 0.175521 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:12:15,871 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:12:15,871 [INFO] Sum of grad norms: 2.615575
2019-03-07 16:12:15,872 [INFO] ---------------------------------
2019-03-07 16:12:35,228 [INFO] ---------------------------------
2019-03-07 16:12:35,229 [INFO] Summary:
2019-03-07 16:12:35,230 [INFO] Batch 192000, worst loss 0.173186 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:12:35,231 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:12:35,231 [INFO] Sum of grad norms: 1.846544
2019-03-07 16:12:35,232 [INFO] ---------------------------------
2019-03-07 16:12:54,312 [INFO] ---------------------------------
2019-03-07 16:12:54,313 [INFO] Summary:
2019-03-07 16:12:54,314 [INFO] Batch 193000, worst loss 0.231777 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:12:54,314 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:12:54,315 [INFO] Sum of grad norms: 0.282769
2019-03-07 16:12:54,316 [INFO] ---------------------------------
2019-03-07 16:13:13,753 [INFO] ---------------------------------
2019-03-07 16:13:13,754 [INFO] Summary:
2019-03-07 16:13:13,754 [INFO] Batch 194000, worst loss 0.231777 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:13:13,755 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:13:13,756 [INFO] Sum of grad norms: 0.347517
2019-03-07 16:13:13,757 [INFO] ---------------------------------
2019-03-07 16:13:33,206 [INFO] ---------------------------------
2019-03-07 16:13:33,207 [INFO] Summary:
2019-03-07 16:13:33,208 [INFO] Batch 195000, worst loss 0.180550 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:13:33,209 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:13:33,209 [INFO] Sum of grad norms: 3.170004
2019-03-07 16:13:33,210 [INFO] ---------------------------------
2019-03-07 16:13:52,480 [INFO] ---------------------------------
2019-03-07 16:13:52,481 [INFO] Summary:
2019-03-07 16:13:52,482 [INFO] Batch 196000, worst loss 0.229338 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:13:52,483 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:13:52,484 [INFO] Sum of grad norms: 6.447752
2019-03-07 16:13:52,485 [INFO] ---------------------------------
2019-03-07 16:14:11,835 [INFO] ---------------------------------
2019-03-07 16:14:11,836 [INFO] Summary:
2019-03-07 16:14:11,837 [INFO] Batch 197000, worst loss 0.229338 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:14:11,837 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:14:11,838 [INFO] Sum of grad norms: 1.823306
2019-03-07 16:14:11,839 [INFO] ---------------------------------
2019-03-07 16:14:31,220 [INFO] ---------------------------------
2019-03-07 16:14:31,221 [INFO] Summary:
2019-03-07 16:14:31,222 [INFO] Batch 198000, worst loss 0.212441 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:14:31,223 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:14:31,224 [INFO] Sum of grad norms: 4.392993
2019-03-07 16:14:31,224 [INFO] ---------------------------------
2019-03-07 16:14:50,498 [INFO] ---------------------------------
2019-03-07 16:14:50,499 [INFO] Summary:
2019-03-07 16:14:50,499 [INFO] Batch 199000, worst loss 0.214134 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:14:50,501 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:14:50,501 [INFO] Sum of grad norms: 4.620430
2019-03-07 16:14:50,502 [INFO] ---------------------------------
2019-03-07 16:15:10,104 [INFO] ---------------------------------
2019-03-07 16:15:10,105 [INFO] Summary:
2019-03-07 16:15:10,106 [INFO] Batch 200000, worst loss 0.199465 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:15:10,106 [INFO] Regularization: 5447.255371 * 0.0000010000 = 0.0054472554
2019-03-07 16:15:10,107 [INFO] Sum of grad norms: 5.633098
2019-03-07 16:15:10,109 [INFO] ---------------------------------
2019-03-07 16:15:23,217 [INFO] ---------------------------------
2019-03-07 16:15:23,219 [INFO] Evaluation:
2019-03-07 16:15:23,219 [INFO] Batch 200000, worst loss 0.177562 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:15:23,221 [INFO] ---------------------------------
2019-03-07 16:15:23,223 [INFO] Finished training, saved to file classifier/1551963312/1551971723_1_classifier_final.pth
2019-03-07 16:15:23,494 [INFO] Training model #2: (9, 64, 402) @ 2
2019-03-07 16:15:42,848 [INFO] ---------------------------------
2019-03-07 16:15:42,849 [INFO] Summary:
2019-03-07 16:15:42,851 [INFO] Batch 1000, worst loss 1680.562134 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:15:42,852 [INFO] Regularization: 15584.175781 * 0.0000010000 = 0.0155841755
2019-03-07 16:15:42,852 [INFO] Sum of grad norms: 0.007120
2019-03-07 16:15:42,853 [INFO] ---------------------------------
2019-03-07 16:16:02,303 [INFO] ---------------------------------
2019-03-07 16:16:02,304 [INFO] Summary:
2019-03-07 16:16:02,305 [INFO] Batch 2000, worst loss 1.127056 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:16:02,306 [INFO] Regularization: 13708.731445 * 0.0000010000 = 0.0137087312
2019-03-07 16:16:02,307 [INFO] Sum of grad norms: 3.147111
2019-03-07 16:16:02,308 [INFO] ---------------------------------
2019-03-07 16:16:22,066 [INFO] ---------------------------------
2019-03-07 16:16:22,067 [INFO] Summary:
2019-03-07 16:16:22,067 [INFO] Batch 3000, worst loss 0.646148 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:16:22,068 [INFO] Regularization: 13265.143555 * 0.0000010000 = 0.0132651431
2019-03-07 16:16:22,069 [INFO] Sum of grad norms: 5.674157
2019-03-07 16:16:22,070 [INFO] ---------------------------------
2019-03-07 16:16:41,455 [INFO] ---------------------------------
2019-03-07 16:16:41,456 [INFO] Summary:
2019-03-07 16:16:41,456 [INFO] Batch 4000, worst loss 0.447640 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:16:41,457 [INFO] Regularization: 12634.838867 * 0.0000010000 = 0.0126348389
2019-03-07 16:16:41,457 [INFO] Sum of grad norms: 28.321587
2019-03-07 16:16:41,458 [INFO] ---------------------------------
2019-03-07 16:17:00,582 [INFO] ---------------------------------
2019-03-07 16:17:00,583 [INFO] Summary:
2019-03-07 16:17:00,583 [INFO] Batch 5000, worst loss 0.435568 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:17:00,584 [INFO] Regularization: 11773.583984 * 0.0000010000 = 0.0117735844
2019-03-07 16:17:00,585 [INFO] Sum of grad norms: 4.533246
2019-03-07 16:17:00,586 [INFO] ---------------------------------
2019-03-07 16:17:20,077 [INFO] ---------------------------------
2019-03-07 16:17:20,079 [INFO] Summary:
2019-03-07 16:17:20,079 [INFO] Batch 6000, worst loss 0.364983 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:17:20,080 [INFO] Regularization: 10708.509766 * 0.0000010000 = 0.0107085099
2019-03-07 16:17:20,080 [INFO] Sum of grad norms: 7.125810
2019-03-07 16:17:20,081 [INFO] ---------------------------------
2019-03-07 16:17:39,763 [INFO] ---------------------------------
2019-03-07 16:17:39,764 [INFO] Summary:
2019-03-07 16:17:39,765 [INFO] Batch 7000, worst loss 0.382342 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:17:39,766 [INFO] Regularization: 9634.127930 * 0.0000010000 = 0.0096341278
2019-03-07 16:17:39,767 [INFO] Sum of grad norms: 3.729399
2019-03-07 16:17:39,768 [INFO] ---------------------------------
2019-03-07 16:17:59,021 [INFO] ---------------------------------
2019-03-07 16:17:59,022 [INFO] Summary:
2019-03-07 16:17:59,023 [INFO] Batch 8000, worst loss 0.316636 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:17:59,024 [INFO] Regularization: 8585.763672 * 0.0000010000 = 0.0085857641
2019-03-07 16:17:59,025 [INFO] Sum of grad norms: 6.048470
2019-03-07 16:17:59,025 [INFO] ---------------------------------
2019-03-07 16:18:18,890 [INFO] ---------------------------------
2019-03-07 16:18:18,891 [INFO] Summary:
2019-03-07 16:18:18,892 [INFO] Batch 9000, worst loss 0.307590 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:18:18,893 [INFO] Regularization: 7892.565918 * 0.0000010000 = 0.0078925658
2019-03-07 16:18:18,893 [INFO] Sum of grad norms: 6.161745
2019-03-07 16:18:18,894 [INFO] ---------------------------------
2019-03-07 16:18:38,415 [INFO] ---------------------------------
2019-03-07 16:18:38,415 [INFO] Summary:
2019-03-07 16:18:38,416 [INFO] Batch 10000, worst loss 0.455341 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:18:38,417 [INFO] Regularization: 7433.315430 * 0.0000010000 = 0.0074333153
2019-03-07 16:18:38,418 [INFO] Sum of grad norms: 4.390884
2019-03-07 16:18:38,418 [INFO] ---------------------------------
2019-03-07 16:18:51,550 [INFO] ---------------------------------
2019-03-07 16:18:51,551 [INFO] Evaluation:
2019-03-07 16:18:51,551 [INFO] Batch 10000, worst loss 0.320613 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:18:51,552 [INFO] ---------------------------------
2019-03-07 16:19:11,095 [INFO] ---------------------------------
2019-03-07 16:19:11,096 [INFO] Summary:
2019-03-07 16:19:11,097 [INFO] Batch 11000, worst loss 0.387720 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:19:11,098 [INFO] Regularization: 7246.650391 * 0.0000010000 = 0.0072466503
2019-03-07 16:19:11,099 [INFO] Sum of grad norms: 1.349780
2019-03-07 16:19:11,099 [INFO] ---------------------------------
2019-03-07 16:19:30,543 [INFO] ---------------------------------
2019-03-07 16:19:30,544 [INFO] Summary:
2019-03-07 16:19:30,545 [INFO] Batch 12000, worst loss 0.303755 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:19:30,546 [INFO] Regularization: 7020.879883 * 0.0000010000 = 0.0070208800
2019-03-07 16:19:30,546 [INFO] Sum of grad norms: 3.092676
2019-03-07 16:19:30,547 [INFO] ---------------------------------
2019-03-07 16:19:49,967 [INFO] ---------------------------------
2019-03-07 16:19:49,968 [INFO] Summary:
2019-03-07 16:19:49,968 [INFO] Batch 13000, worst loss 0.384728 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:19:49,969 [INFO] Regularization: 6939.020020 * 0.0000010000 = 0.0069390200
2019-03-07 16:19:49,970 [INFO] Sum of grad norms: 1.365256
2019-03-07 16:19:49,971 [INFO] ---------------------------------
2019-03-07 16:20:09,186 [INFO] ---------------------------------
2019-03-07 16:20:09,187 [INFO] Summary:
2019-03-07 16:20:09,188 [INFO] Batch 14000, worst loss 0.292450 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:20:09,188 [INFO] Regularization: 6774.252441 * 0.0000010000 = 0.0067742523
2019-03-07 16:20:09,190 [INFO] Sum of grad norms: 9.826309
2019-03-07 16:20:09,190 [INFO] ---------------------------------
2019-03-07 16:20:28,534 [INFO] ---------------------------------
2019-03-07 16:20:28,536 [INFO] Summary:
2019-03-07 16:20:28,536 [INFO] Batch 15000, worst loss 0.309730 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:20:28,537 [INFO] Regularization: 6696.682617 * 0.0000010000 = 0.0066966824
2019-03-07 16:20:28,538 [INFO] Sum of grad norms: 4.463830
2019-03-07 16:20:28,538 [INFO] ---------------------------------
2019-03-07 16:20:48,062 [INFO] ---------------------------------
2019-03-07 16:20:48,064 [INFO] Summary:
2019-03-07 16:20:48,064 [INFO] Batch 16000, worst loss 0.372760 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:20:48,067 [INFO] Regularization: 6627.684570 * 0.0000010000 = 0.0066276845
2019-03-07 16:20:48,068 [INFO] Sum of grad norms: 6.285491
2019-03-07 16:20:48,069 [INFO] ---------------------------------
2019-03-07 16:21:07,953 [INFO] ---------------------------------
2019-03-07 16:21:07,954 [INFO] Summary:
2019-03-07 16:21:07,955 [INFO] Batch 17000, worst loss 0.295274 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:21:07,956 [INFO] Regularization: 6576.632324 * 0.0000010000 = 0.0065766321
2019-03-07 16:21:07,956 [INFO] Sum of grad norms: 1.517114
2019-03-07 16:21:07,957 [INFO] ---------------------------------
2019-03-07 16:21:27,248 [INFO] ---------------------------------
2019-03-07 16:21:27,249 [INFO] Summary:
2019-03-07 16:21:27,250 [INFO] Batch 18000, worst loss 0.339525 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:21:27,250 [INFO] Regularization: 6509.448730 * 0.0000010000 = 0.0065094489
2019-03-07 16:21:27,251 [INFO] Sum of grad norms: 3.915808
2019-03-07 16:21:27,252 [INFO] ---------------------------------
2019-03-07 16:21:46,675 [INFO] ---------------------------------
2019-03-07 16:21:46,676 [INFO] Summary:
2019-03-07 16:21:46,676 [INFO] Batch 19000, worst loss 0.459189 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:21:46,677 [INFO] Regularization: 6758.350586 * 0.0000010000 = 0.0067583504
2019-03-07 16:21:46,678 [INFO] Sum of grad norms: 11.086505
2019-03-07 16:21:46,678 [INFO] ---------------------------------
2019-03-07 16:22:06,091 [INFO] ---------------------------------
2019-03-07 16:22:06,092 [INFO] Summary:
2019-03-07 16:22:06,093 [INFO] Batch 20000, worst loss 0.335174 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:22:06,093 [INFO] Regularization: 6374.111816 * 0.0000010000 = 0.0063741119
2019-03-07 16:22:06,094 [INFO] Sum of grad norms: 4.279611
2019-03-07 16:22:06,095 [INFO] ---------------------------------
2019-03-07 16:22:19,225 [INFO] ---------------------------------
2019-03-07 16:22:19,226 [INFO] Evaluation:
2019-03-07 16:22:19,227 [INFO] Batch 20000, worst loss 0.335601 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:22:19,228 [INFO] ---------------------------------
2019-03-07 16:22:38,998 [INFO] ---------------------------------
2019-03-07 16:22:38,999 [INFO] Summary:
2019-03-07 16:22:39,000 [INFO] Batch 21000, worst loss 0.310162 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:22:39,001 [INFO] Regularization: 6334.227539 * 0.0000010000 = 0.0063342275
2019-03-07 16:22:39,004 [INFO] Sum of grad norms: 2.547886
2019-03-07 16:22:39,005 [INFO] ---------------------------------
2019-03-07 16:22:58,549 [INFO] ---------------------------------
2019-03-07 16:22:58,550 [INFO] Summary:
2019-03-07 16:22:58,551 [INFO] Batch 22000, worst loss 0.326694 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:22:58,552 [INFO] Regularization: 6312.814941 * 0.0000010000 = 0.0063128150
2019-03-07 16:22:58,552 [INFO] Sum of grad norms: 2.178746
2019-03-07 16:22:58,553 [INFO] ---------------------------------
2019-03-07 16:23:18,103 [INFO] ---------------------------------
2019-03-07 16:23:18,104 [INFO] Summary:
2019-03-07 16:23:18,104 [INFO] Batch 23000, worst loss 0.386043 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:23:18,105 [INFO] Regularization: 6428.597168 * 0.0000010000 = 0.0064285970
2019-03-07 16:23:18,106 [INFO] Sum of grad norms: 1.553773
2019-03-07 16:23:18,106 [INFO] ---------------------------------
2019-03-07 16:23:37,411 [INFO] ---------------------------------
2019-03-07 16:23:37,413 [INFO] Summary:
2019-03-07 16:23:37,413 [INFO] Batch 24000, worst loss 0.358597 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:23:37,414 [INFO] Regularization: 6229.577637 * 0.0000010000 = 0.0062295776
2019-03-07 16:23:37,415 [INFO] Sum of grad norms: 2.040752
2019-03-07 16:23:37,416 [INFO] ---------------------------------
2019-03-07 16:23:56,925 [INFO] ---------------------------------
2019-03-07 16:23:56,926 [INFO] Summary:
2019-03-07 16:23:56,926 [INFO] Batch 25000, worst loss 0.295643 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:23:56,927 [INFO] Regularization: 6160.615234 * 0.0000010000 = 0.0061606150
2019-03-07 16:23:56,928 [INFO] Sum of grad norms: 10.532276
2019-03-07 16:23:56,929 [INFO] ---------------------------------
2019-03-07 16:24:16,232 [INFO] ---------------------------------
2019-03-07 16:24:16,233 [INFO] Summary:
2019-03-07 16:24:16,234 [INFO] Batch 26000, worst loss 0.326679 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:24:16,235 [INFO] Regularization: 6181.012207 * 0.0000010000 = 0.0061810124
2019-03-07 16:24:16,236 [INFO] Sum of grad norms: 5.930036
2019-03-07 16:24:16,237 [INFO] ---------------------------------
2019-03-07 16:24:35,478 [INFO] ---------------------------------
2019-03-07 16:24:35,479 [INFO] Summary:
2019-03-07 16:24:35,480 [INFO] Batch 27000, worst loss 0.322174 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:24:35,480 [INFO] Regularization: 6114.165039 * 0.0000010000 = 0.0061141648
2019-03-07 16:24:35,481 [INFO] Sum of grad norms: 7.323875
2019-03-07 16:24:35,482 [INFO] ---------------------------------
2019-03-07 16:24:55,069 [INFO] ---------------------------------
2019-03-07 16:24:55,070 [INFO] Summary:
2019-03-07 16:24:55,071 [INFO] Batch 28000, worst loss 0.350220 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:24:55,072 [INFO] Regularization: 6049.866699 * 0.0000010000 = 0.0060498668
2019-03-07 16:24:55,073 [INFO] Sum of grad norms: 1.824831
2019-03-07 16:24:55,073 [INFO] ---------------------------------
2019-03-07 16:25:14,415 [INFO] ---------------------------------
2019-03-07 16:25:14,416 [INFO] Summary:
2019-03-07 16:25:14,419 [INFO] Batch 29000, worst loss 0.230139 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:25:14,420 [INFO] Regularization: 6067.341797 * 0.0000010000 = 0.0060673417
2019-03-07 16:25:14,420 [INFO] Sum of grad norms: 0.697126
2019-03-07 16:25:14,421 [INFO] ---------------------------------
2019-03-07 16:25:33,882 [INFO] ---------------------------------
2019-03-07 16:25:33,883 [INFO] Summary:
2019-03-07 16:25:33,884 [INFO] Batch 30000, worst loss 0.403460 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:25:33,885 [INFO] Regularization: 6031.144043 * 0.0000010000 = 0.0060311439
2019-03-07 16:25:33,885 [INFO] Sum of grad norms: 3.632254
2019-03-07 16:25:33,886 [INFO] ---------------------------------
2019-03-07 16:25:47,031 [INFO] ---------------------------------
2019-03-07 16:25:47,032 [INFO] Evaluation:
2019-03-07 16:25:47,033 [INFO] Batch 30000, worst loss 0.207345 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:25:47,033 [INFO] ---------------------------------
2019-03-07 16:26:06,531 [INFO] ---------------------------------
2019-03-07 16:26:06,532 [INFO] Summary:
2019-03-07 16:26:06,533 [INFO] Batch 31000, worst loss 0.294983 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:26:06,534 [INFO] Regularization: 5936.279785 * 0.0000010000 = 0.0059362799
2019-03-07 16:26:06,534 [INFO] Sum of grad norms: 9.278629
2019-03-07 16:26:06,535 [INFO] ---------------------------------
2019-03-07 16:26:26,262 [INFO] ---------------------------------
2019-03-07 16:26:26,263 [INFO] Summary:
2019-03-07 16:26:26,264 [INFO] Batch 32000, worst loss 0.431772 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:26:26,265 [INFO] Regularization: 5880.354492 * 0.0000010000 = 0.0058803544
2019-03-07 16:26:26,266 [INFO] Sum of grad norms: 3.226399
2019-03-07 16:26:26,266 [INFO] ---------------------------------
2019-03-07 16:26:45,670 [INFO] ---------------------------------
2019-03-07 16:26:45,671 [INFO] Summary:
2019-03-07 16:26:45,672 [INFO] Batch 33000, worst loss 0.335443 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:26:45,673 [INFO] Regularization: 5883.156738 * 0.0000010000 = 0.0058831568
2019-03-07 16:26:45,673 [INFO] Sum of grad norms: 3.228666
2019-03-07 16:26:45,674 [INFO] ---------------------------------
2019-03-07 16:27:04,787 [INFO] ---------------------------------
2019-03-07 16:27:04,787 [INFO] Summary:
2019-03-07 16:27:04,788 [INFO] Batch 34000, worst loss 0.287709 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:27:04,788 [INFO] Regularization: 5829.038086 * 0.0000010000 = 0.0058290381
2019-03-07 16:27:04,789 [INFO] Sum of grad norms: 4.627767
2019-03-07 16:27:04,790 [INFO] ---------------------------------
2019-03-07 16:27:24,119 [INFO] ---------------------------------
2019-03-07 16:27:24,120 [INFO] Summary:
2019-03-07 16:27:24,121 [INFO] Batch 35000, worst loss 0.299265 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:27:24,122 [INFO] Regularization: 5758.432617 * 0.0000010000 = 0.0057584327
2019-03-07 16:27:24,123 [INFO] Sum of grad norms: 3.551059
2019-03-07 16:27:24,123 [INFO] ---------------------------------
2019-03-07 16:27:43,517 [INFO] ---------------------------------
2019-03-07 16:27:43,518 [INFO] Summary:
2019-03-07 16:27:43,521 [INFO] Batch 36000, worst loss 0.353263 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:27:43,523 [INFO] Regularization: 5714.869141 * 0.0000010000 = 0.0057148691
2019-03-07 16:27:43,524 [INFO] Sum of grad norms: 15.464916
2019-03-07 16:27:43,525 [INFO] ---------------------------------
2019-03-07 16:28:02,780 [INFO] ---------------------------------
2019-03-07 16:28:02,781 [INFO] Summary:
2019-03-07 16:28:02,781 [INFO] Batch 37000, worst loss 0.277242 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:28:02,782 [INFO] Regularization: 5687.581543 * 0.0000010000 = 0.0056875814
2019-03-07 16:28:02,783 [INFO] Sum of grad norms: 2.553745
2019-03-07 16:28:02,784 [INFO] ---------------------------------
2019-03-07 16:28:22,274 [INFO] ---------------------------------
2019-03-07 16:28:22,275 [INFO] Summary:
2019-03-07 16:28:22,276 [INFO] Batch 38000, worst loss 0.302493 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:28:22,277 [INFO] Regularization: 5674.406250 * 0.0000010000 = 0.0056744064
2019-03-07 16:28:22,278 [INFO] Sum of grad norms: 5.798085
2019-03-07 16:28:22,279 [INFO] ---------------------------------
2019-03-07 16:28:41,979 [INFO] ---------------------------------
2019-03-07 16:28:41,981 [INFO] Summary:
2019-03-07 16:28:41,981 [INFO] Batch 39000, worst loss 0.297727 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:28:41,982 [INFO] Regularization: 5605.121094 * 0.0000010000 = 0.0056051211
2019-03-07 16:28:41,983 [INFO] Sum of grad norms: 4.405013
2019-03-07 16:28:41,984 [INFO] ---------------------------------
2019-03-07 16:29:01,401 [INFO] ---------------------------------
2019-03-07 16:29:01,402 [INFO] Summary:
2019-03-07 16:29:01,403 [INFO] Batch 40000, worst loss 0.308048 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:29:01,403 [INFO] Regularization: 5565.810059 * 0.0000010000 = 0.0055658100
2019-03-07 16:29:01,404 [INFO] Sum of grad norms: 3.196143
2019-03-07 16:29:01,405 [INFO] ---------------------------------
2019-03-07 16:29:14,519 [INFO] ---------------------------------
2019-03-07 16:29:14,520 [INFO] Evaluation:
2019-03-07 16:29:14,521 [INFO] Batch 40000, worst loss 0.218435 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 16:29:14,522 [INFO] ---------------------------------
2019-03-07 16:29:33,655 [INFO] ---------------------------------
2019-03-07 16:29:33,656 [INFO] Summary:
2019-03-07 16:29:33,656 [INFO] Batch 41000, worst loss 0.253306 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:29:33,657 [INFO] Regularization: 5571.022949 * 0.0000010000 = 0.0055710231
2019-03-07 16:29:33,658 [INFO] Sum of grad norms: 4.638857
2019-03-07 16:29:33,659 [INFO] ---------------------------------
2019-03-07 16:29:53,351 [INFO] ---------------------------------
2019-03-07 16:29:53,352 [INFO] Summary:
2019-03-07 16:29:53,353 [INFO] Batch 42000, worst loss 0.260659 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:29:53,355 [INFO] Regularization: 5420.362793 * 0.0000010000 = 0.0054203626
2019-03-07 16:29:53,356 [INFO] Sum of grad norms: 0.516352
2019-03-07 16:29:53,356 [INFO] ---------------------------------
2019-03-07 16:30:12,463 [INFO] ---------------------------------
2019-03-07 16:30:12,464 [INFO] Summary:
2019-03-07 16:30:12,465 [INFO] Batch 43000, worst loss 0.237025 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:30:12,466 [INFO] Regularization: 5413.669434 * 0.0000010000 = 0.0054136696
2019-03-07 16:30:12,467 [INFO] Sum of grad norms: 7.061841
2019-03-07 16:30:12,467 [INFO] ---------------------------------
2019-03-07 16:30:32,050 [INFO] ---------------------------------
2019-03-07 16:30:32,051 [INFO] Summary:
2019-03-07 16:30:32,052 [INFO] Batch 44000, worst loss 0.263669 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:30:32,054 [INFO] Regularization: 5404.949707 * 0.0000010000 = 0.0054049497
2019-03-07 16:30:32,056 [INFO] Sum of grad norms: 1.297139
2019-03-07 16:30:32,057 [INFO] ---------------------------------
2019-03-07 16:30:51,270 [INFO] ---------------------------------
2019-03-07 16:30:51,271 [INFO] Summary:
2019-03-07 16:30:51,272 [INFO] Batch 45000, worst loss 0.219086 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:30:51,273 [INFO] Regularization: 5363.841309 * 0.0000010000 = 0.0053638411
2019-03-07 16:30:51,273 [INFO] Sum of grad norms: 1.652949
2019-03-07 16:30:51,274 [INFO] ---------------------------------
2019-03-07 16:31:11,037 [INFO] ---------------------------------
2019-03-07 16:31:11,038 [INFO] Summary:
2019-03-07 16:31:11,039 [INFO] Batch 46000, worst loss 0.200859 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:31:11,039 [INFO] Regularization: 5364.242676 * 0.0000010000 = 0.0053642425
2019-03-07 16:31:11,040 [INFO] Sum of grad norms: 7.750175
2019-03-07 16:31:11,041 [INFO] ---------------------------------
2019-03-07 16:31:30,218 [INFO] ---------------------------------
2019-03-07 16:31:30,219 [INFO] Summary:
2019-03-07 16:31:30,219 [INFO] Batch 47000, worst loss 0.279777 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:31:30,220 [INFO] Regularization: 5332.053711 * 0.0000010000 = 0.0053320536
2019-03-07 16:31:30,220 [INFO] Sum of grad norms: 12.115197
2019-03-07 16:31:30,221 [INFO] ---------------------------------
2019-03-07 16:31:49,464 [INFO] ---------------------------------
2019-03-07 16:31:49,465 [INFO] Summary:
2019-03-07 16:31:49,466 [INFO] Batch 48000, worst loss 0.209002 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:31:49,467 [INFO] Regularization: 5302.077148 * 0.0000010000 = 0.0053020772
2019-03-07 16:31:49,468 [INFO] Sum of grad norms: 1.046213
2019-03-07 16:31:49,469 [INFO] ---------------------------------
2019-03-07 16:32:09,124 [INFO] ---------------------------------
2019-03-07 16:32:09,125 [INFO] Summary:
2019-03-07 16:32:09,126 [INFO] Batch 49000, worst loss 0.318929 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:32:09,126 [INFO] Regularization: 5314.295898 * 0.0000010000 = 0.0053142961
2019-03-07 16:32:09,127 [INFO] Sum of grad norms: 1.406024
2019-03-07 16:32:09,128 [INFO] ---------------------------------
2019-03-07 16:32:28,271 [INFO] ---------------------------------
2019-03-07 16:32:28,272 [INFO] Summary:
2019-03-07 16:32:28,272 [INFO] Batch 50000, worst loss 0.195897 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:32:28,274 [INFO] Regularization: 5268.184082 * 0.0000010000 = 0.0052681840
2019-03-07 16:32:28,275 [INFO] Sum of grad norms: 14.024525
2019-03-07 16:32:28,276 [INFO] ---------------------------------
2019-03-07 16:32:41,427 [INFO] ---------------------------------
2019-03-07 16:32:41,428 [INFO] Evaluation:
2019-03-07 16:32:41,429 [INFO] Batch 50000, worst loss 0.215894 (without reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 16:32:41,430 [INFO] ---------------------------------
2019-03-07 16:33:00,942 [INFO] ---------------------------------
2019-03-07 16:33:00,943 [INFO] Summary:
2019-03-07 16:33:00,943 [INFO] Batch 51000, worst loss 0.262811 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:33:00,944 [INFO] Regularization: 5269.214355 * 0.0000010000 = 0.0052692145
2019-03-07 16:33:00,945 [INFO] Sum of grad norms: 0.590010
2019-03-07 16:33:00,946 [INFO] ---------------------------------
2019-03-07 16:33:20,207 [INFO] ---------------------------------
2019-03-07 16:33:20,209 [INFO] Summary:
2019-03-07 16:33:20,209 [INFO] Batch 52000, worst loss 0.181410 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:33:20,210 [INFO] Regularization: 5247.099121 * 0.0000010000 = 0.0052470993
2019-03-07 16:33:20,211 [INFO] Sum of grad norms: 7.771496
2019-03-07 16:33:20,212 [INFO] ---------------------------------
2019-03-07 16:33:39,742 [INFO] ---------------------------------
2019-03-07 16:33:39,744 [INFO] Summary:
2019-03-07 16:33:39,744 [INFO] Batch 53000, worst loss 0.238943 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:33:39,745 [INFO] Regularization: 5233.144043 * 0.0000010000 = 0.0052331439
2019-03-07 16:33:39,747 [INFO] Sum of grad norms: 1.797595
2019-03-07 16:33:39,748 [INFO] ---------------------------------
2019-03-07 16:33:59,182 [INFO] ---------------------------------
2019-03-07 16:33:59,183 [INFO] Summary:
2019-03-07 16:33:59,184 [INFO] Batch 54000, worst loss 0.245727 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:33:59,184 [INFO] Regularization: 5223.720703 * 0.0000010000 = 0.0052237208
2019-03-07 16:33:59,185 [INFO] Sum of grad norms: 0.251837
2019-03-07 16:33:59,186 [INFO] ---------------------------------
2019-03-07 16:34:18,418 [INFO] ---------------------------------
2019-03-07 16:34:18,419 [INFO] Summary:
2019-03-07 16:34:18,420 [INFO] Batch 55000, worst loss 0.191755 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:34:18,421 [INFO] Regularization: 5219.937012 * 0.0000010000 = 0.0052199368
2019-03-07 16:34:18,422 [INFO] Sum of grad norms: 0.296305
2019-03-07 16:34:18,423 [INFO] ---------------------------------
2019-03-07 16:34:37,835 [INFO] ---------------------------------
2019-03-07 16:34:37,836 [INFO] Summary:
2019-03-07 16:34:37,836 [INFO] Batch 56000, worst loss 0.276984 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:34:37,837 [INFO] Regularization: 5208.446289 * 0.0000010000 = 0.0052084462
2019-03-07 16:34:37,837 [INFO] Sum of grad norms: 0.506474
2019-03-07 16:34:37,838 [INFO] ---------------------------------
2019-03-07 16:34:57,418 [INFO] ---------------------------------
2019-03-07 16:34:57,419 [INFO] Summary:
2019-03-07 16:34:57,420 [INFO] Batch 57000, worst loss 0.203106 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:34:57,421 [INFO] Regularization: 5208.680176 * 0.0000010000 = 0.0052086799
2019-03-07 16:34:57,421 [INFO] Sum of grad norms: 4.968905
2019-03-07 16:34:57,422 [INFO] ---------------------------------
2019-03-07 16:35:16,689 [INFO] ---------------------------------
2019-03-07 16:35:16,690 [INFO] Summary:
2019-03-07 16:35:16,691 [INFO] Batch 58000, worst loss 0.272835 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:35:16,691 [INFO] Regularization: 5209.415527 * 0.0000010000 = 0.0052094157
2019-03-07 16:35:16,692 [INFO] Sum of grad norms: 0.303804
2019-03-07 16:35:16,693 [INFO] ---------------------------------
2019-03-07 16:35:36,068 [INFO] ---------------------------------
2019-03-07 16:35:36,069 [INFO] Summary:
2019-03-07 16:35:36,070 [INFO] Batch 59000, worst loss 0.259539 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:35:36,071 [INFO] Regularization: 5200.734375 * 0.0000010000 = 0.0052007344
2019-03-07 16:35:36,072 [INFO] Sum of grad norms: 17.357420
2019-03-07 16:35:36,073 [INFO] ---------------------------------
2019-03-07 16:35:55,378 [INFO] ---------------------------------
2019-03-07 16:35:55,379 [INFO] Summary:
2019-03-07 16:35:55,379 [INFO] Batch 60000, worst loss 0.263132 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:35:55,380 [INFO] Regularization: 5199.023926 * 0.0000010000 = 0.0051990240
2019-03-07 16:35:55,381 [INFO] Sum of grad norms: 0.340165
2019-03-07 16:35:55,382 [INFO] ---------------------------------
2019-03-07 16:36:08,449 [INFO] ---------------------------------
2019-03-07 16:36:08,450 [INFO] Evaluation:
2019-03-07 16:36:08,451 [INFO] Batch 60000, worst loss 0.201787 (without reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 16:36:08,452 [INFO] ---------------------------------
2019-03-07 16:36:28,092 [INFO] ---------------------------------
2019-03-07 16:36:28,093 [INFO] Summary:
2019-03-07 16:36:28,093 [INFO] Batch 61000, worst loss 0.187820 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:36:28,094 [INFO] Regularization: 5189.073242 * 0.0000010000 = 0.0051890733
2019-03-07 16:36:28,095 [INFO] Sum of grad norms: 5.493223
2019-03-07 16:36:28,096 [INFO] ---------------------------------
2019-03-07 16:36:47,511 [INFO] ---------------------------------
2019-03-07 16:36:47,512 [INFO] Summary:
2019-03-07 16:36:47,513 [INFO] Batch 62000, worst loss 0.248853 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:36:47,513 [INFO] Regularization: 5184.136719 * 0.0000010000 = 0.0051841368
2019-03-07 16:36:47,514 [INFO] Sum of grad norms: 0.168097
2019-03-07 16:36:47,515 [INFO] ---------------------------------
2019-03-07 16:37:06,634 [INFO] ---------------------------------
2019-03-07 16:37:06,635 [INFO] Summary:
2019-03-07 16:37:06,635 [INFO] Batch 63000, worst loss 0.230789 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:37:06,636 [INFO] Regularization: 5183.049316 * 0.0000010000 = 0.0051830495
2019-03-07 16:37:06,637 [INFO] Sum of grad norms: 1.267897
2019-03-07 16:37:06,638 [INFO] ---------------------------------
2019-03-07 16:37:26,098 [INFO] ---------------------------------
2019-03-07 16:37:26,099 [INFO] Summary:
2019-03-07 16:37:26,100 [INFO] Batch 64000, worst loss 0.205287 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:37:26,101 [INFO] Regularization: 5180.428223 * 0.0000010000 = 0.0051804283
2019-03-07 16:37:26,101 [INFO] Sum of grad norms: 0.372241
2019-03-07 16:37:26,102 [INFO] ---------------------------------
2019-03-07 16:37:45,610 [INFO] ---------------------------------
2019-03-07 16:37:45,611 [INFO] Summary:
2019-03-07 16:37:45,612 [INFO] Batch 65000, worst loss 0.148298 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:37:45,613 [INFO] Regularization: 5179.712891 * 0.0000010000 = 0.0051797130
2019-03-07 16:37:45,614 [INFO] Sum of grad norms: 0.198733
2019-03-07 16:37:45,615 [INFO] ---------------------------------
2019-03-07 16:38:05,237 [INFO] ---------------------------------
2019-03-07 16:38:05,238 [INFO] Summary:
2019-03-07 16:38:05,239 [INFO] Batch 66000, worst loss 0.211628 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:38:05,240 [INFO] Regularization: 5179.695801 * 0.0000010000 = 0.0051796958
2019-03-07 16:38:05,240 [INFO] Sum of grad norms: 3.500548
2019-03-07 16:38:05,241 [INFO] ---------------------------------
2019-03-07 16:38:24,743 [INFO] ---------------------------------
2019-03-07 16:38:24,744 [INFO] Summary:
2019-03-07 16:38:24,744 [INFO] Batch 67000, worst loss 0.191500 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:38:24,745 [INFO] Regularization: 5177.260254 * 0.0000010000 = 0.0051772604
2019-03-07 16:38:24,746 [INFO] Sum of grad norms: 6.950188
2019-03-07 16:38:24,746 [INFO] ---------------------------------
2019-03-07 16:38:44,043 [INFO] ---------------------------------
2019-03-07 16:38:44,044 [INFO] Summary:
2019-03-07 16:38:44,045 [INFO] Batch 68000, worst loss 0.167636 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:38:44,045 [INFO] Regularization: 5174.464355 * 0.0000010000 = 0.0051744645
2019-03-07 16:38:44,046 [INFO] Sum of grad norms: 3.982417
2019-03-07 16:38:44,047 [INFO] ---------------------------------
2019-03-07 16:39:03,073 [INFO] ---------------------------------
2019-03-07 16:39:03,074 [INFO] Summary:
2019-03-07 16:39:03,074 [INFO] Batch 69000, worst loss 0.197431 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:39:03,075 [INFO] Regularization: 5174.861816 * 0.0000010000 = 0.0051748618
2019-03-07 16:39:03,075 [INFO] Sum of grad norms: 1.401139
2019-03-07 16:39:03,076 [INFO] ---------------------------------
2019-03-07 16:39:22,272 [INFO] ---------------------------------
2019-03-07 16:39:22,272 [INFO] Summary:
2019-03-07 16:39:22,273 [INFO] Batch 70000, worst loss 0.167414 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:39:22,274 [INFO] Regularization: 5173.916504 * 0.0000010000 = 0.0051739165
2019-03-07 16:39:22,275 [INFO] Sum of grad norms: 3.470883
2019-03-07 16:39:22,276 [INFO] ---------------------------------
2019-03-07 16:39:35,404 [INFO] ---------------------------------
2019-03-07 16:39:35,405 [INFO] Evaluation:
2019-03-07 16:39:35,407 [INFO] Batch 70000, worst loss 0.238902 (without reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 16:39:35,408 [INFO] ---------------------------------
2019-03-07 16:39:54,700 [INFO] ---------------------------------
2019-03-07 16:39:54,701 [INFO] Summary:
2019-03-07 16:39:54,702 [INFO] Batch 71000, worst loss 0.192029 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:39:54,703 [INFO] Regularization: 5172.478516 * 0.0000010000 = 0.0051724785
2019-03-07 16:39:54,704 [INFO] Sum of grad norms: 2.889145
2019-03-07 16:39:54,705 [INFO] ---------------------------------
2019-03-07 16:40:14,168 [INFO] ---------------------------------
2019-03-07 16:40:14,169 [INFO] Summary:
2019-03-07 16:40:14,169 [INFO] Batch 72000, worst loss 0.180282 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:40:14,170 [INFO] Regularization: 5170.369141 * 0.0000010000 = 0.0051703691
2019-03-07 16:40:14,171 [INFO] Sum of grad norms: 3.918664
2019-03-07 16:40:14,171 [INFO] ---------------------------------
2019-03-07 16:40:33,677 [INFO] ---------------------------------
2019-03-07 16:40:33,678 [INFO] Summary:
2019-03-07 16:40:33,679 [INFO] Batch 73000, worst loss 0.187601 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:40:33,679 [INFO] Regularization: 5169.336426 * 0.0000010000 = 0.0051693362
2019-03-07 16:40:33,680 [INFO] Sum of grad norms: 6.580605
2019-03-07 16:40:33,680 [INFO] ---------------------------------
2019-03-07 16:40:52,975 [INFO] ---------------------------------
2019-03-07 16:40:52,976 [INFO] Summary:
2019-03-07 16:40:52,977 [INFO] Batch 74000, worst loss 0.190862 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:40:52,978 [INFO] Regularization: 5169.212891 * 0.0000010000 = 0.0051692128
2019-03-07 16:40:52,979 [INFO] Sum of grad norms: 1.966429
2019-03-07 16:40:52,980 [INFO] ---------------------------------
2019-03-07 16:41:12,297 [INFO] ---------------------------------
2019-03-07 16:41:12,298 [INFO] Summary:
2019-03-07 16:41:12,298 [INFO] Batch 75000, worst loss 0.208541 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:41:12,300 [INFO] Regularization: 5168.969727 * 0.0000010000 = 0.0051689697
2019-03-07 16:41:12,300 [INFO] Sum of grad norms: 0.197844
2019-03-07 16:41:12,301 [INFO] ---------------------------------
2019-03-07 16:41:31,748 [INFO] ---------------------------------
2019-03-07 16:41:31,749 [INFO] Summary:
2019-03-07 16:41:31,750 [INFO] Batch 76000, worst loss 0.177932 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:41:31,751 [INFO] Regularization: 5168.542480 * 0.0000010000 = 0.0051685423
2019-03-07 16:41:31,752 [INFO] Sum of grad norms: 13.934744
2019-03-07 16:41:31,753 [INFO] ---------------------------------
2019-03-07 16:41:51,226 [INFO] ---------------------------------
2019-03-07 16:41:51,227 [INFO] Summary:
2019-03-07 16:41:51,227 [INFO] Batch 77000, worst loss 0.187983 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:41:51,227 [INFO] Regularization: 5167.991211 * 0.0000010000 = 0.0051679914
2019-03-07 16:41:51,228 [INFO] Sum of grad norms: 3.660076
2019-03-07 16:41:51,229 [INFO] ---------------------------------
2019-03-07 16:42:10,693 [INFO] ---------------------------------
2019-03-07 16:42:10,694 [INFO] Summary:
2019-03-07 16:42:10,696 [INFO] Batch 78000, worst loss 0.182124 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:42:10,697 [INFO] Regularization: 5167.696289 * 0.0000010000 = 0.0051676962
2019-03-07 16:42:10,698 [INFO] Sum of grad norms: 2.883439
2019-03-07 16:42:10,699 [INFO] ---------------------------------
2019-03-07 16:42:30,178 [INFO] ---------------------------------
2019-03-07 16:42:30,179 [INFO] Summary:
2019-03-07 16:42:30,180 [INFO] Batch 79000, worst loss 0.229823 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:42:30,181 [INFO] Regularization: 5167.095703 * 0.0000010000 = 0.0051670959
2019-03-07 16:42:30,182 [INFO] Sum of grad norms: 2.461461
2019-03-07 16:42:30,182 [INFO] ---------------------------------
2019-03-07 16:42:49,843 [INFO] ---------------------------------
2019-03-07 16:42:49,844 [INFO] Summary:
2019-03-07 16:42:49,844 [INFO] Batch 80000, worst loss 0.226428 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:42:49,845 [INFO] Regularization: 5166.705078 * 0.0000010000 = 0.0051667052
2019-03-07 16:42:49,845 [INFO] Sum of grad norms: 1.970665
2019-03-07 16:42:49,846 [INFO] ---------------------------------
2019-03-07 16:43:02,930 [INFO] ---------------------------------
2019-03-07 16:43:02,934 [INFO] Evaluation:
2019-03-07 16:43:02,936 [INFO] Batch 80000, worst loss 0.148447 (without reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 16:43:02,938 [INFO] ---------------------------------
2019-03-07 16:43:22,653 [INFO] ---------------------------------
2019-03-07 16:43:22,654 [INFO] Summary:
2019-03-07 16:43:22,654 [INFO] Batch 81000, worst loss 0.177773 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:43:22,655 [INFO] Regularization: 5166.601074 * 0.0000010000 = 0.0051666009
2019-03-07 16:43:22,656 [INFO] Sum of grad norms: 4.110006
2019-03-07 16:43:22,657 [INFO] ---------------------------------
2019-03-07 16:43:41,587 [INFO] ---------------------------------
2019-03-07 16:43:41,588 [INFO] Summary:
2019-03-07 16:43:41,589 [INFO] Batch 82000, worst loss 0.193874 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:43:41,590 [INFO] Regularization: 5166.238281 * 0.0000010000 = 0.0051662382
2019-03-07 16:43:41,591 [INFO] Sum of grad norms: 2.494479
2019-03-07 16:43:41,591 [INFO] ---------------------------------
2019-03-07 16:44:00,752 [INFO] ---------------------------------
2019-03-07 16:44:00,753 [INFO] Summary:
2019-03-07 16:44:00,754 [INFO] Batch 83000, worst loss 0.190580 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:44:00,755 [INFO] Regularization: 5166.057617 * 0.0000010000 = 0.0051660575
2019-03-07 16:44:00,755 [INFO] Sum of grad norms: 0.172634
2019-03-07 16:44:00,756 [INFO] ---------------------------------
2019-03-07 16:44:20,336 [INFO] ---------------------------------
2019-03-07 16:44:20,337 [INFO] Summary:
2019-03-07 16:44:20,337 [INFO] Batch 84000, worst loss 0.180374 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:44:20,338 [INFO] Regularization: 5165.904297 * 0.0000010000 = 0.0051659043
2019-03-07 16:44:20,339 [INFO] Sum of grad norms: 7.932870
2019-03-07 16:44:20,340 [INFO] ---------------------------------
2019-03-07 16:44:39,577 [INFO] ---------------------------------
2019-03-07 16:44:39,578 [INFO] Summary:
2019-03-07 16:44:39,579 [INFO] Batch 85000, worst loss 0.190632 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:44:39,579 [INFO] Regularization: 5165.823730 * 0.0000010000 = 0.0051658237
2019-03-07 16:44:39,580 [INFO] Sum of grad norms: 4.155670
2019-03-07 16:44:39,581 [INFO] ---------------------------------
2019-03-07 16:44:59,165 [INFO] ---------------------------------
2019-03-07 16:44:59,166 [INFO] Summary:
2019-03-07 16:44:59,170 [INFO] Batch 86000, worst loss 0.182220 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:44:59,170 [INFO] Regularization: 5165.696777 * 0.0000010000 = 0.0051656966
2019-03-07 16:44:59,171 [INFO] Sum of grad norms: 8.145331
2019-03-07 16:44:59,172 [INFO] ---------------------------------
2019-03-07 16:45:18,693 [INFO] ---------------------------------
2019-03-07 16:45:18,694 [INFO] Summary:
2019-03-07 16:45:18,694 [INFO] Batch 87000, worst loss 0.272097 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:45:18,695 [INFO] Regularization: 5165.501465 * 0.0000010000 = 0.0051655015
2019-03-07 16:45:18,696 [INFO] Sum of grad norms: 0.735239
2019-03-07 16:45:18,697 [INFO] ---------------------------------
2019-03-07 16:45:38,189 [INFO] ---------------------------------
2019-03-07 16:45:38,189 [INFO] Summary:
2019-03-07 16:45:38,192 [INFO] Batch 88000, worst loss 0.230092 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:45:38,193 [INFO] Regularization: 5165.455566 * 0.0000010000 = 0.0051654554
2019-03-07 16:45:38,193 [INFO] Sum of grad norms: 0.270145
2019-03-07 16:45:38,194 [INFO] ---------------------------------
2019-03-07 16:45:57,713 [INFO] ---------------------------------
2019-03-07 16:45:57,714 [INFO] Summary:
2019-03-07 16:45:57,715 [INFO] Batch 89000, worst loss 0.227065 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:45:57,716 [INFO] Regularization: 5165.354980 * 0.0000010000 = 0.0051653548
2019-03-07 16:45:57,717 [INFO] Sum of grad norms: 0.633388
2019-03-07 16:45:57,719 [INFO] ---------------------------------
2019-03-07 16:46:16,911 [INFO] ---------------------------------
2019-03-07 16:46:16,912 [INFO] Summary:
2019-03-07 16:46:16,913 [INFO] Batch 90000, worst loss 0.249578 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:46:16,914 [INFO] Regularization: 5165.327637 * 0.0000010000 = 0.0051653278
2019-03-07 16:46:16,914 [INFO] Sum of grad norms: 1.618536
2019-03-07 16:46:16,915 [INFO] ---------------------------------
2019-03-07 16:46:29,954 [INFO] ---------------------------------
2019-03-07 16:46:29,955 [INFO] Evaluation:
2019-03-07 16:46:29,956 [INFO] Batch 90000, worst loss 0.160075 (without reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 16:46:29,956 [INFO] ---------------------------------
2019-03-07 16:46:49,385 [INFO] ---------------------------------
2019-03-07 16:46:49,386 [INFO] Summary:
2019-03-07 16:46:49,389 [INFO] Batch 91000, worst loss 0.161239 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:46:49,390 [INFO] Regularization: 5165.277832 * 0.0000010000 = 0.0051652780
2019-03-07 16:46:49,390 [INFO] Sum of grad norms: 0.258962
2019-03-07 16:46:49,391 [INFO] ---------------------------------
2019-03-07 16:47:08,895 [INFO] ---------------------------------
2019-03-07 16:47:08,895 [INFO] Summary:
2019-03-07 16:47:08,896 [INFO] Batch 92000, worst loss 0.301542 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:47:08,896 [INFO] Regularization: 5165.163574 * 0.0000010000 = 0.0051651634
2019-03-07 16:47:08,897 [INFO] Sum of grad norms: 0.323272
2019-03-07 16:47:08,898 [INFO] ---------------------------------
2019-03-07 16:47:28,483 [INFO] ---------------------------------
2019-03-07 16:47:28,484 [INFO] Summary:
2019-03-07 16:47:28,485 [INFO] Batch 93000, worst loss 0.173805 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:47:28,486 [INFO] Regularization: 5165.118652 * 0.0000010000 = 0.0051651187
2019-03-07 16:47:28,487 [INFO] Sum of grad norms: 0.200920
2019-03-07 16:47:28,488 [INFO] ---------------------------------
2019-03-07 16:47:47,868 [INFO] ---------------------------------
2019-03-07 16:47:47,869 [INFO] Summary:
2019-03-07 16:47:47,869 [INFO] Batch 94000, worst loss 0.253590 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:47:47,870 [INFO] Regularization: 5165.087891 * 0.0000010000 = 0.0051650880
2019-03-07 16:47:47,871 [INFO] Sum of grad norms: 0.221735
2019-03-07 16:47:47,872 [INFO] ---------------------------------
2019-03-07 16:48:07,386 [INFO] ---------------------------------
2019-03-07 16:48:07,387 [INFO] Summary:
2019-03-07 16:48:07,388 [INFO] Batch 95000, worst loss 0.166029 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:48:07,389 [INFO] Regularization: 5165.026855 * 0.0000010000 = 0.0051650270
2019-03-07 16:48:07,390 [INFO] Sum of grad norms: 0.154137
2019-03-07 16:48:07,391 [INFO] ---------------------------------
2019-03-07 16:48:26,848 [INFO] ---------------------------------
2019-03-07 16:48:26,849 [INFO] Summary:
2019-03-07 16:48:26,849 [INFO] Batch 96000, worst loss 0.181525 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:48:26,850 [INFO] Regularization: 5165.010254 * 0.0000010000 = 0.0051650102
2019-03-07 16:48:26,851 [INFO] Sum of grad norms: 21.650114
2019-03-07 16:48:26,852 [INFO] ---------------------------------
2019-03-07 16:48:46,170 [INFO] ---------------------------------
2019-03-07 16:48:46,171 [INFO] Summary:
2019-03-07 16:48:46,171 [INFO] Batch 97000, worst loss 0.198054 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:48:46,172 [INFO] Regularization: 5164.982422 * 0.0000010000 = 0.0051649823
2019-03-07 16:48:46,173 [INFO] Sum of grad norms: 0.219640
2019-03-07 16:48:46,174 [INFO] ---------------------------------
2019-03-07 16:49:05,526 [INFO] ---------------------------------
2019-03-07 16:49:05,527 [INFO] Summary:
2019-03-07 16:49:05,528 [INFO] Batch 98000, worst loss 0.198032 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:49:05,529 [INFO] Regularization: 5164.948730 * 0.0000010000 = 0.0051649488
2019-03-07 16:49:05,530 [INFO] Sum of grad norms: 5.072640
2019-03-07 16:49:05,530 [INFO] ---------------------------------
2019-03-07 16:49:24,556 [INFO] ---------------------------------
2019-03-07 16:49:24,557 [INFO] Summary:
2019-03-07 16:49:24,558 [INFO] Batch 99000, worst loss 0.194780 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:49:24,558 [INFO] Regularization: 5164.925293 * 0.0000010000 = 0.0051649255
2019-03-07 16:49:24,559 [INFO] Sum of grad norms: 0.154173
2019-03-07 16:49:24,560 [INFO] ---------------------------------
2019-03-07 16:49:44,079 [INFO] ---------------------------------
2019-03-07 16:49:44,080 [INFO] Summary:
2019-03-07 16:49:44,081 [INFO] Batch 100000, worst loss 0.222866 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:49:44,082 [INFO] Regularization: 5164.857910 * 0.0000010000 = 0.0051648580
2019-03-07 16:49:44,082 [INFO] Sum of grad norms: 5.930615
2019-03-07 16:49:44,083 [INFO] ---------------------------------
2019-03-07 16:49:57,188 [INFO] ---------------------------------
2019-03-07 16:49:57,189 [INFO] Evaluation:
2019-03-07 16:49:57,190 [INFO] Batch 100000, worst loss 0.223132 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:49:57,193 [INFO] ---------------------------------
2019-03-07 16:50:16,575 [INFO] ---------------------------------
2019-03-07 16:50:16,575 [INFO] Summary:
2019-03-07 16:50:16,576 [INFO] Batch 101000, worst loss 0.187188 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:50:16,576 [INFO] Regularization: 5164.809570 * 0.0000010000 = 0.0051648095
2019-03-07 16:50:16,577 [INFO] Sum of grad norms: 8.980337
2019-03-07 16:50:16,577 [INFO] ---------------------------------
2019-03-07 16:50:36,054 [INFO] ---------------------------------
2019-03-07 16:50:36,055 [INFO] Summary:
2019-03-07 16:50:36,056 [INFO] Batch 102000, worst loss 0.158061 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:50:36,057 [INFO] Regularization: 5164.776855 * 0.0000010000 = 0.0051647769
2019-03-07 16:50:36,058 [INFO] Sum of grad norms: 0.425846
2019-03-07 16:50:36,059 [INFO] ---------------------------------
2019-03-07 16:50:55,268 [INFO] ---------------------------------
2019-03-07 16:50:55,269 [INFO] Summary:
2019-03-07 16:50:55,270 [INFO] Batch 103000, worst loss 0.158010 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:50:55,270 [INFO] Regularization: 5164.770996 * 0.0000010000 = 0.0051647709
2019-03-07 16:50:55,272 [INFO] Sum of grad norms: 1.910451
2019-03-07 16:50:55,272 [INFO] ---------------------------------
2019-03-07 16:51:14,531 [INFO] ---------------------------------
2019-03-07 16:51:14,532 [INFO] Summary:
2019-03-07 16:51:14,533 [INFO] Batch 104000, worst loss 0.183129 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:51:14,536 [INFO] Regularization: 5164.762207 * 0.0000010000 = 0.0051647620
2019-03-07 16:51:14,537 [INFO] Sum of grad norms: 3.514339
2019-03-07 16:51:14,538 [INFO] ---------------------------------
2019-03-07 16:51:33,816 [INFO] ---------------------------------
2019-03-07 16:51:33,817 [INFO] Summary:
2019-03-07 16:51:33,817 [INFO] Batch 105000, worst loss 0.199612 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:51:33,818 [INFO] Regularization: 5164.763184 * 0.0000010000 = 0.0051647630
2019-03-07 16:51:33,819 [INFO] Sum of grad norms: 2.411152
2019-03-07 16:51:33,820 [INFO] ---------------------------------
2019-03-07 16:51:52,955 [INFO] ---------------------------------
2019-03-07 16:51:52,956 [INFO] Summary:
2019-03-07 16:51:52,957 [INFO] Batch 106000, worst loss 0.191703 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:51:52,957 [INFO] Regularization: 5164.741699 * 0.0000010000 = 0.0051647415
2019-03-07 16:51:52,958 [INFO] Sum of grad norms: 17.425520
2019-03-07 16:51:52,960 [INFO] ---------------------------------
2019-03-07 16:52:12,426 [INFO] ---------------------------------
2019-03-07 16:52:12,427 [INFO] Summary:
2019-03-07 16:52:12,427 [INFO] Batch 107000, worst loss 0.179161 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:52:12,428 [INFO] Regularization: 5164.733398 * 0.0000010000 = 0.0051647332
2019-03-07 16:52:12,429 [INFO] Sum of grad norms: 1.949938
2019-03-07 16:52:12,430 [INFO] ---------------------------------
2019-03-07 16:52:31,679 [INFO] ---------------------------------
2019-03-07 16:52:31,680 [INFO] Summary:
2019-03-07 16:52:31,681 [INFO] Batch 108000, worst loss 0.203509 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:52:31,681 [INFO] Regularization: 5164.735352 * 0.0000010000 = 0.0051647355
2019-03-07 16:52:31,682 [INFO] Sum of grad norms: 0.329599
2019-03-07 16:52:31,683 [INFO] ---------------------------------
2019-03-07 16:52:51,236 [INFO] ---------------------------------
2019-03-07 16:52:51,238 [INFO] Summary:
2019-03-07 16:52:51,238 [INFO] Batch 109000, worst loss 0.181582 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:52:51,239 [INFO] Regularization: 5164.726074 * 0.0000010000 = 0.0051647262
2019-03-07 16:52:51,240 [INFO] Sum of grad norms: 3.378237
2019-03-07 16:52:51,241 [INFO] ---------------------------------
2019-03-07 16:53:10,893 [INFO] ---------------------------------
2019-03-07 16:53:10,894 [INFO] Summary:
2019-03-07 16:53:10,895 [INFO] Batch 110000, worst loss 0.181406 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:53:10,896 [INFO] Regularization: 5164.719238 * 0.0000010000 = 0.0051647192
2019-03-07 16:53:10,897 [INFO] Sum of grad norms: 0.294763
2019-03-07 16:53:10,897 [INFO] ---------------------------------
2019-03-07 16:53:24,572 [INFO] ---------------------------------
2019-03-07 16:53:24,573 [INFO] Evaluation:
2019-03-07 16:53:24,573 [INFO] Batch 110000, worst loss 0.182319 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:53:24,575 [INFO] ---------------------------------
2019-03-07 16:53:44,042 [INFO] ---------------------------------
2019-03-07 16:53:44,043 [INFO] Summary:
2019-03-07 16:53:44,043 [INFO] Batch 111000, worst loss 0.187471 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:53:44,044 [INFO] Regularization: 5164.715332 * 0.0000010000 = 0.0051647155
2019-03-07 16:53:44,045 [INFO] Sum of grad norms: 2.391309
2019-03-07 16:53:44,045 [INFO] ---------------------------------
2019-03-07 16:54:03,249 [INFO] ---------------------------------
2019-03-07 16:54:03,250 [INFO] Summary:
2019-03-07 16:54:03,251 [INFO] Batch 112000, worst loss 0.203674 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:54:03,251 [INFO] Regularization: 5164.709473 * 0.0000010000 = 0.0051647094
2019-03-07 16:54:03,252 [INFO] Sum of grad norms: 0.169071
2019-03-07 16:54:03,253 [INFO] ---------------------------------
2019-03-07 16:54:22,714 [INFO] ---------------------------------
2019-03-07 16:54:22,715 [INFO] Summary:
2019-03-07 16:54:22,716 [INFO] Batch 113000, worst loss 0.178932 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:54:22,716 [INFO] Regularization: 5164.708008 * 0.0000010000 = 0.0051647080
2019-03-07 16:54:22,717 [INFO] Sum of grad norms: 3.253585
2019-03-07 16:54:22,718 [INFO] ---------------------------------
2019-03-07 16:54:42,074 [INFO] ---------------------------------
2019-03-07 16:54:42,075 [INFO] Summary:
2019-03-07 16:54:42,076 [INFO] Batch 114000, worst loss 0.178934 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:54:42,077 [INFO] Regularization: 5164.708496 * 0.0000010000 = 0.0051647085
2019-03-07 16:54:42,078 [INFO] Sum of grad norms: 4.354023
2019-03-07 16:54:42,079 [INFO] ---------------------------------
2019-03-07 16:55:01,320 [INFO] ---------------------------------
2019-03-07 16:55:01,321 [INFO] Summary:
2019-03-07 16:55:01,321 [INFO] Batch 115000, worst loss 0.195078 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:55:01,322 [INFO] Regularization: 5164.705566 * 0.0000010000 = 0.0051647057
2019-03-07 16:55:01,323 [INFO] Sum of grad norms: 4.034073
2019-03-07 16:55:01,324 [INFO] ---------------------------------
2019-03-07 16:55:21,113 [INFO] ---------------------------------
2019-03-07 16:55:21,114 [INFO] Summary:
2019-03-07 16:55:21,114 [INFO] Batch 116000, worst loss 0.206657 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:55:21,115 [INFO] Regularization: 5164.701172 * 0.0000010000 = 0.0051647010
2019-03-07 16:55:21,116 [INFO] Sum of grad norms: 0.349110
2019-03-07 16:55:21,117 [INFO] ---------------------------------
2019-03-07 16:55:40,641 [INFO] ---------------------------------
2019-03-07 16:55:40,642 [INFO] Summary:
2019-03-07 16:55:40,643 [INFO] Batch 117000, worst loss 0.206668 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:55:40,643 [INFO] Regularization: 5164.700195 * 0.0000010000 = 0.0051647001
2019-03-07 16:55:40,644 [INFO] Sum of grad norms: 0.546332
2019-03-07 16:55:40,645 [INFO] ---------------------------------
2019-03-07 16:55:59,732 [INFO] ---------------------------------
2019-03-07 16:55:59,733 [INFO] Summary:
2019-03-07 16:55:59,734 [INFO] Batch 118000, worst loss 0.234175 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:55:59,735 [INFO] Regularization: 5164.701660 * 0.0000010000 = 0.0051647015
2019-03-07 16:55:59,736 [INFO] Sum of grad norms: 0.950620
2019-03-07 16:55:59,736 [INFO] ---------------------------------
2019-03-07 16:56:19,393 [INFO] ---------------------------------
2019-03-07 16:56:19,395 [INFO] Summary:
2019-03-07 16:56:19,395 [INFO] Batch 119000, worst loss 0.180024 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:56:19,396 [INFO] Regularization: 5164.696289 * 0.0000010000 = 0.0051646964
2019-03-07 16:56:19,396 [INFO] Sum of grad norms: 7.426789
2019-03-07 16:56:19,397 [INFO] ---------------------------------
2019-03-07 16:56:38,823 [INFO] ---------------------------------
2019-03-07 16:56:38,824 [INFO] Summary:
2019-03-07 16:56:38,825 [INFO] Batch 120000, worst loss 0.193188 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:56:38,826 [INFO] Regularization: 5164.695312 * 0.0000010000 = 0.0051646954
2019-03-07 16:56:38,827 [INFO] Sum of grad norms: 9.541392
2019-03-07 16:56:38,827 [INFO] ---------------------------------
2019-03-07 16:56:51,930 [INFO] ---------------------------------
2019-03-07 16:56:51,931 [INFO] Evaluation:
2019-03-07 16:56:51,932 [INFO] Batch 120000, worst loss 0.177544 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:56:51,934 [INFO] ---------------------------------
2019-03-07 16:57:11,270 [INFO] ---------------------------------
2019-03-07 16:57:11,271 [INFO] Summary:
2019-03-07 16:57:11,271 [INFO] Batch 121000, worst loss 0.185293 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:57:11,272 [INFO] Regularization: 5164.694336 * 0.0000010000 = 0.0051646945
2019-03-07 16:57:11,273 [INFO] Sum of grad norms: 8.837991
2019-03-07 16:57:11,274 [INFO] ---------------------------------
2019-03-07 16:57:30,716 [INFO] ---------------------------------
2019-03-07 16:57:30,717 [INFO] Summary:
2019-03-07 16:57:30,718 [INFO] Batch 122000, worst loss 0.166817 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:57:30,719 [INFO] Regularization: 5164.693359 * 0.0000010000 = 0.0051646936
2019-03-07 16:57:30,720 [INFO] Sum of grad norms: 0.288304
2019-03-07 16:57:30,721 [INFO] ---------------------------------
2019-03-07 16:57:50,325 [INFO] ---------------------------------
2019-03-07 16:57:50,326 [INFO] Summary:
2019-03-07 16:57:50,326 [INFO] Batch 123000, worst loss 0.198055 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:57:50,327 [INFO] Regularization: 5164.693359 * 0.0000010000 = 0.0051646936
2019-03-07 16:57:50,328 [INFO] Sum of grad norms: 2.582595
2019-03-07 16:57:50,329 [INFO] ---------------------------------
2019-03-07 16:58:10,169 [INFO] ---------------------------------
2019-03-07 16:58:10,171 [INFO] Summary:
2019-03-07 16:58:10,171 [INFO] Batch 124000, worst loss 0.232227 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:58:10,172 [INFO] Regularization: 5164.692871 * 0.0000010000 = 0.0051646926
2019-03-07 16:58:10,173 [INFO] Sum of grad norms: 4.220412
2019-03-07 16:58:10,173 [INFO] ---------------------------------
2019-03-07 16:58:29,909 [INFO] ---------------------------------
2019-03-07 16:58:29,911 [INFO] Summary:
2019-03-07 16:58:29,912 [INFO] Batch 125000, worst loss 0.232226 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:58:29,912 [INFO] Regularization: 5164.692383 * 0.0000010000 = 0.0051646922
2019-03-07 16:58:29,913 [INFO] Sum of grad norms: 2.981687
2019-03-07 16:58:29,914 [INFO] ---------------------------------
2019-03-07 16:58:49,583 [INFO] ---------------------------------
2019-03-07 16:58:49,584 [INFO] Summary:
2019-03-07 16:58:49,584 [INFO] Batch 126000, worst loss 0.225304 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:58:49,585 [INFO] Regularization: 5164.692383 * 0.0000010000 = 0.0051646922
2019-03-07 16:58:49,586 [INFO] Sum of grad norms: 8.209631
2019-03-07 16:58:49,590 [INFO] ---------------------------------
2019-03-07 16:59:08,910 [INFO] ---------------------------------
2019-03-07 16:59:08,911 [INFO] Summary:
2019-03-07 16:59:08,912 [INFO] Batch 127000, worst loss 0.225304 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:59:08,912 [INFO] Regularization: 5164.691895 * 0.0000010000 = 0.0051646917
2019-03-07 16:59:08,913 [INFO] Sum of grad norms: 11.379389
2019-03-07 16:59:08,913 [INFO] ---------------------------------
2019-03-07 16:59:28,349 [INFO] ---------------------------------
2019-03-07 16:59:28,350 [INFO] Summary:
2019-03-07 16:59:28,351 [INFO] Batch 128000, worst loss 0.184074 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:59:28,351 [INFO] Regularization: 5164.691895 * 0.0000010000 = 0.0051646917
2019-03-07 16:59:28,352 [INFO] Sum of grad norms: 0.165154
2019-03-07 16:59:28,353 [INFO] ---------------------------------
2019-03-07 16:59:47,831 [INFO] ---------------------------------
2019-03-07 16:59:47,832 [INFO] Summary:
2019-03-07 16:59:47,833 [INFO] Batch 129000, worst loss 0.185983 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 16:59:47,833 [INFO] Regularization: 5164.691406 * 0.0000010000 = 0.0051646912
2019-03-07 16:59:47,834 [INFO] Sum of grad norms: 0.509874
2019-03-07 16:59:47,834 [INFO] ---------------------------------
2019-03-07 17:00:07,103 [INFO] ---------------------------------
2019-03-07 17:00:07,104 [INFO] Summary:
2019-03-07 17:00:07,104 [INFO] Batch 130000, worst loss 0.190597 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:00:07,105 [INFO] Regularization: 5164.691406 * 0.0000010000 = 0.0051646912
2019-03-07 17:00:07,106 [INFO] Sum of grad norms: 0.578530
2019-03-07 17:00:07,107 [INFO] ---------------------------------
2019-03-07 17:00:20,257 [INFO] ---------------------------------
2019-03-07 17:00:20,258 [INFO] Evaluation:
2019-03-07 17:00:20,259 [INFO] Batch 130000, worst loss 0.189363 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:00:20,260 [INFO] ---------------------------------
2019-03-07 17:00:39,477 [INFO] ---------------------------------
2019-03-07 17:00:39,478 [INFO] Summary:
2019-03-07 17:00:39,478 [INFO] Batch 131000, worst loss 0.198821 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:00:39,479 [INFO] Regularization: 5164.691406 * 0.0000010000 = 0.0051646912
2019-03-07 17:00:39,480 [INFO] Sum of grad norms: 0.278073
2019-03-07 17:00:39,481 [INFO] ---------------------------------
2019-03-07 17:00:58,891 [INFO] ---------------------------------
2019-03-07 17:00:58,892 [INFO] Summary:
2019-03-07 17:00:58,893 [INFO] Batch 132000, worst loss 0.198821 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:00:58,894 [INFO] Regularization: 5164.690918 * 0.0000010000 = 0.0051646908
2019-03-07 17:00:58,894 [INFO] Sum of grad norms: 0.989371
2019-03-07 17:00:58,895 [INFO] ---------------------------------
2019-03-07 17:01:18,224 [INFO] ---------------------------------
2019-03-07 17:01:18,225 [INFO] Summary:
2019-03-07 17:01:18,226 [INFO] Batch 133000, worst loss 0.173252 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:01:18,226 [INFO] Regularization: 5164.690918 * 0.0000010000 = 0.0051646908
2019-03-07 17:01:18,227 [INFO] Sum of grad norms: 2.293998
2019-03-07 17:01:18,228 [INFO] ---------------------------------
2019-03-07 17:01:37,468 [INFO] ---------------------------------
2019-03-07 17:01:37,469 [INFO] Summary:
2019-03-07 17:01:37,470 [INFO] Batch 134000, worst loss 0.187857 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:01:37,471 [INFO] Regularization: 5164.690918 * 0.0000010000 = 0.0051646908
2019-03-07 17:01:37,472 [INFO] Sum of grad norms: 0.186186
2019-03-07 17:01:37,472 [INFO] ---------------------------------
2019-03-07 17:01:56,949 [INFO] ---------------------------------
2019-03-07 17:01:56,950 [INFO] Summary:
2019-03-07 17:01:56,951 [INFO] Batch 135000, worst loss 0.187857 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:01:56,954 [INFO] Regularization: 5164.690918 * 0.0000010000 = 0.0051646908
2019-03-07 17:01:56,955 [INFO] Sum of grad norms: 2.115819
2019-03-07 17:01:56,956 [INFO] ---------------------------------
2019-03-07 17:02:16,682 [INFO] ---------------------------------
2019-03-07 17:02:16,683 [INFO] Summary:
2019-03-07 17:02:16,684 [INFO] Batch 136000, worst loss 0.167547 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:02:16,684 [INFO] Regularization: 5164.690918 * 0.0000010000 = 0.0051646908
2019-03-07 17:02:16,685 [INFO] Sum of grad norms: 0.448750
2019-03-07 17:02:16,686 [INFO] ---------------------------------
2019-03-07 17:02:36,166 [INFO] ---------------------------------
2019-03-07 17:02:36,167 [INFO] Summary:
2019-03-07 17:02:36,168 [INFO] Batch 137000, worst loss 0.187379 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:02:36,169 [INFO] Regularization: 5164.690918 * 0.0000010000 = 0.0051646908
2019-03-07 17:02:36,170 [INFO] Sum of grad norms: 1.377679
2019-03-07 17:02:36,171 [INFO] ---------------------------------
2019-03-07 17:02:55,770 [INFO] ---------------------------------
2019-03-07 17:02:55,771 [INFO] Summary:
2019-03-07 17:02:55,772 [INFO] Batch 138000, worst loss 0.163125 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:02:55,773 [INFO] Regularization: 5164.690430 * 0.0000010000 = 0.0051646903
2019-03-07 17:02:55,774 [INFO] Sum of grad norms: 0.251865
2019-03-07 17:02:55,774 [INFO] ---------------------------------
2019-03-07 17:03:15,162 [INFO] ---------------------------------
2019-03-07 17:03:15,163 [INFO] Summary:
2019-03-07 17:03:15,165 [INFO] Batch 139000, worst loss 0.198762 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:03:15,165 [INFO] Regularization: 5164.690430 * 0.0000010000 = 0.0051646903
2019-03-07 17:03:15,166 [INFO] Sum of grad norms: 18.264154
2019-03-07 17:03:15,167 [INFO] ---------------------------------
2019-03-07 17:03:34,914 [INFO] ---------------------------------
2019-03-07 17:03:34,915 [INFO] Summary:
2019-03-07 17:03:34,916 [INFO] Batch 140000, worst loss 0.233999 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:03:34,917 [INFO] Regularization: 5164.690430 * 0.0000010000 = 0.0051646903
2019-03-07 17:03:34,918 [INFO] Sum of grad norms: 4.227551
2019-03-07 17:03:34,918 [INFO] ---------------------------------
2019-03-07 17:03:48,441 [INFO] ---------------------------------
2019-03-07 17:03:48,443 [INFO] Evaluation:
2019-03-07 17:03:48,444 [INFO] Batch 140000, worst loss 0.184939 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:03:48,444 [INFO] ---------------------------------
2019-03-07 17:04:07,994 [INFO] ---------------------------------
2019-03-07 17:04:07,995 [INFO] Summary:
2019-03-07 17:04:07,998 [INFO] Batch 141000, worst loss 0.190104 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:04:07,999 [INFO] Regularization: 5164.690430 * 0.0000010000 = 0.0051646903
2019-03-07 17:04:08,000 [INFO] Sum of grad norms: 0.189966
2019-03-07 17:04:08,001 [INFO] ---------------------------------
2019-03-07 17:04:27,501 [INFO] ---------------------------------
2019-03-07 17:04:27,502 [INFO] Summary:
2019-03-07 17:04:27,503 [INFO] Batch 142000, worst loss 0.189750 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:04:27,504 [INFO] Regularization: 5164.690430 * 0.0000010000 = 0.0051646903
2019-03-07 17:04:27,505 [INFO] Sum of grad norms: 2.955717
2019-03-07 17:04:27,506 [INFO] ---------------------------------
2019-03-07 17:04:46,825 [INFO] ---------------------------------
2019-03-07 17:04:46,825 [INFO] Summary:
2019-03-07 17:04:46,826 [INFO] Batch 143000, worst loss 0.207454 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:04:46,827 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:04:46,828 [INFO] Sum of grad norms: 10.435127
2019-03-07 17:04:46,828 [INFO] ---------------------------------
2019-03-07 17:05:06,436 [INFO] ---------------------------------
2019-03-07 17:05:06,437 [INFO] Summary:
2019-03-07 17:05:06,438 [INFO] Batch 144000, worst loss 0.209790 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:05:06,439 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:05:06,440 [INFO] Sum of grad norms: 0.600800
2019-03-07 17:05:06,441 [INFO] ---------------------------------
2019-03-07 17:05:25,995 [INFO] ---------------------------------
2019-03-07 17:05:25,996 [INFO] Summary:
2019-03-07 17:05:25,998 [INFO] Batch 145000, worst loss 0.209790 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:05:25,999 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:05:25,999 [INFO] Sum of grad norms: 10.832943
2019-03-07 17:05:26,000 [INFO] ---------------------------------
2019-03-07 17:05:45,415 [INFO] ---------------------------------
2019-03-07 17:05:45,415 [INFO] Summary:
2019-03-07 17:05:45,416 [INFO] Batch 146000, worst loss 0.168969 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:05:45,416 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:05:45,417 [INFO] Sum of grad norms: 7.859115
2019-03-07 17:05:45,417 [INFO] ---------------------------------
2019-03-07 17:06:05,033 [INFO] ---------------------------------
2019-03-07 17:06:05,034 [INFO] Summary:
2019-03-07 17:06:05,035 [INFO] Batch 147000, worst loss 0.193533 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:06:05,036 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:06:05,036 [INFO] Sum of grad norms: 4.122905
2019-03-07 17:06:05,037 [INFO] ---------------------------------
2019-03-07 17:06:24,433 [INFO] ---------------------------------
2019-03-07 17:06:24,434 [INFO] Summary:
2019-03-07 17:06:24,434 [INFO] Batch 148000, worst loss 0.229193 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:06:24,435 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:06:24,436 [INFO] Sum of grad norms: 0.286477
2019-03-07 17:06:24,436 [INFO] ---------------------------------
2019-03-07 17:06:43,794 [INFO] ---------------------------------
2019-03-07 17:06:43,795 [INFO] Summary:
2019-03-07 17:06:43,795 [INFO] Batch 149000, worst loss 0.229193 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:06:43,796 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:06:43,797 [INFO] Sum of grad norms: 0.571347
2019-03-07 17:06:43,797 [INFO] ---------------------------------
2019-03-07 17:07:03,022 [INFO] ---------------------------------
2019-03-07 17:07:03,024 [INFO] Summary:
2019-03-07 17:07:03,024 [INFO] Batch 150000, worst loss 0.211296 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:07:03,025 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:07:03,027 [INFO] Sum of grad norms: 24.879450
2019-03-07 17:07:03,028 [INFO] ---------------------------------
2019-03-07 17:07:16,251 [INFO] ---------------------------------
2019-03-07 17:07:16,253 [INFO] Evaluation:
2019-03-07 17:07:16,254 [INFO] Batch 150000, worst loss 0.203829 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:07:16,255 [INFO] ---------------------------------
2019-03-07 17:07:35,826 [INFO] ---------------------------------
2019-03-07 17:07:35,827 [INFO] Summary:
2019-03-07 17:07:35,827 [INFO] Batch 151000, worst loss 0.220769 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:07:35,828 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:07:35,829 [INFO] Sum of grad norms: 0.374013
2019-03-07 17:07:35,829 [INFO] ---------------------------------
2019-03-07 17:07:55,346 [INFO] ---------------------------------
2019-03-07 17:07:55,347 [INFO] Summary:
2019-03-07 17:07:55,347 [INFO] Batch 152000, worst loss 0.198978 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:07:55,348 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:07:55,348 [INFO] Sum of grad norms: 10.715062
2019-03-07 17:07:55,349 [INFO] ---------------------------------
2019-03-07 17:08:14,781 [INFO] ---------------------------------
2019-03-07 17:08:14,783 [INFO] Summary:
2019-03-07 17:08:14,783 [INFO] Batch 153000, worst loss 0.181649 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:08:14,784 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:08:14,785 [INFO] Sum of grad norms: 0.219809
2019-03-07 17:08:14,786 [INFO] ---------------------------------
2019-03-07 17:08:34,615 [INFO] ---------------------------------
2019-03-07 17:08:34,616 [INFO] Summary:
2019-03-07 17:08:34,616 [INFO] Batch 154000, worst loss 0.186120 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:08:34,617 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:08:34,617 [INFO] Sum of grad norms: 0.092856
2019-03-07 17:08:34,618 [INFO] ---------------------------------
2019-03-07 17:08:54,029 [INFO] ---------------------------------
2019-03-07 17:08:54,029 [INFO] Summary:
2019-03-07 17:08:54,030 [INFO] Batch 155000, worst loss 0.186464 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:08:54,031 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:08:54,032 [INFO] Sum of grad norms: 0.269046
2019-03-07 17:08:54,033 [INFO] ---------------------------------
2019-03-07 17:09:13,493 [INFO] ---------------------------------
2019-03-07 17:09:13,494 [INFO] Summary:
2019-03-07 17:09:13,495 [INFO] Batch 156000, worst loss 0.173480 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:09:13,495 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:09:13,496 [INFO] Sum of grad norms: 0.316153
2019-03-07 17:09:13,497 [INFO] ---------------------------------
2019-03-07 17:09:33,135 [INFO] ---------------------------------
2019-03-07 17:09:33,136 [INFO] Summary:
2019-03-07 17:09:33,137 [INFO] Batch 157000, worst loss 0.183746 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:09:33,138 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:09:33,138 [INFO] Sum of grad norms: 0.602259
2019-03-07 17:09:33,139 [INFO] ---------------------------------
2019-03-07 17:09:52,367 [INFO] ---------------------------------
2019-03-07 17:09:52,367 [INFO] Summary:
2019-03-07 17:09:52,368 [INFO] Batch 158000, worst loss 0.183746 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:09:52,368 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:09:52,369 [INFO] Sum of grad norms: 3.257333
2019-03-07 17:09:52,370 [INFO] ---------------------------------
2019-03-07 17:10:11,819 [INFO] ---------------------------------
2019-03-07 17:10:11,820 [INFO] Summary:
2019-03-07 17:10:11,821 [INFO] Batch 159000, worst loss 0.205435 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:10:11,824 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:10:11,825 [INFO] Sum of grad norms: 8.379195
2019-03-07 17:10:11,826 [INFO] ---------------------------------
2019-03-07 17:10:31,256 [INFO] ---------------------------------
2019-03-07 17:10:31,257 [INFO] Summary:
2019-03-07 17:10:31,257 [INFO] Batch 160000, worst loss 0.280193 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:10:31,258 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:10:31,258 [INFO] Sum of grad norms: 0.264337
2019-03-07 17:10:31,259 [INFO] ---------------------------------
2019-03-07 17:10:44,338 [INFO] ---------------------------------
2019-03-07 17:10:44,339 [INFO] Evaluation:
2019-03-07 17:10:44,339 [INFO] Batch 160000, worst loss 0.275029 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:10:44,340 [INFO] ---------------------------------
2019-03-07 17:11:03,877 [INFO] ---------------------------------
2019-03-07 17:11:03,878 [INFO] Summary:
2019-03-07 17:11:03,879 [INFO] Batch 161000, worst loss 0.171506 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:11:03,880 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:11:03,881 [INFO] Sum of grad norms: 0.475979
2019-03-07 17:11:03,881 [INFO] ---------------------------------
2019-03-07 17:11:23,449 [INFO] ---------------------------------
2019-03-07 17:11:23,450 [INFO] Summary:
2019-03-07 17:11:23,451 [INFO] Batch 162000, worst loss 0.173963 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:11:23,451 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:11:23,452 [INFO] Sum of grad norms: 0.266962
2019-03-07 17:11:23,453 [INFO] ---------------------------------
2019-03-07 17:11:43,064 [INFO] ---------------------------------
2019-03-07 17:11:43,065 [INFO] Summary:
2019-03-07 17:11:43,066 [INFO] Batch 163000, worst loss 0.198237 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:11:43,066 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:11:43,067 [INFO] Sum of grad norms: 10.063651
2019-03-07 17:11:43,068 [INFO] ---------------------------------
2019-03-07 17:12:02,289 [INFO] ---------------------------------
2019-03-07 17:12:02,290 [INFO] Summary:
2019-03-07 17:12:02,291 [INFO] Batch 164000, worst loss 0.194318 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:12:02,292 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:12:02,293 [INFO] Sum of grad norms: 18.530092
2019-03-07 17:12:02,293 [INFO] ---------------------------------
2019-03-07 17:12:21,728 [INFO] ---------------------------------
2019-03-07 17:12:21,729 [INFO] Summary:
2019-03-07 17:12:21,731 [INFO] Batch 165000, worst loss 0.192395 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:12:21,731 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:12:21,732 [INFO] Sum of grad norms: 8.706350
2019-03-07 17:12:21,733 [INFO] ---------------------------------
2019-03-07 17:12:41,295 [INFO] ---------------------------------
2019-03-07 17:12:41,296 [INFO] Summary:
2019-03-07 17:12:41,297 [INFO] Batch 166000, worst loss 0.168492 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:12:41,297 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:12:41,299 [INFO] Sum of grad norms: 0.498036
2019-03-07 17:12:41,300 [INFO] ---------------------------------
2019-03-07 17:13:00,638 [INFO] ---------------------------------
2019-03-07 17:13:00,639 [INFO] Summary:
2019-03-07 17:13:00,640 [INFO] Batch 167000, worst loss 0.179016 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:13:00,641 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:13:00,641 [INFO] Sum of grad norms: 0.150844
2019-03-07 17:13:00,642 [INFO] ---------------------------------
2019-03-07 17:13:20,529 [INFO] ---------------------------------
2019-03-07 17:13:20,530 [INFO] Summary:
2019-03-07 17:13:20,530 [INFO] Batch 168000, worst loss 0.204140 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:13:20,531 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:13:20,532 [INFO] Sum of grad norms: 6.184736
2019-03-07 17:13:20,533 [INFO] ---------------------------------
2019-03-07 17:13:41,814 [INFO] ---------------------------------
2019-03-07 17:13:41,815 [INFO] Summary:
2019-03-07 17:13:41,815 [INFO] Batch 169000, worst loss 0.226015 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:13:41,816 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:13:41,817 [INFO] Sum of grad norms: 5.849566
2019-03-07 17:13:41,818 [INFO] ---------------------------------
2019-03-07 17:14:01,482 [INFO] ---------------------------------
2019-03-07 17:14:01,483 [INFO] Summary:
2019-03-07 17:14:01,483 [INFO] Batch 170000, worst loss 0.205921 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:14:01,484 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:14:01,485 [INFO] Sum of grad norms: 11.783415
2019-03-07 17:14:01,485 [INFO] ---------------------------------
2019-03-07 17:14:15,078 [INFO] ---------------------------------
2019-03-07 17:14:15,079 [INFO] Evaluation:
2019-03-07 17:14:15,081 [INFO] Batch 170000, worst loss 0.190628 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:14:15,082 [INFO] ---------------------------------
2019-03-07 17:14:34,568 [INFO] ---------------------------------
2019-03-07 17:14:34,569 [INFO] Summary:
2019-03-07 17:14:34,569 [INFO] Batch 171000, worst loss 0.186143 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:14:34,570 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:14:34,571 [INFO] Sum of grad norms: 0.323953
2019-03-07 17:14:34,572 [INFO] ---------------------------------
2019-03-07 17:14:53,775 [INFO] ---------------------------------
2019-03-07 17:14:53,776 [INFO] Summary:
2019-03-07 17:14:53,777 [INFO] Batch 172000, worst loss 0.200254 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:14:53,777 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:14:53,778 [INFO] Sum of grad norms: 21.351545
2019-03-07 17:14:53,779 [INFO] ---------------------------------
2019-03-07 17:15:13,084 [INFO] ---------------------------------
2019-03-07 17:15:13,085 [INFO] Summary:
2019-03-07 17:15:13,086 [INFO] Batch 173000, worst loss 0.200254 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:15:13,086 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:15:13,087 [INFO] Sum of grad norms: 0.498605
2019-03-07 17:15:13,088 [INFO] ---------------------------------
2019-03-07 17:15:32,592 [INFO] ---------------------------------
2019-03-07 17:15:32,593 [INFO] Summary:
2019-03-07 17:15:32,594 [INFO] Batch 174000, worst loss 0.159151 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:15:32,594 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:15:32,595 [INFO] Sum of grad norms: 0.317412
2019-03-07 17:15:32,596 [INFO] ---------------------------------
2019-03-07 17:15:52,110 [INFO] ---------------------------------
2019-03-07 17:15:52,111 [INFO] Summary:
2019-03-07 17:15:52,112 [INFO] Batch 175000, worst loss 0.186603 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:15:52,113 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:15:52,114 [INFO] Sum of grad norms: 5.083400
2019-03-07 17:15:52,115 [INFO] ---------------------------------
2019-03-07 17:16:11,337 [INFO] ---------------------------------
2019-03-07 17:16:11,338 [INFO] Summary:
2019-03-07 17:16:11,339 [INFO] Batch 176000, worst loss 0.186603 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:16:11,340 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:16:11,341 [INFO] Sum of grad norms: 2.321748
2019-03-07 17:16:11,342 [INFO] ---------------------------------
2019-03-07 17:16:30,534 [INFO] ---------------------------------
2019-03-07 17:16:30,535 [INFO] Summary:
2019-03-07 17:16:30,536 [INFO] Batch 177000, worst loss 0.183478 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:16:30,537 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:16:30,539 [INFO] Sum of grad norms: 10.659386
2019-03-07 17:16:30,540 [INFO] ---------------------------------
2019-03-07 17:16:49,579 [INFO] ---------------------------------
2019-03-07 17:16:49,580 [INFO] Summary:
2019-03-07 17:16:49,581 [INFO] Batch 178000, worst loss 0.188096 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:16:49,582 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:16:49,583 [INFO] Sum of grad norms: 9.006017
2019-03-07 17:16:49,583 [INFO] ---------------------------------
2019-03-07 17:17:08,956 [INFO] ---------------------------------
2019-03-07 17:17:08,957 [INFO] Summary:
2019-03-07 17:17:08,957 [INFO] Batch 179000, worst loss 0.188096 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:17:08,958 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:17:08,959 [INFO] Sum of grad norms: 1.322256
2019-03-07 17:17:08,960 [INFO] ---------------------------------
2019-03-07 17:17:28,190 [INFO] ---------------------------------
2019-03-07 17:17:28,191 [INFO] Summary:
2019-03-07 17:17:28,192 [INFO] Batch 180000, worst loss 0.187971 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:17:28,192 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:17:28,193 [INFO] Sum of grad norms: 0.177536
2019-03-07 17:17:28,194 [INFO] ---------------------------------
2019-03-07 17:17:41,481 [INFO] ---------------------------------
2019-03-07 17:17:41,482 [INFO] Evaluation:
2019-03-07 17:17:41,483 [INFO] Batch 180000, worst loss 0.182252 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:17:41,484 [INFO] ---------------------------------
2019-03-07 17:18:00,767 [INFO] ---------------------------------
2019-03-07 17:18:00,768 [INFO] Summary:
2019-03-07 17:18:00,768 [INFO] Batch 181000, worst loss 0.176027 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:18:00,769 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:18:00,770 [INFO] Sum of grad norms: 0.850177
2019-03-07 17:18:00,771 [INFO] ---------------------------------
2019-03-07 17:18:20,389 [INFO] ---------------------------------
2019-03-07 17:18:20,390 [INFO] Summary:
2019-03-07 17:18:20,390 [INFO] Batch 182000, worst loss 0.211445 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:18:20,391 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:18:20,392 [INFO] Sum of grad norms: 0.160993
2019-03-07 17:18:20,393 [INFO] ---------------------------------
2019-03-07 17:18:39,106 [INFO] ---------------------------------
2019-03-07 17:18:39,107 [INFO] Summary:
2019-03-07 17:18:39,107 [INFO] Batch 183000, worst loss 0.211445 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:18:39,108 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:18:39,109 [INFO] Sum of grad norms: 8.812114
2019-03-07 17:18:39,110 [INFO] ---------------------------------
2019-03-07 17:18:58,773 [INFO] ---------------------------------
2019-03-07 17:18:58,774 [INFO] Summary:
2019-03-07 17:18:58,775 [INFO] Batch 184000, worst loss 0.182615 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:18:58,776 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:18:58,776 [INFO] Sum of grad norms: 15.819551
2019-03-07 17:18:58,777 [INFO] ---------------------------------
2019-03-07 17:19:17,741 [INFO] ---------------------------------
2019-03-07 17:19:17,742 [INFO] Summary:
2019-03-07 17:19:17,742 [INFO] Batch 185000, worst loss 0.202332 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:19:17,743 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:19:17,744 [INFO] Sum of grad norms: 4.342382
2019-03-07 17:19:17,745 [INFO] ---------------------------------
2019-03-07 17:19:37,392 [INFO] ---------------------------------
2019-03-07 17:19:37,393 [INFO] Summary:
2019-03-07 17:19:37,394 [INFO] Batch 186000, worst loss 0.241067 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:19:37,395 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:19:37,395 [INFO] Sum of grad norms: 4.657811
2019-03-07 17:19:37,396 [INFO] ---------------------------------
2019-03-07 17:19:56,733 [INFO] ---------------------------------
2019-03-07 17:19:56,734 [INFO] Summary:
2019-03-07 17:19:56,735 [INFO] Batch 187000, worst loss 0.231135 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:19:56,735 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:19:56,736 [INFO] Sum of grad norms: 0.382557
2019-03-07 17:19:56,737 [INFO] ---------------------------------
2019-03-07 17:20:16,299 [INFO] ---------------------------------
2019-03-07 17:20:16,300 [INFO] Summary:
2019-03-07 17:20:16,301 [INFO] Batch 188000, worst loss 0.167629 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:20:16,302 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:20:16,303 [INFO] Sum of grad norms: 3.356432
2019-03-07 17:20:16,304 [INFO] ---------------------------------
2019-03-07 17:20:35,700 [INFO] ---------------------------------
2019-03-07 17:20:35,701 [INFO] Summary:
2019-03-07 17:20:35,702 [INFO] Batch 189000, worst loss 0.199478 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:20:35,704 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:20:35,708 [INFO] Sum of grad norms: 0.741063
2019-03-07 17:20:35,708 [INFO] ---------------------------------
2019-03-07 17:20:54,831 [INFO] ---------------------------------
2019-03-07 17:20:54,832 [INFO] Summary:
2019-03-07 17:20:54,834 [INFO] Batch 190000, worst loss 0.210924 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:20:54,835 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:20:54,836 [INFO] Sum of grad norms: 0.307513
2019-03-07 17:20:54,837 [INFO] ---------------------------------
2019-03-07 17:21:08,030 [INFO] ---------------------------------
2019-03-07 17:21:08,031 [INFO] Evaluation:
2019-03-07 17:21:08,032 [INFO] Batch 190000, worst loss 0.205759 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:21:08,034 [INFO] ---------------------------------
2019-03-07 17:21:27,632 [INFO] ---------------------------------
2019-03-07 17:21:27,633 [INFO] Summary:
2019-03-07 17:21:27,633 [INFO] Batch 191000, worst loss 0.155263 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:21:27,635 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:21:27,635 [INFO] Sum of grad norms: 3.184495
2019-03-07 17:21:27,636 [INFO] ---------------------------------
2019-03-07 17:21:47,161 [INFO] ---------------------------------
2019-03-07 17:21:47,162 [INFO] Summary:
2019-03-07 17:21:47,163 [INFO] Batch 192000, worst loss 0.173221 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:21:47,163 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:21:47,164 [INFO] Sum of grad norms: 3.036793
2019-03-07 17:21:47,165 [INFO] ---------------------------------
2019-03-07 17:22:06,829 [INFO] ---------------------------------
2019-03-07 17:22:06,830 [INFO] Summary:
2019-03-07 17:22:06,831 [INFO] Batch 193000, worst loss 0.223276 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:22:06,831 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:22:06,832 [INFO] Sum of grad norms: 0.168342
2019-03-07 17:22:06,832 [INFO] ---------------------------------
2019-03-07 17:22:26,204 [INFO] ---------------------------------
2019-03-07 17:22:26,205 [INFO] Summary:
2019-03-07 17:22:26,206 [INFO] Batch 194000, worst loss 0.219068 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:22:26,206 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:22:26,207 [INFO] Sum of grad norms: 11.698818
2019-03-07 17:22:26,208 [INFO] ---------------------------------
2019-03-07 17:22:45,736 [INFO] ---------------------------------
2019-03-07 17:22:45,737 [INFO] Summary:
2019-03-07 17:22:45,738 [INFO] Batch 195000, worst loss 0.180680 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:22:45,741 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:22:45,743 [INFO] Sum of grad norms: 5.385006
2019-03-07 17:22:45,744 [INFO] ---------------------------------
2019-03-07 17:23:05,183 [INFO] ---------------------------------
2019-03-07 17:23:05,184 [INFO] Summary:
2019-03-07 17:23:05,185 [INFO] Batch 196000, worst loss 0.231341 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:23:05,186 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:23:05,187 [INFO] Sum of grad norms: 1.622572
2019-03-07 17:23:05,187 [INFO] ---------------------------------
2019-03-07 17:23:24,884 [INFO] ---------------------------------
2019-03-07 17:23:24,885 [INFO] Summary:
2019-03-07 17:23:24,886 [INFO] Batch 197000, worst loss 0.231341 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:23:24,887 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:23:24,887 [INFO] Sum of grad norms: 1.493294
2019-03-07 17:23:24,888 [INFO] ---------------------------------
2019-03-07 17:23:44,610 [INFO] ---------------------------------
2019-03-07 17:23:44,611 [INFO] Summary:
2019-03-07 17:23:44,611 [INFO] Batch 198000, worst loss 0.179785 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:23:44,612 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:23:44,613 [INFO] Sum of grad norms: 0.081571
2019-03-07 17:23:44,614 [INFO] ---------------------------------
2019-03-07 17:24:04,162 [INFO] ---------------------------------
2019-03-07 17:24:04,163 [INFO] Summary:
2019-03-07 17:24:04,164 [INFO] Batch 199000, worst loss 0.204475 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:24:04,165 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:24:04,166 [INFO] Sum of grad norms: 3.656976
2019-03-07 17:24:04,166 [INFO] ---------------------------------
2019-03-07 17:24:23,687 [INFO] ---------------------------------
2019-03-07 17:24:23,688 [INFO] Summary:
2019-03-07 17:24:23,689 [INFO] Batch 200000, worst loss 0.181493 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:24:23,690 [INFO] Regularization: 5164.689941 * 0.0000010000 = 0.0051646899
2019-03-07 17:24:23,691 [INFO] Sum of grad norms: 0.933617
2019-03-07 17:24:23,691 [INFO] ---------------------------------
2019-03-07 17:24:36,961 [INFO] ---------------------------------
2019-03-07 17:24:36,962 [INFO] Evaluation:
2019-03-07 17:24:36,963 [INFO] Batch 200000, worst loss 0.173145 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:24:36,965 [INFO] ---------------------------------
2019-03-07 17:24:36,967 [INFO] Finished training, saved to file classifier/1551963312/1551975876_2_classifier_final.pth
2019-03-07 17:24:37,247 [INFO] Training model #3: (9, 64, 402) @ 2
2019-03-07 17:24:56,198 [INFO] ---------------------------------
2019-03-07 17:24:56,199 [INFO] Summary:
2019-03-07 17:24:56,200 [INFO] Batch 1000, worst loss 1568.154053 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:24:56,201 [INFO] Regularization: 15567.418945 * 0.0000010000 = 0.0155674191
2019-03-07 17:24:56,201 [INFO] Sum of grad norms: 0.005785
2019-03-07 17:24:56,202 [INFO] ---------------------------------
2019-03-07 17:25:15,470 [INFO] ---------------------------------
2019-03-07 17:25:15,470 [INFO] Summary:
2019-03-07 17:25:15,471 [INFO] Batch 2000, worst loss 1.134291 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:25:15,472 [INFO] Regularization: 10765.798828 * 0.0000010000 = 0.0107657984
2019-03-07 17:25:15,473 [INFO] Sum of grad norms: 0.005745
2019-03-07 17:25:15,473 [INFO] ---------------------------------
2019-03-07 17:25:34,770 [INFO] ---------------------------------
2019-03-07 17:25:34,771 [INFO] Summary:
2019-03-07 17:25:34,772 [INFO] Batch 3000, worst loss 0.778374 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:25:34,772 [INFO] Regularization: 10287.998047 * 0.0000010000 = 0.0102879982
2019-03-07 17:25:34,773 [INFO] Sum of grad norms: 5.863052
2019-03-07 17:25:34,774 [INFO] ---------------------------------
2019-03-07 17:25:54,349 [INFO] ---------------------------------
2019-03-07 17:25:54,350 [INFO] Summary:
2019-03-07 17:25:54,351 [INFO] Batch 4000, worst loss 0.463964 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:25:54,352 [INFO] Regularization: 9989.778320 * 0.0000010000 = 0.0099897785
2019-03-07 17:25:54,352 [INFO] Sum of grad norms: 8.894127
2019-03-07 17:25:54,353 [INFO] ---------------------------------
2019-03-07 17:26:13,639 [INFO] ---------------------------------
2019-03-07 17:26:13,640 [INFO] Summary:
2019-03-07 17:26:13,641 [INFO] Batch 5000, worst loss 0.531959 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:26:13,641 [INFO] Regularization: 9499.869141 * 0.0000010000 = 0.0094998693
2019-03-07 17:26:13,642 [INFO] Sum of grad norms: 3.073421
2019-03-07 17:26:13,643 [INFO] ---------------------------------
2019-03-07 17:26:33,004 [INFO] ---------------------------------
2019-03-07 17:26:33,005 [INFO] Summary:
2019-03-07 17:26:33,005 [INFO] Batch 6000, worst loss 0.395933 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:26:33,006 [INFO] Regularization: 8868.228516 * 0.0000010000 = 0.0088682286
2019-03-07 17:26:33,007 [INFO] Sum of grad norms: 4.734892
2019-03-07 17:26:33,007 [INFO] ---------------------------------
2019-03-07 17:26:52,523 [INFO] ---------------------------------
2019-03-07 17:26:52,524 [INFO] Summary:
2019-03-07 17:26:52,524 [INFO] Batch 7000, worst loss 0.362714 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:26:52,525 [INFO] Regularization: 8140.354004 * 0.0000010000 = 0.0081403544
2019-03-07 17:26:52,526 [INFO] Sum of grad norms: 2.489019
2019-03-07 17:26:52,527 [INFO] ---------------------------------
2019-03-07 17:27:11,922 [INFO] ---------------------------------
2019-03-07 17:27:11,923 [INFO] Summary:
2019-03-07 17:27:11,924 [INFO] Batch 8000, worst loss 0.325668 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:27:11,924 [INFO] Regularization: 7550.994141 * 0.0000010000 = 0.0075509939
2019-03-07 17:27:11,925 [INFO] Sum of grad norms: 3.317430
2019-03-07 17:27:11,926 [INFO] ---------------------------------
2019-03-07 17:27:31,186 [INFO] ---------------------------------
2019-03-07 17:27:31,187 [INFO] Summary:
2019-03-07 17:27:31,187 [INFO] Batch 9000, worst loss 0.377538 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:27:31,188 [INFO] Regularization: 7080.472656 * 0.0000010000 = 0.0070804725
2019-03-07 17:27:31,189 [INFO] Sum of grad norms: 3.378342
2019-03-07 17:27:31,190 [INFO] ---------------------------------
2019-03-07 17:27:50,553 [INFO] ---------------------------------
2019-03-07 17:27:50,554 [INFO] Summary:
2019-03-07 17:27:50,555 [INFO] Batch 10000, worst loss 0.312508 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:27:50,556 [INFO] Regularization: 6710.509766 * 0.0000010000 = 0.0067105098
2019-03-07 17:27:50,557 [INFO] Sum of grad norms: 4.980002
2019-03-07 17:27:50,557 [INFO] ---------------------------------
2019-03-07 17:28:03,679 [INFO] ---------------------------------
2019-03-07 17:28:03,683 [INFO] Evaluation:
2019-03-07 17:28:03,685 [INFO] Batch 10000, worst loss 0.273344 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:28:03,687 [INFO] ---------------------------------
2019-03-07 17:28:23,219 [INFO] ---------------------------------
2019-03-07 17:28:23,220 [INFO] Summary:
2019-03-07 17:28:23,220 [INFO] Batch 11000, worst loss 0.319999 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:28:23,221 [INFO] Regularization: 6391.921387 * 0.0000010000 = 0.0063919215
2019-03-07 17:28:23,222 [INFO] Sum of grad norms: 2.539727
2019-03-07 17:28:23,223 [INFO] ---------------------------------
2019-03-07 17:28:42,582 [INFO] ---------------------------------
2019-03-07 17:28:42,583 [INFO] Summary:
2019-03-07 17:28:42,583 [INFO] Batch 12000, worst loss 0.324811 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:28:42,584 [INFO] Regularization: 6170.019531 * 0.0000010000 = 0.0061700195
2019-03-07 17:28:42,585 [INFO] Sum of grad norms: 1.227887
2019-03-07 17:28:42,586 [INFO] ---------------------------------
2019-03-07 17:29:02,134 [INFO] ---------------------------------
2019-03-07 17:29:02,135 [INFO] Summary:
2019-03-07 17:29:02,135 [INFO] Batch 13000, worst loss 0.317140 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:29:02,136 [INFO] Regularization: 6008.081055 * 0.0000010000 = 0.0060080811
2019-03-07 17:29:02,137 [INFO] Sum of grad norms: 5.145102
2019-03-07 17:29:02,138 [INFO] ---------------------------------
2019-03-07 17:29:21,785 [INFO] ---------------------------------
2019-03-07 17:29:21,786 [INFO] Summary:
2019-03-07 17:29:21,790 [INFO] Batch 14000, worst loss 0.302619 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:29:21,791 [INFO] Regularization: 5864.716797 * 0.0000010000 = 0.0058647166
2019-03-07 17:29:21,792 [INFO] Sum of grad norms: 9.163040
2019-03-07 17:29:21,793 [INFO] ---------------------------------
2019-03-07 17:29:41,113 [INFO] ---------------------------------
2019-03-07 17:29:41,114 [INFO] Summary:
2019-03-07 17:29:41,114 [INFO] Batch 15000, worst loss 0.331471 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:29:41,115 [INFO] Regularization: 5752.056152 * 0.0000010000 = 0.0057520559
2019-03-07 17:29:41,116 [INFO] Sum of grad norms: 4.579263
2019-03-07 17:29:41,117 [INFO] ---------------------------------
2019-03-07 17:30:00,661 [INFO] ---------------------------------
2019-03-07 17:30:00,662 [INFO] Summary:
2019-03-07 17:30:00,663 [INFO] Batch 16000, worst loss 0.456146 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:30:00,664 [INFO] Regularization: 5619.287598 * 0.0000010000 = 0.0056192875
2019-03-07 17:30:00,665 [INFO] Sum of grad norms: 4.757135
2019-03-07 17:30:00,665 [INFO] ---------------------------------
2019-03-07 17:30:20,053 [INFO] ---------------------------------
2019-03-07 17:30:20,054 [INFO] Summary:
2019-03-07 17:30:20,055 [INFO] Batch 17000, worst loss 0.384662 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:30:20,055 [INFO] Regularization: 5652.903320 * 0.0000010000 = 0.0056529031
2019-03-07 17:30:20,056 [INFO] Sum of grad norms: 2.104421
2019-03-07 17:30:20,057 [INFO] ---------------------------------
2019-03-07 17:30:39,206 [INFO] ---------------------------------
2019-03-07 17:30:39,207 [INFO] Summary:
2019-03-07 17:30:39,208 [INFO] Batch 18000, worst loss 0.288458 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:30:39,208 [INFO] Regularization: 5549.274414 * 0.0000010000 = 0.0055492744
2019-03-07 17:30:39,209 [INFO] Sum of grad norms: 8.180529
2019-03-07 17:30:39,210 [INFO] ---------------------------------
2019-03-07 17:30:58,636 [INFO] ---------------------------------
2019-03-07 17:30:58,637 [INFO] Summary:
2019-03-07 17:30:58,638 [INFO] Batch 19000, worst loss 0.490673 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:30:58,638 [INFO] Regularization: 5538.980469 * 0.0000010000 = 0.0055389805
2019-03-07 17:30:58,639 [INFO] Sum of grad norms: 10.194870
2019-03-07 17:30:58,640 [INFO] ---------------------------------
2019-03-07 17:31:18,026 [INFO] ---------------------------------
2019-03-07 17:31:18,027 [INFO] Summary:
2019-03-07 17:31:18,027 [INFO] Batch 20000, worst loss 0.389317 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:31:18,028 [INFO] Regularization: 5509.979980 * 0.0000010000 = 0.0055099800
2019-03-07 17:31:18,029 [INFO] Sum of grad norms: 8.339840
2019-03-07 17:31:18,030 [INFO] ---------------------------------
2019-03-07 17:31:31,162 [INFO] ---------------------------------
2019-03-07 17:31:31,163 [INFO] Evaluation:
2019-03-07 17:31:31,163 [INFO] Batch 20000, worst loss 0.262595 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:31:31,164 [INFO] ---------------------------------
2019-03-07 17:31:50,561 [INFO] ---------------------------------
2019-03-07 17:31:50,562 [INFO] Summary:
2019-03-07 17:31:50,563 [INFO] Batch 21000, worst loss 0.354940 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:31:50,564 [INFO] Regularization: 5531.952148 * 0.0000010000 = 0.0055319523
2019-03-07 17:31:50,565 [INFO] Sum of grad norms: 11.564391
2019-03-07 17:31:50,566 [INFO] ---------------------------------
2019-03-07 17:32:10,251 [INFO] ---------------------------------
2019-03-07 17:32:10,252 [INFO] Summary:
2019-03-07 17:32:10,252 [INFO] Batch 22000, worst loss 0.379493 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:32:10,253 [INFO] Regularization: 5412.975098 * 0.0000010000 = 0.0054129749
2019-03-07 17:32:10,254 [INFO] Sum of grad norms: 7.011730
2019-03-07 17:32:10,255 [INFO] ---------------------------------
2019-03-07 17:32:29,828 [INFO] ---------------------------------
2019-03-07 17:32:29,829 [INFO] Summary:
2019-03-07 17:32:29,830 [INFO] Batch 23000, worst loss 0.312223 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:32:29,831 [INFO] Regularization: 5386.139648 * 0.0000010000 = 0.0053861397
2019-03-07 17:32:29,831 [INFO] Sum of grad norms: 3.507084
2019-03-07 17:32:29,833 [INFO] ---------------------------------
2019-03-07 17:32:49,073 [INFO] ---------------------------------
2019-03-07 17:32:49,074 [INFO] Summary:
2019-03-07 17:32:49,074 [INFO] Batch 24000, worst loss 0.322822 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:32:49,075 [INFO] Regularization: 5498.789551 * 0.0000010000 = 0.0054987897
2019-03-07 17:32:49,076 [INFO] Sum of grad norms: 1.390545
2019-03-07 17:32:49,076 [INFO] ---------------------------------
2019-03-07 17:33:08,912 [INFO] ---------------------------------
2019-03-07 17:33:08,913 [INFO] Summary:
2019-03-07 17:33:08,913 [INFO] Batch 25000, worst loss 0.338278 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:33:08,915 [INFO] Regularization: 5387.925293 * 0.0000010000 = 0.0053879251
2019-03-07 17:33:08,915 [INFO] Sum of grad norms: 7.450976
2019-03-07 17:33:08,916 [INFO] ---------------------------------
2019-03-07 17:33:28,151 [INFO] ---------------------------------
2019-03-07 17:33:28,152 [INFO] Summary:
2019-03-07 17:33:28,152 [INFO] Batch 26000, worst loss 0.346766 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:33:28,154 [INFO] Regularization: 5444.091797 * 0.0000010000 = 0.0054440917
2019-03-07 17:33:28,155 [INFO] Sum of grad norms: 2.743558
2019-03-07 17:33:28,156 [INFO] ---------------------------------
2019-03-07 17:33:47,669 [INFO] ---------------------------------
2019-03-07 17:33:47,670 [INFO] Summary:
2019-03-07 17:33:47,672 [INFO] Batch 27000, worst loss 0.296059 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:33:47,672 [INFO] Regularization: 5375.702637 * 0.0000010000 = 0.0053757024
2019-03-07 17:33:47,674 [INFO] Sum of grad norms: 7.010742
2019-03-07 17:33:47,676 [INFO] ---------------------------------
2019-03-07 17:34:07,291 [INFO] ---------------------------------
2019-03-07 17:34:07,292 [INFO] Summary:
2019-03-07 17:34:07,293 [INFO] Batch 28000, worst loss 0.272716 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:34:07,293 [INFO] Regularization: 5338.731934 * 0.0000010000 = 0.0053387317
2019-03-07 17:34:07,294 [INFO] Sum of grad norms: 4.289318
2019-03-07 17:34:07,295 [INFO] ---------------------------------
2019-03-07 17:34:26,730 [INFO] ---------------------------------
2019-03-07 17:34:26,731 [INFO] Summary:
2019-03-07 17:34:26,732 [INFO] Batch 29000, worst loss 0.329587 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:34:26,733 [INFO] Regularization: 5235.809570 * 0.0000010000 = 0.0052358094
2019-03-07 17:34:26,734 [INFO] Sum of grad norms: 1.071620
2019-03-07 17:34:26,734 [INFO] ---------------------------------
2019-03-07 17:34:46,184 [INFO] ---------------------------------
2019-03-07 17:34:46,185 [INFO] Summary:
2019-03-07 17:34:46,185 [INFO] Batch 30000, worst loss 0.299208 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:34:46,186 [INFO] Regularization: 5213.511719 * 0.0000010000 = 0.0052135116
2019-03-07 17:34:46,187 [INFO] Sum of grad norms: 5.114139
2019-03-07 17:34:46,187 [INFO] ---------------------------------
2019-03-07 17:34:59,341 [INFO] ---------------------------------
2019-03-07 17:34:59,342 [INFO] Evaluation:
2019-03-07 17:34:59,342 [INFO] Batch 30000, worst loss 0.272213 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:34:59,343 [INFO] ---------------------------------
2019-03-07 17:35:18,753 [INFO] ---------------------------------
2019-03-07 17:35:18,754 [INFO] Summary:
2019-03-07 17:35:18,755 [INFO] Batch 31000, worst loss 0.232172 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:35:18,756 [INFO] Regularization: 5181.862305 * 0.0000010000 = 0.0051818625
2019-03-07 17:35:18,756 [INFO] Sum of grad norms: 2.508524
2019-03-07 17:35:18,757 [INFO] ---------------------------------
2019-03-07 17:35:38,476 [INFO] ---------------------------------
2019-03-07 17:35:38,477 [INFO] Summary:
2019-03-07 17:35:38,478 [INFO] Batch 32000, worst loss 0.344606 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:35:38,478 [INFO] Regularization: 5175.217285 * 0.0000010000 = 0.0051752171
2019-03-07 17:35:38,479 [INFO] Sum of grad norms: 4.129191
2019-03-07 17:35:38,480 [INFO] ---------------------------------
2019-03-07 17:35:58,017 [INFO] ---------------------------------
2019-03-07 17:35:58,018 [INFO] Summary:
2019-03-07 17:35:58,018 [INFO] Batch 33000, worst loss 0.273696 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:35:58,019 [INFO] Regularization: 5203.842285 * 0.0000010000 = 0.0052038422
2019-03-07 17:35:58,019 [INFO] Sum of grad norms: 6.486147
2019-03-07 17:35:58,020 [INFO] ---------------------------------
2019-03-07 17:36:17,506 [INFO] ---------------------------------
2019-03-07 17:36:17,507 [INFO] Summary:
2019-03-07 17:36:17,508 [INFO] Batch 34000, worst loss 0.224686 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:36:17,509 [INFO] Regularization: 5121.126953 * 0.0000010000 = 0.0051211268
2019-03-07 17:36:17,509 [INFO] Sum of grad norms: 1.747414
2019-03-07 17:36:17,510 [INFO] ---------------------------------
2019-03-07 17:36:36,479 [INFO] ---------------------------------
2019-03-07 17:36:36,480 [INFO] Summary:
2019-03-07 17:36:36,481 [INFO] Batch 35000, worst loss 0.287719 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:36:36,482 [INFO] Regularization: 5061.379883 * 0.0000010000 = 0.0050613801
2019-03-07 17:36:36,483 [INFO] Sum of grad norms: 10.779064
2019-03-07 17:36:36,484 [INFO] ---------------------------------
2019-03-07 17:36:55,864 [INFO] ---------------------------------
2019-03-07 17:36:55,865 [INFO] Summary:
2019-03-07 17:36:55,866 [INFO] Batch 36000, worst loss 0.406749 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:36:55,867 [INFO] Regularization: 5098.215820 * 0.0000010000 = 0.0050982158
2019-03-07 17:36:55,868 [INFO] Sum of grad norms: 3.926088
2019-03-07 17:36:55,868 [INFO] ---------------------------------
2019-03-07 17:37:15,432 [INFO] ---------------------------------
2019-03-07 17:37:15,433 [INFO] Summary:
2019-03-07 17:37:15,434 [INFO] Batch 37000, worst loss 0.282354 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:37:15,435 [INFO] Regularization: 5040.360840 * 0.0000010000 = 0.0050403606
2019-03-07 17:37:15,435 [INFO] Sum of grad norms: 1.965488
2019-03-07 17:37:15,436 [INFO] ---------------------------------
2019-03-07 17:37:34,862 [INFO] ---------------------------------
2019-03-07 17:37:34,863 [INFO] Summary:
2019-03-07 17:37:34,864 [INFO] Batch 38000, worst loss 0.388069 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:37:34,865 [INFO] Regularization: 5052.291504 * 0.0000010000 = 0.0050522913
2019-03-07 17:37:34,866 [INFO] Sum of grad norms: 5.817238
2019-03-07 17:37:34,866 [INFO] ---------------------------------
2019-03-07 17:37:54,557 [INFO] ---------------------------------
2019-03-07 17:37:54,559 [INFO] Summary:
2019-03-07 17:37:54,559 [INFO] Batch 39000, worst loss 0.259024 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:37:54,560 [INFO] Regularization: 5140.474609 * 0.0000010000 = 0.0051404745
2019-03-07 17:37:54,560 [INFO] Sum of grad norms: 3.778160
2019-03-07 17:37:54,561 [INFO] ---------------------------------
2019-03-07 17:38:14,239 [INFO] ---------------------------------
2019-03-07 17:38:14,241 [INFO] Summary:
2019-03-07 17:38:14,243 [INFO] Batch 40000, worst loss 0.303298 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:38:14,244 [INFO] Regularization: 5113.156738 * 0.0000010000 = 0.0051131565
2019-03-07 17:38:14,245 [INFO] Sum of grad norms: 1.674995
2019-03-07 17:38:14,246 [INFO] ---------------------------------
2019-03-07 17:38:27,244 [INFO] ---------------------------------
2019-03-07 17:38:27,245 [INFO] Evaluation:
2019-03-07 17:38:27,246 [INFO] Batch 40000, worst loss 0.255867 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 17:38:27,248 [INFO] ---------------------------------
2019-03-07 17:38:46,773 [INFO] ---------------------------------
2019-03-07 17:38:46,774 [INFO] Summary:
2019-03-07 17:38:46,774 [INFO] Batch 41000, worst loss 0.296519 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:38:46,775 [INFO] Regularization: 5062.361328 * 0.0000010000 = 0.0050623612
2019-03-07 17:38:46,776 [INFO] Sum of grad norms: 3.206025
2019-03-07 17:38:46,776 [INFO] ---------------------------------
2019-03-07 17:39:06,311 [INFO] ---------------------------------
2019-03-07 17:39:06,312 [INFO] Summary:
2019-03-07 17:39:06,314 [INFO] Batch 42000, worst loss 0.268199 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:39:06,315 [INFO] Regularization: 4955.144043 * 0.0000010000 = 0.0049551441
2019-03-07 17:39:06,316 [INFO] Sum of grad norms: 4.778244
2019-03-07 17:39:06,317 [INFO] ---------------------------------
2019-03-07 17:39:25,974 [INFO] ---------------------------------
2019-03-07 17:39:25,975 [INFO] Summary:
2019-03-07 17:39:25,976 [INFO] Batch 43000, worst loss 0.185787 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:39:25,977 [INFO] Regularization: 4882.665527 * 0.0000010000 = 0.0048826654
2019-03-07 17:39:25,978 [INFO] Sum of grad norms: 0.458384
2019-03-07 17:39:25,978 [INFO] ---------------------------------
2019-03-07 17:39:45,592 [INFO] ---------------------------------
2019-03-07 17:39:45,594 [INFO] Summary:
2019-03-07 17:39:45,594 [INFO] Batch 44000, worst loss 0.215600 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:39:45,595 [INFO] Regularization: 4872.329102 * 0.0000010000 = 0.0048723291
2019-03-07 17:39:45,596 [INFO] Sum of grad norms: 0.601387
2019-03-07 17:39:45,597 [INFO] ---------------------------------
2019-03-07 17:40:05,222 [INFO] ---------------------------------
2019-03-07 17:40:05,223 [INFO] Summary:
2019-03-07 17:40:05,223 [INFO] Batch 45000, worst loss 0.212083 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:40:05,224 [INFO] Regularization: 4858.250488 * 0.0000010000 = 0.0048582503
2019-03-07 17:40:05,225 [INFO] Sum of grad norms: 7.745533
2019-03-07 17:40:05,226 [INFO] ---------------------------------
2019-03-07 17:40:24,479 [INFO] ---------------------------------
2019-03-07 17:40:24,480 [INFO] Summary:
2019-03-07 17:40:24,480 [INFO] Batch 46000, worst loss 0.221011 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:40:24,481 [INFO] Regularization: 4858.276367 * 0.0000010000 = 0.0048582763
2019-03-07 17:40:24,481 [INFO] Sum of grad norms: 4.529067
2019-03-07 17:40:24,482 [INFO] ---------------------------------
2019-03-07 17:40:44,036 [INFO] ---------------------------------
2019-03-07 17:40:44,037 [INFO] Summary:
2019-03-07 17:40:44,037 [INFO] Batch 47000, worst loss 0.232228 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:40:44,039 [INFO] Regularization: 4873.517090 * 0.0000010000 = 0.0048735170
2019-03-07 17:40:44,039 [INFO] Sum of grad norms: 6.943739
2019-03-07 17:40:44,040 [INFO] ---------------------------------
2019-03-07 17:41:03,103 [INFO] ---------------------------------
2019-03-07 17:41:03,104 [INFO] Summary:
2019-03-07 17:41:03,107 [INFO] Batch 48000, worst loss 0.268188 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:41:03,107 [INFO] Regularization: 4825.561523 * 0.0000010000 = 0.0048255613
2019-03-07 17:41:03,109 [INFO] Sum of grad norms: 4.373373
2019-03-07 17:41:03,110 [INFO] ---------------------------------
2019-03-07 17:41:22,328 [INFO] ---------------------------------
2019-03-07 17:41:22,329 [INFO] Summary:
2019-03-07 17:41:22,330 [INFO] Batch 49000, worst loss 0.232739 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:41:22,331 [INFO] Regularization: 4832.902344 * 0.0000010000 = 0.0048329025
2019-03-07 17:41:22,331 [INFO] Sum of grad norms: 7.251850
2019-03-07 17:41:22,332 [INFO] ---------------------------------
2019-03-07 17:41:42,124 [INFO] ---------------------------------
2019-03-07 17:41:42,125 [INFO] Summary:
2019-03-07 17:41:42,126 [INFO] Batch 50000, worst loss 0.263732 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:41:42,126 [INFO] Regularization: 4801.707031 * 0.0000010000 = 0.0048017069
2019-03-07 17:41:42,127 [INFO] Sum of grad norms: 0.341623
2019-03-07 17:41:42,128 [INFO] ---------------------------------
2019-03-07 17:41:55,198 [INFO] ---------------------------------
2019-03-07 17:41:55,200 [INFO] Evaluation:
2019-03-07 17:41:55,200 [INFO] Batch 50000, worst loss 0.174334 (without reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-07 17:41:55,201 [INFO] ---------------------------------
2019-03-07 17:42:14,829 [INFO] ---------------------------------
2019-03-07 17:42:14,830 [INFO] Summary:
2019-03-07 17:42:14,830 [INFO] Batch 51000, worst loss 0.286317 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:42:14,831 [INFO] Regularization: 4804.330078 * 0.0000010000 = 0.0048043299
2019-03-07 17:42:14,832 [INFO] Sum of grad norms: 7.469728
2019-03-07 17:42:14,832 [INFO] ---------------------------------
2019-03-07 17:42:34,184 [INFO] ---------------------------------
2019-03-07 17:42:34,185 [INFO] Summary:
2019-03-07 17:42:34,186 [INFO] Batch 52000, worst loss 0.190585 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:42:34,186 [INFO] Regularization: 4775.921387 * 0.0000010000 = 0.0047759213
2019-03-07 17:42:34,188 [INFO] Sum of grad norms: 0.856222
2019-03-07 17:42:34,188 [INFO] ---------------------------------
2019-03-07 17:42:53,813 [INFO] ---------------------------------
2019-03-07 17:42:53,813 [INFO] Summary:
2019-03-07 17:42:53,814 [INFO] Batch 53000, worst loss 0.224415 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:42:53,815 [INFO] Regularization: 4769.346191 * 0.0000010000 = 0.0047693462
2019-03-07 17:42:53,815 [INFO] Sum of grad norms: 0.735969
2019-03-07 17:42:53,816 [INFO] ---------------------------------
2019-03-07 17:43:12,933 [INFO] ---------------------------------
2019-03-07 17:43:12,934 [INFO] Summary:
2019-03-07 17:43:12,936 [INFO] Batch 54000, worst loss 0.185937 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:43:12,937 [INFO] Regularization: 4771.254395 * 0.0000010000 = 0.0047712545
2019-03-07 17:43:12,938 [INFO] Sum of grad norms: 0.261242
2019-03-07 17:43:12,939 [INFO] ---------------------------------
2019-03-07 17:43:32,315 [INFO] ---------------------------------
2019-03-07 17:43:32,316 [INFO] Summary:
2019-03-07 17:43:32,316 [INFO] Batch 55000, worst loss 0.163627 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:43:32,317 [INFO] Regularization: 4761.897461 * 0.0000010000 = 0.0047618975
2019-03-07 17:43:32,317 [INFO] Sum of grad norms: 0.474140
2019-03-07 17:43:32,318 [INFO] ---------------------------------
2019-03-07 17:43:51,559 [INFO] ---------------------------------
2019-03-07 17:43:51,560 [INFO] Summary:
2019-03-07 17:43:51,561 [INFO] Batch 56000, worst loss 0.217017 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:43:51,561 [INFO] Regularization: 4763.111328 * 0.0000010000 = 0.0047631115
2019-03-07 17:43:51,562 [INFO] Sum of grad norms: 1.993053
2019-03-07 17:43:51,563 [INFO] ---------------------------------
2019-03-07 17:44:11,119 [INFO] ---------------------------------
2019-03-07 17:44:11,120 [INFO] Summary:
2019-03-07 17:44:11,121 [INFO] Batch 57000, worst loss 0.210802 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:44:11,127 [INFO] Regularization: 4759.426758 * 0.0000010000 = 0.0047594267
2019-03-07 17:44:11,128 [INFO] Sum of grad norms: 4.260375
2019-03-07 17:44:11,129 [INFO] ---------------------------------
2019-03-07 17:44:30,682 [INFO] ---------------------------------
2019-03-07 17:44:30,683 [INFO] Summary:
2019-03-07 17:44:30,684 [INFO] Batch 58000, worst loss 0.220391 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:44:30,684 [INFO] Regularization: 4753.720703 * 0.0000010000 = 0.0047537205
2019-03-07 17:44:30,685 [INFO] Sum of grad norms: 0.156264
2019-03-07 17:44:30,686 [INFO] ---------------------------------
2019-03-07 17:44:49,712 [INFO] ---------------------------------
2019-03-07 17:44:49,712 [INFO] Summary:
2019-03-07 17:44:49,713 [INFO] Batch 59000, worst loss 0.217846 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:44:49,714 [INFO] Regularization: 4753.725098 * 0.0000010000 = 0.0047537251
2019-03-07 17:44:49,715 [INFO] Sum of grad norms: 4.951966
2019-03-07 17:44:49,715 [INFO] ---------------------------------
2019-03-07 17:45:09,132 [INFO] ---------------------------------
2019-03-07 17:45:09,133 [INFO] Summary:
2019-03-07 17:45:09,133 [INFO] Batch 60000, worst loss 0.227277 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:45:09,134 [INFO] Regularization: 4746.552734 * 0.0000010000 = 0.0047465526
2019-03-07 17:45:09,135 [INFO] Sum of grad norms: 0.399790
2019-03-07 17:45:09,135 [INFO] ---------------------------------
2019-03-07 17:45:22,226 [INFO] ---------------------------------
2019-03-07 17:45:22,227 [INFO] Evaluation:
2019-03-07 17:45:22,228 [INFO] Batch 60000, worst loss 0.159067 (without reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-07 17:45:22,229 [INFO] ---------------------------------
2019-03-07 17:45:42,079 [INFO] ---------------------------------
2019-03-07 17:45:42,081 [INFO] Summary:
2019-03-07 17:45:42,081 [INFO] Batch 61000, worst loss 0.179755 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:45:42,082 [INFO] Regularization: 4747.901855 * 0.0000010000 = 0.0047479020
2019-03-07 17:45:42,083 [INFO] Sum of grad norms: 0.988358
2019-03-07 17:45:42,084 [INFO] ---------------------------------
2019-03-07 17:46:01,618 [INFO] ---------------------------------
2019-03-07 17:46:01,619 [INFO] Summary:
2019-03-07 17:46:01,620 [INFO] Batch 62000, worst loss 0.180938 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:46:01,621 [INFO] Regularization: 4741.400391 * 0.0000010000 = 0.0047414005
2019-03-07 17:46:01,621 [INFO] Sum of grad norms: 5.311560
2019-03-07 17:46:01,622 [INFO] ---------------------------------
2019-03-07 17:46:20,984 [INFO] ---------------------------------
2019-03-07 17:46:20,985 [INFO] Summary:
2019-03-07 17:46:20,986 [INFO] Batch 63000, worst loss 0.173783 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:46:20,987 [INFO] Regularization: 4738.404297 * 0.0000010000 = 0.0047384044
2019-03-07 17:46:20,987 [INFO] Sum of grad norms: 3.125540
2019-03-07 17:46:20,988 [INFO] ---------------------------------
2019-03-07 17:46:40,047 [INFO] ---------------------------------
2019-03-07 17:46:40,048 [INFO] Summary:
2019-03-07 17:46:40,048 [INFO] Batch 64000, worst loss 0.138328 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:46:40,049 [INFO] Regularization: 4737.357910 * 0.0000010000 = 0.0047373581
2019-03-07 17:46:40,050 [INFO] Sum of grad norms: 2.655912
2019-03-07 17:46:40,051 [INFO] ---------------------------------
2019-03-07 17:46:59,481 [INFO] ---------------------------------
2019-03-07 17:46:59,482 [INFO] Summary:
2019-03-07 17:46:59,483 [INFO] Batch 65000, worst loss 0.149052 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:46:59,484 [INFO] Regularization: 4735.842285 * 0.0000010000 = 0.0047358423
2019-03-07 17:46:59,485 [INFO] Sum of grad norms: 0.141512
2019-03-07 17:46:59,485 [INFO] ---------------------------------
2019-03-07 17:47:18,672 [INFO] ---------------------------------
2019-03-07 17:47:18,673 [INFO] Summary:
2019-03-07 17:47:18,674 [INFO] Batch 66000, worst loss 0.230897 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:47:18,675 [INFO] Regularization: 4734.247070 * 0.0000010000 = 0.0047342470
2019-03-07 17:47:18,675 [INFO] Sum of grad norms: 9.451469
2019-03-07 17:47:18,676 [INFO] ---------------------------------
2019-03-07 17:47:38,379 [INFO] ---------------------------------
2019-03-07 17:47:38,380 [INFO] Summary:
2019-03-07 17:47:38,381 [INFO] Batch 67000, worst loss 0.173653 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:47:38,381 [INFO] Regularization: 4733.903320 * 0.0000010000 = 0.0047339033
2019-03-07 17:47:38,382 [INFO] Sum of grad norms: 8.155925
2019-03-07 17:47:38,383 [INFO] ---------------------------------
2019-03-07 17:47:57,667 [INFO] ---------------------------------
2019-03-07 17:47:57,668 [INFO] Summary:
2019-03-07 17:47:57,668 [INFO] Batch 68000, worst loss 0.218763 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:47:57,669 [INFO] Regularization: 4732.887207 * 0.0000010000 = 0.0047328873
2019-03-07 17:47:57,670 [INFO] Sum of grad norms: 2.097994
2019-03-07 17:47:57,670 [INFO] ---------------------------------
2019-03-07 17:48:17,098 [INFO] ---------------------------------
2019-03-07 17:48:17,099 [INFO] Summary:
2019-03-07 17:48:17,100 [INFO] Batch 69000, worst loss 0.207198 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:48:17,100 [INFO] Regularization: 4731.878418 * 0.0000010000 = 0.0047318786
2019-03-07 17:48:17,101 [INFO] Sum of grad norms: 8.777597
2019-03-07 17:48:17,102 [INFO] ---------------------------------
2019-03-07 17:48:36,249 [INFO] ---------------------------------
2019-03-07 17:48:36,250 [INFO] Summary:
2019-03-07 17:48:36,251 [INFO] Batch 70000, worst loss 0.159362 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:48:36,252 [INFO] Regularization: 4732.047852 * 0.0000010000 = 0.0047320477
2019-03-07 17:48:36,252 [INFO] Sum of grad norms: 5.454829
2019-03-07 17:48:36,253 [INFO] ---------------------------------
2019-03-07 17:48:49,342 [INFO] ---------------------------------
2019-03-07 17:48:49,343 [INFO] Evaluation:
2019-03-07 17:48:49,344 [INFO] Batch 70000, worst loss 0.180518 (without reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-07 17:48:49,347 [INFO] ---------------------------------
2019-03-07 17:49:08,779 [INFO] ---------------------------------
2019-03-07 17:49:08,780 [INFO] Summary:
2019-03-07 17:49:08,781 [INFO] Batch 71000, worst loss 0.162770 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:49:08,781 [INFO] Regularization: 4729.354004 * 0.0000010000 = 0.0047293538
2019-03-07 17:49:08,782 [INFO] Sum of grad norms: 10.294228
2019-03-07 17:49:08,783 [INFO] ---------------------------------
2019-03-07 17:49:28,021 [INFO] ---------------------------------
2019-03-07 17:49:28,021 [INFO] Summary:
2019-03-07 17:49:28,022 [INFO] Batch 72000, worst loss 0.182037 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:49:28,022 [INFO] Regularization: 4728.240723 * 0.0000010000 = 0.0047282409
2019-03-07 17:49:28,023 [INFO] Sum of grad norms: 1.210220
2019-03-07 17:49:28,024 [INFO] ---------------------------------
2019-03-07 17:49:47,292 [INFO] ---------------------------------
2019-03-07 17:49:47,293 [INFO] Summary:
2019-03-07 17:49:47,293 [INFO] Batch 73000, worst loss 0.196410 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:49:47,294 [INFO] Regularization: 4727.188477 * 0.0000010000 = 0.0047271885
2019-03-07 17:49:47,295 [INFO] Sum of grad norms: 0.293474
2019-03-07 17:49:47,296 [INFO] ---------------------------------
2019-03-07 17:50:06,966 [INFO] ---------------------------------
2019-03-07 17:50:06,967 [INFO] Summary:
2019-03-07 17:50:06,968 [INFO] Batch 74000, worst loss 0.169066 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:50:06,968 [INFO] Regularization: 4727.424316 * 0.0000010000 = 0.0047274241
2019-03-07 17:50:06,969 [INFO] Sum of grad norms: 0.294439
2019-03-07 17:50:06,969 [INFO] ---------------------------------
2019-03-07 17:50:26,247 [INFO] ---------------------------------
2019-03-07 17:50:26,248 [INFO] Summary:
2019-03-07 17:50:26,249 [INFO] Batch 75000, worst loss 0.161259 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:50:26,249 [INFO] Regularization: 4726.705566 * 0.0000010000 = 0.0047267056
2019-03-07 17:50:26,250 [INFO] Sum of grad norms: 2.571692
2019-03-07 17:50:26,251 [INFO] ---------------------------------
2019-03-07 17:50:45,804 [INFO] ---------------------------------
2019-03-07 17:50:45,805 [INFO] Summary:
2019-03-07 17:50:45,805 [INFO] Batch 76000, worst loss 0.155795 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:50:45,806 [INFO] Regularization: 4726.309570 * 0.0000010000 = 0.0047263093
2019-03-07 17:50:45,806 [INFO] Sum of grad norms: 7.581843
2019-03-07 17:50:45,807 [INFO] ---------------------------------
2019-03-07 17:51:05,450 [INFO] ---------------------------------
2019-03-07 17:51:05,451 [INFO] Summary:
2019-03-07 17:51:05,451 [INFO] Batch 77000, worst loss 0.194587 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:51:05,452 [INFO] Regularization: 4725.743652 * 0.0000010000 = 0.0047257436
2019-03-07 17:51:05,453 [INFO] Sum of grad norms: 1.298774
2019-03-07 17:51:05,454 [INFO] ---------------------------------
2019-03-07 17:51:24,525 [INFO] ---------------------------------
2019-03-07 17:51:24,526 [INFO] Summary:
2019-03-07 17:51:24,527 [INFO] Batch 78000, worst loss 0.180201 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:51:24,527 [INFO] Regularization: 4725.156250 * 0.0000010000 = 0.0047251564
2019-03-07 17:51:24,528 [INFO] Sum of grad norms: 1.131800
2019-03-07 17:51:24,529 [INFO] ---------------------------------
2019-03-07 17:51:43,524 [INFO] ---------------------------------
2019-03-07 17:51:43,525 [INFO] Summary:
2019-03-07 17:51:43,526 [INFO] Batch 79000, worst loss 0.208304 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:51:43,527 [INFO] Regularization: 4725.074707 * 0.0000010000 = 0.0047250749
2019-03-07 17:51:43,528 [INFO] Sum of grad norms: 7.352540
2019-03-07 17:51:43,529 [INFO] ---------------------------------
2019-03-07 17:52:02,216 [INFO] ---------------------------------
2019-03-07 17:52:02,217 [INFO] Summary:
2019-03-07 17:52:02,217 [INFO] Batch 80000, worst loss 0.199432 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:52:02,218 [INFO] Regularization: 4724.876953 * 0.0000010000 = 0.0047248770
2019-03-07 17:52:02,218 [INFO] Sum of grad norms: 1.106332
2019-03-07 17:52:02,219 [INFO] ---------------------------------
2019-03-07 17:52:15,214 [INFO] ---------------------------------
2019-03-07 17:52:15,215 [INFO] Evaluation:
2019-03-07 17:52:15,218 [INFO] Batch 80000, worst loss 0.137372 (without reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-07 17:52:15,222 [INFO] ---------------------------------
2019-03-07 17:52:34,852 [INFO] ---------------------------------
2019-03-07 17:52:34,853 [INFO] Summary:
2019-03-07 17:52:34,854 [INFO] Batch 81000, worst loss 0.178187 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:52:34,855 [INFO] Regularization: 4724.421875 * 0.0000010000 = 0.0047244220
2019-03-07 17:52:34,855 [INFO] Sum of grad norms: 5.774725
2019-03-07 17:52:34,856 [INFO] ---------------------------------
2019-03-07 17:52:54,340 [INFO] ---------------------------------
2019-03-07 17:52:54,341 [INFO] Summary:
2019-03-07 17:52:54,342 [INFO] Batch 82000, worst loss 0.201048 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:52:54,342 [INFO] Regularization: 4724.250488 * 0.0000010000 = 0.0047242506
2019-03-07 17:52:54,343 [INFO] Sum of grad norms: 25.007074
2019-03-07 17:52:54,344 [INFO] ---------------------------------
2019-03-07 17:53:13,870 [INFO] ---------------------------------
2019-03-07 17:53:13,871 [INFO] Summary:
2019-03-07 17:53:13,871 [INFO] Batch 83000, worst loss 0.239828 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:53:13,872 [INFO] Regularization: 4724.163086 * 0.0000010000 = 0.0047241631
2019-03-07 17:53:13,873 [INFO] Sum of grad norms: 0.125662
2019-03-07 17:53:13,873 [INFO] ---------------------------------
2019-03-07 17:53:33,086 [INFO] ---------------------------------
2019-03-07 17:53:33,087 [INFO] Summary:
2019-03-07 17:53:33,087 [INFO] Batch 84000, worst loss 0.194427 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:53:33,088 [INFO] Regularization: 4724.215820 * 0.0000010000 = 0.0047242157
2019-03-07 17:53:33,089 [INFO] Sum of grad norms: 3.008341
2019-03-07 17:53:33,090 [INFO] ---------------------------------
2019-03-07 17:53:52,563 [INFO] ---------------------------------
2019-03-07 17:53:52,564 [INFO] Summary:
2019-03-07 17:53:52,565 [INFO] Batch 85000, worst loss 0.190042 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:53:52,566 [INFO] Regularization: 4723.937012 * 0.0000010000 = 0.0047239368
2019-03-07 17:53:52,567 [INFO] Sum of grad norms: 7.739800
2019-03-07 17:53:52,568 [INFO] ---------------------------------
2019-03-07 17:54:12,093 [INFO] ---------------------------------
2019-03-07 17:54:12,094 [INFO] Summary:
2019-03-07 17:54:12,095 [INFO] Batch 86000, worst loss 0.184775 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:54:12,096 [INFO] Regularization: 4723.878906 * 0.0000010000 = 0.0047238790
2019-03-07 17:54:12,097 [INFO] Sum of grad norms: 6.704670
2019-03-07 17:54:12,098 [INFO] ---------------------------------
2019-03-07 17:54:31,210 [INFO] ---------------------------------
2019-03-07 17:54:31,211 [INFO] Summary:
2019-03-07 17:54:31,212 [INFO] Batch 87000, worst loss 0.211005 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:54:31,213 [INFO] Regularization: 4723.792480 * 0.0000010000 = 0.0047237924
2019-03-07 17:54:31,213 [INFO] Sum of grad norms: 7.393929
2019-03-07 17:54:31,214 [INFO] ---------------------------------
2019-03-07 17:54:50,533 [INFO] ---------------------------------
2019-03-07 17:54:50,534 [INFO] Summary:
2019-03-07 17:54:50,534 [INFO] Batch 88000, worst loss 0.167445 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:54:50,536 [INFO] Regularization: 4723.767578 * 0.0000010000 = 0.0047237677
2019-03-07 17:54:50,536 [INFO] Sum of grad norms: 1.599760
2019-03-07 17:54:50,537 [INFO] ---------------------------------
2019-03-07 17:55:09,811 [INFO] ---------------------------------
2019-03-07 17:55:09,812 [INFO] Summary:
2019-03-07 17:55:09,813 [INFO] Batch 89000, worst loss 0.162113 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:55:09,813 [INFO] Regularization: 4723.666992 * 0.0000010000 = 0.0047236672
2019-03-07 17:55:09,814 [INFO] Sum of grad norms: 0.106269
2019-03-07 17:55:09,815 [INFO] ---------------------------------
2019-03-07 17:55:29,056 [INFO] ---------------------------------
2019-03-07 17:55:29,057 [INFO] Summary:
2019-03-07 17:55:29,058 [INFO] Batch 90000, worst loss 0.201271 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:55:29,059 [INFO] Regularization: 4723.686523 * 0.0000010000 = 0.0047236867
2019-03-07 17:55:29,060 [INFO] Sum of grad norms: 9.259181
2019-03-07 17:55:29,061 [INFO] ---------------------------------
2019-03-07 17:55:42,180 [INFO] ---------------------------------
2019-03-07 17:55:42,181 [INFO] Evaluation:
2019-03-07 17:55:42,182 [INFO] Batch 90000, worst loss 0.160660 (without reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-07 17:55:42,183 [INFO] ---------------------------------
2019-03-07 17:56:01,398 [INFO] ---------------------------------
2019-03-07 17:56:01,399 [INFO] Summary:
2019-03-07 17:56:01,400 [INFO] Batch 91000, worst loss 0.171479 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:56:01,400 [INFO] Regularization: 4723.625977 * 0.0000010000 = 0.0047236262
2019-03-07 17:56:01,401 [INFO] Sum of grad norms: 5.351074
2019-03-07 17:56:01,401 [INFO] ---------------------------------
2019-03-07 17:56:21,000 [INFO] ---------------------------------
2019-03-07 17:56:21,001 [INFO] Summary:
2019-03-07 17:56:21,002 [INFO] Batch 92000, worst loss 0.235232 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:56:21,002 [INFO] Regularization: 4723.500000 * 0.0000010000 = 0.0047235000
2019-03-07 17:56:21,003 [INFO] Sum of grad norms: 8.214576
2019-03-07 17:56:21,004 [INFO] ---------------------------------
2019-03-07 17:56:40,233 [INFO] ---------------------------------
2019-03-07 17:56:40,234 [INFO] Summary:
2019-03-07 17:56:40,235 [INFO] Batch 93000, worst loss 0.154184 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:56:40,237 [INFO] Regularization: 4723.482910 * 0.0000010000 = 0.0047234828
2019-03-07 17:56:40,238 [INFO] Sum of grad norms: 0.197430
2019-03-07 17:56:40,241 [INFO] ---------------------------------
2019-03-07 17:56:59,360 [INFO] ---------------------------------
2019-03-07 17:56:59,361 [INFO] Summary:
2019-03-07 17:56:59,361 [INFO] Batch 94000, worst loss 0.190787 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:56:59,362 [INFO] Regularization: 4723.407715 * 0.0000010000 = 0.0047234078
2019-03-07 17:56:59,362 [INFO] Sum of grad norms: 0.790457
2019-03-07 17:56:59,363 [INFO] ---------------------------------
2019-03-07 17:57:18,760 [INFO] ---------------------------------
2019-03-07 17:57:18,761 [INFO] Summary:
2019-03-07 17:57:18,762 [INFO] Batch 95000, worst loss 0.168855 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:57:18,762 [INFO] Regularization: 4723.395996 * 0.0000010000 = 0.0047233962
2019-03-07 17:57:18,763 [INFO] Sum of grad norms: 0.165272
2019-03-07 17:57:18,764 [INFO] ---------------------------------
2019-03-07 17:57:38,146 [INFO] ---------------------------------
2019-03-07 17:57:38,148 [INFO] Summary:
2019-03-07 17:57:38,148 [INFO] Batch 96000, worst loss 0.186767 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:57:38,149 [INFO] Regularization: 4723.408203 * 0.0000010000 = 0.0047234083
2019-03-07 17:57:38,150 [INFO] Sum of grad norms: 10.503059
2019-03-07 17:57:38,151 [INFO] ---------------------------------
2019-03-07 17:57:57,628 [INFO] ---------------------------------
2019-03-07 17:57:57,629 [INFO] Summary:
2019-03-07 17:57:57,630 [INFO] Batch 97000, worst loss 0.185854 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:57:57,631 [INFO] Regularization: 4723.421387 * 0.0000010000 = 0.0047234213
2019-03-07 17:57:57,631 [INFO] Sum of grad norms: 0.260032
2019-03-07 17:57:57,632 [INFO] ---------------------------------
2019-03-07 17:58:17,145 [INFO] ---------------------------------
2019-03-07 17:58:17,146 [INFO] Summary:
2019-03-07 17:58:17,147 [INFO] Batch 98000, worst loss 0.187903 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:58:17,148 [INFO] Regularization: 4723.391113 * 0.0000010000 = 0.0047233910
2019-03-07 17:58:17,148 [INFO] Sum of grad norms: 1.680188
2019-03-07 17:58:17,149 [INFO] ---------------------------------
2019-03-07 17:58:36,744 [INFO] ---------------------------------
2019-03-07 17:58:36,745 [INFO] Summary:
2019-03-07 17:58:36,746 [INFO] Batch 99000, worst loss 0.202610 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:58:36,747 [INFO] Regularization: 4723.381836 * 0.0000010000 = 0.0047233817
2019-03-07 17:58:36,748 [INFO] Sum of grad norms: 0.463505
2019-03-07 17:58:36,749 [INFO] ---------------------------------
2019-03-07 17:58:56,319 [INFO] ---------------------------------
2019-03-07 17:58:56,322 [INFO] Summary:
2019-03-07 17:58:56,323 [INFO] Batch 100000, worst loss 0.176997 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:58:56,326 [INFO] Regularization: 4723.333984 * 0.0000010000 = 0.0047233338
2019-03-07 17:58:56,327 [INFO] Sum of grad norms: 0.496948
2019-03-07 17:58:56,327 [INFO] ---------------------------------
2019-03-07 17:59:09,378 [INFO] ---------------------------------
2019-03-07 17:59:09,380 [INFO] Evaluation:
2019-03-07 17:59:09,380 [INFO] Batch 100000, worst loss 0.170573 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:59:09,381 [INFO] ---------------------------------
2019-03-07 17:59:28,885 [INFO] ---------------------------------
2019-03-07 17:59:28,886 [INFO] Summary:
2019-03-07 17:59:28,887 [INFO] Batch 101000, worst loss 0.164248 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:59:28,888 [INFO] Regularization: 4723.293457 * 0.0000010000 = 0.0047232932
2019-03-07 17:59:28,888 [INFO] Sum of grad norms: 2.549958
2019-03-07 17:59:28,889 [INFO] ---------------------------------
2019-03-07 17:59:48,179 [INFO] ---------------------------------
2019-03-07 17:59:48,180 [INFO] Summary:
2019-03-07 17:59:48,181 [INFO] Batch 102000, worst loss 0.161335 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 17:59:48,181 [INFO] Regularization: 4723.265137 * 0.0000010000 = 0.0047232653
2019-03-07 17:59:48,182 [INFO] Sum of grad norms: 6.044923
2019-03-07 17:59:48,183 [INFO] ---------------------------------
2019-03-07 18:00:07,684 [INFO] ---------------------------------
2019-03-07 18:00:07,685 [INFO] Summary:
2019-03-07 18:00:07,686 [INFO] Batch 103000, worst loss 0.148320 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:00:07,687 [INFO] Regularization: 4723.265137 * 0.0000010000 = 0.0047232653
2019-03-07 18:00:07,688 [INFO] Sum of grad norms: 8.893107
2019-03-07 18:00:07,688 [INFO] ---------------------------------
2019-03-07 18:00:26,956 [INFO] ---------------------------------
2019-03-07 18:00:26,957 [INFO] Summary:
2019-03-07 18:00:26,958 [INFO] Batch 104000, worst loss 0.178281 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:00:26,959 [INFO] Regularization: 4723.245117 * 0.0000010000 = 0.0047232453
2019-03-07 18:00:26,960 [INFO] Sum of grad norms: 0.466817
2019-03-07 18:00:26,961 [INFO] ---------------------------------
2019-03-07 18:00:46,430 [INFO] ---------------------------------
2019-03-07 18:00:46,431 [INFO] Summary:
2019-03-07 18:00:46,431 [INFO] Batch 105000, worst loss 0.179589 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:00:46,432 [INFO] Regularization: 4723.250488 * 0.0000010000 = 0.0047232504
2019-03-07 18:00:46,433 [INFO] Sum of grad norms: 0.208856
2019-03-07 18:00:46,434 [INFO] ---------------------------------
2019-03-07 18:01:05,682 [INFO] ---------------------------------
2019-03-07 18:01:05,683 [INFO] Summary:
2019-03-07 18:01:05,683 [INFO] Batch 106000, worst loss 0.195922 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:01:05,684 [INFO] Regularization: 4723.233887 * 0.0000010000 = 0.0047232341
2019-03-07 18:01:05,685 [INFO] Sum of grad norms: 3.704104
2019-03-07 18:01:05,685 [INFO] ---------------------------------
2019-03-07 18:01:25,179 [INFO] ---------------------------------
2019-03-07 18:01:25,180 [INFO] Summary:
2019-03-07 18:01:25,181 [INFO] Batch 107000, worst loss 0.195723 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:01:25,182 [INFO] Regularization: 4723.226074 * 0.0000010000 = 0.0047232262
2019-03-07 18:01:25,182 [INFO] Sum of grad norms: 5.109040
2019-03-07 18:01:25,183 [INFO] ---------------------------------
2019-03-07 18:01:44,959 [INFO] ---------------------------------
2019-03-07 18:01:44,960 [INFO] Summary:
2019-03-07 18:01:44,960 [INFO] Batch 108000, worst loss 0.191074 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:01:44,961 [INFO] Regularization: 4723.224121 * 0.0000010000 = 0.0047232243
2019-03-07 18:01:44,962 [INFO] Sum of grad norms: 2.172791
2019-03-07 18:01:44,963 [INFO] ---------------------------------
2019-03-07 18:02:04,632 [INFO] ---------------------------------
2019-03-07 18:02:04,633 [INFO] Summary:
2019-03-07 18:02:04,634 [INFO] Batch 109000, worst loss 0.154426 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:02:04,634 [INFO] Regularization: 4723.226562 * 0.0000010000 = 0.0047232267
2019-03-07 18:02:04,635 [INFO] Sum of grad norms: 0.138835
2019-03-07 18:02:04,635 [INFO] ---------------------------------
2019-03-07 18:02:24,205 [INFO] ---------------------------------
2019-03-07 18:02:24,206 [INFO] Summary:
2019-03-07 18:02:24,207 [INFO] Batch 110000, worst loss 0.149073 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:02:24,207 [INFO] Regularization: 4723.212891 * 0.0000010000 = 0.0047232127
2019-03-07 18:02:24,208 [INFO] Sum of grad norms: 1.590972
2019-03-07 18:02:24,209 [INFO] ---------------------------------
2019-03-07 18:02:37,624 [INFO] ---------------------------------
2019-03-07 18:02:37,625 [INFO] Evaluation:
2019-03-07 18:02:37,626 [INFO] Batch 110000, worst loss 0.168216 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:02:37,627 [INFO] ---------------------------------
2019-03-07 18:02:57,465 [INFO] ---------------------------------
2019-03-07 18:02:57,466 [INFO] Summary:
2019-03-07 18:02:57,466 [INFO] Batch 111000, worst loss 0.172929 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:02:57,467 [INFO] Regularization: 4723.206055 * 0.0000010000 = 0.0047232062
2019-03-07 18:02:57,468 [INFO] Sum of grad norms: 0.249413
2019-03-07 18:02:57,468 [INFO] ---------------------------------
2019-03-07 18:03:16,946 [INFO] ---------------------------------
2019-03-07 18:03:16,947 [INFO] Summary:
2019-03-07 18:03:16,948 [INFO] Batch 112000, worst loss 0.197511 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:03:16,950 [INFO] Regularization: 4723.200684 * 0.0000010000 = 0.0047232006
2019-03-07 18:03:16,951 [INFO] Sum of grad norms: 6.437685
2019-03-07 18:03:16,952 [INFO] ---------------------------------
2019-03-07 18:03:36,277 [INFO] ---------------------------------
2019-03-07 18:03:36,278 [INFO] Summary:
2019-03-07 18:03:36,279 [INFO] Batch 113000, worst loss 0.171716 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:03:36,280 [INFO] Regularization: 4723.200195 * 0.0000010000 = 0.0047232001
2019-03-07 18:03:36,281 [INFO] Sum of grad norms: 0.231160
2019-03-07 18:03:36,282 [INFO] ---------------------------------
2019-03-07 18:03:55,595 [INFO] ---------------------------------
2019-03-07 18:03:55,596 [INFO] Summary:
2019-03-07 18:03:55,597 [INFO] Batch 114000, worst loss 0.171730 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:03:55,598 [INFO] Regularization: 4723.198242 * 0.0000010000 = 0.0047231982
2019-03-07 18:03:55,598 [INFO] Sum of grad norms: 1.982297
2019-03-07 18:03:55,599 [INFO] ---------------------------------
2019-03-07 18:04:14,968 [INFO] ---------------------------------
2019-03-07 18:04:14,970 [INFO] Summary:
2019-03-07 18:04:14,970 [INFO] Batch 115000, worst loss 0.163884 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:04:14,971 [INFO] Regularization: 4723.194336 * 0.0000010000 = 0.0047231945
2019-03-07 18:04:14,971 [INFO] Sum of grad norms: 0.986198
2019-03-07 18:04:14,972 [INFO] ---------------------------------
2019-03-07 18:04:34,497 [INFO] ---------------------------------
2019-03-07 18:04:34,498 [INFO] Summary:
2019-03-07 18:04:34,498 [INFO] Batch 116000, worst loss 0.184416 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:04:34,499 [INFO] Regularization: 4723.197754 * 0.0000010000 = 0.0047231978
2019-03-07 18:04:34,500 [INFO] Sum of grad norms: 0.584655
2019-03-07 18:04:34,501 [INFO] ---------------------------------
2019-03-07 18:04:53,804 [INFO] ---------------------------------
2019-03-07 18:04:53,805 [INFO] Summary:
2019-03-07 18:04:53,805 [INFO] Batch 117000, worst loss 0.144789 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:04:53,807 [INFO] Regularization: 4723.194824 * 0.0000010000 = 0.0047231950
2019-03-07 18:04:53,807 [INFO] Sum of grad norms: 8.326993
2019-03-07 18:04:53,808 [INFO] ---------------------------------
2019-03-07 18:05:13,151 [INFO] ---------------------------------
2019-03-07 18:05:13,152 [INFO] Summary:
2019-03-07 18:05:13,152 [INFO] Batch 118000, worst loss 0.215427 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:05:13,153 [INFO] Regularization: 4723.192383 * 0.0000010000 = 0.0047231922
2019-03-07 18:05:13,154 [INFO] Sum of grad norms: 0.805836
2019-03-07 18:05:13,155 [INFO] ---------------------------------
2019-03-07 18:05:32,662 [INFO] ---------------------------------
2019-03-07 18:05:32,663 [INFO] Summary:
2019-03-07 18:05:32,664 [INFO] Batch 119000, worst loss 0.214178 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:05:32,665 [INFO] Regularization: 4723.191406 * 0.0000010000 = 0.0047231913
2019-03-07 18:05:32,666 [INFO] Sum of grad norms: 8.561295
2019-03-07 18:05:32,666 [INFO] ---------------------------------
2019-03-07 18:05:52,066 [INFO] ---------------------------------
2019-03-07 18:05:52,067 [INFO] Summary:
2019-03-07 18:05:52,068 [INFO] Batch 120000, worst loss 0.170760 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:05:52,068 [INFO] Regularization: 4723.194336 * 0.0000010000 = 0.0047231945
2019-03-07 18:05:52,069 [INFO] Sum of grad norms: 0.322219
2019-03-07 18:05:52,069 [INFO] ---------------------------------
2019-03-07 18:06:05,270 [INFO] ---------------------------------
2019-03-07 18:06:05,271 [INFO] Evaluation:
2019-03-07 18:06:05,271 [INFO] Batch 120000, worst loss 0.170132 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:06:05,272 [INFO] ---------------------------------
2019-03-07 18:06:24,605 [INFO] ---------------------------------
2019-03-07 18:06:24,606 [INFO] Summary:
2019-03-07 18:06:24,607 [INFO] Batch 121000, worst loss 0.168478 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:06:24,607 [INFO] Regularization: 4723.191895 * 0.0000010000 = 0.0047231917
2019-03-07 18:06:24,608 [INFO] Sum of grad norms: 4.575600
2019-03-07 18:06:24,609 [INFO] ---------------------------------
2019-03-07 18:06:44,130 [INFO] ---------------------------------
2019-03-07 18:06:44,131 [INFO] Summary:
2019-03-07 18:06:44,131 [INFO] Batch 122000, worst loss 0.148134 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:06:44,132 [INFO] Regularization: 4723.190918 * 0.0000010000 = 0.0047231908
2019-03-07 18:06:44,132 [INFO] Sum of grad norms: 0.181014
2019-03-07 18:06:44,133 [INFO] ---------------------------------
2019-03-07 18:07:03,797 [INFO] ---------------------------------
2019-03-07 18:07:03,798 [INFO] Summary:
2019-03-07 18:07:03,799 [INFO] Batch 123000, worst loss 0.167778 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:07:03,800 [INFO] Regularization: 4723.190918 * 0.0000010000 = 0.0047231908
2019-03-07 18:07:03,801 [INFO] Sum of grad norms: 4.559369
2019-03-07 18:07:03,802 [INFO] ---------------------------------
2019-03-07 18:07:23,432 [INFO] ---------------------------------
2019-03-07 18:07:23,433 [INFO] Summary:
2019-03-07 18:07:23,434 [INFO] Batch 124000, worst loss 0.199945 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:07:23,434 [INFO] Regularization: 4723.190918 * 0.0000010000 = 0.0047231908
2019-03-07 18:07:23,435 [INFO] Sum of grad norms: 5.041653
2019-03-07 18:07:23,437 [INFO] ---------------------------------
2019-03-07 18:07:43,065 [INFO] ---------------------------------
2019-03-07 18:07:43,066 [INFO] Summary:
2019-03-07 18:07:43,067 [INFO] Batch 125000, worst loss 0.199945 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:07:43,067 [INFO] Regularization: 4723.190918 * 0.0000010000 = 0.0047231908
2019-03-07 18:07:43,068 [INFO] Sum of grad norms: 2.279670
2019-03-07 18:07:43,069 [INFO] ---------------------------------
2019-03-07 18:08:02,882 [INFO] ---------------------------------
2019-03-07 18:08:02,883 [INFO] Summary:
2019-03-07 18:08:02,884 [INFO] Batch 126000, worst loss 0.177426 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:08:02,885 [INFO] Regularization: 4723.190430 * 0.0000010000 = 0.0047231903
2019-03-07 18:08:02,885 [INFO] Sum of grad norms: 8.880424
2019-03-07 18:08:02,886 [INFO] ---------------------------------
2019-03-07 18:08:22,422 [INFO] ---------------------------------
2019-03-07 18:08:22,423 [INFO] Summary:
2019-03-07 18:08:22,424 [INFO] Batch 127000, worst loss 0.188022 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:08:22,425 [INFO] Regularization: 4723.190430 * 0.0000010000 = 0.0047231903
2019-03-07 18:08:22,426 [INFO] Sum of grad norms: 4.570173
2019-03-07 18:08:22,427 [INFO] ---------------------------------
2019-03-07 18:08:41,919 [INFO] ---------------------------------
2019-03-07 18:08:41,919 [INFO] Summary:
2019-03-07 18:08:41,920 [INFO] Batch 128000, worst loss 0.170795 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:08:41,921 [INFO] Regularization: 4723.189941 * 0.0000010000 = 0.0047231899
2019-03-07 18:08:41,922 [INFO] Sum of grad norms: 1.145853
2019-03-07 18:08:41,922 [INFO] ---------------------------------
2019-03-07 18:09:01,400 [INFO] ---------------------------------
2019-03-07 18:09:01,401 [INFO] Summary:
2019-03-07 18:09:01,402 [INFO] Batch 129000, worst loss 0.170793 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:09:01,403 [INFO] Regularization: 4723.189941 * 0.0000010000 = 0.0047231899
2019-03-07 18:09:01,403 [INFO] Sum of grad norms: 0.195820
2019-03-07 18:09:01,404 [INFO] ---------------------------------
2019-03-07 18:09:20,643 [INFO] ---------------------------------
2019-03-07 18:09:20,644 [INFO] Summary:
2019-03-07 18:09:20,644 [INFO] Batch 130000, worst loss 0.182353 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:09:20,645 [INFO] Regularization: 4723.189941 * 0.0000010000 = 0.0047231899
2019-03-07 18:09:20,645 [INFO] Sum of grad norms: 2.885731
2019-03-07 18:09:20,646 [INFO] ---------------------------------
2019-03-07 18:09:33,664 [INFO] ---------------------------------
2019-03-07 18:09:33,665 [INFO] Evaluation:
2019-03-07 18:09:33,665 [INFO] Batch 130000, worst loss 0.185427 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:09:33,666 [INFO] ---------------------------------
2019-03-07 18:09:53,238 [INFO] ---------------------------------
2019-03-07 18:09:53,239 [INFO] Summary:
2019-03-07 18:09:53,239 [INFO] Batch 131000, worst loss 0.190150 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:09:53,240 [INFO] Regularization: 4723.189941 * 0.0000010000 = 0.0047231899
2019-03-07 18:09:53,241 [INFO] Sum of grad norms: 1.039960
2019-03-07 18:09:53,242 [INFO] ---------------------------------
2019-03-07 18:10:12,435 [INFO] ---------------------------------
2019-03-07 18:10:12,436 [INFO] Summary:
2019-03-07 18:10:12,437 [INFO] Batch 132000, worst loss 0.163371 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:10:12,438 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:10:12,439 [INFO] Sum of grad norms: 2.501344
2019-03-07 18:10:12,440 [INFO] ---------------------------------
2019-03-07 18:10:31,428 [INFO] ---------------------------------
2019-03-07 18:10:31,429 [INFO] Summary:
2019-03-07 18:10:31,429 [INFO] Batch 133000, worst loss 0.183654 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:10:31,430 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:10:31,431 [INFO] Sum of grad norms: 12.922306
2019-03-07 18:10:31,431 [INFO] ---------------------------------
2019-03-07 18:10:50,490 [INFO] ---------------------------------
2019-03-07 18:10:50,492 [INFO] Summary:
2019-03-07 18:10:50,494 [INFO] Batch 134000, worst loss 0.178694 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:10:50,495 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:10:50,496 [INFO] Sum of grad norms: 1.459374
2019-03-07 18:10:50,496 [INFO] ---------------------------------
2019-03-07 18:11:09,927 [INFO] ---------------------------------
2019-03-07 18:11:09,928 [INFO] Summary:
2019-03-07 18:11:09,929 [INFO] Batch 135000, worst loss 0.181891 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:11:09,929 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:11:09,930 [INFO] Sum of grad norms: 1.348313
2019-03-07 18:11:09,931 [INFO] ---------------------------------
2019-03-07 18:11:29,088 [INFO] ---------------------------------
2019-03-07 18:11:29,089 [INFO] Summary:
2019-03-07 18:11:29,089 [INFO] Batch 136000, worst loss 0.174809 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:11:29,090 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:11:29,091 [INFO] Sum of grad norms: 6.113113
2019-03-07 18:11:29,092 [INFO] ---------------------------------
2019-03-07 18:11:48,211 [INFO] ---------------------------------
2019-03-07 18:11:48,212 [INFO] Summary:
2019-03-07 18:11:48,213 [INFO] Batch 137000, worst loss 0.173689 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:11:48,215 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:11:48,216 [INFO] Sum of grad norms: 0.363759
2019-03-07 18:11:48,216 [INFO] ---------------------------------
2019-03-07 18:12:07,581 [INFO] ---------------------------------
2019-03-07 18:12:07,582 [INFO] Summary:
2019-03-07 18:12:07,582 [INFO] Batch 138000, worst loss 0.154844 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:12:07,583 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:12:07,584 [INFO] Sum of grad norms: 7.865170
2019-03-07 18:12:07,585 [INFO] ---------------------------------
2019-03-07 18:12:26,914 [INFO] ---------------------------------
2019-03-07 18:12:26,915 [INFO] Summary:
2019-03-07 18:12:26,916 [INFO] Batch 139000, worst loss 0.167661 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:12:26,917 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:12:26,917 [INFO] Sum of grad norms: 4.315897
2019-03-07 18:12:26,918 [INFO] ---------------------------------
2019-03-07 18:12:46,382 [INFO] ---------------------------------
2019-03-07 18:12:46,384 [INFO] Summary:
2019-03-07 18:12:46,384 [INFO] Batch 140000, worst loss 0.169565 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:12:46,385 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:12:46,386 [INFO] Sum of grad norms: 0.119357
2019-03-07 18:12:46,387 [INFO] ---------------------------------
2019-03-07 18:12:59,869 [INFO] ---------------------------------
2019-03-07 18:12:59,870 [INFO] Evaluation:
2019-03-07 18:12:59,870 [INFO] Batch 140000, worst loss 0.189864 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:12:59,871 [INFO] ---------------------------------
2019-03-07 18:13:19,590 [INFO] ---------------------------------
2019-03-07 18:13:19,591 [INFO] Summary:
2019-03-07 18:13:19,592 [INFO] Batch 141000, worst loss 0.194588 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:13:19,593 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:13:19,593 [INFO] Sum of grad norms: 0.214969
2019-03-07 18:13:19,594 [INFO] ---------------------------------
2019-03-07 18:13:38,790 [INFO] ---------------------------------
2019-03-07 18:13:38,791 [INFO] Summary:
2019-03-07 18:13:38,792 [INFO] Batch 142000, worst loss 0.159916 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:13:38,792 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:13:38,793 [INFO] Sum of grad norms: 1.745261
2019-03-07 18:13:38,794 [INFO] ---------------------------------
2019-03-07 18:13:58,311 [INFO] ---------------------------------
2019-03-07 18:13:58,312 [INFO] Summary:
2019-03-07 18:13:58,312 [INFO] Batch 143000, worst loss 0.186475 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:13:58,313 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:13:58,314 [INFO] Sum of grad norms: 11.976102
2019-03-07 18:13:58,315 [INFO] ---------------------------------
2019-03-07 18:14:17,868 [INFO] ---------------------------------
2019-03-07 18:14:17,869 [INFO] Summary:
2019-03-07 18:14:17,870 [INFO] Batch 144000, worst loss 0.176399 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:14:17,870 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:14:17,871 [INFO] Sum of grad norms: 1.917987
2019-03-07 18:14:17,872 [INFO] ---------------------------------
2019-03-07 18:14:37,091 [INFO] ---------------------------------
2019-03-07 18:14:37,092 [INFO] Summary:
2019-03-07 18:14:37,093 [INFO] Batch 145000, worst loss 0.172998 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:14:37,093 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:14:37,094 [INFO] Sum of grad norms: 0.163914
2019-03-07 18:14:37,095 [INFO] ---------------------------------
2019-03-07 18:14:56,647 [INFO] ---------------------------------
2019-03-07 18:14:56,649 [INFO] Summary:
2019-03-07 18:14:56,649 [INFO] Batch 146000, worst loss 0.153564 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:14:56,650 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:14:56,651 [INFO] Sum of grad norms: 2.416279
2019-03-07 18:14:56,652 [INFO] ---------------------------------
2019-03-07 18:15:15,918 [INFO] ---------------------------------
2019-03-07 18:15:15,919 [INFO] Summary:
2019-03-07 18:15:15,919 [INFO] Batch 147000, worst loss 0.162562 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:15:15,920 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:15:15,921 [INFO] Sum of grad norms: 2.643868
2019-03-07 18:15:15,922 [INFO] ---------------------------------
2019-03-07 18:15:35,159 [INFO] ---------------------------------
2019-03-07 18:15:35,160 [INFO] Summary:
2019-03-07 18:15:35,161 [INFO] Batch 148000, worst loss 0.184478 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:15:35,162 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:15:35,162 [INFO] Sum of grad norms: 2.582276
2019-03-07 18:15:35,163 [INFO] ---------------------------------
2019-03-07 18:15:54,724 [INFO] ---------------------------------
2019-03-07 18:15:54,725 [INFO] Summary:
2019-03-07 18:15:54,725 [INFO] Batch 149000, worst loss 0.162989 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:15:54,726 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:15:54,727 [INFO] Sum of grad norms: 2.572936
2019-03-07 18:15:54,728 [INFO] ---------------------------------
2019-03-07 18:16:14,157 [INFO] ---------------------------------
2019-03-07 18:16:14,159 [INFO] Summary:
2019-03-07 18:16:14,159 [INFO] Batch 150000, worst loss 0.148716 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:16:14,163 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:16:14,165 [INFO] Sum of grad norms: 3.509234
2019-03-07 18:16:14,166 [INFO] ---------------------------------
2019-03-07 18:16:27,482 [INFO] ---------------------------------
2019-03-07 18:16:27,482 [INFO] Evaluation:
2019-03-07 18:16:27,486 [INFO] Batch 150000, worst loss 0.141971 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:16:27,487 [INFO] ---------------------------------
2019-03-07 18:16:47,303 [INFO] ---------------------------------
2019-03-07 18:16:47,304 [INFO] Summary:
2019-03-07 18:16:47,304 [INFO] Batch 151000, worst loss 0.237719 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:16:47,305 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:16:47,306 [INFO] Sum of grad norms: 8.797428
2019-03-07 18:16:47,307 [INFO] ---------------------------------
2019-03-07 18:17:06,486 [INFO] ---------------------------------
2019-03-07 18:17:06,487 [INFO] Summary:
2019-03-07 18:17:06,488 [INFO] Batch 152000, worst loss 0.206334 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:17:06,489 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:17:06,491 [INFO] Sum of grad norms: 5.566382
2019-03-07 18:17:06,492 [INFO] ---------------------------------
2019-03-07 18:17:26,112 [INFO] ---------------------------------
2019-03-07 18:17:26,113 [INFO] Summary:
2019-03-07 18:17:26,114 [INFO] Batch 153000, worst loss 0.150491 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:17:26,114 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:17:26,115 [INFO] Sum of grad norms: 1.153173
2019-03-07 18:17:26,116 [INFO] ---------------------------------
2019-03-07 18:17:45,689 [INFO] ---------------------------------
2019-03-07 18:17:45,690 [INFO] Summary:
2019-03-07 18:17:45,690 [INFO] Batch 154000, worst loss 0.215179 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:17:45,691 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:17:45,693 [INFO] Sum of grad norms: 0.170870
2019-03-07 18:17:45,694 [INFO] ---------------------------------
2019-03-07 18:18:05,357 [INFO] ---------------------------------
2019-03-07 18:18:05,358 [INFO] Summary:
2019-03-07 18:18:05,359 [INFO] Batch 155000, worst loss 0.207459 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:18:05,360 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:18:05,360 [INFO] Sum of grad norms: 6.230646
2019-03-07 18:18:05,361 [INFO] ---------------------------------
2019-03-07 18:18:24,928 [INFO] ---------------------------------
2019-03-07 18:18:24,929 [INFO] Summary:
2019-03-07 18:18:24,931 [INFO] Batch 156000, worst loss 0.146152 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:18:24,932 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:18:24,933 [INFO] Sum of grad norms: 1.889996
2019-03-07 18:18:24,934 [INFO] ---------------------------------
2019-03-07 18:18:44,289 [INFO] ---------------------------------
2019-03-07 18:18:44,290 [INFO] Summary:
2019-03-07 18:18:44,291 [INFO] Batch 157000, worst loss 0.162966 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:18:44,292 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:18:44,293 [INFO] Sum of grad norms: 0.127229
2019-03-07 18:18:44,293 [INFO] ---------------------------------
2019-03-07 18:19:03,240 [INFO] ---------------------------------
2019-03-07 18:19:03,241 [INFO] Summary:
2019-03-07 18:19:03,242 [INFO] Batch 158000, worst loss 0.179495 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:19:03,243 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:19:03,244 [INFO] Sum of grad norms: 0.146026
2019-03-07 18:19:03,245 [INFO] ---------------------------------
2019-03-07 18:19:22,757 [INFO] ---------------------------------
2019-03-07 18:19:22,759 [INFO] Summary:
2019-03-07 18:19:22,759 [INFO] Batch 159000, worst loss 0.188467 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:19:22,760 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:19:22,761 [INFO] Sum of grad norms: 0.136603
2019-03-07 18:19:22,762 [INFO] ---------------------------------
2019-03-07 18:19:42,282 [INFO] ---------------------------------
2019-03-07 18:19:42,283 [INFO] Summary:
2019-03-07 18:19:42,284 [INFO] Batch 160000, worst loss 0.208319 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:19:42,284 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:19:42,285 [INFO] Sum of grad norms: 0.260662
2019-03-07 18:19:42,286 [INFO] ---------------------------------
2019-03-07 18:19:55,279 [INFO] ---------------------------------
2019-03-07 18:19:55,281 [INFO] Evaluation:
2019-03-07 18:19:55,282 [INFO] Batch 160000, worst loss 0.203596 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:19:55,283 [INFO] ---------------------------------
2019-03-07 18:20:14,732 [INFO] ---------------------------------
2019-03-07 18:20:14,734 [INFO] Summary:
2019-03-07 18:20:14,734 [INFO] Batch 161000, worst loss 0.147687 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:20:14,735 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:20:14,736 [INFO] Sum of grad norms: 0.280297
2019-03-07 18:20:14,737 [INFO] ---------------------------------
2019-03-07 18:20:34,047 [INFO] ---------------------------------
2019-03-07 18:20:34,048 [INFO] Summary:
2019-03-07 18:20:34,049 [INFO] Batch 162000, worst loss 0.199868 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:20:34,050 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:20:34,051 [INFO] Sum of grad norms: 1.390570
2019-03-07 18:20:34,051 [INFO] ---------------------------------
2019-03-07 18:20:53,688 [INFO] ---------------------------------
2019-03-07 18:20:53,689 [INFO] Summary:
2019-03-07 18:20:53,690 [INFO] Batch 163000, worst loss 0.165809 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:20:53,691 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:20:53,692 [INFO] Sum of grad norms: 2.807837
2019-03-07 18:20:53,692 [INFO] ---------------------------------
2019-03-07 18:21:13,366 [INFO] ---------------------------------
2019-03-07 18:21:13,367 [INFO] Summary:
2019-03-07 18:21:13,368 [INFO] Batch 164000, worst loss 0.164041 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:21:13,369 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:21:13,370 [INFO] Sum of grad norms: 3.033998
2019-03-07 18:21:13,370 [INFO] ---------------------------------
2019-03-07 18:21:32,790 [INFO] ---------------------------------
2019-03-07 18:21:32,791 [INFO] Summary:
2019-03-07 18:21:32,791 [INFO] Batch 165000, worst loss 0.176382 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:21:32,792 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:21:32,793 [INFO] Sum of grad norms: 6.826960
2019-03-07 18:21:32,794 [INFO] ---------------------------------
2019-03-07 18:21:52,288 [INFO] ---------------------------------
2019-03-07 18:21:52,289 [INFO] Summary:
2019-03-07 18:21:52,290 [INFO] Batch 166000, worst loss 0.163267 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:21:52,290 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:21:52,291 [INFO] Sum of grad norms: 0.143495
2019-03-07 18:21:52,293 [INFO] ---------------------------------
2019-03-07 18:22:11,868 [INFO] ---------------------------------
2019-03-07 18:22:11,870 [INFO] Summary:
2019-03-07 18:22:11,870 [INFO] Batch 167000, worst loss 0.170665 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:22:11,871 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:22:11,872 [INFO] Sum of grad norms: 7.322738
2019-03-07 18:22:11,873 [INFO] ---------------------------------
2019-03-07 18:22:31,667 [INFO] ---------------------------------
2019-03-07 18:22:31,668 [INFO] Summary:
2019-03-07 18:22:31,669 [INFO] Batch 168000, worst loss 0.195881 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:22:31,670 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:22:31,671 [INFO] Sum of grad norms: 0.244219
2019-03-07 18:22:31,672 [INFO] ---------------------------------
2019-03-07 18:22:51,235 [INFO] ---------------------------------
2019-03-07 18:22:51,236 [INFO] Summary:
2019-03-07 18:22:51,237 [INFO] Batch 169000, worst loss 0.171931 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:22:51,237 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:22:51,238 [INFO] Sum of grad norms: 9.998530
2019-03-07 18:22:51,239 [INFO] ---------------------------------
2019-03-07 18:23:10,754 [INFO] ---------------------------------
2019-03-07 18:23:10,755 [INFO] Summary:
2019-03-07 18:23:10,756 [INFO] Batch 170000, worst loss 0.156193 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:23:10,756 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:23:10,757 [INFO] Sum of grad norms: 8.230360
2019-03-07 18:23:10,758 [INFO] ---------------------------------
2019-03-07 18:23:24,074 [INFO] ---------------------------------
2019-03-07 18:23:24,075 [INFO] Evaluation:
2019-03-07 18:23:24,076 [INFO] Batch 170000, worst loss 0.184285 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:23:24,077 [INFO] ---------------------------------
2019-03-07 18:23:43,417 [INFO] ---------------------------------
2019-03-07 18:23:43,418 [INFO] Summary:
2019-03-07 18:23:43,419 [INFO] Batch 171000, worst loss 0.161962 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:23:43,419 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:23:43,420 [INFO] Sum of grad norms: 3.460157
2019-03-07 18:23:43,421 [INFO] ---------------------------------
2019-03-07 18:24:02,781 [INFO] ---------------------------------
2019-03-07 18:24:02,783 [INFO] Summary:
2019-03-07 18:24:02,784 [INFO] Batch 172000, worst loss 0.166519 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:24:02,785 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:24:02,786 [INFO] Sum of grad norms: 0.334828
2019-03-07 18:24:02,787 [INFO] ---------------------------------
2019-03-07 18:24:22,143 [INFO] ---------------------------------
2019-03-07 18:24:22,144 [INFO] Summary:
2019-03-07 18:24:22,145 [INFO] Batch 173000, worst loss 0.153559 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:24:22,145 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:24:22,146 [INFO] Sum of grad norms: 0.253434
2019-03-07 18:24:22,147 [INFO] ---------------------------------
2019-03-07 18:24:41,569 [INFO] ---------------------------------
2019-03-07 18:24:41,570 [INFO] Summary:
2019-03-07 18:24:41,571 [INFO] Batch 174000, worst loss 0.149233 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:24:41,571 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:24:41,572 [INFO] Sum of grad norms: 0.419186
2019-03-07 18:24:41,573 [INFO] ---------------------------------
2019-03-07 18:25:00,938 [INFO] ---------------------------------
2019-03-07 18:25:00,939 [INFO] Summary:
2019-03-07 18:25:00,939 [INFO] Batch 175000, worst loss 0.160468 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:25:00,941 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:25:00,942 [INFO] Sum of grad norms: 0.320317
2019-03-07 18:25:00,943 [INFO] ---------------------------------
2019-03-07 18:25:20,522 [INFO] ---------------------------------
2019-03-07 18:25:20,523 [INFO] Summary:
2019-03-07 18:25:20,524 [INFO] Batch 176000, worst loss 0.173266 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:25:20,524 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:25:20,525 [INFO] Sum of grad norms: 5.096164
2019-03-07 18:25:20,526 [INFO] ---------------------------------
2019-03-07 18:25:39,846 [INFO] ---------------------------------
2019-03-07 18:25:39,847 [INFO] Summary:
2019-03-07 18:25:39,848 [INFO] Batch 177000, worst loss 0.156880 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:25:39,849 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:25:39,849 [INFO] Sum of grad norms: 2.296289
2019-03-07 18:25:39,850 [INFO] ---------------------------------
2019-03-07 18:25:59,385 [INFO] ---------------------------------
2019-03-07 18:25:59,387 [INFO] Summary:
2019-03-07 18:25:59,387 [INFO] Batch 178000, worst loss 0.156880 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:25:59,389 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:25:59,389 [INFO] Sum of grad norms: 10.018800
2019-03-07 18:25:59,390 [INFO] ---------------------------------
2019-03-07 18:26:18,590 [INFO] ---------------------------------
2019-03-07 18:26:18,591 [INFO] Summary:
2019-03-07 18:26:18,591 [INFO] Batch 179000, worst loss 0.172108 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:26:18,592 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:26:18,593 [INFO] Sum of grad norms: 10.493472
2019-03-07 18:26:18,594 [INFO] ---------------------------------
2019-03-07 18:26:38,207 [INFO] ---------------------------------
2019-03-07 18:26:38,208 [INFO] Summary:
2019-03-07 18:26:38,208 [INFO] Batch 180000, worst loss 0.164795 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:26:38,209 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:26:38,209 [INFO] Sum of grad norms: 6.794498
2019-03-07 18:26:38,210 [INFO] ---------------------------------
2019-03-07 18:26:51,518 [INFO] ---------------------------------
2019-03-07 18:26:51,519 [INFO] Evaluation:
2019-03-07 18:26:51,520 [INFO] Batch 180000, worst loss 0.150009 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:26:51,522 [INFO] ---------------------------------
2019-03-07 18:27:11,213 [INFO] ---------------------------------
2019-03-07 18:27:11,214 [INFO] Summary:
2019-03-07 18:27:11,215 [INFO] Batch 181000, worst loss 0.149214 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:27:11,215 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:27:11,216 [INFO] Sum of grad norms: 0.181432
2019-03-07 18:27:11,216 [INFO] ---------------------------------
2019-03-07 18:27:30,806 [INFO] ---------------------------------
2019-03-07 18:27:30,807 [INFO] Summary:
2019-03-07 18:27:30,807 [INFO] Batch 182000, worst loss 0.182531 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:27:30,808 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:27:30,809 [INFO] Sum of grad norms: 13.764931
2019-03-07 18:27:30,809 [INFO] ---------------------------------
2019-03-07 18:27:50,338 [INFO] ---------------------------------
2019-03-07 18:27:50,339 [INFO] Summary:
2019-03-07 18:27:50,339 [INFO] Batch 183000, worst loss 0.163081 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:27:50,340 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:27:50,341 [INFO] Sum of grad norms: 0.145092
2019-03-07 18:27:50,341 [INFO] ---------------------------------
2019-03-07 18:28:10,300 [INFO] ---------------------------------
2019-03-07 18:28:10,301 [INFO] Summary:
2019-03-07 18:28:10,302 [INFO] Batch 184000, worst loss 0.164696 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:28:10,302 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:28:10,303 [INFO] Sum of grad norms: 0.326681
2019-03-07 18:28:10,304 [INFO] ---------------------------------
2019-03-07 18:28:30,039 [INFO] ---------------------------------
2019-03-07 18:28:30,040 [INFO] Summary:
2019-03-07 18:28:30,040 [INFO] Batch 185000, worst loss 0.170510 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:28:30,041 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:28:30,042 [INFO] Sum of grad norms: 5.481423
2019-03-07 18:28:30,043 [INFO] ---------------------------------
2019-03-07 18:28:49,734 [INFO] ---------------------------------
2019-03-07 18:28:49,735 [INFO] Summary:
2019-03-07 18:28:49,736 [INFO] Batch 186000, worst loss 0.213437 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:28:49,736 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:28:49,737 [INFO] Sum of grad norms: 1.204254
2019-03-07 18:28:49,738 [INFO] ---------------------------------
2019-03-07 18:29:09,144 [INFO] ---------------------------------
2019-03-07 18:29:09,145 [INFO] Summary:
2019-03-07 18:29:09,145 [INFO] Batch 187000, worst loss 0.169659 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:29:09,146 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:29:09,147 [INFO] Sum of grad norms: 8.977845
2019-03-07 18:29:09,147 [INFO] ---------------------------------
2019-03-07 18:29:28,349 [INFO] ---------------------------------
2019-03-07 18:29:28,350 [INFO] Summary:
2019-03-07 18:29:28,350 [INFO] Batch 188000, worst loss 0.157408 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:29:28,350 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:29:28,351 [INFO] Sum of grad norms: 11.230602
2019-03-07 18:29:28,352 [INFO] ---------------------------------
2019-03-07 18:29:47,916 [INFO] ---------------------------------
2019-03-07 18:29:47,917 [INFO] Summary:
2019-03-07 18:29:47,917 [INFO] Batch 189000, worst loss 0.204691 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:29:47,918 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:29:47,918 [INFO] Sum of grad norms: 1.706155
2019-03-07 18:29:47,919 [INFO] ---------------------------------
2019-03-07 18:30:07,545 [INFO] ---------------------------------
2019-03-07 18:30:07,546 [INFO] Summary:
2019-03-07 18:30:07,547 [INFO] Batch 190000, worst loss 0.204691 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:30:07,548 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:30:07,549 [INFO] Sum of grad norms: 4.886823
2019-03-07 18:30:07,549 [INFO] ---------------------------------
2019-03-07 18:30:20,701 [INFO] ---------------------------------
2019-03-07 18:30:20,702 [INFO] Evaluation:
2019-03-07 18:30:20,703 [INFO] Batch 190000, worst loss 0.155573 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:30:20,703 [INFO] ---------------------------------
2019-03-07 18:30:40,104 [INFO] ---------------------------------
2019-03-07 18:30:40,105 [INFO] Summary:
2019-03-07 18:30:40,106 [INFO] Batch 191000, worst loss 0.177384 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:30:40,107 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:30:40,108 [INFO] Sum of grad norms: 7.446808
2019-03-07 18:30:40,108 [INFO] ---------------------------------
2019-03-07 18:30:59,270 [INFO] ---------------------------------
2019-03-07 18:30:59,272 [INFO] Summary:
2019-03-07 18:30:59,273 [INFO] Batch 192000, worst loss 0.173946 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:30:59,274 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:30:59,274 [INFO] Sum of grad norms: 1.081397
2019-03-07 18:30:59,276 [INFO] ---------------------------------
2019-03-07 18:31:18,894 [INFO] ---------------------------------
2019-03-07 18:31:18,895 [INFO] Summary:
2019-03-07 18:31:18,896 [INFO] Batch 193000, worst loss 0.215831 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:31:18,896 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:31:18,897 [INFO] Sum of grad norms: 0.125873
2019-03-07 18:31:18,898 [INFO] ---------------------------------
2019-03-07 18:31:38,466 [INFO] ---------------------------------
2019-03-07 18:31:38,467 [INFO] Summary:
2019-03-07 18:31:38,468 [INFO] Batch 194000, worst loss 0.142350 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:31:38,468 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:31:38,469 [INFO] Sum of grad norms: 1.774419
2019-03-07 18:31:38,469 [INFO] ---------------------------------
2019-03-07 18:31:57,944 [INFO] ---------------------------------
2019-03-07 18:31:57,945 [INFO] Summary:
2019-03-07 18:31:57,946 [INFO] Batch 195000, worst loss 0.179480 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:31:57,947 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:31:57,947 [INFO] Sum of grad norms: 6.055139
2019-03-07 18:31:57,948 [INFO] ---------------------------------
2019-03-07 18:32:17,305 [INFO] ---------------------------------
2019-03-07 18:32:17,306 [INFO] Summary:
2019-03-07 18:32:17,307 [INFO] Batch 196000, worst loss 0.182774 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:32:17,308 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:32:17,309 [INFO] Sum of grad norms: 0.700257
2019-03-07 18:32:17,310 [INFO] ---------------------------------
2019-03-07 18:32:36,904 [INFO] ---------------------------------
2019-03-07 18:32:36,905 [INFO] Summary:
2019-03-07 18:32:36,905 [INFO] Batch 197000, worst loss 0.187174 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:32:36,906 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:32:36,907 [INFO] Sum of grad norms: 0.118188
2019-03-07 18:32:36,908 [INFO] ---------------------------------
2019-03-07 18:32:56,819 [INFO] ---------------------------------
2019-03-07 18:32:56,820 [INFO] Summary:
2019-03-07 18:32:56,821 [INFO] Batch 198000, worst loss 0.145416 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:32:56,822 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:32:56,823 [INFO] Sum of grad norms: 10.707406
2019-03-07 18:32:56,823 [INFO] ---------------------------------
2019-03-07 18:33:16,248 [INFO] ---------------------------------
2019-03-07 18:33:16,249 [INFO] Summary:
2019-03-07 18:33:16,249 [INFO] Batch 199000, worst loss 0.196111 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:33:16,250 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:33:16,251 [INFO] Sum of grad norms: 4.770188
2019-03-07 18:33:16,252 [INFO] ---------------------------------
2019-03-07 18:33:35,769 [INFO] ---------------------------------
2019-03-07 18:33:35,770 [INFO] Summary:
2019-03-07 18:33:35,771 [INFO] Batch 200000, worst loss 0.171470 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:33:35,771 [INFO] Regularization: 4723.189453 * 0.0000010000 = 0.0047231894
2019-03-07 18:33:35,772 [INFO] Sum of grad norms: 4.787026
2019-03-07 18:33:35,772 [INFO] ---------------------------------
2019-03-07 18:33:48,932 [INFO] ---------------------------------
2019-03-07 18:33:48,933 [INFO] Evaluation:
2019-03-07 18:33:48,934 [INFO] Batch 200000, worst loss 0.164583 (without reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-07 18:33:48,935 [INFO] ---------------------------------
2019-03-07 18:33:48,935 [INFO] Finished training, saved to file classifier/1551963312/1551980028_3_classifier_final.pth
2019-03-07 18:33:49,217 [INFO] Training model #4: (9, 64, 402) @ 2
2019-03-07 18:34:08,147 [INFO] ---------------------------------
2019-03-07 18:34:08,148 [INFO] Summary:
2019-03-07 18:34:08,149 [INFO] Batch 1000, worst loss 1848.072876 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:34:08,149 [INFO] Regularization: 14786.924805 * 0.0000010000 = 0.0147869252
2019-03-07 18:34:08,150 [INFO] Sum of grad norms: 0.007293
2019-03-07 18:34:08,151 [INFO] ---------------------------------
2019-03-07 18:34:27,298 [INFO] ---------------------------------
2019-03-07 18:34:27,299 [INFO] Summary:
2019-03-07 18:34:27,300 [INFO] Batch 2000, worst loss 1.504420 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:34:27,301 [INFO] Regularization: 10381.772461 * 0.0000010000 = 0.0103817722
2019-03-07 18:34:27,302 [INFO] Sum of grad norms: 0.005858
2019-03-07 18:34:27,303 [INFO] ---------------------------------
2019-03-07 18:34:46,516 [INFO] ---------------------------------
2019-03-07 18:34:46,517 [INFO] Summary:
2019-03-07 18:34:46,518 [INFO] Batch 3000, worst loss 0.700202 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:34:46,518 [INFO] Regularization: 7714.860840 * 0.0000010000 = 0.0077148606
2019-03-07 18:34:46,519 [INFO] Sum of grad norms: 0.007096
2019-03-07 18:34:46,519 [INFO] ---------------------------------
2019-03-07 18:35:05,579 [INFO] ---------------------------------
2019-03-07 18:35:05,580 [INFO] Summary:
2019-03-07 18:35:05,581 [INFO] Batch 4000, worst loss 1.362833 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:35:05,581 [INFO] Regularization: 7602.024902 * 0.0000010000 = 0.0076020248
2019-03-07 18:35:05,582 [INFO] Sum of grad norms: 11.710948
2019-03-07 18:35:05,582 [INFO] ---------------------------------
2019-03-07 18:35:25,074 [INFO] ---------------------------------
2019-03-07 18:35:25,075 [INFO] Summary:
2019-03-07 18:35:25,075 [INFO] Batch 5000, worst loss 0.450362 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:35:25,076 [INFO] Regularization: 7495.016113 * 0.0000010000 = 0.0074950163
2019-03-07 18:35:25,077 [INFO] Sum of grad norms: 6.754228
2019-03-07 18:35:25,079 [INFO] ---------------------------------
2019-03-07 18:35:44,768 [INFO] ---------------------------------
2019-03-07 18:35:44,769 [INFO] Summary:
2019-03-07 18:35:44,770 [INFO] Batch 6000, worst loss 0.342816 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:35:44,771 [INFO] Regularization: 7311.725586 * 0.0000010000 = 0.0073117255
2019-03-07 18:35:44,772 [INFO] Sum of grad norms: 0.752320
2019-03-07 18:35:44,773 [INFO] ---------------------------------
2019-03-07 18:36:03,927 [INFO] ---------------------------------
2019-03-07 18:36:03,928 [INFO] Summary:
2019-03-07 18:36:03,929 [INFO] Batch 7000, worst loss 0.394368 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:36:03,930 [INFO] Regularization: 7098.108887 * 0.0000010000 = 0.0070981090
2019-03-07 18:36:03,931 [INFO] Sum of grad norms: 7.349253
2019-03-07 18:36:03,932 [INFO] ---------------------------------
2019-03-07 18:36:23,290 [INFO] ---------------------------------
2019-03-07 18:36:23,291 [INFO] Summary:
2019-03-07 18:36:23,292 [INFO] Batch 8000, worst loss 0.387430 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:36:23,293 [INFO] Regularization: 6865.384766 * 0.0000010000 = 0.0068653845
2019-03-07 18:36:23,293 [INFO] Sum of grad norms: 5.382931
2019-03-07 18:36:23,294 [INFO] ---------------------------------
2019-03-07 18:36:42,822 [INFO] ---------------------------------
2019-03-07 18:36:42,823 [INFO] Summary:
2019-03-07 18:36:42,824 [INFO] Batch 9000, worst loss 0.375191 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:36:42,825 [INFO] Regularization: 6645.962891 * 0.0000010000 = 0.0066459631
2019-03-07 18:36:42,825 [INFO] Sum of grad norms: 6.363722
2019-03-07 18:36:42,826 [INFO] ---------------------------------
2019-03-07 18:37:02,092 [INFO] ---------------------------------
2019-03-07 18:37:02,093 [INFO] Summary:
2019-03-07 18:37:02,093 [INFO] Batch 10000, worst loss 0.293513 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:37:02,094 [INFO] Regularization: 6509.510742 * 0.0000010000 = 0.0065095108
2019-03-07 18:37:02,095 [INFO] Sum of grad norms: 1.650673
2019-03-07 18:37:02,096 [INFO] ---------------------------------
2019-03-07 18:37:15,146 [INFO] ---------------------------------
2019-03-07 18:37:15,148 [INFO] Evaluation:
2019-03-07 18:37:15,149 [INFO] Batch 10000, worst loss 0.265901 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:37:15,150 [INFO] ---------------------------------
2019-03-07 18:37:34,610 [INFO] ---------------------------------
2019-03-07 18:37:34,612 [INFO] Summary:
2019-03-07 18:37:34,613 [INFO] Batch 11000, worst loss 0.331180 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:37:34,614 [INFO] Regularization: 6331.239746 * 0.0000010000 = 0.0063312398
2019-03-07 18:37:34,615 [INFO] Sum of grad norms: 5.488784
2019-03-07 18:37:34,617 [INFO] ---------------------------------
2019-03-07 18:37:54,077 [INFO] ---------------------------------
2019-03-07 18:37:54,078 [INFO] Summary:
2019-03-07 18:37:54,079 [INFO] Batch 12000, worst loss 0.349040 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:37:54,080 [INFO] Regularization: 6207.410156 * 0.0000010000 = 0.0062074102
2019-03-07 18:37:54,081 [INFO] Sum of grad norms: 5.458011
2019-03-07 18:37:54,081 [INFO] ---------------------------------
2019-03-07 18:38:13,262 [INFO] ---------------------------------
2019-03-07 18:38:13,263 [INFO] Summary:
2019-03-07 18:38:13,264 [INFO] Batch 13000, worst loss 0.346537 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:38:13,265 [INFO] Regularization: 6133.658203 * 0.0000010000 = 0.0061336583
2019-03-07 18:38:13,266 [INFO] Sum of grad norms: 2.962481
2019-03-07 18:38:13,266 [INFO] ---------------------------------
2019-03-07 18:38:32,705 [INFO] ---------------------------------
2019-03-07 18:38:32,706 [INFO] Summary:
2019-03-07 18:38:32,706 [INFO] Batch 14000, worst loss 0.320589 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:38:32,707 [INFO] Regularization: 5882.347168 * 0.0000010000 = 0.0058823470
2019-03-07 18:38:32,708 [INFO] Sum of grad norms: 4.903368
2019-03-07 18:38:32,709 [INFO] ---------------------------------
2019-03-07 18:38:52,027 [INFO] ---------------------------------
2019-03-07 18:38:52,028 [INFO] Summary:
2019-03-07 18:38:52,029 [INFO] Batch 15000, worst loss 0.338852 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:38:52,029 [INFO] Regularization: 5810.941406 * 0.0000010000 = 0.0058109416
2019-03-07 18:38:52,030 [INFO] Sum of grad norms: 2.181957
2019-03-07 18:38:52,031 [INFO] ---------------------------------
2019-03-07 18:39:11,456 [INFO] ---------------------------------
2019-03-07 18:39:11,457 [INFO] Summary:
2019-03-07 18:39:11,457 [INFO] Batch 16000, worst loss 0.308027 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:39:11,458 [INFO] Regularization: 5774.060547 * 0.0000010000 = 0.0057740607
2019-03-07 18:39:11,459 [INFO] Sum of grad norms: 1.330161
2019-03-07 18:39:11,460 [INFO] ---------------------------------
2019-03-07 18:39:30,582 [INFO] ---------------------------------
2019-03-07 18:39:30,583 [INFO] Summary:
2019-03-07 18:39:30,584 [INFO] Batch 17000, worst loss 0.379870 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:39:30,585 [INFO] Regularization: 5694.754883 * 0.0000010000 = 0.0056947549
2019-03-07 18:39:30,586 [INFO] Sum of grad norms: 7.073238
2019-03-07 18:39:30,587 [INFO] ---------------------------------
2019-03-07 18:39:49,746 [INFO] ---------------------------------
2019-03-07 18:39:49,747 [INFO] Summary:
2019-03-07 18:39:49,750 [INFO] Batch 18000, worst loss 0.355263 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:39:49,750 [INFO] Regularization: 5627.812988 * 0.0000010000 = 0.0056278128
2019-03-07 18:39:49,751 [INFO] Sum of grad norms: 4.559099
2019-03-07 18:39:49,751 [INFO] ---------------------------------
2019-03-07 18:40:08,954 [INFO] ---------------------------------
2019-03-07 18:40:08,955 [INFO] Summary:
2019-03-07 18:40:08,955 [INFO] Batch 19000, worst loss 0.340392 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:40:08,956 [INFO] Regularization: 5597.226562 * 0.0000010000 = 0.0055972263
2019-03-07 18:40:08,957 [INFO] Sum of grad norms: 2.656298
2019-03-07 18:40:08,958 [INFO] ---------------------------------
2019-03-07 18:40:28,582 [INFO] ---------------------------------
2019-03-07 18:40:28,584 [INFO] Summary:
2019-03-07 18:40:28,584 [INFO] Batch 20000, worst loss 0.275983 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:40:28,585 [INFO] Regularization: 5529.564453 * 0.0000010000 = 0.0055295643
2019-03-07 18:40:28,586 [INFO] Sum of grad norms: 4.519516
2019-03-07 18:40:28,587 [INFO] ---------------------------------
2019-03-07 18:40:41,745 [INFO] ---------------------------------
2019-03-07 18:40:41,746 [INFO] Evaluation:
2019-03-07 18:40:41,747 [INFO] Batch 20000, worst loss 0.259788 (without reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:40:41,747 [INFO] ---------------------------------
2019-03-07 18:41:01,228 [INFO] ---------------------------------
2019-03-07 18:41:01,230 [INFO] Summary:
2019-03-07 18:41:01,232 [INFO] Batch 21000, worst loss 0.287000 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:41:01,233 [INFO] Regularization: 5579.817383 * 0.0000010000 = 0.0055798176
2019-03-07 18:41:01,235 [INFO] Sum of grad norms: 5.543907
2019-03-07 18:41:01,237 [INFO] ---------------------------------
2019-03-07 18:41:20,491 [INFO] ---------------------------------
2019-03-07 18:41:20,492 [INFO] Summary:
2019-03-07 18:41:20,493 [INFO] Batch 22000, worst loss 0.300594 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:41:20,493 [INFO] Regularization: 5481.920898 * 0.0000010000 = 0.0054819207
2019-03-07 18:41:20,494 [INFO] Sum of grad norms: 5.737449
2019-03-07 18:41:20,495 [INFO] ---------------------------------
2019-03-07 18:41:39,918 [INFO] ---------------------------------
2019-03-07 18:41:39,919 [INFO] Summary:
2019-03-07 18:41:39,920 [INFO] Batch 23000, worst loss 0.330837 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:41:39,920 [INFO] Regularization: 5409.184082 * 0.0000010000 = 0.0054091839
2019-03-07 18:41:39,921 [INFO] Sum of grad norms: 6.572744
2019-03-07 18:41:39,922 [INFO] ---------------------------------
2019-03-07 18:41:59,370 [INFO] ---------------------------------
2019-03-07 18:41:59,371 [INFO] Summary:
2019-03-07 18:41:59,371 [INFO] Batch 24000, worst loss 0.272610 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:41:59,372 [INFO] Regularization: 5366.637695 * 0.0000010000 = 0.0053666378
2019-03-07 18:41:59,373 [INFO] Sum of grad norms: 2.745782
2019-03-07 18:41:59,374 [INFO] ---------------------------------
2019-03-07 18:42:18,562 [INFO] ---------------------------------
2019-03-07 18:42:18,563 [INFO] Summary:
2019-03-07 18:42:18,566 [INFO] Batch 25000, worst loss 0.339701 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-07 18:42:18,566 [INFO] Regularization: 5337.439453 * 0.0000010000 = 0.0053374395
2019-03-07 18:42:18,567 [INFO] Sum of grad norms: 2.587099
2019-03-07 18:42:18,568 [INFO] ---------------------------------
