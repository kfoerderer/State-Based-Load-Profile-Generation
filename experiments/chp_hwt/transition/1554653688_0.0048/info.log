2019-04-07 18:14:48,232 [INFO] batch_size: 32
2019-04-07 18:14:48,233 [INFO] learning_rate_initialization: 0.001000, learning_rate_loss_factor: 1.000000, learning_rate_decay_after: 20000, learning_rate_decay_at: 10000, learning_rate_decay_factor: 0.500000
2019-04-07 18:14:48,236 [INFO] weigths: tensor([ 1.,  1., 20., 80., 10., 10.,  1.,  1.], device='cuda:0')
2019-04-07 18:14:48,236 [INFO] regularization factor: 0.0000000100000000
2019-04-07 18:14:48,237 [INFO] unfolding_after: 1000, unfolding_at: 1000, unfolding_delta: 1, unfolding_share: 0.000000
2019-04-07 18:14:48,389 [INFO] ---------------------------------
2019-04-07 18:14:48,392 [INFO] Training model #0: (11, 64, 8) @ 3
2019-04-07 18:15:10,265 [INFO] ---------------------------------
2019-04-07 18:15:10,266 [INFO] Summary:
2019-04-07 18:15:10,266 [INFO] Batch 1000, worst loss 23.587412 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:15:10,267 [INFO] Regularization: 6730.597168 * 0.0000000100 = 0.0000673060 loss
2019-04-07 18:15:10,267 [INFO] unfolding 0, single step 1001
2019-04-07 18:15:10,268 [INFO] Sum of grad norms of most recent batch: 13.501610
2019-04-07 18:15:10,269 [INFO] ---------------------------------
2019-04-07 18:15:31,800 [INFO] ---------------------------------
2019-04-07 18:15:31,801 [INFO] Summary:
2019-04-07 18:15:31,802 [INFO] Batch 2000, worst loss 0.132069 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:15:31,803 [INFO] Regularization: 3895.441650 * 0.0000000100 = 0.0000389544 loss
2019-04-07 18:15:31,804 [INFO] unfolding 0, single step 2001
2019-04-07 18:15:31,805 [INFO] Sum of grad norms of most recent batch: 2.609828
2019-04-07 18:15:31,806 [INFO] ---------------------------------
2019-04-07 18:15:52,866 [INFO] ---------------------------------
2019-04-07 18:15:52,867 [INFO] Summary:
2019-04-07 18:15:52,868 [INFO] Batch 3000, worst loss 0.132288 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:15:52,869 [INFO] Regularization: 3021.489990 * 0.0000000100 = 0.0000302149 loss
2019-04-07 18:15:52,870 [INFO] unfolding 0, single step 3001
2019-04-07 18:15:52,871 [INFO] Sum of grad norms of most recent batch: 2.463405
2019-04-07 18:15:52,872 [INFO] ---------------------------------
2019-04-07 18:16:14,170 [INFO] ---------------------------------
2019-04-07 18:16:14,171 [INFO] Summary:
2019-04-07 18:16:14,171 [INFO] Batch 4000, worst loss 0.037635 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:16:14,172 [INFO] Regularization: 2696.145264 * 0.0000000100 = 0.0000269615 loss
2019-04-07 18:16:14,172 [INFO] unfolding 0, single step 4001
2019-04-07 18:16:14,173 [INFO] Sum of grad norms of most recent batch: 2.968331
2019-04-07 18:16:14,173 [INFO] ---------------------------------
2019-04-07 18:16:35,807 [INFO] ---------------------------------
2019-04-07 18:16:35,808 [INFO] Summary:
2019-04-07 18:16:35,808 [INFO] Batch 5000, worst loss 0.156712 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:16:35,809 [INFO] Regularization: 2432.386963 * 0.0000000100 = 0.0000243239 loss
2019-04-07 18:16:35,809 [INFO] unfolding 0, single step 5001
2019-04-07 18:16:35,810 [INFO] Sum of grad norms of most recent batch: 2.235706
2019-04-07 18:16:35,810 [INFO] ---------------------------------
2019-04-07 18:16:57,551 [INFO] ---------------------------------
2019-04-07 18:16:57,552 [INFO] Summary:
2019-04-07 18:16:57,553 [INFO] Batch 6000, worst loss 0.166335 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:16:57,553 [INFO] Regularization: 2251.617920 * 0.0000000100 = 0.0000225162 loss
2019-04-07 18:16:57,553 [INFO] unfolding 0, single step 6001
2019-04-07 18:16:57,554 [INFO] Sum of grad norms of most recent batch: 2.916917
2019-04-07 18:16:57,555 [INFO] ---------------------------------
2019-04-07 18:17:19,412 [INFO] ---------------------------------
2019-04-07 18:17:19,413 [INFO] Summary:
2019-04-07 18:17:19,414 [INFO] Batch 7000, worst loss 0.053650 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:17:19,415 [INFO] Regularization: 2100.044678 * 0.0000000100 = 0.0000210004 loss
2019-04-07 18:17:19,416 [INFO] unfolding 0, single step 7001
2019-04-07 18:17:19,417 [INFO] Sum of grad norms of most recent batch: 1.459610
2019-04-07 18:17:19,418 [INFO] ---------------------------------
2019-04-07 18:17:41,656 [INFO] ---------------------------------
2019-04-07 18:17:41,657 [INFO] Summary:
2019-04-07 18:17:41,659 [INFO] Batch 8000, worst loss 0.080341 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:17:41,659 [INFO] Regularization: 1973.724487 * 0.0000000100 = 0.0000197372 loss
2019-04-07 18:17:41,659 [INFO] unfolding 0, single step 8001
2019-04-07 18:17:41,660 [INFO] Sum of grad norms of most recent batch: 3.668078
2019-04-07 18:17:41,661 [INFO] ---------------------------------
2019-04-07 18:18:03,558 [INFO] ---------------------------------
2019-04-07 18:18:03,559 [INFO] Summary:
2019-04-07 18:18:03,560 [INFO] Batch 9000, worst loss 0.068867 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:18:03,561 [INFO] Regularization: 1884.071899 * 0.0000000100 = 0.0000188407 loss
2019-04-07 18:18:03,562 [INFO] unfolding 0, single step 9001
2019-04-07 18:18:03,563 [INFO] Sum of grad norms of most recent batch: 0.916302
2019-04-07 18:18:03,563 [INFO] ---------------------------------
2019-04-07 18:18:25,628 [INFO] ---------------------------------
2019-04-07 18:18:25,629 [INFO] Summary:
2019-04-07 18:18:25,630 [INFO] Batch 10000, worst loss 0.161409 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:18:25,630 [INFO] Regularization: 1848.537476 * 0.0000000100 = 0.0000184854 loss
2019-04-07 18:18:25,630 [INFO] unfolding 0, single step 10001
2019-04-07 18:18:25,631 [INFO] Sum of grad norms of most recent batch: 2.609658
2019-04-07 18:18:25,631 [INFO] ---------------------------------
2019-04-07 18:19:03,081 [INFO] ---------------------------------
2019-04-07 18:19:03,082 [INFO] Evaluation:
2019-04-07 18:19:03,083 [INFO] Batch 10000, worst loss 0.157642 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:19:03,083 [INFO] ---------------------------------
2019-04-07 18:19:23,980 [INFO] ---------------------------------
2019-04-07 18:19:23,981 [INFO] Summary:
2019-04-07 18:19:23,982 [INFO] Batch 11000, worst loss 0.052225 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:19:23,982 [INFO] Regularization: 1808.336548 * 0.0000000100 = 0.0000180834 loss
2019-04-07 18:19:23,982 [INFO] unfolding 0, single step 11001
2019-04-07 18:19:23,983 [INFO] Sum of grad norms of most recent batch: 1.540305
2019-04-07 18:19:23,984 [INFO] ---------------------------------
2019-04-07 18:19:45,806 [INFO] ---------------------------------
2019-04-07 18:19:45,807 [INFO] Summary:
2019-04-07 18:19:45,807 [INFO] Batch 12000, worst loss 0.099118 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:19:45,808 [INFO] Regularization: 1781.901367 * 0.0000000100 = 0.0000178190 loss
2019-04-07 18:19:45,809 [INFO] unfolding 0, single step 12001
2019-04-07 18:19:45,809 [INFO] Sum of grad norms of most recent batch: 0.613092
2019-04-07 18:19:45,810 [INFO] ---------------------------------
2019-04-07 18:20:07,430 [INFO] ---------------------------------
2019-04-07 18:20:07,431 [INFO] Summary:
2019-04-07 18:20:07,432 [INFO] Batch 13000, worst loss 0.158370 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:20:07,432 [INFO] Regularization: 1748.922729 * 0.0000000100 = 0.0000174892 loss
2019-04-07 18:20:07,433 [INFO] unfolding 0, single step 13001
2019-04-07 18:20:07,433 [INFO] Sum of grad norms of most recent batch: 1.769884
2019-04-07 18:20:07,434 [INFO] ---------------------------------
2019-04-07 18:20:29,288 [INFO] ---------------------------------
2019-04-07 18:20:29,289 [INFO] Summary:
2019-04-07 18:20:29,290 [INFO] Batch 14000, worst loss 0.104335 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:20:29,290 [INFO] Regularization: 1742.176758 * 0.0000000100 = 0.0000174218 loss
2019-04-07 18:20:29,290 [INFO] unfolding 0, single step 14001
2019-04-07 18:20:29,291 [INFO] Sum of grad norms of most recent batch: 2.428213
2019-04-07 18:20:29,291 [INFO] ---------------------------------
2019-04-07 18:20:51,101 [INFO] ---------------------------------
2019-04-07 18:20:51,102 [INFO] Summary:
2019-04-07 18:20:51,102 [INFO] Batch 15000, worst loss 0.203212 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:20:51,103 [INFO] Regularization: 1699.665405 * 0.0000000100 = 0.0000169967 loss
2019-04-07 18:20:51,103 [INFO] unfolding 0, single step 15001
2019-04-07 18:20:51,104 [INFO] Sum of grad norms of most recent batch: 2.588441
2019-04-07 18:20:51,104 [INFO] ---------------------------------
2019-04-07 18:21:12,536 [INFO] ---------------------------------
2019-04-07 18:21:12,538 [INFO] Summary:
2019-04-07 18:21:12,538 [INFO] Batch 16000, worst loss 0.133811 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:21:12,539 [INFO] Regularization: 1661.929688 * 0.0000000100 = 0.0000166193 loss
2019-04-07 18:21:12,539 [INFO] unfolding 0, single step 16001
2019-04-07 18:21:12,540 [INFO] Sum of grad norms of most recent batch: 4.427480
2019-04-07 18:21:12,540 [INFO] ---------------------------------
2019-04-07 18:21:34,561 [INFO] ---------------------------------
2019-04-07 18:21:34,562 [INFO] Summary:
2019-04-07 18:21:34,562 [INFO] Batch 17000, worst loss 0.133961 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:21:34,563 [INFO] Regularization: 1619.592163 * 0.0000000100 = 0.0000161959 loss
2019-04-07 18:21:34,563 [INFO] unfolding 0, single step 17001
2019-04-07 18:21:34,564 [INFO] Sum of grad norms of most recent batch: 1.613964
2019-04-07 18:21:34,565 [INFO] ---------------------------------
2019-04-07 18:21:55,829 [INFO] ---------------------------------
2019-04-07 18:21:55,830 [INFO] Summary:
2019-04-07 18:21:55,830 [INFO] Batch 18000, worst loss 0.049993 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:21:55,831 [INFO] Regularization: 1657.622314 * 0.0000000100 = 0.0000165762 loss
2019-04-07 18:21:55,831 [INFO] unfolding 0, single step 18001
2019-04-07 18:21:55,832 [INFO] Sum of grad norms of most recent batch: 1.066950
2019-04-07 18:21:55,833 [INFO] ---------------------------------
2019-04-07 18:22:17,321 [INFO] ---------------------------------
2019-04-07 18:22:17,322 [INFO] Summary:
2019-04-07 18:22:17,323 [INFO] Batch 19000, worst loss 0.042191 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:22:17,323 [INFO] Regularization: 1627.613770 * 0.0000000100 = 0.0000162761 loss
2019-04-07 18:22:17,323 [INFO] unfolding 0, single step 19001
2019-04-07 18:22:17,324 [INFO] Sum of grad norms of most recent batch: 1.455288
2019-04-07 18:22:17,324 [INFO] ---------------------------------
2019-04-07 18:22:39,004 [INFO] ---------------------------------
2019-04-07 18:22:39,005 [INFO] Summary:
2019-04-07 18:22:39,006 [INFO] Batch 20000, worst loss 0.111833 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:22:39,006 [INFO] Regularization: 1648.345703 * 0.0000000100 = 0.0000164835 loss
2019-04-07 18:22:39,006 [INFO] unfolding 0, single step 20001
2019-04-07 18:22:39,007 [INFO] Sum of grad norms of most recent batch: 1.492674
2019-04-07 18:22:39,007 [INFO] ---------------------------------
2019-04-07 18:23:16,173 [INFO] ---------------------------------
2019-04-07 18:23:16,174 [INFO] Evaluation:
2019-04-07 18:23:16,174 [INFO] Batch 20000, worst loss 0.097623 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:23:16,175 [INFO] ---------------------------------
2019-04-07 18:23:38,135 [INFO] ---------------------------------
2019-04-07 18:23:38,136 [INFO] Summary:
2019-04-07 18:23:38,136 [INFO] Batch 21000, worst loss 0.037354 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:23:38,137 [INFO] Regularization: 1689.316528 * 0.0000000100 = 0.0000168932 loss
2019-04-07 18:23:38,137 [INFO] unfolding 0, single step 21001
2019-04-07 18:23:38,138 [INFO] Sum of grad norms of most recent batch: 2.396368
2019-04-07 18:23:38,138 [INFO] ---------------------------------
2019-04-07 18:23:58,805 [INFO] ---------------------------------
2019-04-07 18:23:58,806 [INFO] Summary:
2019-04-07 18:23:58,807 [INFO] Batch 22000, worst loss 0.099268 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:23:58,808 [INFO] Regularization: 1719.833618 * 0.0000000100 = 0.0000171983 loss
2019-04-07 18:23:58,809 [INFO] unfolding 0, single step 22001
2019-04-07 18:23:58,809 [INFO] Sum of grad norms of most recent batch: 2.394248
2019-04-07 18:23:58,810 [INFO] ---------------------------------
2019-04-07 18:24:19,434 [INFO] ---------------------------------
2019-04-07 18:24:19,435 [INFO] Summary:
2019-04-07 18:24:19,436 [INFO] Batch 23000, worst loss 0.040062 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:24:19,437 [INFO] Regularization: 1682.375854 * 0.0000000100 = 0.0000168238 loss
2019-04-07 18:24:19,438 [INFO] unfolding 0, single step 23001
2019-04-07 18:24:19,439 [INFO] Sum of grad norms of most recent batch: 2.386905
2019-04-07 18:24:19,440 [INFO] ---------------------------------
2019-04-07 18:24:41,153 [INFO] ---------------------------------
2019-04-07 18:24:41,154 [INFO] Summary:
2019-04-07 18:24:41,155 [INFO] Batch 24000, worst loss 0.089743 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:24:41,156 [INFO] Regularization: 1742.635742 * 0.0000000100 = 0.0000174264 loss
2019-04-07 18:24:41,157 [INFO] unfolding 0, single step 24001
2019-04-07 18:24:41,158 [INFO] Sum of grad norms of most recent batch: 1.310224
2019-04-07 18:24:41,159 [INFO] ---------------------------------
2019-04-07 18:25:03,457 [INFO] ---------------------------------
2019-04-07 18:25:03,458 [INFO] Summary:
2019-04-07 18:25:03,458 [INFO] Batch 25000, worst loss 0.134452 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:25:03,459 [INFO] Regularization: 1666.066406 * 0.0000000100 = 0.0000166607 loss
2019-04-07 18:25:03,459 [INFO] unfolding 0, single step 25001
2019-04-07 18:25:03,460 [INFO] Sum of grad norms of most recent batch: 5.525459
2019-04-07 18:25:03,460 [INFO] ---------------------------------
2019-04-07 18:25:25,267 [INFO] ---------------------------------
2019-04-07 18:25:25,268 [INFO] Summary:
2019-04-07 18:25:25,268 [INFO] Batch 26000, worst loss 0.110745 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:25:25,269 [INFO] Regularization: 1631.905640 * 0.0000000100 = 0.0000163191 loss
2019-04-07 18:25:25,269 [INFO] unfolding 0, single step 26001
2019-04-07 18:25:25,270 [INFO] Sum of grad norms of most recent batch: 2.209973
2019-04-07 18:25:25,270 [INFO] ---------------------------------
2019-04-07 18:25:47,096 [INFO] ---------------------------------
2019-04-07 18:25:47,097 [INFO] Summary:
2019-04-07 18:25:47,098 [INFO] Batch 27000, worst loss 0.156489 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:25:47,098 [INFO] Regularization: 1594.087524 * 0.0000000100 = 0.0000159409 loss
2019-04-07 18:25:47,099 [INFO] unfolding 0, single step 27001
2019-04-07 18:25:47,099 [INFO] Sum of grad norms of most recent batch: 3.239517
2019-04-07 18:25:47,100 [INFO] ---------------------------------
2019-04-07 18:26:08,610 [INFO] ---------------------------------
2019-04-07 18:26:08,611 [INFO] Summary:
2019-04-07 18:26:08,612 [INFO] Batch 28000, worst loss 0.130653 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:26:08,612 [INFO] Regularization: 1559.899902 * 0.0000000100 = 0.0000155990 loss
2019-04-07 18:26:08,612 [INFO] unfolding 0, single step 28001
2019-04-07 18:26:08,613 [INFO] Sum of grad norms of most recent batch: 1.807691
2019-04-07 18:26:08,613 [INFO] ---------------------------------
2019-04-07 18:26:30,035 [INFO] ---------------------------------
2019-04-07 18:26:30,036 [INFO] Summary:
2019-04-07 18:26:30,037 [INFO] Batch 29000, worst loss 0.104231 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:26:30,038 [INFO] Regularization: 1612.419678 * 0.0000000100 = 0.0000161242 loss
2019-04-07 18:26:30,038 [INFO] unfolding 0, single step 29001
2019-04-07 18:26:30,039 [INFO] Sum of grad norms of most recent batch: 0.951088
2019-04-07 18:26:30,040 [INFO] ---------------------------------
2019-04-07 18:26:51,283 [INFO] ---------------------------------
2019-04-07 18:26:51,284 [INFO] Summary:
2019-04-07 18:26:51,285 [INFO] Batch 30000, worst loss 0.292006 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 18:26:51,286 [INFO] Regularization: 1566.413940 * 0.0000000100 = 0.0000156641 loss
2019-04-07 18:26:51,286 [INFO] unfolding 0, single step 30001
2019-04-07 18:26:51,287 [INFO] Sum of grad norms of most recent batch: 1.051965
2019-04-07 18:26:51,288 [INFO] ---------------------------------
2019-04-07 18:27:28,529 [INFO] ---------------------------------
2019-04-07 18:27:28,530 [INFO] Evaluation:
2019-04-07 18:27:28,530 [INFO] Batch 30000, worst loss 0.164985 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:27:28,531 [INFO] ---------------------------------
2019-04-07 18:27:50,541 [INFO] ---------------------------------
2019-04-07 18:27:50,543 [INFO] Summary:
2019-04-07 18:27:50,544 [INFO] Batch 31000, worst loss 0.112400 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 18:27:50,545 [INFO] Regularization: 1598.707886 * 0.0000000100 = 0.0000159871 loss
2019-04-07 18:27:50,546 [INFO] unfolding 0, single step 31001
2019-04-07 18:27:50,547 [INFO] Sum of grad norms of most recent batch: 1.629580
2019-04-07 18:27:50,548 [INFO] ---------------------------------
2019-04-07 18:28:12,184 [INFO] ---------------------------------
2019-04-07 18:28:12,185 [INFO] Summary:
2019-04-07 18:28:12,186 [INFO] Batch 32000, worst loss 0.120493 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 18:28:12,186 [INFO] Regularization: 1526.713501 * 0.0000000100 = 0.0000152671 loss
2019-04-07 18:28:12,186 [INFO] unfolding 0, single step 32001
2019-04-07 18:28:12,187 [INFO] Sum of grad norms of most recent batch: 1.126804
2019-04-07 18:28:12,188 [INFO] ---------------------------------
2019-04-07 18:28:33,595 [INFO] ---------------------------------
2019-04-07 18:28:33,596 [INFO] Summary:
2019-04-07 18:28:33,596 [INFO] Batch 33000, worst loss 0.031746 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 18:28:33,597 [INFO] Regularization: 1458.088989 * 0.0000000100 = 0.0000145809 loss
2019-04-07 18:28:33,597 [INFO] unfolding 0, single step 33001
2019-04-07 18:28:33,598 [INFO] Sum of grad norms of most recent batch: 0.442686
2019-04-07 18:28:33,598 [INFO] ---------------------------------
2019-04-07 18:28:55,641 [INFO] ---------------------------------
2019-04-07 18:28:55,642 [INFO] Summary:
2019-04-07 18:28:55,643 [INFO] Batch 34000, worst loss 0.057835 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 18:28:55,643 [INFO] Regularization: 1414.678711 * 0.0000000100 = 0.0000141468 loss
2019-04-07 18:28:55,644 [INFO] unfolding 0, single step 34001
2019-04-07 18:28:55,644 [INFO] Sum of grad norms of most recent batch: 0.572064
2019-04-07 18:28:55,645 [INFO] ---------------------------------
2019-04-07 18:29:17,579 [INFO] ---------------------------------
2019-04-07 18:29:17,580 [INFO] Summary:
2019-04-07 18:29:17,581 [INFO] Batch 35000, worst loss 0.108397 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 18:29:17,581 [INFO] Regularization: 1407.631958 * 0.0000000100 = 0.0000140763 loss
2019-04-07 18:29:17,582 [INFO] unfolding 0, single step 35001
2019-04-07 18:29:17,582 [INFO] Sum of grad norms of most recent batch: 1.253675
2019-04-07 18:29:17,583 [INFO] ---------------------------------
2019-04-07 18:29:39,465 [INFO] ---------------------------------
2019-04-07 18:29:39,466 [INFO] Summary:
2019-04-07 18:29:39,467 [INFO] Batch 36000, worst loss 0.121993 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 18:29:39,468 [INFO] Regularization: 1368.306152 * 0.0000000100 = 0.0000136831 loss
2019-04-07 18:29:39,468 [INFO] unfolding 0, single step 36001
2019-04-07 18:29:39,469 [INFO] Sum of grad norms of most recent batch: 0.397193
2019-04-07 18:29:39,469 [INFO] ---------------------------------
2019-04-07 18:30:01,172 [INFO] ---------------------------------
2019-04-07 18:30:01,173 [INFO] Summary:
2019-04-07 18:30:01,174 [INFO] Batch 37000, worst loss 0.045087 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 18:30:01,175 [INFO] Regularization: 1328.999756 * 0.0000000100 = 0.0000132900 loss
2019-04-07 18:30:01,175 [INFO] unfolding 0, single step 37001
2019-04-07 18:30:01,176 [INFO] Sum of grad norms of most recent batch: 0.611140
2019-04-07 18:30:01,176 [INFO] ---------------------------------
2019-04-07 18:30:23,141 [INFO] ---------------------------------
2019-04-07 18:30:23,142 [INFO] Summary:
2019-04-07 18:30:23,142 [INFO] Batch 38000, worst loss 0.035246 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 18:30:23,143 [INFO] Regularization: 1307.764648 * 0.0000000100 = 0.0000130776 loss
2019-04-07 18:30:23,143 [INFO] unfolding 0, single step 38001
2019-04-07 18:30:23,143 [INFO] Sum of grad norms of most recent batch: 0.726431
2019-04-07 18:30:23,144 [INFO] ---------------------------------
2019-04-07 18:30:44,696 [INFO] ---------------------------------
2019-04-07 18:30:44,697 [INFO] Summary:
2019-04-07 18:30:44,697 [INFO] Batch 39000, worst loss 0.106400 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 18:30:44,698 [INFO] Regularization: 1284.252197 * 0.0000000100 = 0.0000128425 loss
2019-04-07 18:30:44,698 [INFO] unfolding 0, single step 39001
2019-04-07 18:30:44,699 [INFO] Sum of grad norms of most recent batch: 0.529678
2019-04-07 18:30:44,699 [INFO] ---------------------------------
2019-04-07 18:31:06,830 [INFO] ---------------------------------
2019-04-07 18:31:06,831 [INFO] Summary:
2019-04-07 18:31:06,832 [INFO] Batch 40000, worst loss 0.053093 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 18:31:06,832 [INFO] Regularization: 1284.771484 * 0.0000000100 = 0.0000128477 loss
2019-04-07 18:31:06,832 [INFO] unfolding 0, single step 40001
2019-04-07 18:31:06,833 [INFO] Sum of grad norms of most recent batch: 0.241436
2019-04-07 18:31:06,833 [INFO] ---------------------------------
2019-04-07 18:31:43,888 [INFO] ---------------------------------
2019-04-07 18:31:43,889 [INFO] Evaluation:
2019-04-07 18:31:43,890 [INFO] Batch 40000, worst loss 0.132070 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:31:43,891 [INFO] ---------------------------------
2019-04-07 18:32:05,342 [INFO] ---------------------------------
2019-04-07 18:32:05,343 [INFO] Summary:
2019-04-07 18:32:05,344 [INFO] Batch 41000, worst loss 0.107695 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 18:32:05,345 [INFO] Regularization: 1262.184082 * 0.0000000100 = 0.0000126218 loss
2019-04-07 18:32:05,346 [INFO] unfolding 0, single step 41001
2019-04-07 18:32:05,348 [INFO] Sum of grad norms of most recent batch: 0.533771
2019-04-07 18:32:05,349 [INFO] ---------------------------------
2019-04-07 18:32:26,611 [INFO] ---------------------------------
2019-04-07 18:32:26,612 [INFO] Summary:
2019-04-07 18:32:26,613 [INFO] Batch 42000, worst loss 0.069189 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 18:32:26,613 [INFO] Regularization: 1263.183472 * 0.0000000100 = 0.0000126318 loss
2019-04-07 18:32:26,614 [INFO] unfolding 0, single step 42001
2019-04-07 18:32:26,615 [INFO] Sum of grad norms of most recent batch: 1.269098
2019-04-07 18:32:26,615 [INFO] ---------------------------------
2019-04-07 18:32:48,558 [INFO] ---------------------------------
2019-04-07 18:32:48,559 [INFO] Summary:
2019-04-07 18:32:48,560 [INFO] Batch 43000, worst loss 0.050395 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 18:32:48,561 [INFO] Regularization: 1247.821411 * 0.0000000100 = 0.0000124782 loss
2019-04-07 18:32:48,561 [INFO] unfolding 0, single step 43001
2019-04-07 18:32:48,562 [INFO] Sum of grad norms of most recent batch: 0.130631
2019-04-07 18:32:48,563 [INFO] ---------------------------------
2019-04-07 18:33:10,478 [INFO] ---------------------------------
2019-04-07 18:33:10,479 [INFO] Summary:
2019-04-07 18:33:10,480 [INFO] Batch 44000, worst loss 0.055958 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 18:33:10,481 [INFO] Regularization: 1249.607178 * 0.0000000100 = 0.0000124961 loss
2019-04-07 18:33:10,482 [INFO] unfolding 0, single step 44001
2019-04-07 18:33:10,483 [INFO] Sum of grad norms of most recent batch: 0.478311
2019-04-07 18:33:10,484 [INFO] ---------------------------------
2019-04-07 18:33:31,802 [INFO] ---------------------------------
2019-04-07 18:33:31,803 [INFO] Summary:
2019-04-07 18:33:31,804 [INFO] Batch 45000, worst loss 0.055501 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 18:33:31,805 [INFO] Regularization: 1232.584351 * 0.0000000100 = 0.0000123258 loss
2019-04-07 18:33:31,805 [INFO] unfolding 0, single step 45001
2019-04-07 18:33:31,806 [INFO] Sum of grad norms of most recent batch: 1.332368
2019-04-07 18:33:31,806 [INFO] ---------------------------------
2019-04-07 18:33:53,537 [INFO] ---------------------------------
2019-04-07 18:33:53,538 [INFO] Summary:
2019-04-07 18:33:53,540 [INFO] Batch 46000, worst loss 0.022826 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 18:33:53,540 [INFO] Regularization: 1226.588623 * 0.0000000100 = 0.0000122659 loss
2019-04-07 18:33:53,541 [INFO] unfolding 0, single step 46001
2019-04-07 18:33:53,542 [INFO] Sum of grad norms of most recent batch: 0.272546
2019-04-07 18:33:53,543 [INFO] ---------------------------------
2019-04-07 18:34:14,575 [INFO] ---------------------------------
2019-04-07 18:34:14,576 [INFO] Summary:
2019-04-07 18:34:14,577 [INFO] Batch 47000, worst loss 0.054762 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 18:34:14,578 [INFO] Regularization: 1204.567627 * 0.0000000100 = 0.0000120457 loss
2019-04-07 18:34:14,579 [INFO] unfolding 0, single step 47001
2019-04-07 18:34:14,579 [INFO] Sum of grad norms of most recent batch: 0.171039
2019-04-07 18:34:14,580 [INFO] ---------------------------------
2019-04-07 18:34:36,218 [INFO] ---------------------------------
2019-04-07 18:34:36,219 [INFO] Summary:
2019-04-07 18:34:36,219 [INFO] Batch 48000, worst loss 0.074508 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 18:34:36,220 [INFO] Regularization: 1189.653442 * 0.0000000100 = 0.0000118965 loss
2019-04-07 18:34:36,220 [INFO] unfolding 0, single step 48001
2019-04-07 18:34:36,221 [INFO] Sum of grad norms of most recent batch: 0.984591
2019-04-07 18:34:36,221 [INFO] ---------------------------------
2019-04-07 18:34:57,681 [INFO] ---------------------------------
2019-04-07 18:34:57,682 [INFO] Summary:
2019-04-07 18:34:57,683 [INFO] Batch 49000, worst loss 0.048116 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 18:34:57,683 [INFO] Regularization: 1177.606934 * 0.0000000100 = 0.0000117761 loss
2019-04-07 18:34:57,683 [INFO] unfolding 0, single step 49001
2019-04-07 18:34:57,684 [INFO] Sum of grad norms of most recent batch: 0.513572
2019-04-07 18:34:57,684 [INFO] ---------------------------------
2019-04-07 18:35:18,724 [INFO] ---------------------------------
2019-04-07 18:35:18,725 [INFO] Summary:
2019-04-07 18:35:18,726 [INFO] Batch 50000, worst loss 0.064168 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 18:35:18,726 [INFO] Regularization: 1157.278076 * 0.0000000100 = 0.0000115728 loss
2019-04-07 18:35:18,727 [INFO] unfolding 0, single step 50001
2019-04-07 18:35:18,727 [INFO] Sum of grad norms of most recent batch: 0.556164
2019-04-07 18:35:18,728 [INFO] ---------------------------------
2019-04-07 18:35:56,066 [INFO] ---------------------------------
2019-04-07 18:35:56,067 [INFO] Evaluation:
2019-04-07 18:35:56,067 [INFO] Batch 50000, worst loss 0.072377 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:35:56,068 [INFO] ---------------------------------
2019-04-07 18:36:17,389 [INFO] ---------------------------------
2019-04-07 18:36:17,390 [INFO] Summary:
2019-04-07 18:36:17,390 [INFO] Batch 51000, worst loss 0.059968 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 18:36:17,391 [INFO] Regularization: 1156.815430 * 0.0000000100 = 0.0000115682 loss
2019-04-07 18:36:17,391 [INFO] unfolding 0, single step 51001
2019-04-07 18:36:17,392 [INFO] Sum of grad norms of most recent batch: 0.213560
2019-04-07 18:36:17,393 [INFO] ---------------------------------
2019-04-07 18:36:39,188 [INFO] ---------------------------------
2019-04-07 18:36:39,189 [INFO] Summary:
2019-04-07 18:36:39,190 [INFO] Batch 52000, worst loss 0.100299 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 18:36:39,191 [INFO] Regularization: 1135.785522 * 0.0000000100 = 0.0000113579 loss
2019-04-07 18:36:39,191 [INFO] unfolding 0, single step 52001
2019-04-07 18:36:39,192 [INFO] Sum of grad norms of most recent batch: 0.186393
2019-04-07 18:36:39,192 [INFO] ---------------------------------
2019-04-07 18:37:00,455 [INFO] ---------------------------------
2019-04-07 18:37:00,456 [INFO] Summary:
2019-04-07 18:37:00,457 [INFO] Batch 53000, worst loss 0.085131 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 18:37:00,458 [INFO] Regularization: 1125.701538 * 0.0000000100 = 0.0000112570 loss
2019-04-07 18:37:00,459 [INFO] unfolding 0, single step 53001
2019-04-07 18:37:00,460 [INFO] Sum of grad norms of most recent batch: 0.700712
2019-04-07 18:37:00,461 [INFO] ---------------------------------
2019-04-07 18:37:21,192 [INFO] ---------------------------------
2019-04-07 18:37:21,193 [INFO] Summary:
2019-04-07 18:37:21,193 [INFO] Batch 54000, worst loss 0.076706 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 18:37:21,194 [INFO] Regularization: 1120.119385 * 0.0000000100 = 0.0000112012 loss
2019-04-07 18:37:21,194 [INFO] unfolding 0, single step 54001
2019-04-07 18:37:21,195 [INFO] Sum of grad norms of most recent batch: 0.568400
2019-04-07 18:37:21,196 [INFO] ---------------------------------
2019-04-07 18:37:42,814 [INFO] ---------------------------------
2019-04-07 18:37:42,815 [INFO] Summary:
2019-04-07 18:37:42,816 [INFO] Batch 55000, worst loss 0.080365 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 18:37:42,816 [INFO] Regularization: 1115.186401 * 0.0000000100 = 0.0000111519 loss
2019-04-07 18:37:42,817 [INFO] unfolding 0, single step 55001
2019-04-07 18:37:42,817 [INFO] Sum of grad norms of most recent batch: 0.160655
2019-04-07 18:37:42,818 [INFO] ---------------------------------
2019-04-07 18:38:04,641 [INFO] ---------------------------------
2019-04-07 18:38:04,642 [INFO] Summary:
2019-04-07 18:38:04,643 [INFO] Batch 56000, worst loss 0.024909 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 18:38:04,643 [INFO] Regularization: 1108.000732 * 0.0000000100 = 0.0000110800 loss
2019-04-07 18:38:04,644 [INFO] unfolding 0, single step 56001
2019-04-07 18:38:04,644 [INFO] Sum of grad norms of most recent batch: 0.215454
2019-04-07 18:38:04,645 [INFO] ---------------------------------
2019-04-07 18:38:26,215 [INFO] ---------------------------------
2019-04-07 18:38:26,216 [INFO] Summary:
2019-04-07 18:38:26,217 [INFO] Batch 57000, worst loss 0.042582 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 18:38:26,217 [INFO] Regularization: 1097.976929 * 0.0000000100 = 0.0000109798 loss
2019-04-07 18:38:26,218 [INFO] unfolding 0, single step 57001
2019-04-07 18:38:26,218 [INFO] Sum of grad norms of most recent batch: 0.093914
2019-04-07 18:38:26,219 [INFO] ---------------------------------
2019-04-07 18:38:48,018 [INFO] ---------------------------------
2019-04-07 18:38:48,019 [INFO] Summary:
2019-04-07 18:38:48,020 [INFO] Batch 58000, worst loss 0.031154 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 18:38:48,021 [INFO] Regularization: 1090.725830 * 0.0000000100 = 0.0000109073 loss
2019-04-07 18:38:48,021 [INFO] unfolding 0, single step 58001
2019-04-07 18:38:48,022 [INFO] Sum of grad norms of most recent batch: 0.271365
2019-04-07 18:38:48,023 [INFO] ---------------------------------
2019-04-07 18:39:09,304 [INFO] ---------------------------------
2019-04-07 18:39:09,305 [INFO] Summary:
2019-04-07 18:39:09,306 [INFO] Batch 59000, worst loss 0.026343 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 18:39:09,306 [INFO] Regularization: 1082.523438 * 0.0000000100 = 0.0000108252 loss
2019-04-07 18:39:09,307 [INFO] unfolding 0, single step 59001
2019-04-07 18:39:09,307 [INFO] Sum of grad norms of most recent batch: 0.203145
2019-04-07 18:39:09,308 [INFO] ---------------------------------
2019-04-07 18:39:30,539 [INFO] ---------------------------------
2019-04-07 18:39:30,540 [INFO] Summary:
2019-04-07 18:39:30,541 [INFO] Batch 60000, worst loss 0.034823 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 18:39:30,542 [INFO] Regularization: 1069.678101 * 0.0000000100 = 0.0000106968 loss
2019-04-07 18:39:30,543 [INFO] unfolding 0, single step 60001
2019-04-07 18:39:30,544 [INFO] Sum of grad norms of most recent batch: 0.407067
2019-04-07 18:39:30,544 [INFO] ---------------------------------
2019-04-07 18:40:07,883 [INFO] ---------------------------------
2019-04-07 18:40:07,884 [INFO] Evaluation:
2019-04-07 18:40:07,885 [INFO] Batch 60000, worst loss 0.078555 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:40:07,886 [INFO] ---------------------------------
2019-04-07 18:40:28,568 [INFO] ---------------------------------
2019-04-07 18:40:28,569 [INFO] Summary:
2019-04-07 18:40:28,570 [INFO] Batch 61000, worst loss 0.042713 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 18:40:28,570 [INFO] Regularization: 1073.614258 * 0.0000000100 = 0.0000107361 loss
2019-04-07 18:40:28,571 [INFO] unfolding 0, single step 61001
2019-04-07 18:40:28,571 [INFO] Sum of grad norms of most recent batch: 0.157789
2019-04-07 18:40:28,572 [INFO] ---------------------------------
2019-04-07 18:40:49,429 [INFO] ---------------------------------
2019-04-07 18:40:49,430 [INFO] Summary:
2019-04-07 18:40:49,431 [INFO] Batch 62000, worst loss 0.035743 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 18:40:49,432 [INFO] Regularization: 1069.062012 * 0.0000000100 = 0.0000106906 loss
2019-04-07 18:40:49,432 [INFO] unfolding 0, single step 62001
2019-04-07 18:40:49,433 [INFO] Sum of grad norms of most recent batch: 0.681514
2019-04-07 18:40:49,433 [INFO] ---------------------------------
2019-04-07 18:41:09,923 [INFO] ---------------------------------
2019-04-07 18:41:09,924 [INFO] Summary:
2019-04-07 18:41:09,924 [INFO] Batch 63000, worst loss 0.014192 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 18:41:09,926 [INFO] Regularization: 1065.965698 * 0.0000000100 = 0.0000106597 loss
2019-04-07 18:41:09,926 [INFO] unfolding 0, single step 63001
2019-04-07 18:41:09,927 [INFO] Sum of grad norms of most recent batch: 0.076809
2019-04-07 18:41:09,928 [INFO] ---------------------------------
2019-04-07 18:41:31,279 [INFO] ---------------------------------
2019-04-07 18:41:31,280 [INFO] Summary:
2019-04-07 18:41:31,281 [INFO] Batch 64000, worst loss 0.067583 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 18:41:31,281 [INFO] Regularization: 1063.877686 * 0.0000000100 = 0.0000106388 loss
2019-04-07 18:41:31,281 [INFO] unfolding 0, single step 64001
2019-04-07 18:41:31,282 [INFO] Sum of grad norms of most recent batch: 0.198604
2019-04-07 18:41:31,283 [INFO] ---------------------------------
2019-04-07 18:41:52,488 [INFO] ---------------------------------
2019-04-07 18:41:52,489 [INFO] Summary:
2019-04-07 18:41:52,490 [INFO] Batch 65000, worst loss 0.043755 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 18:41:52,490 [INFO] Regularization: 1059.730591 * 0.0000000100 = 0.0000105973 loss
2019-04-07 18:41:52,490 [INFO] unfolding 0, single step 65001
2019-04-07 18:41:52,491 [INFO] Sum of grad norms of most recent batch: 0.293245
2019-04-07 18:41:52,491 [INFO] ---------------------------------
2019-04-07 18:42:13,923 [INFO] ---------------------------------
2019-04-07 18:42:13,924 [INFO] Summary:
2019-04-07 18:42:13,924 [INFO] Batch 66000, worst loss 0.030117 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 18:42:13,925 [INFO] Regularization: 1055.685181 * 0.0000000100 = 0.0000105569 loss
2019-04-07 18:42:13,925 [INFO] unfolding 0, single step 66001
2019-04-07 18:42:13,926 [INFO] Sum of grad norms of most recent batch: 0.148046
2019-04-07 18:42:13,926 [INFO] ---------------------------------
2019-04-07 18:42:34,851 [INFO] ---------------------------------
2019-04-07 18:42:34,852 [INFO] Summary:
2019-04-07 18:42:34,853 [INFO] Batch 67000, worst loss 0.023127 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 18:42:34,854 [INFO] Regularization: 1056.049194 * 0.0000000100 = 0.0000105605 loss
2019-04-07 18:42:34,855 [INFO] unfolding 0, single step 67001
2019-04-07 18:42:34,856 [INFO] Sum of grad norms of most recent batch: 0.079329
2019-04-07 18:42:34,856 [INFO] ---------------------------------
2019-04-07 18:42:56,050 [INFO] ---------------------------------
2019-04-07 18:42:56,051 [INFO] Summary:
2019-04-07 18:42:56,051 [INFO] Batch 68000, worst loss 0.053536 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 18:42:56,052 [INFO] Regularization: 1054.454590 * 0.0000000100 = 0.0000105445 loss
2019-04-07 18:42:56,052 [INFO] unfolding 0, single step 68001
2019-04-07 18:42:56,053 [INFO] Sum of grad norms of most recent batch: 0.108915
2019-04-07 18:42:56,053 [INFO] ---------------------------------
2019-04-07 18:43:16,648 [INFO] ---------------------------------
2019-04-07 18:43:16,649 [INFO] Summary:
2019-04-07 18:43:16,650 [INFO] Batch 69000, worst loss 0.029184 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 18:43:16,651 [INFO] Regularization: 1051.804443 * 0.0000000100 = 0.0000105180 loss
2019-04-07 18:43:16,652 [INFO] unfolding 0, single step 69001
2019-04-07 18:43:16,653 [INFO] Sum of grad norms of most recent batch: 0.105486
2019-04-07 18:43:16,654 [INFO] ---------------------------------
2019-04-07 18:43:37,750 [INFO] ---------------------------------
2019-04-07 18:43:37,751 [INFO] Summary:
2019-04-07 18:43:37,751 [INFO] Batch 70000, worst loss 0.020617 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 18:43:37,752 [INFO] Regularization: 1048.555298 * 0.0000000100 = 0.0000104856 loss
2019-04-07 18:43:37,752 [INFO] unfolding 0, single step 70001
2019-04-07 18:43:37,753 [INFO] Sum of grad norms of most recent batch: 0.053780
2019-04-07 18:43:37,753 [INFO] ---------------------------------
2019-04-07 18:44:15,046 [INFO] ---------------------------------
2019-04-07 18:44:15,047 [INFO] Evaluation:
2019-04-07 18:44:15,047 [INFO] Batch 70000, worst loss 0.073488 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:44:15,048 [INFO] ---------------------------------
2019-04-07 18:44:36,757 [INFO] ---------------------------------
2019-04-07 18:44:36,757 [INFO] Summary:
2019-04-07 18:44:36,758 [INFO] Batch 71000, worst loss 0.028784 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 18:44:36,759 [INFO] Regularization: 1044.772095 * 0.0000000100 = 0.0000104477 loss
2019-04-07 18:44:36,759 [INFO] unfolding 0, single step 71001
2019-04-07 18:44:36,760 [INFO] Sum of grad norms of most recent batch: 0.069712
2019-04-07 18:44:36,760 [INFO] ---------------------------------
2019-04-07 18:44:58,000 [INFO] ---------------------------------
2019-04-07 18:44:58,001 [INFO] Summary:
2019-04-07 18:44:58,002 [INFO] Batch 72000, worst loss 0.009388 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 18:44:58,002 [INFO] Regularization: 1042.019409 * 0.0000000100 = 0.0000104202 loss
2019-04-07 18:44:58,003 [INFO] unfolding 0, single step 72001
2019-04-07 18:44:58,003 [INFO] Sum of grad norms of most recent batch: 0.103157
2019-04-07 18:44:58,004 [INFO] ---------------------------------
2019-04-07 18:45:19,674 [INFO] ---------------------------------
2019-04-07 18:45:19,675 [INFO] Summary:
2019-04-07 18:45:19,676 [INFO] Batch 73000, worst loss 0.013771 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 18:45:19,676 [INFO] Regularization: 1040.200684 * 0.0000000100 = 0.0000104020 loss
2019-04-07 18:45:19,676 [INFO] unfolding 0, single step 73001
2019-04-07 18:45:19,677 [INFO] Sum of grad norms of most recent batch: 0.056877
2019-04-07 18:45:19,678 [INFO] ---------------------------------
2019-04-07 18:45:41,383 [INFO] ---------------------------------
2019-04-07 18:45:41,384 [INFO] Summary:
2019-04-07 18:45:41,385 [INFO] Batch 74000, worst loss 0.048543 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 18:45:41,385 [INFO] Regularization: 1040.166626 * 0.0000000100 = 0.0000104017 loss
2019-04-07 18:45:41,386 [INFO] unfolding 0, single step 74001
2019-04-07 18:45:41,387 [INFO] Sum of grad norms of most recent batch: 0.084005
2019-04-07 18:45:41,388 [INFO] ---------------------------------
2019-04-07 18:46:02,980 [INFO] ---------------------------------
2019-04-07 18:46:02,981 [INFO] Summary:
2019-04-07 18:46:02,982 [INFO] Batch 75000, worst loss 0.007227 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 18:46:02,982 [INFO] Regularization: 1037.660034 * 0.0000000100 = 0.0000103766 loss
2019-04-07 18:46:02,983 [INFO] unfolding 0, single step 75001
2019-04-07 18:46:02,983 [INFO] Sum of grad norms of most recent batch: 0.067902
2019-04-07 18:46:02,984 [INFO] ---------------------------------
2019-04-07 18:46:24,443 [INFO] ---------------------------------
2019-04-07 18:46:24,444 [INFO] Summary:
2019-04-07 18:46:24,445 [INFO] Batch 76000, worst loss 0.007349 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 18:46:24,446 [INFO] Regularization: 1035.338135 * 0.0000000100 = 0.0000103534 loss
2019-04-07 18:46:24,447 [INFO] unfolding 0, single step 76001
2019-04-07 18:46:24,447 [INFO] Sum of grad norms of most recent batch: 0.129473
2019-04-07 18:46:24,448 [INFO] ---------------------------------
2019-04-07 18:46:45,811 [INFO] ---------------------------------
2019-04-07 18:46:45,812 [INFO] Summary:
2019-04-07 18:46:45,813 [INFO] Batch 77000, worst loss 0.010002 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 18:46:45,813 [INFO] Regularization: 1032.983032 * 0.0000000100 = 0.0000103298 loss
2019-04-07 18:46:45,813 [INFO] unfolding 0, single step 77001
2019-04-07 18:46:45,814 [INFO] Sum of grad norms of most recent batch: 0.167527
2019-04-07 18:46:45,814 [INFO] ---------------------------------
2019-04-07 18:47:07,281 [INFO] ---------------------------------
2019-04-07 18:47:07,282 [INFO] Summary:
2019-04-07 18:47:07,283 [INFO] Batch 78000, worst loss 0.012242 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 18:47:07,284 [INFO] Regularization: 1030.629517 * 0.0000000100 = 0.0000103063 loss
2019-04-07 18:47:07,285 [INFO] unfolding 0, single step 78001
2019-04-07 18:47:07,286 [INFO] Sum of grad norms of most recent batch: 0.074560
2019-04-07 18:47:07,286 [INFO] ---------------------------------
2019-04-07 18:47:28,457 [INFO] ---------------------------------
2019-04-07 18:47:28,458 [INFO] Summary:
2019-04-07 18:47:28,459 [INFO] Batch 79000, worst loss 0.030962 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 18:47:28,459 [INFO] Regularization: 1032.031250 * 0.0000000100 = 0.0000103203 loss
2019-04-07 18:47:28,460 [INFO] unfolding 0, single step 79001
2019-04-07 18:47:28,460 [INFO] Sum of grad norms of most recent batch: 0.403456
2019-04-07 18:47:28,461 [INFO] ---------------------------------
2019-04-07 18:47:49,185 [INFO] ---------------------------------
2019-04-07 18:47:49,186 [INFO] Summary:
2019-04-07 18:47:49,187 [INFO] Batch 80000, worst loss 0.017656 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 18:47:49,187 [INFO] Regularization: 1030.436035 * 0.0000000100 = 0.0000103044 loss
2019-04-07 18:47:49,188 [INFO] unfolding 0, single step 80001
2019-04-07 18:47:49,188 [INFO] Sum of grad norms of most recent batch: 0.081245
2019-04-07 18:47:49,189 [INFO] ---------------------------------
2019-04-07 18:48:26,566 [INFO] ---------------------------------
2019-04-07 18:48:26,567 [INFO] Evaluation:
2019-04-07 18:48:26,568 [INFO] Batch 80000, worst loss 0.041220 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:48:26,569 [INFO] ---------------------------------
2019-04-07 18:48:48,174 [INFO] ---------------------------------
2019-04-07 18:48:48,175 [INFO] Summary:
2019-04-07 18:48:48,175 [INFO] Batch 81000, worst loss 0.029718 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 18:48:48,176 [INFO] Regularization: 1029.316528 * 0.0000000100 = 0.0000102932 loss
2019-04-07 18:48:48,176 [INFO] unfolding 0, single step 81001
2019-04-07 18:48:48,177 [INFO] Sum of grad norms of most recent batch: 0.061079
2019-04-07 18:48:48,177 [INFO] ---------------------------------
2019-04-07 18:49:10,127 [INFO] ---------------------------------
2019-04-07 18:49:10,128 [INFO] Summary:
2019-04-07 18:49:10,128 [INFO] Batch 82000, worst loss 0.033205 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 18:49:10,129 [INFO] Regularization: 1028.770508 * 0.0000000100 = 0.0000102877 loss
2019-04-07 18:49:10,129 [INFO] unfolding 0, single step 82001
2019-04-07 18:49:10,130 [INFO] Sum of grad norms of most recent batch: 0.135677
2019-04-07 18:49:10,131 [INFO] ---------------------------------
2019-04-07 18:49:31,358 [INFO] ---------------------------------
2019-04-07 18:49:31,359 [INFO] Summary:
2019-04-07 18:49:31,359 [INFO] Batch 83000, worst loss 0.013861 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 18:49:31,360 [INFO] Regularization: 1027.610718 * 0.0000000100 = 0.0000102761 loss
2019-04-07 18:49:31,361 [INFO] unfolding 0, single step 83001
2019-04-07 18:49:31,361 [INFO] Sum of grad norms of most recent batch: 0.064870
2019-04-07 18:49:31,362 [INFO] ---------------------------------
2019-04-07 18:49:52,824 [INFO] ---------------------------------
2019-04-07 18:49:52,825 [INFO] Summary:
2019-04-07 18:49:52,825 [INFO] Batch 84000, worst loss 0.008688 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 18:49:52,826 [INFO] Regularization: 1027.165649 * 0.0000000100 = 0.0000102717 loss
2019-04-07 18:49:52,826 [INFO] unfolding 0, single step 84001
2019-04-07 18:49:52,827 [INFO] Sum of grad norms of most recent batch: 0.052986
2019-04-07 18:49:52,827 [INFO] ---------------------------------
2019-04-07 18:50:14,417 [INFO] ---------------------------------
2019-04-07 18:50:14,418 [INFO] Summary:
2019-04-07 18:50:14,419 [INFO] Batch 85000, worst loss 0.010700 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 18:50:14,419 [INFO] Regularization: 1026.384644 * 0.0000000100 = 0.0000102638 loss
2019-04-07 18:50:14,420 [INFO] unfolding 0, single step 85001
2019-04-07 18:50:14,421 [INFO] Sum of grad norms of most recent batch: 0.084110
2019-04-07 18:50:14,421 [INFO] ---------------------------------
2019-04-07 18:50:35,912 [INFO] ---------------------------------
2019-04-07 18:50:35,914 [INFO] Summary:
2019-04-07 18:50:35,914 [INFO] Batch 86000, worst loss 0.016993 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 18:50:35,915 [INFO] Regularization: 1026.184814 * 0.0000000100 = 0.0000102618 loss
2019-04-07 18:50:35,915 [INFO] unfolding 0, single step 86001
2019-04-07 18:50:35,916 [INFO] Sum of grad norms of most recent batch: 0.212974
2019-04-07 18:50:35,916 [INFO] ---------------------------------
2019-04-07 18:50:57,911 [INFO] ---------------------------------
2019-04-07 18:50:57,912 [INFO] Summary:
2019-04-07 18:50:57,912 [INFO] Batch 87000, worst loss 0.053721 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 18:50:57,913 [INFO] Regularization: 1026.703125 * 0.0000000100 = 0.0000102670 loss
2019-04-07 18:50:57,913 [INFO] unfolding 0, single step 87001
2019-04-07 18:50:57,914 [INFO] Sum of grad norms of most recent batch: 0.068425
2019-04-07 18:50:57,914 [INFO] ---------------------------------
2019-04-07 18:51:18,795 [INFO] ---------------------------------
2019-04-07 18:51:18,796 [INFO] Summary:
2019-04-07 18:51:18,797 [INFO] Batch 88000, worst loss 0.025535 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 18:51:18,797 [INFO] Regularization: 1025.857544 * 0.0000000100 = 0.0000102586 loss
2019-04-07 18:51:18,798 [INFO] unfolding 0, single step 88001
2019-04-07 18:51:18,799 [INFO] Sum of grad norms of most recent batch: 0.145706
2019-04-07 18:51:18,799 [INFO] ---------------------------------
2019-04-07 18:51:39,310 [INFO] ---------------------------------
2019-04-07 18:51:39,311 [INFO] Summary:
2019-04-07 18:51:39,311 [INFO] Batch 89000, worst loss 0.011669 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 18:51:39,312 [INFO] Regularization: 1025.047241 * 0.0000000100 = 0.0000102505 loss
2019-04-07 18:51:39,312 [INFO] unfolding 0, single step 89001
2019-04-07 18:51:39,313 [INFO] Sum of grad norms of most recent batch: 0.106904
2019-04-07 18:51:39,313 [INFO] ---------------------------------
2019-04-07 18:52:00,824 [INFO] ---------------------------------
2019-04-07 18:52:00,825 [INFO] Summary:
2019-04-07 18:52:00,826 [INFO] Batch 90000, worst loss 0.032250 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 18:52:00,827 [INFO] Regularization: 1024.507324 * 0.0000000100 = 0.0000102451 loss
2019-04-07 18:52:00,827 [INFO] unfolding 0, single step 90001
2019-04-07 18:52:00,827 [INFO] Sum of grad norms of most recent batch: 0.105767
2019-04-07 18:52:00,828 [INFO] ---------------------------------
2019-04-07 18:52:38,264 [INFO] ---------------------------------
2019-04-07 18:52:38,265 [INFO] Evaluation:
2019-04-07 18:52:38,266 [INFO] Batch 90000, worst loss 0.031284 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:52:38,267 [INFO] ---------------------------------
2019-04-07 18:52:59,527 [INFO] ---------------------------------
2019-04-07 18:52:59,528 [INFO] Summary:
2019-04-07 18:52:59,529 [INFO] Batch 91000, worst loss 0.009703 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 18:52:59,530 [INFO] Regularization: 1024.199341 * 0.0000000100 = 0.0000102420 loss
2019-04-07 18:52:59,531 [INFO] unfolding 0, single step 91001
2019-04-07 18:52:59,532 [INFO] Sum of grad norms of most recent batch: 0.176992
2019-04-07 18:52:59,533 [INFO] ---------------------------------
2019-04-07 18:53:20,980 [INFO] ---------------------------------
2019-04-07 18:53:20,981 [INFO] Summary:
2019-04-07 18:53:20,981 [INFO] Batch 92000, worst loss 0.013566 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 18:53:20,982 [INFO] Regularization: 1023.713745 * 0.0000000100 = 0.0000102371 loss
2019-04-07 18:53:20,982 [INFO] unfolding 0, single step 92001
2019-04-07 18:53:20,983 [INFO] Sum of grad norms of most recent batch: 0.034434
2019-04-07 18:53:20,983 [INFO] ---------------------------------
2019-04-07 18:53:42,110 [INFO] ---------------------------------
2019-04-07 18:53:42,111 [INFO] Summary:
2019-04-07 18:53:42,112 [INFO] Batch 93000, worst loss 0.004174 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 18:53:42,112 [INFO] Regularization: 1023.386780 * 0.0000000100 = 0.0000102339 loss
2019-04-07 18:53:42,113 [INFO] unfolding 0, single step 93001
2019-04-07 18:53:42,113 [INFO] Sum of grad norms of most recent batch: 0.086247
2019-04-07 18:53:42,114 [INFO] ---------------------------------
2019-04-07 18:54:19,243 [INFO] ---------------------------------
2019-04-07 18:54:19,244 [INFO] Evaluation:
2019-04-07 18:54:19,245 [INFO] Batch 93000, worst loss 0.030745 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:54:19,245 [INFO] ---------------------------------
2019-04-07 18:54:40,102 [INFO] ---------------------------------
2019-04-07 18:54:40,103 [INFO] Summary:
2019-04-07 18:54:40,104 [INFO] Batch 94000, worst loss 0.010566 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 18:54:40,104 [INFO] Regularization: 1023.292236 * 0.0000000100 = 0.0000102329 loss
2019-04-07 18:54:40,105 [INFO] unfolding 0, single step 94001
2019-04-07 18:54:40,106 [INFO] Sum of grad norms of most recent batch: 0.076303
2019-04-07 18:54:40,106 [INFO] ---------------------------------
2019-04-07 18:55:01,300 [INFO] ---------------------------------
2019-04-07 18:55:01,301 [INFO] Summary:
2019-04-07 18:55:01,301 [INFO] Batch 95000, worst loss 0.011490 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 18:55:01,302 [INFO] Regularization: 1022.752563 * 0.0000000100 = 0.0000102275 loss
2019-04-07 18:55:01,302 [INFO] unfolding 0, single step 95001
2019-04-07 18:55:01,303 [INFO] Sum of grad norms of most recent batch: 0.080949
2019-04-07 18:55:01,303 [INFO] ---------------------------------
2019-04-07 18:55:22,652 [INFO] ---------------------------------
2019-04-07 18:55:22,653 [INFO] Summary:
2019-04-07 18:55:22,655 [INFO] Batch 96000, worst loss 0.030896 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 18:55:22,655 [INFO] Regularization: 1022.382324 * 0.0000000100 = 0.0000102238 loss
2019-04-07 18:55:22,656 [INFO] unfolding 0, single step 96001
2019-04-07 18:55:22,656 [INFO] Sum of grad norms of most recent batch: 0.075654
2019-04-07 18:55:22,657 [INFO] ---------------------------------
2019-04-07 18:55:44,473 [INFO] ---------------------------------
2019-04-07 18:55:44,474 [INFO] Summary:
2019-04-07 18:55:44,474 [INFO] Batch 97000, worst loss 0.010322 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 18:55:44,475 [INFO] Regularization: 1022.121277 * 0.0000000100 = 0.0000102212 loss
2019-04-07 18:55:44,475 [INFO] unfolding 0, single step 97001
2019-04-07 18:55:44,476 [INFO] Sum of grad norms of most recent batch: 0.649594
2019-04-07 18:55:44,476 [INFO] ---------------------------------
2019-04-07 18:56:05,661 [INFO] ---------------------------------
2019-04-07 18:56:05,663 [INFO] Summary:
2019-04-07 18:56:05,663 [INFO] Batch 98000, worst loss 0.017039 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 18:56:05,664 [INFO] Regularization: 1021.577698 * 0.0000000100 = 0.0000102158 loss
2019-04-07 18:56:05,664 [INFO] unfolding 0, single step 98001
2019-04-07 18:56:05,665 [INFO] Sum of grad norms of most recent batch: 0.044501
2019-04-07 18:56:05,666 [INFO] ---------------------------------
2019-04-07 18:56:27,226 [INFO] ---------------------------------
2019-04-07 18:56:27,227 [INFO] Summary:
2019-04-07 18:56:27,228 [INFO] Batch 99000, worst loss 0.009157 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 18:56:27,228 [INFO] Regularization: 1021.199463 * 0.0000000100 = 0.0000102120 loss
2019-04-07 18:56:27,229 [INFO] unfolding 0, single step 99001
2019-04-07 18:56:27,229 [INFO] Sum of grad norms of most recent batch: 0.071168
2019-04-07 18:56:27,230 [INFO] ---------------------------------
2019-04-07 18:56:48,576 [INFO] ---------------------------------
2019-04-07 18:56:48,577 [INFO] Summary:
2019-04-07 18:56:48,577 [INFO] Batch 100000, worst loss 0.005845 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 18:56:48,578 [INFO] Regularization: 1020.656189 * 0.0000000100 = 0.0000102066 loss
2019-04-07 18:56:48,578 [INFO] unfolding 0, single step 100001
2019-04-07 18:56:48,579 [INFO] Sum of grad norms of most recent batch: 0.034045
2019-04-07 18:56:48,579 [INFO] ---------------------------------
2019-04-07 18:57:25,829 [INFO] ---------------------------------
2019-04-07 18:57:25,830 [INFO] Evaluation:
2019-04-07 18:57:25,830 [INFO] Batch 100000, worst loss 0.019940 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 18:57:25,831 [INFO] ---------------------------------
2019-04-07 18:57:47,138 [INFO] ---------------------------------
2019-04-07 18:57:47,139 [INFO] Summary:
2019-04-07 18:57:47,140 [INFO] Batch 101000, worst loss 0.007672 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 18:57:47,141 [INFO] Regularization: 1020.366455 * 0.0000000100 = 0.0000102037 loss
2019-04-07 18:57:47,141 [INFO] unfolding 0, single step 101001
2019-04-07 18:57:47,142 [INFO] Sum of grad norms of most recent batch: 0.045463
2019-04-07 18:57:47,142 [INFO] ---------------------------------
2019-04-07 18:58:08,155 [INFO] ---------------------------------
2019-04-07 18:58:08,156 [INFO] Summary:
2019-04-07 18:58:08,156 [INFO] Batch 102000, worst loss 0.009440 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 18:58:08,157 [INFO] Regularization: 1020.089355 * 0.0000000100 = 0.0000102009 loss
2019-04-07 18:58:08,157 [INFO] unfolding 0, single step 102001
2019-04-07 18:58:08,158 [INFO] Sum of grad norms of most recent batch: 0.187429
2019-04-07 18:58:08,158 [INFO] ---------------------------------
2019-04-07 18:58:28,912 [INFO] ---------------------------------
2019-04-07 18:58:28,913 [INFO] Summary:
2019-04-07 18:58:28,914 [INFO] Batch 103000, worst loss 0.011831 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 18:58:28,914 [INFO] Regularization: 1019.895569 * 0.0000000100 = 0.0000101990 loss
2019-04-07 18:58:28,915 [INFO] unfolding 0, single step 103001
2019-04-07 18:58:28,915 [INFO] Sum of grad norms of most recent batch: 0.078377
2019-04-07 18:58:28,916 [INFO] ---------------------------------
2019-04-07 18:58:50,253 [INFO] ---------------------------------
2019-04-07 18:58:50,254 [INFO] Summary:
2019-04-07 18:58:50,254 [INFO] Batch 104000, worst loss 0.005649 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 18:58:50,255 [INFO] Regularization: 1019.673706 * 0.0000000100 = 0.0000101967 loss
2019-04-07 18:58:50,255 [INFO] unfolding 0, single step 104001
2019-04-07 18:58:50,256 [INFO] Sum of grad norms of most recent batch: 0.063872
2019-04-07 18:58:50,256 [INFO] ---------------------------------
2019-04-07 18:59:11,548 [INFO] ---------------------------------
2019-04-07 18:59:11,549 [INFO] Summary:
2019-04-07 18:59:11,550 [INFO] Batch 105000, worst loss 0.014789 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 18:59:11,550 [INFO] Regularization: 1019.484863 * 0.0000000100 = 0.0000101948 loss
2019-04-07 18:59:11,550 [INFO] unfolding 0, single step 105001
2019-04-07 18:59:11,551 [INFO] Sum of grad norms of most recent batch: 0.063972
2019-04-07 18:59:11,551 [INFO] ---------------------------------
2019-04-07 18:59:33,105 [INFO] ---------------------------------
2019-04-07 18:59:33,107 [INFO] Summary:
2019-04-07 18:59:33,107 [INFO] Batch 106000, worst loss 0.008616 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 18:59:33,108 [INFO] Regularization: 1019.336243 * 0.0000000100 = 0.0000101934 loss
2019-04-07 18:59:33,108 [INFO] unfolding 0, single step 106001
2019-04-07 18:59:33,108 [INFO] Sum of grad norms of most recent batch: 0.191501
2019-04-07 18:59:33,109 [INFO] ---------------------------------
2019-04-07 18:59:54,313 [INFO] ---------------------------------
2019-04-07 18:59:54,314 [INFO] Summary:
2019-04-07 18:59:54,314 [INFO] Batch 107000, worst loss 0.012714 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 18:59:54,315 [INFO] Regularization: 1019.123962 * 0.0000000100 = 0.0000101912 loss
2019-04-07 18:59:54,315 [INFO] unfolding 0, single step 107001
2019-04-07 18:59:54,316 [INFO] Sum of grad norms of most recent batch: 2.738600
2019-04-07 18:59:54,316 [INFO] ---------------------------------
2019-04-07 19:00:15,733 [INFO] ---------------------------------
2019-04-07 19:00:15,734 [INFO] Summary:
2019-04-07 19:00:15,734 [INFO] Batch 108000, worst loss 0.017925 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 19:00:15,735 [INFO] Regularization: 1019.167786 * 0.0000000100 = 0.0000101917 loss
2019-04-07 19:00:15,735 [INFO] unfolding 0, single step 108001
2019-04-07 19:00:15,736 [INFO] Sum of grad norms of most recent batch: 0.126070
2019-04-07 19:00:15,736 [INFO] ---------------------------------
2019-04-07 19:00:37,170 [INFO] ---------------------------------
2019-04-07 19:00:37,171 [INFO] Summary:
2019-04-07 19:00:37,171 [INFO] Batch 109000, worst loss 0.006099 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 19:00:37,172 [INFO] Regularization: 1019.125549 * 0.0000000100 = 0.0000101913 loss
2019-04-07 19:00:37,172 [INFO] unfolding 0, single step 109001
2019-04-07 19:00:37,173 [INFO] Sum of grad norms of most recent batch: 0.084120
2019-04-07 19:00:37,173 [INFO] ---------------------------------
2019-04-07 19:00:59,226 [INFO] ---------------------------------
2019-04-07 19:00:59,227 [INFO] Summary:
2019-04-07 19:00:59,228 [INFO] Batch 110000, worst loss 0.005529 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 19:00:59,228 [INFO] Regularization: 1018.876160 * 0.0000000100 = 0.0000101888 loss
2019-04-07 19:00:59,229 [INFO] unfolding 0, single step 110001
2019-04-07 19:00:59,229 [INFO] Sum of grad norms of most recent batch: 0.095911
2019-04-07 19:00:59,230 [INFO] ---------------------------------
2019-04-07 19:01:36,420 [INFO] ---------------------------------
2019-04-07 19:01:36,421 [INFO] Evaluation:
2019-04-07 19:01:36,421 [INFO] Batch 110000, worst loss 0.030503 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:01:36,422 [INFO] ---------------------------------
2019-04-07 19:01:57,912 [INFO] ---------------------------------
2019-04-07 19:01:57,913 [INFO] Summary:
2019-04-07 19:01:57,914 [INFO] Batch 111000, worst loss 0.011538 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 19:01:57,914 [INFO] Regularization: 1018.727112 * 0.0000000100 = 0.0000101873 loss
2019-04-07 19:01:57,915 [INFO] unfolding 0, single step 111001
2019-04-07 19:01:57,915 [INFO] Sum of grad norms of most recent batch: 0.045865
2019-04-07 19:01:57,916 [INFO] ---------------------------------
2019-04-07 19:02:19,566 [INFO] ---------------------------------
2019-04-07 19:02:19,567 [INFO] Summary:
2019-04-07 19:02:19,568 [INFO] Batch 112000, worst loss 0.014138 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 19:02:19,568 [INFO] Regularization: 1018.698975 * 0.0000000100 = 0.0000101870 loss
2019-04-07 19:02:19,569 [INFO] unfolding 0, single step 112001
2019-04-07 19:02:19,570 [INFO] Sum of grad norms of most recent batch: 0.065626
2019-04-07 19:02:19,570 [INFO] ---------------------------------
2019-04-07 19:02:40,930 [INFO] ---------------------------------
2019-04-07 19:02:40,931 [INFO] Summary:
2019-04-07 19:02:40,932 [INFO] Batch 113000, worst loss 0.008469 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 19:02:40,932 [INFO] Regularization: 1018.527039 * 0.0000000100 = 0.0000101853 loss
2019-04-07 19:02:40,932 [INFO] unfolding 0, single step 113001
2019-04-07 19:02:40,933 [INFO] Sum of grad norms of most recent batch: 0.033509
2019-04-07 19:02:40,933 [INFO] ---------------------------------
2019-04-07 19:03:01,855 [INFO] ---------------------------------
2019-04-07 19:03:01,856 [INFO] Summary:
2019-04-07 19:03:01,856 [INFO] Batch 114000, worst loss 0.009410 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 19:03:01,857 [INFO] Regularization: 1018.516235 * 0.0000000100 = 0.0000101852 loss
2019-04-07 19:03:01,857 [INFO] unfolding 0, single step 114001
2019-04-07 19:03:01,858 [INFO] Sum of grad norms of most recent batch: 0.045520
2019-04-07 19:03:01,858 [INFO] ---------------------------------
2019-04-07 19:03:22,874 [INFO] ---------------------------------
2019-04-07 19:03:22,875 [INFO] Summary:
2019-04-07 19:03:22,876 [INFO] Batch 115000, worst loss 0.008607 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 19:03:22,876 [INFO] Regularization: 1018.450989 * 0.0000000100 = 0.0000101845 loss
2019-04-07 19:03:22,877 [INFO] unfolding 0, single step 115001
2019-04-07 19:03:22,877 [INFO] Sum of grad norms of most recent batch: 0.073688
2019-04-07 19:03:22,878 [INFO] ---------------------------------
2019-04-07 19:03:43,479 [INFO] ---------------------------------
2019-04-07 19:03:43,480 [INFO] Summary:
2019-04-07 19:03:43,480 [INFO] Batch 116000, worst loss 0.013240 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 19:03:43,481 [INFO] Regularization: 1018.446655 * 0.0000000100 = 0.0000101845 loss
2019-04-07 19:03:43,481 [INFO] unfolding 0, single step 116001
2019-04-07 19:03:43,482 [INFO] Sum of grad norms of most recent batch: 0.084998
2019-04-07 19:03:43,482 [INFO] ---------------------------------
2019-04-07 19:04:05,248 [INFO] ---------------------------------
2019-04-07 19:04:05,249 [INFO] Summary:
2019-04-07 19:04:05,250 [INFO] Batch 117000, worst loss 0.004865 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 19:04:05,250 [INFO] Regularization: 1018.321472 * 0.0000000100 = 0.0000101832 loss
2019-04-07 19:04:05,251 [INFO] unfolding 0, single step 117001
2019-04-07 19:04:05,251 [INFO] Sum of grad norms of most recent batch: 0.047475
2019-04-07 19:04:05,252 [INFO] ---------------------------------
2019-04-07 19:04:42,607 [INFO] ---------------------------------
2019-04-07 19:04:42,607 [INFO] Evaluation:
2019-04-07 19:04:42,608 [INFO] Batch 117000, worst loss 0.017609 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:04:42,608 [INFO] ---------------------------------
2019-04-07 19:05:03,942 [INFO] ---------------------------------
2019-04-07 19:05:03,943 [INFO] Summary:
2019-04-07 19:05:03,943 [INFO] Batch 118000, worst loss 0.004863 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 19:05:03,944 [INFO] Regularization: 1018.214172 * 0.0000000100 = 0.0000101821 loss
2019-04-07 19:05:03,944 [INFO] unfolding 0, single step 118001
2019-04-07 19:05:03,945 [INFO] Sum of grad norms of most recent batch: 0.069821
2019-04-07 19:05:03,945 [INFO] ---------------------------------
2019-04-07 19:05:41,427 [INFO] ---------------------------------
2019-04-07 19:05:41,428 [INFO] Evaluation:
2019-04-07 19:05:41,428 [INFO] Batch 118000, worst loss 0.023158 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:05:41,429 [INFO] ---------------------------------
2019-04-07 19:06:02,461 [INFO] ---------------------------------
2019-04-07 19:06:02,461 [INFO] Summary:
2019-04-07 19:06:02,462 [INFO] Batch 119000, worst loss 0.008965 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 19:06:02,462 [INFO] Regularization: 1018.183105 * 0.0000000100 = 0.0000101818 loss
2019-04-07 19:06:02,462 [INFO] unfolding 0, single step 119001
2019-04-07 19:06:02,463 [INFO] Sum of grad norms of most recent batch: 0.221071
2019-04-07 19:06:02,463 [INFO] ---------------------------------
2019-04-07 19:06:23,771 [INFO] ---------------------------------
2019-04-07 19:06:23,772 [INFO] Summary:
2019-04-07 19:06:23,772 [INFO] Batch 120000, worst loss 0.008925 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 19:06:23,773 [INFO] Regularization: 1018.054810 * 0.0000000100 = 0.0000101805 loss
2019-04-07 19:06:23,774 [INFO] unfolding 0, single step 120001
2019-04-07 19:06:23,775 [INFO] Sum of grad norms of most recent batch: 0.046260
2019-04-07 19:06:23,776 [INFO] ---------------------------------
2019-04-07 19:07:01,171 [INFO] ---------------------------------
2019-04-07 19:07:01,172 [INFO] Evaluation:
2019-04-07 19:07:01,173 [INFO] Batch 120000, worst loss 0.037195 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:07:01,173 [INFO] ---------------------------------
2019-04-07 19:07:23,193 [INFO] ---------------------------------
2019-04-07 19:07:23,194 [INFO] Summary:
2019-04-07 19:07:23,195 [INFO] Batch 121000, worst loss 0.016773 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 19:07:23,195 [INFO] Regularization: 1018.011475 * 0.0000000100 = 0.0000101801 loss
2019-04-07 19:07:23,195 [INFO] unfolding 0, single step 121001
2019-04-07 19:07:23,196 [INFO] Sum of grad norms of most recent batch: 0.037232
2019-04-07 19:07:23,196 [INFO] ---------------------------------
2019-04-07 19:07:44,059 [INFO] ---------------------------------
2019-04-07 19:07:44,060 [INFO] Summary:
2019-04-07 19:07:44,061 [INFO] Batch 122000, worst loss 0.010042 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 19:07:44,061 [INFO] Regularization: 1017.960327 * 0.0000000100 = 0.0000101796 loss
2019-04-07 19:07:44,062 [INFO] unfolding 0, single step 122001
2019-04-07 19:07:44,062 [INFO] Sum of grad norms of most recent batch: 0.043565
2019-04-07 19:07:44,063 [INFO] ---------------------------------
2019-04-07 19:08:05,115 [INFO] ---------------------------------
2019-04-07 19:08:05,116 [INFO] Summary:
2019-04-07 19:08:05,117 [INFO] Batch 123000, worst loss 0.010044 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 19:08:05,117 [INFO] Regularization: 1017.914856 * 0.0000000100 = 0.0000101791 loss
2019-04-07 19:08:05,118 [INFO] unfolding 0, single step 123001
2019-04-07 19:08:05,118 [INFO] Sum of grad norms of most recent batch: 0.043011
2019-04-07 19:08:05,119 [INFO] ---------------------------------
2019-04-07 19:08:27,196 [INFO] ---------------------------------
2019-04-07 19:08:27,197 [INFO] Summary:
2019-04-07 19:08:27,197 [INFO] Batch 124000, worst loss 0.027334 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 19:08:27,198 [INFO] Regularization: 1017.875122 * 0.0000000100 = 0.0000101788 loss
2019-04-07 19:08:27,198 [INFO] unfolding 0, single step 124001
2019-04-07 19:08:27,199 [INFO] Sum of grad norms of most recent batch: 1.665116
2019-04-07 19:08:27,199 [INFO] ---------------------------------
2019-04-07 19:08:48,657 [INFO] ---------------------------------
2019-04-07 19:08:48,658 [INFO] Summary:
2019-04-07 19:08:48,659 [INFO] Batch 125000, worst loss 0.005183 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 19:08:48,660 [INFO] Regularization: 1017.873352 * 0.0000000100 = 0.0000101787 loss
2019-04-07 19:08:48,660 [INFO] unfolding 0, single step 125001
2019-04-07 19:08:48,661 [INFO] Sum of grad norms of most recent batch: 0.026694
2019-04-07 19:08:48,661 [INFO] ---------------------------------
2019-04-07 19:09:10,383 [INFO] ---------------------------------
2019-04-07 19:09:10,384 [INFO] Summary:
2019-04-07 19:09:10,385 [INFO] Batch 126000, worst loss 0.013473 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 19:09:10,385 [INFO] Regularization: 1017.832764 * 0.0000000100 = 0.0000101783 loss
2019-04-07 19:09:10,385 [INFO] unfolding 0, single step 126001
2019-04-07 19:09:10,386 [INFO] Sum of grad norms of most recent batch: 0.057878
2019-04-07 19:09:10,386 [INFO] ---------------------------------
2019-04-07 19:09:31,353 [INFO] ---------------------------------
2019-04-07 19:09:31,354 [INFO] Summary:
2019-04-07 19:09:31,355 [INFO] Batch 127000, worst loss 0.032683 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 19:09:31,355 [INFO] Regularization: 1017.821106 * 0.0000000100 = 0.0000101782 loss
2019-04-07 19:09:31,356 [INFO] unfolding 0, single step 127001
2019-04-07 19:09:31,357 [INFO] Sum of grad norms of most recent batch: 0.034967
2019-04-07 19:09:31,357 [INFO] ---------------------------------
2019-04-07 19:09:52,922 [INFO] ---------------------------------
2019-04-07 19:09:52,923 [INFO] Summary:
2019-04-07 19:09:52,923 [INFO] Batch 128000, worst loss 0.005546 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 19:09:52,924 [INFO] Regularization: 1017.769104 * 0.0000000100 = 0.0000101777 loss
2019-04-07 19:09:52,924 [INFO] unfolding 0, single step 128001
2019-04-07 19:09:52,924 [INFO] Sum of grad norms of most recent batch: 0.026613
2019-04-07 19:09:52,925 [INFO] ---------------------------------
2019-04-07 19:10:14,423 [INFO] ---------------------------------
2019-04-07 19:10:14,424 [INFO] Summary:
2019-04-07 19:10:14,424 [INFO] Batch 129000, worst loss 0.008380 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 19:10:14,425 [INFO] Regularization: 1017.722595 * 0.0000000100 = 0.0000101772 loss
2019-04-07 19:10:14,425 [INFO] unfolding 0, single step 129001
2019-04-07 19:10:14,425 [INFO] Sum of grad norms of most recent batch: 0.057937
2019-04-07 19:10:14,426 [INFO] ---------------------------------
2019-04-07 19:10:35,273 [INFO] ---------------------------------
2019-04-07 19:10:35,274 [INFO] Summary:
2019-04-07 19:10:35,275 [INFO] Batch 130000, worst loss 0.008309 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 19:10:35,277 [INFO] Regularization: 1017.748779 * 0.0000000100 = 0.0000101775 loss
2019-04-07 19:10:35,277 [INFO] unfolding 0, single step 130001
2019-04-07 19:10:35,279 [INFO] Sum of grad norms of most recent batch: 0.072341
2019-04-07 19:10:35,280 [INFO] ---------------------------------
2019-04-07 19:11:12,797 [INFO] ---------------------------------
2019-04-07 19:11:12,798 [INFO] Evaluation:
2019-04-07 19:11:12,798 [INFO] Batch 130000, worst loss 0.021421 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:11:12,799 [INFO] ---------------------------------
2019-04-07 19:11:34,434 [INFO] ---------------------------------
2019-04-07 19:11:34,435 [INFO] Summary:
2019-04-07 19:11:34,436 [INFO] Batch 131000, worst loss 0.007791 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:11:34,436 [INFO] Regularization: 1017.684937 * 0.0000000100 = 0.0000101768 loss
2019-04-07 19:11:34,436 [INFO] unfolding 0, single step 131001
2019-04-07 19:11:34,437 [INFO] Sum of grad norms of most recent batch: 0.063397
2019-04-07 19:11:34,437 [INFO] ---------------------------------
2019-04-07 19:11:55,588 [INFO] ---------------------------------
2019-04-07 19:11:55,589 [INFO] Summary:
2019-04-07 19:11:55,589 [INFO] Batch 132000, worst loss 0.010690 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:11:55,590 [INFO] Regularization: 1017.654358 * 0.0000000100 = 0.0000101765 loss
2019-04-07 19:11:55,590 [INFO] unfolding 0, single step 132001
2019-04-07 19:11:55,591 [INFO] Sum of grad norms of most recent batch: 0.170671
2019-04-07 19:11:55,591 [INFO] ---------------------------------
2019-04-07 19:12:16,747 [INFO] ---------------------------------
2019-04-07 19:12:16,748 [INFO] Summary:
2019-04-07 19:12:16,749 [INFO] Batch 133000, worst loss 0.006439 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:12:16,750 [INFO] Regularization: 1017.634399 * 0.0000000100 = 0.0000101763 loss
2019-04-07 19:12:16,751 [INFO] unfolding 0, single step 133001
2019-04-07 19:12:16,752 [INFO] Sum of grad norms of most recent batch: 0.016458
2019-04-07 19:12:16,753 [INFO] ---------------------------------
2019-04-07 19:12:37,805 [INFO] ---------------------------------
2019-04-07 19:12:37,806 [INFO] Summary:
2019-04-07 19:12:37,807 [INFO] Batch 134000, worst loss 0.015920 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:12:37,807 [INFO] Regularization: 1017.585938 * 0.0000000100 = 0.0000101759 loss
2019-04-07 19:12:37,808 [INFO] unfolding 0, single step 134001
2019-04-07 19:12:37,808 [INFO] Sum of grad norms of most recent batch: 0.035215
2019-04-07 19:12:37,809 [INFO] ---------------------------------
2019-04-07 19:12:59,678 [INFO] ---------------------------------
2019-04-07 19:12:59,680 [INFO] Summary:
2019-04-07 19:12:59,681 [INFO] Batch 135000, worst loss 0.007581 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:12:59,682 [INFO] Regularization: 1017.585327 * 0.0000000100 = 0.0000101759 loss
2019-04-07 19:12:59,682 [INFO] unfolding 0, single step 135001
2019-04-07 19:12:59,683 [INFO] Sum of grad norms of most recent batch: 0.082481
2019-04-07 19:12:59,684 [INFO] ---------------------------------
2019-04-07 19:13:21,180 [INFO] ---------------------------------
2019-04-07 19:13:21,181 [INFO] Summary:
2019-04-07 19:13:21,182 [INFO] Batch 136000, worst loss 0.007144 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:13:21,182 [INFO] Regularization: 1017.595215 * 0.0000000100 = 0.0000101760 loss
2019-04-07 19:13:21,183 [INFO] unfolding 0, single step 136001
2019-04-07 19:13:21,183 [INFO] Sum of grad norms of most recent batch: 0.052958
2019-04-07 19:13:21,184 [INFO] ---------------------------------
2019-04-07 19:13:42,500 [INFO] ---------------------------------
2019-04-07 19:13:42,501 [INFO] Summary:
2019-04-07 19:13:42,502 [INFO] Batch 137000, worst loss 0.014283 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:13:42,503 [INFO] Regularization: 1017.592163 * 0.0000000100 = 0.0000101759 loss
2019-04-07 19:13:42,503 [INFO] unfolding 0, single step 137001
2019-04-07 19:13:42,504 [INFO] Sum of grad norms of most recent batch: 0.034800
2019-04-07 19:13:42,504 [INFO] ---------------------------------
2019-04-07 19:14:03,639 [INFO] ---------------------------------
2019-04-07 19:14:03,641 [INFO] Summary:
2019-04-07 19:14:03,641 [INFO] Batch 138000, worst loss 0.012375 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:14:03,642 [INFO] Regularization: 1017.549011 * 0.0000000100 = 0.0000101755 loss
2019-04-07 19:14:03,642 [INFO] unfolding 0, single step 138001
2019-04-07 19:14:03,643 [INFO] Sum of grad norms of most recent batch: 0.033152
2019-04-07 19:14:03,643 [INFO] ---------------------------------
2019-04-07 19:14:24,514 [INFO] ---------------------------------
2019-04-07 19:14:24,515 [INFO] Summary:
2019-04-07 19:14:24,516 [INFO] Batch 139000, worst loss 0.011687 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:14:24,516 [INFO] Regularization: 1017.560181 * 0.0000000100 = 0.0000101756 loss
2019-04-07 19:14:24,517 [INFO] unfolding 0, single step 139001
2019-04-07 19:14:24,517 [INFO] Sum of grad norms of most recent batch: 1.247719
2019-04-07 19:14:24,518 [INFO] ---------------------------------
2019-04-07 19:14:45,843 [INFO] ---------------------------------
2019-04-07 19:14:45,844 [INFO] Summary:
2019-04-07 19:14:45,844 [INFO] Batch 140000, worst loss 0.005025 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:14:45,845 [INFO] Regularization: 1017.507568 * 0.0000000100 = 0.0000101751 loss
2019-04-07 19:14:45,845 [INFO] unfolding 0, single step 140001
2019-04-07 19:14:45,846 [INFO] Sum of grad norms of most recent batch: 0.054711
2019-04-07 19:14:45,846 [INFO] ---------------------------------
2019-04-07 19:15:23,040 [INFO] ---------------------------------
2019-04-07 19:15:23,041 [INFO] Evaluation:
2019-04-07 19:15:23,041 [INFO] Batch 140000, worst loss 0.023815 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:15:23,042 [INFO] ---------------------------------
2019-04-07 19:15:44,319 [INFO] ---------------------------------
2019-04-07 19:15:44,321 [INFO] Summary:
2019-04-07 19:15:44,321 [INFO] Batch 141000, worst loss 0.008475 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:15:44,322 [INFO] Regularization: 1017.502319 * 0.0000000100 = 0.0000101750 loss
2019-04-07 19:15:44,323 [INFO] unfolding 0, single step 141001
2019-04-07 19:15:44,323 [INFO] Sum of grad norms of most recent batch: 0.073029
2019-04-07 19:15:44,324 [INFO] ---------------------------------
2019-04-07 19:16:05,661 [INFO] ---------------------------------
2019-04-07 19:16:05,662 [INFO] Summary:
2019-04-07 19:16:05,663 [INFO] Batch 142000, worst loss 0.005612 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:16:05,663 [INFO] Regularization: 1017.496521 * 0.0000000100 = 0.0000101750 loss
2019-04-07 19:16:05,664 [INFO] unfolding 0, single step 142001
2019-04-07 19:16:05,664 [INFO] Sum of grad norms of most recent batch: 0.044002
2019-04-07 19:16:05,665 [INFO] ---------------------------------
2019-04-07 19:16:26,834 [INFO] ---------------------------------
2019-04-07 19:16:26,835 [INFO] Summary:
2019-04-07 19:16:26,836 [INFO] Batch 143000, worst loss 0.005397 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:16:26,837 [INFO] Regularization: 1017.478333 * 0.0000000100 = 0.0000101748 loss
2019-04-07 19:16:26,837 [INFO] unfolding 0, single step 143001
2019-04-07 19:16:26,838 [INFO] Sum of grad norms of most recent batch: 0.057059
2019-04-07 19:16:26,838 [INFO] ---------------------------------
2019-04-07 19:16:48,104 [INFO] ---------------------------------
2019-04-07 19:16:48,105 [INFO] Summary:
2019-04-07 19:16:48,105 [INFO] Batch 144000, worst loss 0.015959 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:16:48,106 [INFO] Regularization: 1017.463379 * 0.0000000100 = 0.0000101746 loss
2019-04-07 19:16:48,106 [INFO] unfolding 0, single step 144001
2019-04-07 19:16:48,107 [INFO] Sum of grad norms of most recent batch: 0.057750
2019-04-07 19:16:48,108 [INFO] ---------------------------------
2019-04-07 19:17:09,409 [INFO] ---------------------------------
2019-04-07 19:17:09,410 [INFO] Summary:
2019-04-07 19:17:09,410 [INFO] Batch 145000, worst loss 0.003696 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:17:09,411 [INFO] Regularization: 1017.435669 * 0.0000000100 = 0.0000101744 loss
2019-04-07 19:17:09,411 [INFO] unfolding 0, single step 145001
2019-04-07 19:17:09,412 [INFO] Sum of grad norms of most recent batch: 0.032236
2019-04-07 19:17:09,413 [INFO] ---------------------------------
2019-04-07 19:17:46,854 [INFO] ---------------------------------
2019-04-07 19:17:46,855 [INFO] Evaluation:
2019-04-07 19:17:46,855 [INFO] Batch 145000, worst loss 0.031700 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:17:46,856 [INFO] ---------------------------------
2019-04-07 19:18:08,369 [INFO] ---------------------------------
2019-04-07 19:18:08,370 [INFO] Summary:
2019-04-07 19:18:08,370 [INFO] Batch 146000, worst loss 0.051322 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:18:08,371 [INFO] Regularization: 1017.437805 * 0.0000000100 = 0.0000101744 loss
2019-04-07 19:18:08,371 [INFO] unfolding 0, single step 146001
2019-04-07 19:18:08,372 [INFO] Sum of grad norms of most recent batch: 0.037174
2019-04-07 19:18:08,373 [INFO] ---------------------------------
2019-04-07 19:18:29,950 [INFO] ---------------------------------
2019-04-07 19:18:29,951 [INFO] Summary:
2019-04-07 19:18:29,951 [INFO] Batch 147000, worst loss 0.019400 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:18:29,952 [INFO] Regularization: 1017.421021 * 0.0000000100 = 0.0000101742 loss
2019-04-07 19:18:29,952 [INFO] unfolding 0, single step 147001
2019-04-07 19:18:29,952 [INFO] Sum of grad norms of most recent batch: 0.078513
2019-04-07 19:18:29,953 [INFO] ---------------------------------
2019-04-07 19:18:50,675 [INFO] ---------------------------------
2019-04-07 19:18:50,676 [INFO] Summary:
2019-04-07 19:18:50,677 [INFO] Batch 148000, worst loss 0.018958 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:18:50,677 [INFO] Regularization: 1017.416138 * 0.0000000100 = 0.0000101742 loss
2019-04-07 19:18:50,677 [INFO] unfolding 0, single step 148001
2019-04-07 19:18:50,678 [INFO] Sum of grad norms of most recent batch: 0.042497
2019-04-07 19:18:50,678 [INFO] ---------------------------------
2019-04-07 19:19:11,875 [INFO] ---------------------------------
2019-04-07 19:19:11,876 [INFO] Summary:
2019-04-07 19:19:11,876 [INFO] Batch 149000, worst loss 0.008366 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:19:11,877 [INFO] Regularization: 1017.400879 * 0.0000000100 = 0.0000101740 loss
2019-04-07 19:19:11,878 [INFO] unfolding 0, single step 149001
2019-04-07 19:19:11,878 [INFO] Sum of grad norms of most recent batch: 0.098898
2019-04-07 19:19:11,879 [INFO] ---------------------------------
2019-04-07 19:19:32,680 [INFO] ---------------------------------
2019-04-07 19:19:32,681 [INFO] Summary:
2019-04-07 19:19:32,681 [INFO] Batch 150000, worst loss 0.009543 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 19:19:32,682 [INFO] Regularization: 1017.385986 * 0.0000000100 = 0.0000101739 loss
2019-04-07 19:19:32,682 [INFO] unfolding 0, single step 150001
2019-04-07 19:19:32,683 [INFO] Sum of grad norms of most recent batch: 0.032804
2019-04-07 19:19:32,684 [INFO] ---------------------------------
2019-04-07 19:20:09,999 [INFO] ---------------------------------
2019-04-07 19:20:10,000 [INFO] Evaluation:
2019-04-07 19:20:10,001 [INFO] Batch 150000, worst loss 0.027739 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:20:10,001 [INFO] ---------------------------------
2019-04-07 19:20:10,002 [INFO] Finished training, saved to file transition/1554653688/1554657610_0_transition_final.pth
2019-04-07 19:20:10,180 [INFO] ---------------------------------
2019-04-07 19:20:10,181 [INFO] Training model #1: (11, 64, 8) @ 3
2019-04-07 19:20:31,896 [INFO] ---------------------------------
2019-04-07 19:20:31,897 [INFO] Summary:
2019-04-07 19:20:31,897 [INFO] Batch 1000, worst loss 103.714523 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:20:31,898 [INFO] Regularization: 6836.545410 * 0.0000000100 = 0.0000683655 loss
2019-04-07 19:20:31,898 [INFO] unfolding 0, single step 1001
2019-04-07 19:20:31,899 [INFO] Sum of grad norms of most recent batch: 14.417445
2019-04-07 19:20:31,899 [INFO] ---------------------------------
2019-04-07 19:20:53,452 [INFO] ---------------------------------
2019-04-07 19:20:53,453 [INFO] Summary:
2019-04-07 19:20:53,454 [INFO] Batch 2000, worst loss 0.092933 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:20:53,454 [INFO] Regularization: 4013.678711 * 0.0000000100 = 0.0000401368 loss
2019-04-07 19:20:53,454 [INFO] unfolding 0, single step 2001
2019-04-07 19:20:53,455 [INFO] Sum of grad norms of most recent batch: 2.507951
2019-04-07 19:20:53,455 [INFO] ---------------------------------
2019-04-07 19:21:15,507 [INFO] ---------------------------------
2019-04-07 19:21:15,508 [INFO] Summary:
2019-04-07 19:21:15,509 [INFO] Batch 3000, worst loss 0.042730 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:21:15,510 [INFO] Regularization: 3177.822754 * 0.0000000100 = 0.0000317782 loss
2019-04-07 19:21:15,510 [INFO] unfolding 0, single step 3001
2019-04-07 19:21:15,512 [INFO] Sum of grad norms of most recent batch: 2.121325
2019-04-07 19:21:15,512 [INFO] ---------------------------------
2019-04-07 19:21:37,480 [INFO] ---------------------------------
2019-04-07 19:21:37,481 [INFO] Summary:
2019-04-07 19:21:37,481 [INFO] Batch 4000, worst loss 0.162577 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:21:37,482 [INFO] Regularization: 2875.016602 * 0.0000000100 = 0.0000287502 loss
2019-04-07 19:21:37,482 [INFO] unfolding 0, single step 4001
2019-04-07 19:21:37,483 [INFO] Sum of grad norms of most recent batch: 8.900644
2019-04-07 19:21:37,483 [INFO] ---------------------------------
2019-04-07 19:21:58,950 [INFO] ---------------------------------
2019-04-07 19:21:58,951 [INFO] Summary:
2019-04-07 19:21:58,952 [INFO] Batch 5000, worst loss 0.133441 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:21:58,952 [INFO] Regularization: 2670.087891 * 0.0000000100 = 0.0000267009 loss
2019-04-07 19:21:58,953 [INFO] unfolding 0, single step 5001
2019-04-07 19:21:58,953 [INFO] Sum of grad norms of most recent batch: 1.407549
2019-04-07 19:21:58,954 [INFO] ---------------------------------
2019-04-07 19:22:20,337 [INFO] ---------------------------------
2019-04-07 19:22:20,338 [INFO] Summary:
2019-04-07 19:22:20,338 [INFO] Batch 6000, worst loss 0.159088 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:22:20,339 [INFO] Regularization: 2430.004150 * 0.0000000100 = 0.0000243000 loss
2019-04-07 19:22:20,339 [INFO] unfolding 0, single step 6001
2019-04-07 19:22:20,340 [INFO] Sum of grad norms of most recent batch: 3.233250
2019-04-07 19:22:20,340 [INFO] ---------------------------------
2019-04-07 19:22:42,208 [INFO] ---------------------------------
2019-04-07 19:22:42,209 [INFO] Summary:
2019-04-07 19:22:42,209 [INFO] Batch 7000, worst loss 0.034443 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:22:42,210 [INFO] Regularization: 2235.817627 * 0.0000000100 = 0.0000223582 loss
2019-04-07 19:22:42,210 [INFO] unfolding 0, single step 7001
2019-04-07 19:22:42,211 [INFO] Sum of grad norms of most recent batch: 1.117880
2019-04-07 19:22:42,211 [INFO] ---------------------------------
2019-04-07 19:23:03,177 [INFO] ---------------------------------
2019-04-07 19:23:03,178 [INFO] Summary:
2019-04-07 19:23:03,178 [INFO] Batch 8000, worst loss 0.120802 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:23:03,179 [INFO] Regularization: 2109.254150 * 0.0000000100 = 0.0000210925 loss
2019-04-07 19:23:03,179 [INFO] unfolding 0, single step 8001
2019-04-07 19:23:03,180 [INFO] Sum of grad norms of most recent batch: 2.152841
2019-04-07 19:23:03,181 [INFO] ---------------------------------
2019-04-07 19:23:25,062 [INFO] ---------------------------------
2019-04-07 19:23:25,063 [INFO] Summary:
2019-04-07 19:23:25,063 [INFO] Batch 9000, worst loss 0.121082 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:23:25,064 [INFO] Regularization: 2001.237061 * 0.0000000100 = 0.0000200124 loss
2019-04-07 19:23:25,064 [INFO] unfolding 0, single step 9001
2019-04-07 19:23:25,065 [INFO] Sum of grad norms of most recent batch: 2.429043
2019-04-07 19:23:25,065 [INFO] ---------------------------------
2019-04-07 19:23:46,736 [INFO] ---------------------------------
2019-04-07 19:23:46,737 [INFO] Summary:
2019-04-07 19:23:46,738 [INFO] Batch 10000, worst loss 0.166070 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:23:46,739 [INFO] Regularization: 1945.212402 * 0.0000000100 = 0.0000194521 loss
2019-04-07 19:23:46,739 [INFO] unfolding 0, single step 10001
2019-04-07 19:23:46,740 [INFO] Sum of grad norms of most recent batch: 1.155733
2019-04-07 19:23:46,740 [INFO] ---------------------------------
2019-04-07 19:24:23,976 [INFO] ---------------------------------
2019-04-07 19:24:23,977 [INFO] Evaluation:
2019-04-07 19:24:23,978 [INFO] Batch 10000, worst loss 0.163669 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:24:23,979 [INFO] ---------------------------------
2019-04-07 19:24:45,707 [INFO] ---------------------------------
2019-04-07 19:24:45,708 [INFO] Summary:
2019-04-07 19:24:45,709 [INFO] Batch 11000, worst loss 0.084328 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:24:45,709 [INFO] Regularization: 1874.955444 * 0.0000000100 = 0.0000187496 loss
2019-04-07 19:24:45,709 [INFO] unfolding 0, single step 11001
2019-04-07 19:24:45,710 [INFO] Sum of grad norms of most recent batch: 0.544612
2019-04-07 19:24:45,710 [INFO] ---------------------------------
2019-04-07 19:25:07,235 [INFO] ---------------------------------
2019-04-07 19:25:07,236 [INFO] Summary:
2019-04-07 19:25:07,237 [INFO] Batch 12000, worst loss 0.147679 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:25:07,238 [INFO] Regularization: 1866.969604 * 0.0000000100 = 0.0000186697 loss
2019-04-07 19:25:07,238 [INFO] unfolding 0, single step 12001
2019-04-07 19:25:07,239 [INFO] Sum of grad norms of most recent batch: 1.730052
2019-04-07 19:25:07,239 [INFO] ---------------------------------
2019-04-07 19:25:28,854 [INFO] ---------------------------------
2019-04-07 19:25:28,855 [INFO] Summary:
2019-04-07 19:25:28,856 [INFO] Batch 13000, worst loss 0.036031 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:25:28,857 [INFO] Regularization: 1807.740845 * 0.0000000100 = 0.0000180774 loss
2019-04-07 19:25:28,857 [INFO] unfolding 0, single step 13001
2019-04-07 19:25:28,858 [INFO] Sum of grad norms of most recent batch: 1.182918
2019-04-07 19:25:28,859 [INFO] ---------------------------------
2019-04-07 19:25:51,267 [INFO] ---------------------------------
2019-04-07 19:25:51,268 [INFO] Summary:
2019-04-07 19:25:51,268 [INFO] Batch 14000, worst loss 0.112480 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:25:51,269 [INFO] Regularization: 1791.779541 * 0.0000000100 = 0.0000179178 loss
2019-04-07 19:25:51,269 [INFO] unfolding 0, single step 14001
2019-04-07 19:25:51,270 [INFO] Sum of grad norms of most recent batch: 1.613924
2019-04-07 19:25:51,270 [INFO] ---------------------------------
2019-04-07 19:26:13,359 [INFO] ---------------------------------
2019-04-07 19:26:13,360 [INFO] Summary:
2019-04-07 19:26:13,361 [INFO] Batch 15000, worst loss 0.155343 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:26:13,362 [INFO] Regularization: 1777.643433 * 0.0000000100 = 0.0000177764 loss
2019-04-07 19:26:13,363 [INFO] unfolding 0, single step 15001
2019-04-07 19:26:13,364 [INFO] Sum of grad norms of most recent batch: 0.780789
2019-04-07 19:26:13,364 [INFO] ---------------------------------
2019-04-07 19:26:34,897 [INFO] ---------------------------------
2019-04-07 19:26:34,898 [INFO] Summary:
2019-04-07 19:26:34,899 [INFO] Batch 16000, worst loss 0.099461 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:26:34,899 [INFO] Regularization: 1773.013672 * 0.0000000100 = 0.0000177301 loss
2019-04-07 19:26:34,899 [INFO] unfolding 0, single step 16001
2019-04-07 19:26:34,900 [INFO] Sum of grad norms of most recent batch: 2.847128
2019-04-07 19:26:34,900 [INFO] ---------------------------------
2019-04-07 19:26:56,747 [INFO] ---------------------------------
2019-04-07 19:26:56,748 [INFO] Summary:
2019-04-07 19:26:56,748 [INFO] Batch 17000, worst loss 0.080888 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:26:56,749 [INFO] Regularization: 1823.752930 * 0.0000000100 = 0.0000182375 loss
2019-04-07 19:26:56,749 [INFO] unfolding 0, single step 17001
2019-04-07 19:26:56,750 [INFO] Sum of grad norms of most recent batch: 1.569241
2019-04-07 19:26:56,750 [INFO] ---------------------------------
2019-04-07 19:27:18,344 [INFO] ---------------------------------
2019-04-07 19:27:18,345 [INFO] Summary:
2019-04-07 19:27:18,345 [INFO] Batch 18000, worst loss 0.071380 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:27:18,347 [INFO] Regularization: 1831.313477 * 0.0000000100 = 0.0000183131 loss
2019-04-07 19:27:18,347 [INFO] unfolding 0, single step 18001
2019-04-07 19:27:18,349 [INFO] Sum of grad norms of most recent batch: 0.785644
2019-04-07 19:27:18,350 [INFO] ---------------------------------
2019-04-07 19:27:40,602 [INFO] ---------------------------------
2019-04-07 19:27:40,603 [INFO] Summary:
2019-04-07 19:27:40,604 [INFO] Batch 19000, worst loss 0.076094 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:27:40,605 [INFO] Regularization: 1797.967896 * 0.0000000100 = 0.0000179797 loss
2019-04-07 19:27:40,605 [INFO] unfolding 0, single step 19001
2019-04-07 19:27:40,606 [INFO] Sum of grad norms of most recent batch: 1.132355
2019-04-07 19:27:40,607 [INFO] ---------------------------------
2019-04-07 19:28:02,838 [INFO] ---------------------------------
2019-04-07 19:28:02,839 [INFO] Summary:
2019-04-07 19:28:02,840 [INFO] Batch 20000, worst loss 0.082156 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:28:02,841 [INFO] Regularization: 1792.969971 * 0.0000000100 = 0.0000179297 loss
2019-04-07 19:28:02,841 [INFO] unfolding 0, single step 20001
2019-04-07 19:28:02,842 [INFO] Sum of grad norms of most recent batch: 1.141416
2019-04-07 19:28:02,842 [INFO] ---------------------------------
2019-04-07 19:28:40,283 [INFO] ---------------------------------
2019-04-07 19:28:40,284 [INFO] Evaluation:
2019-04-07 19:28:40,285 [INFO] Batch 20000, worst loss 0.137327 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:28:40,286 [INFO] ---------------------------------
2019-04-07 19:29:01,740 [INFO] ---------------------------------
2019-04-07 19:29:01,741 [INFO] Summary:
2019-04-07 19:29:01,742 [INFO] Batch 21000, worst loss 0.154708 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:29:01,742 [INFO] Regularization: 1768.235962 * 0.0000000100 = 0.0000176824 loss
2019-04-07 19:29:01,743 [INFO] unfolding 0, single step 21001
2019-04-07 19:29:01,743 [INFO] Sum of grad norms of most recent batch: 1.815327
2019-04-07 19:29:01,744 [INFO] ---------------------------------
2019-04-07 19:29:23,535 [INFO] ---------------------------------
2019-04-07 19:29:23,536 [INFO] Summary:
2019-04-07 19:29:23,537 [INFO] Batch 22000, worst loss 0.138944 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:29:23,537 [INFO] Regularization: 1766.262939 * 0.0000000100 = 0.0000176626 loss
2019-04-07 19:29:23,538 [INFO] unfolding 0, single step 22001
2019-04-07 19:29:23,538 [INFO] Sum of grad norms of most recent batch: 1.690273
2019-04-07 19:29:23,539 [INFO] ---------------------------------
2019-04-07 19:29:45,127 [INFO] ---------------------------------
2019-04-07 19:29:45,128 [INFO] Summary:
2019-04-07 19:29:45,129 [INFO] Batch 23000, worst loss 0.040198 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:29:45,130 [INFO] Regularization: 1744.307007 * 0.0000000100 = 0.0000174431 loss
2019-04-07 19:29:45,131 [INFO] unfolding 0, single step 23001
2019-04-07 19:29:45,132 [INFO] Sum of grad norms of most recent batch: 2.375941
2019-04-07 19:29:45,132 [INFO] ---------------------------------
2019-04-07 19:30:07,048 [INFO] ---------------------------------
2019-04-07 19:30:07,049 [INFO] Summary:
2019-04-07 19:30:07,049 [INFO] Batch 24000, worst loss 0.105737 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:30:07,050 [INFO] Regularization: 1701.193726 * 0.0000000100 = 0.0000170119 loss
2019-04-07 19:30:07,050 [INFO] unfolding 0, single step 24001
2019-04-07 19:30:07,051 [INFO] Sum of grad norms of most recent batch: 1.064704
2019-04-07 19:30:07,051 [INFO] ---------------------------------
2019-04-07 19:30:28,498 [INFO] ---------------------------------
2019-04-07 19:30:28,499 [INFO] Summary:
2019-04-07 19:30:28,499 [INFO] Batch 25000, worst loss 0.108267 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:30:28,500 [INFO] Regularization: 1712.942505 * 0.0000000100 = 0.0000171294 loss
2019-04-07 19:30:28,500 [INFO] unfolding 0, single step 25001
2019-04-07 19:30:28,501 [INFO] Sum of grad norms of most recent batch: 1.672269
2019-04-07 19:30:28,501 [INFO] ---------------------------------
2019-04-07 19:30:49,967 [INFO] ---------------------------------
2019-04-07 19:30:49,968 [INFO] Summary:
2019-04-07 19:30:49,968 [INFO] Batch 26000, worst loss 0.149987 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:30:49,969 [INFO] Regularization: 1720.123169 * 0.0000000100 = 0.0000172012 loss
2019-04-07 19:30:49,969 [INFO] unfolding 0, single step 26001
2019-04-07 19:30:49,970 [INFO] Sum of grad norms of most recent batch: 2.649468
2019-04-07 19:30:49,970 [INFO] ---------------------------------
2019-04-07 19:31:11,756 [INFO] ---------------------------------
2019-04-07 19:31:11,757 [INFO] Summary:
2019-04-07 19:31:11,757 [INFO] Batch 27000, worst loss 0.110870 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:31:11,758 [INFO] Regularization: 1682.030518 * 0.0000000100 = 0.0000168203 loss
2019-04-07 19:31:11,758 [INFO] unfolding 0, single step 27001
2019-04-07 19:31:11,759 [INFO] Sum of grad norms of most recent batch: 1.628535
2019-04-07 19:31:11,759 [INFO] ---------------------------------
2019-04-07 19:31:33,389 [INFO] ---------------------------------
2019-04-07 19:31:33,390 [INFO] Summary:
2019-04-07 19:31:33,391 [INFO] Batch 28000, worst loss 0.126913 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:31:33,391 [INFO] Regularization: 1675.594849 * 0.0000000100 = 0.0000167559 loss
2019-04-07 19:31:33,391 [INFO] unfolding 0, single step 28001
2019-04-07 19:31:33,392 [INFO] Sum of grad norms of most recent batch: 0.634910
2019-04-07 19:31:33,392 [INFO] ---------------------------------
2019-04-07 19:31:55,588 [INFO] ---------------------------------
2019-04-07 19:31:55,589 [INFO] Summary:
2019-04-07 19:31:55,590 [INFO] Batch 29000, worst loss 0.091524 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:31:55,590 [INFO] Regularization: 1669.288330 * 0.0000000100 = 0.0000166929 loss
2019-04-07 19:31:55,591 [INFO] unfolding 0, single step 29001
2019-04-07 19:31:55,591 [INFO] Sum of grad norms of most recent batch: 1.245684
2019-04-07 19:31:55,592 [INFO] ---------------------------------
2019-04-07 19:32:17,094 [INFO] ---------------------------------
2019-04-07 19:32:17,095 [INFO] Summary:
2019-04-07 19:32:17,096 [INFO] Batch 30000, worst loss 0.029470 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 19:32:17,096 [INFO] Regularization: 1630.716064 * 0.0000000100 = 0.0000163072 loss
2019-04-07 19:32:17,097 [INFO] unfolding 0, single step 30001
2019-04-07 19:32:17,097 [INFO] Sum of grad norms of most recent batch: 0.822528
2019-04-07 19:32:17,098 [INFO] ---------------------------------
2019-04-07 19:32:54,349 [INFO] ---------------------------------
2019-04-07 19:32:54,350 [INFO] Evaluation:
2019-04-07 19:32:54,351 [INFO] Batch 30000, worst loss 0.168117 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:32:54,351 [INFO] ---------------------------------
2019-04-07 19:33:16,682 [INFO] ---------------------------------
2019-04-07 19:33:16,683 [INFO] Summary:
2019-04-07 19:33:16,684 [INFO] Batch 31000, worst loss 0.145767 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 19:33:16,685 [INFO] Regularization: 1688.167969 * 0.0000000100 = 0.0000168817 loss
2019-04-07 19:33:16,685 [INFO] unfolding 0, single step 31001
2019-04-07 19:33:16,686 [INFO] Sum of grad norms of most recent batch: 1.325804
2019-04-07 19:33:16,687 [INFO] ---------------------------------
2019-04-07 19:33:39,046 [INFO] ---------------------------------
2019-04-07 19:33:39,047 [INFO] Summary:
2019-04-07 19:33:39,048 [INFO] Batch 32000, worst loss 0.122409 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 19:33:39,048 [INFO] Regularization: 1607.829346 * 0.0000000100 = 0.0000160783 loss
2019-04-07 19:33:39,048 [INFO] unfolding 0, single step 32001
2019-04-07 19:33:39,049 [INFO] Sum of grad norms of most recent batch: 0.515972
2019-04-07 19:33:39,049 [INFO] ---------------------------------
2019-04-07 19:34:00,487 [INFO] ---------------------------------
2019-04-07 19:34:00,488 [INFO] Summary:
2019-04-07 19:34:00,489 [INFO] Batch 33000, worst loss 0.120066 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 19:34:00,489 [INFO] Regularization: 1591.258789 * 0.0000000100 = 0.0000159126 loss
2019-04-07 19:34:00,490 [INFO] unfolding 0, single step 33001
2019-04-07 19:34:00,491 [INFO] Sum of grad norms of most recent batch: 0.813532
2019-04-07 19:34:00,491 [INFO] ---------------------------------
2019-04-07 19:34:22,741 [INFO] ---------------------------------
2019-04-07 19:34:22,742 [INFO] Summary:
2019-04-07 19:34:22,742 [INFO] Batch 34000, worst loss 0.086335 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 19:34:22,743 [INFO] Regularization: 1544.721313 * 0.0000000100 = 0.0000154472 loss
2019-04-07 19:34:22,743 [INFO] unfolding 0, single step 34001
2019-04-07 19:34:22,744 [INFO] Sum of grad norms of most recent batch: 0.357145
2019-04-07 19:34:22,744 [INFO] ---------------------------------
2019-04-07 19:34:44,216 [INFO] ---------------------------------
2019-04-07 19:34:44,217 [INFO] Summary:
2019-04-07 19:34:44,217 [INFO] Batch 35000, worst loss 0.090034 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 19:34:44,218 [INFO] Regularization: 1519.245483 * 0.0000000100 = 0.0000151925 loss
2019-04-07 19:34:44,218 [INFO] unfolding 0, single step 35001
2019-04-07 19:34:44,219 [INFO] Sum of grad norms of most recent batch: 1.156856
2019-04-07 19:34:44,219 [INFO] ---------------------------------
2019-04-07 19:35:05,619 [INFO] ---------------------------------
2019-04-07 19:35:05,619 [INFO] Summary:
2019-04-07 19:35:05,620 [INFO] Batch 36000, worst loss 0.050984 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 19:35:05,621 [INFO] Regularization: 1480.544556 * 0.0000000100 = 0.0000148054 loss
2019-04-07 19:35:05,621 [INFO] unfolding 0, single step 36001
2019-04-07 19:35:05,622 [INFO] Sum of grad norms of most recent batch: 1.774175
2019-04-07 19:35:05,622 [INFO] ---------------------------------
2019-04-07 19:35:27,467 [INFO] ---------------------------------
2019-04-07 19:35:27,468 [INFO] Summary:
2019-04-07 19:35:27,469 [INFO] Batch 37000, worst loss 0.038950 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 19:35:27,470 [INFO] Regularization: 1449.218872 * 0.0000000100 = 0.0000144922 loss
2019-04-07 19:35:27,470 [INFO] unfolding 0, single step 37001
2019-04-07 19:35:27,471 [INFO] Sum of grad norms of most recent batch: 0.199587
2019-04-07 19:35:27,471 [INFO] ---------------------------------
2019-04-07 19:35:49,320 [INFO] ---------------------------------
2019-04-07 19:35:49,321 [INFO] Summary:
2019-04-07 19:35:49,322 [INFO] Batch 38000, worst loss 0.150508 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 19:35:49,323 [INFO] Regularization: 1422.697632 * 0.0000000100 = 0.0000142270 loss
2019-04-07 19:35:49,323 [INFO] unfolding 0, single step 38001
2019-04-07 19:35:49,324 [INFO] Sum of grad norms of most recent batch: 0.466781
2019-04-07 19:35:49,324 [INFO] ---------------------------------
2019-04-07 19:36:10,967 [INFO] ---------------------------------
2019-04-07 19:36:10,968 [INFO] Summary:
2019-04-07 19:36:10,968 [INFO] Batch 39000, worst loss 0.066064 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 19:36:10,969 [INFO] Regularization: 1386.407593 * 0.0000000100 = 0.0000138641 loss
2019-04-07 19:36:10,969 [INFO] unfolding 0, single step 39001
2019-04-07 19:36:10,970 [INFO] Sum of grad norms of most recent batch: 0.363311
2019-04-07 19:36:10,970 [INFO] ---------------------------------
2019-04-07 19:36:32,183 [INFO] ---------------------------------
2019-04-07 19:36:32,184 [INFO] Summary:
2019-04-07 19:36:32,184 [INFO] Batch 40000, worst loss 0.108785 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 19:36:32,185 [INFO] Regularization: 1365.555298 * 0.0000000100 = 0.0000136556 loss
2019-04-07 19:36:32,185 [INFO] unfolding 0, single step 40001
2019-04-07 19:36:32,185 [INFO] Sum of grad norms of most recent batch: 0.423097
2019-04-07 19:36:32,186 [INFO] ---------------------------------
2019-04-07 19:37:09,523 [INFO] ---------------------------------
2019-04-07 19:37:09,524 [INFO] Evaluation:
2019-04-07 19:37:09,524 [INFO] Batch 40000, worst loss 0.143278 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:37:09,525 [INFO] ---------------------------------
2019-04-07 19:37:30,944 [INFO] ---------------------------------
2019-04-07 19:37:30,945 [INFO] Summary:
2019-04-07 19:37:30,946 [INFO] Batch 41000, worst loss 0.107435 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 19:37:30,946 [INFO] Regularization: 1390.248779 * 0.0000000100 = 0.0000139025 loss
2019-04-07 19:37:30,946 [INFO] unfolding 0, single step 41001
2019-04-07 19:37:30,947 [INFO] Sum of grad norms of most recent batch: 0.531876
2019-04-07 19:37:30,947 [INFO] ---------------------------------
2019-04-07 19:37:52,709 [INFO] ---------------------------------
2019-04-07 19:37:52,710 [INFO] Summary:
2019-04-07 19:37:52,711 [INFO] Batch 42000, worst loss 0.074672 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 19:37:52,711 [INFO] Regularization: 1325.883423 * 0.0000000100 = 0.0000132588 loss
2019-04-07 19:37:52,712 [INFO] unfolding 0, single step 42001
2019-04-07 19:37:52,712 [INFO] Sum of grad norms of most recent batch: 0.121061
2019-04-07 19:37:52,713 [INFO] ---------------------------------
2019-04-07 19:38:14,417 [INFO] ---------------------------------
2019-04-07 19:38:14,418 [INFO] Summary:
2019-04-07 19:38:14,419 [INFO] Batch 43000, worst loss 0.090024 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 19:38:14,419 [INFO] Regularization: 1318.475952 * 0.0000000100 = 0.0000131848 loss
2019-04-07 19:38:14,420 [INFO] unfolding 0, single step 43001
2019-04-07 19:38:14,420 [INFO] Sum of grad norms of most recent batch: 0.601526
2019-04-07 19:38:14,421 [INFO] ---------------------------------
2019-04-07 19:38:35,650 [INFO] ---------------------------------
2019-04-07 19:38:35,651 [INFO] Summary:
2019-04-07 19:38:35,652 [INFO] Batch 44000, worst loss 0.062848 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 19:38:35,652 [INFO] Regularization: 1308.069336 * 0.0000000100 = 0.0000130807 loss
2019-04-07 19:38:35,653 [INFO] unfolding 0, single step 44001
2019-04-07 19:38:35,653 [INFO] Sum of grad norms of most recent batch: 0.229607
2019-04-07 19:38:35,654 [INFO] ---------------------------------
2019-04-07 19:38:56,982 [INFO] ---------------------------------
2019-04-07 19:38:56,983 [INFO] Summary:
2019-04-07 19:38:56,984 [INFO] Batch 45000, worst loss 0.120889 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 19:38:56,984 [INFO] Regularization: 1293.344238 * 0.0000000100 = 0.0000129334 loss
2019-04-07 19:38:56,985 [INFO] unfolding 0, single step 45001
2019-04-07 19:38:56,985 [INFO] Sum of grad norms of most recent batch: 0.067849
2019-04-07 19:38:56,986 [INFO] ---------------------------------
2019-04-07 19:39:18,929 [INFO] ---------------------------------
2019-04-07 19:39:18,930 [INFO] Summary:
2019-04-07 19:39:18,930 [INFO] Batch 46000, worst loss 0.097566 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 19:39:18,931 [INFO] Regularization: 1278.853149 * 0.0000000100 = 0.0000127885 loss
2019-04-07 19:39:18,931 [INFO] unfolding 0, single step 46001
2019-04-07 19:39:18,932 [INFO] Sum of grad norms of most recent batch: 0.528172
2019-04-07 19:39:18,933 [INFO] ---------------------------------
2019-04-07 19:39:40,763 [INFO] ---------------------------------
2019-04-07 19:39:40,764 [INFO] Summary:
2019-04-07 19:39:40,764 [INFO] Batch 47000, worst loss 0.106054 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 19:39:40,765 [INFO] Regularization: 1294.088135 * 0.0000000100 = 0.0000129409 loss
2019-04-07 19:39:40,766 [INFO] unfolding 0, single step 47001
2019-04-07 19:39:40,767 [INFO] Sum of grad norms of most recent batch: 0.307192
2019-04-07 19:39:40,767 [INFO] ---------------------------------
2019-04-07 19:40:02,580 [INFO] ---------------------------------
2019-04-07 19:40:02,581 [INFO] Summary:
2019-04-07 19:40:02,581 [INFO] Batch 48000, worst loss 0.090699 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 19:40:02,582 [INFO] Regularization: 1277.755005 * 0.0000000100 = 0.0000127776 loss
2019-04-07 19:40:02,582 [INFO] unfolding 0, single step 48001
2019-04-07 19:40:02,583 [INFO] Sum of grad norms of most recent batch: 1.832775
2019-04-07 19:40:02,584 [INFO] ---------------------------------
2019-04-07 19:40:24,896 [INFO] ---------------------------------
2019-04-07 19:40:24,897 [INFO] Summary:
2019-04-07 19:40:24,898 [INFO] Batch 49000, worst loss 0.061935 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 19:40:24,898 [INFO] Regularization: 1283.758667 * 0.0000000100 = 0.0000128376 loss
2019-04-07 19:40:24,899 [INFO] unfolding 0, single step 49001
2019-04-07 19:40:24,899 [INFO] Sum of grad norms of most recent batch: 0.396554
2019-04-07 19:40:24,900 [INFO] ---------------------------------
2019-04-07 19:40:46,410 [INFO] ---------------------------------
2019-04-07 19:40:46,412 [INFO] Summary:
2019-04-07 19:40:46,413 [INFO] Batch 50000, worst loss 0.056426 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 19:40:46,413 [INFO] Regularization: 1276.158569 * 0.0000000100 = 0.0000127616 loss
2019-04-07 19:40:46,414 [INFO] unfolding 0, single step 50001
2019-04-07 19:40:46,415 [INFO] Sum of grad norms of most recent batch: 0.153910
2019-04-07 19:40:46,416 [INFO] ---------------------------------
2019-04-07 19:41:23,952 [INFO] ---------------------------------
2019-04-07 19:41:23,954 [INFO] Evaluation:
2019-04-07 19:41:23,954 [INFO] Batch 50000, worst loss 0.152499 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:41:23,955 [INFO] ---------------------------------
2019-04-07 19:41:45,131 [INFO] ---------------------------------
2019-04-07 19:41:45,132 [INFO] Summary:
2019-04-07 19:41:45,132 [INFO] Batch 51000, worst loss 0.043434 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 19:41:45,133 [INFO] Regularization: 1266.423950 * 0.0000000100 = 0.0000126642 loss
2019-04-07 19:41:45,133 [INFO] unfolding 0, single step 51001
2019-04-07 19:41:45,133 [INFO] Sum of grad norms of most recent batch: 0.457702
2019-04-07 19:41:45,134 [INFO] ---------------------------------
2019-04-07 19:42:06,918 [INFO] ---------------------------------
2019-04-07 19:42:06,919 [INFO] Summary:
2019-04-07 19:42:06,920 [INFO] Batch 52000, worst loss 0.037436 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 19:42:06,921 [INFO] Regularization: 1248.272583 * 0.0000000100 = 0.0000124827 loss
2019-04-07 19:42:06,921 [INFO] unfolding 0, single step 52001
2019-04-07 19:42:06,922 [INFO] Sum of grad norms of most recent batch: 0.241607
2019-04-07 19:42:06,923 [INFO] ---------------------------------
2019-04-07 19:42:28,550 [INFO] ---------------------------------
2019-04-07 19:42:28,551 [INFO] Summary:
2019-04-07 19:42:28,551 [INFO] Batch 53000, worst loss 0.087988 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 19:42:28,552 [INFO] Regularization: 1239.711670 * 0.0000000100 = 0.0000123971 loss
2019-04-07 19:42:28,552 [INFO] unfolding 0, single step 53001
2019-04-07 19:42:28,553 [INFO] Sum of grad norms of most recent batch: 0.127009
2019-04-07 19:42:28,553 [INFO] ---------------------------------
2019-04-07 19:42:50,169 [INFO] ---------------------------------
2019-04-07 19:42:50,169 [INFO] Summary:
2019-04-07 19:42:50,170 [INFO] Batch 54000, worst loss 0.047009 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 19:42:50,171 [INFO] Regularization: 1242.292725 * 0.0000000100 = 0.0000124229 loss
2019-04-07 19:42:50,171 [INFO] unfolding 0, single step 54001
2019-04-07 19:42:50,171 [INFO] Sum of grad norms of most recent batch: 0.060067
2019-04-07 19:42:50,172 [INFO] ---------------------------------
2019-04-07 19:43:11,665 [INFO] ---------------------------------
2019-04-07 19:43:11,666 [INFO] Summary:
2019-04-07 19:43:11,667 [INFO] Batch 55000, worst loss 0.098895 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 19:43:11,667 [INFO] Regularization: 1231.476807 * 0.0000000100 = 0.0000123148 loss
2019-04-07 19:43:11,668 [INFO] unfolding 0, single step 55001
2019-04-07 19:43:11,669 [INFO] Sum of grad norms of most recent batch: 0.542468
2019-04-07 19:43:11,669 [INFO] ---------------------------------
2019-04-07 19:43:32,721 [INFO] ---------------------------------
2019-04-07 19:43:32,722 [INFO] Summary:
2019-04-07 19:43:32,722 [INFO] Batch 56000, worst loss 0.104061 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 19:43:32,723 [INFO] Regularization: 1230.836182 * 0.0000000100 = 0.0000123084 loss
2019-04-07 19:43:32,723 [INFO] unfolding 0, single step 56001
2019-04-07 19:43:32,724 [INFO] Sum of grad norms of most recent batch: 0.074685
2019-04-07 19:43:32,724 [INFO] ---------------------------------
2019-04-07 19:43:54,427 [INFO] ---------------------------------
2019-04-07 19:43:54,428 [INFO] Summary:
2019-04-07 19:43:54,428 [INFO] Batch 57000, worst loss 0.106989 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 19:43:54,429 [INFO] Regularization: 1228.887817 * 0.0000000100 = 0.0000122889 loss
2019-04-07 19:43:54,429 [INFO] unfolding 0, single step 57001
2019-04-07 19:43:54,430 [INFO] Sum of grad norms of most recent batch: 0.210025
2019-04-07 19:43:54,430 [INFO] ---------------------------------
2019-04-07 19:44:16,270 [INFO] ---------------------------------
2019-04-07 19:44:16,272 [INFO] Summary:
2019-04-07 19:44:16,275 [INFO] Batch 58000, worst loss 0.040003 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 19:44:16,275 [INFO] Regularization: 1223.147339 * 0.0000000100 = 0.0000122315 loss
2019-04-07 19:44:16,276 [INFO] unfolding 0, single step 58001
2019-04-07 19:44:16,276 [INFO] Sum of grad norms of most recent batch: 0.594377
2019-04-07 19:44:16,277 [INFO] ---------------------------------
2019-04-07 19:44:38,428 [INFO] ---------------------------------
2019-04-07 19:44:38,429 [INFO] Summary:
2019-04-07 19:44:38,429 [INFO] Batch 59000, worst loss 0.061117 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 19:44:38,430 [INFO] Regularization: 1216.026489 * 0.0000000100 = 0.0000121603 loss
2019-04-07 19:44:38,430 [INFO] unfolding 0, single step 59001
2019-04-07 19:44:38,431 [INFO] Sum of grad norms of most recent batch: 0.106166
2019-04-07 19:44:38,431 [INFO] ---------------------------------
2019-04-07 19:45:00,244 [INFO] ---------------------------------
2019-04-07 19:45:00,245 [INFO] Summary:
2019-04-07 19:45:00,246 [INFO] Batch 60000, worst loss 0.101319 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 19:45:00,246 [INFO] Regularization: 1209.069824 * 0.0000000100 = 0.0000120907 loss
2019-04-07 19:45:00,246 [INFO] unfolding 0, single step 60001
2019-04-07 19:45:00,247 [INFO] Sum of grad norms of most recent batch: 0.567118
2019-04-07 19:45:00,247 [INFO] ---------------------------------
2019-04-07 19:45:37,358 [INFO] ---------------------------------
2019-04-07 19:45:37,359 [INFO] Evaluation:
2019-04-07 19:45:37,360 [INFO] Batch 60000, worst loss 0.127410 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:45:37,360 [INFO] ---------------------------------
2019-04-07 19:45:58,570 [INFO] ---------------------------------
2019-04-07 19:45:58,571 [INFO] Summary:
2019-04-07 19:45:58,572 [INFO] Batch 61000, worst loss 0.066145 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 19:45:58,572 [INFO] Regularization: 1211.601440 * 0.0000000100 = 0.0000121160 loss
2019-04-07 19:45:58,573 [INFO] unfolding 0, single step 61001
2019-04-07 19:45:58,573 [INFO] Sum of grad norms of most recent batch: 0.222703
2019-04-07 19:45:58,574 [INFO] ---------------------------------
2019-04-07 19:46:19,951 [INFO] ---------------------------------
2019-04-07 19:46:19,952 [INFO] Summary:
2019-04-07 19:46:19,952 [INFO] Batch 62000, worst loss 0.078120 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 19:46:19,953 [INFO] Regularization: 1202.905762 * 0.0000000100 = 0.0000120291 loss
2019-04-07 19:46:19,953 [INFO] unfolding 0, single step 62001
2019-04-07 19:46:19,954 [INFO] Sum of grad norms of most recent batch: 0.104195
2019-04-07 19:46:19,955 [INFO] ---------------------------------
2019-04-07 19:46:41,114 [INFO] ---------------------------------
2019-04-07 19:46:41,116 [INFO] Summary:
2019-04-07 19:46:41,117 [INFO] Batch 63000, worst loss 0.074314 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 19:46:41,117 [INFO] Regularization: 1198.502686 * 0.0000000100 = 0.0000119850 loss
2019-04-07 19:46:41,118 [INFO] unfolding 0, single step 63001
2019-04-07 19:46:41,119 [INFO] Sum of grad norms of most recent batch: 0.073433
2019-04-07 19:46:41,119 [INFO] ---------------------------------
2019-04-07 19:47:01,864 [INFO] ---------------------------------
2019-04-07 19:47:01,865 [INFO] Summary:
2019-04-07 19:47:01,866 [INFO] Batch 64000, worst loss 0.018808 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 19:47:01,867 [INFO] Regularization: 1197.552979 * 0.0000000100 = 0.0000119755 loss
2019-04-07 19:47:01,868 [INFO] unfolding 0, single step 64001
2019-04-07 19:47:01,869 [INFO] Sum of grad norms of most recent batch: 0.039787
2019-04-07 19:47:01,869 [INFO] ---------------------------------
2019-04-07 19:47:23,914 [INFO] ---------------------------------
2019-04-07 19:47:23,915 [INFO] Summary:
2019-04-07 19:47:23,915 [INFO] Batch 65000, worst loss 0.054359 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 19:47:23,916 [INFO] Regularization: 1195.294678 * 0.0000000100 = 0.0000119529 loss
2019-04-07 19:47:23,916 [INFO] unfolding 0, single step 65001
2019-04-07 19:47:23,917 [INFO] Sum of grad norms of most recent batch: 0.092363
2019-04-07 19:47:23,917 [INFO] ---------------------------------
2019-04-07 19:47:44,946 [INFO] ---------------------------------
2019-04-07 19:47:44,947 [INFO] Summary:
2019-04-07 19:47:44,947 [INFO] Batch 66000, worst loss 0.051093 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 19:47:44,948 [INFO] Regularization: 1189.726807 * 0.0000000100 = 0.0000118973 loss
2019-04-07 19:47:44,948 [INFO] unfolding 0, single step 66001
2019-04-07 19:47:44,949 [INFO] Sum of grad norms of most recent batch: 0.073044
2019-04-07 19:47:44,949 [INFO] ---------------------------------
2019-04-07 19:48:05,635 [INFO] ---------------------------------
2019-04-07 19:48:05,636 [INFO] Summary:
2019-04-07 19:48:05,637 [INFO] Batch 67000, worst loss 0.083346 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 19:48:05,638 [INFO] Regularization: 1192.408569 * 0.0000000100 = 0.0000119241 loss
2019-04-07 19:48:05,638 [INFO] unfolding 0, single step 67001
2019-04-07 19:48:05,639 [INFO] Sum of grad norms of most recent batch: 0.113805
2019-04-07 19:48:05,639 [INFO] ---------------------------------
2019-04-07 19:48:27,103 [INFO] ---------------------------------
2019-04-07 19:48:27,104 [INFO] Summary:
2019-04-07 19:48:27,105 [INFO] Batch 68000, worst loss 0.098310 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 19:48:27,106 [INFO] Regularization: 1188.247803 * 0.0000000100 = 0.0000118825 loss
2019-04-07 19:48:27,106 [INFO] unfolding 0, single step 68001
2019-04-07 19:48:27,107 [INFO] Sum of grad norms of most recent batch: 0.199544
2019-04-07 19:48:27,107 [INFO] ---------------------------------
2019-04-07 19:48:48,629 [INFO] ---------------------------------
2019-04-07 19:48:48,630 [INFO] Summary:
2019-04-07 19:48:48,631 [INFO] Batch 69000, worst loss 0.018619 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 19:48:48,632 [INFO] Regularization: 1183.473999 * 0.0000000100 = 0.0000118347 loss
2019-04-07 19:48:48,632 [INFO] unfolding 0, single step 69001
2019-04-07 19:48:48,633 [INFO] Sum of grad norms of most recent batch: 0.457089
2019-04-07 19:48:48,634 [INFO] ---------------------------------
2019-04-07 19:49:10,014 [INFO] ---------------------------------
2019-04-07 19:49:10,015 [INFO] Summary:
2019-04-07 19:49:10,016 [INFO] Batch 70000, worst loss 0.046101 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 19:49:10,017 [INFO] Regularization: 1182.911377 * 0.0000000100 = 0.0000118291 loss
2019-04-07 19:49:10,017 [INFO] unfolding 0, single step 70001
2019-04-07 19:49:10,018 [INFO] Sum of grad norms of most recent batch: 0.099348
2019-04-07 19:49:10,018 [INFO] ---------------------------------
2019-04-07 19:49:47,361 [INFO] ---------------------------------
2019-04-07 19:49:47,362 [INFO] Evaluation:
2019-04-07 19:49:47,362 [INFO] Batch 70000, worst loss 0.105229 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:49:47,363 [INFO] ---------------------------------
2019-04-07 19:50:09,004 [INFO] ---------------------------------
2019-04-07 19:50:09,005 [INFO] Summary:
2019-04-07 19:50:09,005 [INFO] Batch 71000, worst loss 0.029274 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 19:50:09,006 [INFO] Regularization: 1179.203125 * 0.0000000100 = 0.0000117920 loss
2019-04-07 19:50:09,006 [INFO] unfolding 0, single step 71001
2019-04-07 19:50:09,007 [INFO] Sum of grad norms of most recent batch: 0.226705
2019-04-07 19:50:09,007 [INFO] ---------------------------------
2019-04-07 19:50:30,548 [INFO] ---------------------------------
2019-04-07 19:50:30,549 [INFO] Summary:
2019-04-07 19:50:30,549 [INFO] Batch 72000, worst loss 0.027203 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 19:50:30,550 [INFO] Regularization: 1178.226074 * 0.0000000100 = 0.0000117823 loss
2019-04-07 19:50:30,550 [INFO] unfolding 0, single step 72001
2019-04-07 19:50:30,551 [INFO] Sum of grad norms of most recent batch: 0.410706
2019-04-07 19:50:30,552 [INFO] ---------------------------------
2019-04-07 19:50:51,856 [INFO] ---------------------------------
2019-04-07 19:50:51,857 [INFO] Summary:
2019-04-07 19:50:51,858 [INFO] Batch 73000, worst loss 0.107862 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 19:50:51,858 [INFO] Regularization: 1176.222046 * 0.0000000100 = 0.0000117622 loss
2019-04-07 19:50:51,858 [INFO] unfolding 0, single step 73001
2019-04-07 19:50:51,859 [INFO] Sum of grad norms of most recent batch: 0.045218
2019-04-07 19:50:51,859 [INFO] ---------------------------------
2019-04-07 19:51:13,217 [INFO] ---------------------------------
2019-04-07 19:51:13,218 [INFO] Summary:
2019-04-07 19:51:13,219 [INFO] Batch 74000, worst loss 0.033795 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 19:51:13,219 [INFO] Regularization: 1174.383911 * 0.0000000100 = 0.0000117438 loss
2019-04-07 19:51:13,220 [INFO] unfolding 0, single step 74001
2019-04-07 19:51:13,220 [INFO] Sum of grad norms of most recent batch: 0.115543
2019-04-07 19:51:13,221 [INFO] ---------------------------------
2019-04-07 19:51:34,105 [INFO] ---------------------------------
2019-04-07 19:51:34,106 [INFO] Summary:
2019-04-07 19:51:34,107 [INFO] Batch 75000, worst loss 0.100677 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 19:51:34,107 [INFO] Regularization: 1173.693237 * 0.0000000100 = 0.0000117369 loss
2019-04-07 19:51:34,108 [INFO] unfolding 0, single step 75001
2019-04-07 19:51:34,108 [INFO] Sum of grad norms of most recent batch: 0.226152
2019-04-07 19:51:34,109 [INFO] ---------------------------------
2019-04-07 19:51:55,492 [INFO] ---------------------------------
2019-04-07 19:51:55,493 [INFO] Summary:
2019-04-07 19:51:55,493 [INFO] Batch 76000, worst loss 0.020145 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 19:51:55,494 [INFO] Regularization: 1171.814087 * 0.0000000100 = 0.0000117181 loss
2019-04-07 19:51:55,494 [INFO] unfolding 0, single step 76001
2019-04-07 19:51:55,495 [INFO] Sum of grad norms of most recent batch: 0.047126
2019-04-07 19:51:55,495 [INFO] ---------------------------------
2019-04-07 19:52:17,090 [INFO] ---------------------------------
2019-04-07 19:52:17,091 [INFO] Summary:
2019-04-07 19:52:17,092 [INFO] Batch 77000, worst loss 0.012557 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 19:52:17,092 [INFO] Regularization: 1169.835449 * 0.0000000100 = 0.0000116984 loss
2019-04-07 19:52:17,093 [INFO] unfolding 0, single step 77001
2019-04-07 19:52:17,093 [INFO] Sum of grad norms of most recent batch: 3.439237
2019-04-07 19:52:17,094 [INFO] ---------------------------------
2019-04-07 19:52:38,646 [INFO] ---------------------------------
2019-04-07 19:52:38,648 [INFO] Summary:
2019-04-07 19:52:38,649 [INFO] Batch 78000, worst loss 0.031142 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 19:52:38,649 [INFO] Regularization: 1169.481201 * 0.0000000100 = 0.0000116948 loss
2019-04-07 19:52:38,650 [INFO] unfolding 0, single step 78001
2019-04-07 19:52:38,651 [INFO] Sum of grad norms of most recent batch: 0.120354
2019-04-07 19:52:38,651 [INFO] ---------------------------------
2019-04-07 19:52:59,724 [INFO] ---------------------------------
2019-04-07 19:52:59,725 [INFO] Summary:
2019-04-07 19:52:59,726 [INFO] Batch 79000, worst loss 0.058268 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 19:52:59,726 [INFO] Regularization: 1167.914062 * 0.0000000100 = 0.0000116791 loss
2019-04-07 19:52:59,726 [INFO] unfolding 0, single step 79001
2019-04-07 19:52:59,727 [INFO] Sum of grad norms of most recent batch: 0.081727
2019-04-07 19:52:59,727 [INFO] ---------------------------------
2019-04-07 19:53:21,456 [INFO] ---------------------------------
2019-04-07 19:53:21,457 [INFO] Summary:
2019-04-07 19:53:21,458 [INFO] Batch 80000, worst loss 0.028891 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 19:53:21,458 [INFO] Regularization: 1166.059570 * 0.0000000100 = 0.0000116606 loss
2019-04-07 19:53:21,458 [INFO] unfolding 0, single step 80001
2019-04-07 19:53:21,459 [INFO] Sum of grad norms of most recent batch: 0.077924
2019-04-07 19:53:21,460 [INFO] ---------------------------------
2019-04-07 19:53:58,800 [INFO] ---------------------------------
2019-04-07 19:53:58,800 [INFO] Evaluation:
2019-04-07 19:53:58,801 [INFO] Batch 80000, worst loss 0.102527 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:53:58,802 [INFO] ---------------------------------
2019-04-07 19:54:20,124 [INFO] ---------------------------------
2019-04-07 19:54:20,126 [INFO] Summary:
2019-04-07 19:54:20,126 [INFO] Batch 81000, worst loss 0.060172 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 19:54:20,127 [INFO] Regularization: 1164.786133 * 0.0000000100 = 0.0000116479 loss
2019-04-07 19:54:20,127 [INFO] unfolding 0, single step 81001
2019-04-07 19:54:20,128 [INFO] Sum of grad norms of most recent batch: 0.076844
2019-04-07 19:54:20,128 [INFO] ---------------------------------
2019-04-07 19:54:41,658 [INFO] ---------------------------------
2019-04-07 19:54:41,659 [INFO] Summary:
2019-04-07 19:54:41,660 [INFO] Batch 82000, worst loss 0.202189 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 19:54:41,661 [INFO] Regularization: 1163.984253 * 0.0000000100 = 0.0000116398 loss
2019-04-07 19:54:41,662 [INFO] unfolding 0, single step 82001
2019-04-07 19:54:41,663 [INFO] Sum of grad norms of most recent batch: 0.053950
2019-04-07 19:54:41,664 [INFO] ---------------------------------
2019-04-07 19:55:02,858 [INFO] ---------------------------------
2019-04-07 19:55:02,859 [INFO] Summary:
2019-04-07 19:55:02,859 [INFO] Batch 83000, worst loss 0.061828 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 19:55:02,860 [INFO] Regularization: 1163.787598 * 0.0000000100 = 0.0000116379 loss
2019-04-07 19:55:02,860 [INFO] unfolding 0, single step 83001
2019-04-07 19:55:02,861 [INFO] Sum of grad norms of most recent batch: 0.056453
2019-04-07 19:55:02,861 [INFO] ---------------------------------
2019-04-07 19:55:24,427 [INFO] ---------------------------------
2019-04-07 19:55:24,428 [INFO] Summary:
2019-04-07 19:55:24,429 [INFO] Batch 84000, worst loss 0.066164 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 19:55:24,430 [INFO] Regularization: 1163.310669 * 0.0000000100 = 0.0000116331 loss
2019-04-07 19:55:24,431 [INFO] unfolding 0, single step 84001
2019-04-07 19:55:24,432 [INFO] Sum of grad norms of most recent batch: 0.024441
2019-04-07 19:55:24,432 [INFO] ---------------------------------
2019-04-07 19:55:45,725 [INFO] ---------------------------------
2019-04-07 19:55:45,726 [INFO] Summary:
2019-04-07 19:55:45,726 [INFO] Batch 85000, worst loss 0.085685 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 19:55:45,727 [INFO] Regularization: 1162.587280 * 0.0000000100 = 0.0000116259 loss
2019-04-07 19:55:45,727 [INFO] unfolding 0, single step 85001
2019-04-07 19:55:45,728 [INFO] Sum of grad norms of most recent batch: 0.064198
2019-04-07 19:55:45,728 [INFO] ---------------------------------
2019-04-07 19:56:06,749 [INFO] ---------------------------------
2019-04-07 19:56:06,750 [INFO] Summary:
2019-04-07 19:56:06,750 [INFO] Batch 86000, worst loss 0.039446 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 19:56:06,751 [INFO] Regularization: 1162.342773 * 0.0000000100 = 0.0000116234 loss
2019-04-07 19:56:06,751 [INFO] unfolding 0, single step 86001
2019-04-07 19:56:06,752 [INFO] Sum of grad norms of most recent batch: 0.159273
2019-04-07 19:56:06,752 [INFO] ---------------------------------
2019-04-07 19:56:27,680 [INFO] ---------------------------------
2019-04-07 19:56:27,681 [INFO] Summary:
2019-04-07 19:56:27,682 [INFO] Batch 87000, worst loss 0.027172 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 19:56:27,682 [INFO] Regularization: 1162.015381 * 0.0000000100 = 0.0000116202 loss
2019-04-07 19:56:27,683 [INFO] unfolding 0, single step 87001
2019-04-07 19:56:27,684 [INFO] Sum of grad norms of most recent batch: 0.127334
2019-04-07 19:56:27,684 [INFO] ---------------------------------
2019-04-07 19:56:48,288 [INFO] ---------------------------------
2019-04-07 19:56:48,289 [INFO] Summary:
2019-04-07 19:56:48,290 [INFO] Batch 88000, worst loss 0.041227 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 19:56:48,291 [INFO] Regularization: 1161.141602 * 0.0000000100 = 0.0000116114 loss
2019-04-07 19:56:48,291 [INFO] unfolding 0, single step 88001
2019-04-07 19:56:48,292 [INFO] Sum of grad norms of most recent batch: 0.036688
2019-04-07 19:56:48,292 [INFO] ---------------------------------
2019-04-07 19:57:09,543 [INFO] ---------------------------------
2019-04-07 19:57:09,544 [INFO] Summary:
2019-04-07 19:57:09,545 [INFO] Batch 89000, worst loss 0.042406 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 19:57:09,546 [INFO] Regularization: 1160.628418 * 0.0000000100 = 0.0000116063 loss
2019-04-07 19:57:09,546 [INFO] unfolding 0, single step 89001
2019-04-07 19:57:09,547 [INFO] Sum of grad norms of most recent batch: 0.024691
2019-04-07 19:57:09,548 [INFO] ---------------------------------
2019-04-07 19:57:30,212 [INFO] ---------------------------------
2019-04-07 19:57:30,213 [INFO] Summary:
2019-04-07 19:57:30,214 [INFO] Batch 90000, worst loss 0.056991 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 19:57:30,214 [INFO] Regularization: 1159.643555 * 0.0000000100 = 0.0000115964 loss
2019-04-07 19:57:30,215 [INFO] unfolding 0, single step 90001
2019-04-07 19:57:30,215 [INFO] Sum of grad norms of most recent batch: 0.101127
2019-04-07 19:57:30,216 [INFO] ---------------------------------
2019-04-07 19:58:07,580 [INFO] ---------------------------------
2019-04-07 19:58:07,581 [INFO] Evaluation:
2019-04-07 19:58:07,581 [INFO] Batch 90000, worst loss 0.085211 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 19:58:07,582 [INFO] ---------------------------------
2019-04-07 19:58:29,228 [INFO] ---------------------------------
2019-04-07 19:58:29,229 [INFO] Summary:
2019-04-07 19:58:29,230 [INFO] Batch 91000, worst loss 0.058316 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 19:58:29,230 [INFO] Regularization: 1158.798218 * 0.0000000100 = 0.0000115880 loss
2019-04-07 19:58:29,231 [INFO] unfolding 0, single step 91001
2019-04-07 19:58:29,232 [INFO] Sum of grad norms of most recent batch: 0.019568
2019-04-07 19:58:29,232 [INFO] ---------------------------------
2019-04-07 19:58:50,129 [INFO] ---------------------------------
2019-04-07 19:58:50,130 [INFO] Summary:
2019-04-07 19:58:50,131 [INFO] Batch 92000, worst loss 0.032128 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 19:58:50,131 [INFO] Regularization: 1158.783936 * 0.0000000100 = 0.0000115878 loss
2019-04-07 19:58:50,132 [INFO] unfolding 0, single step 92001
2019-04-07 19:58:50,132 [INFO] Sum of grad norms of most recent batch: 0.045716
2019-04-07 19:58:50,133 [INFO] ---------------------------------
2019-04-07 19:59:11,318 [INFO] ---------------------------------
2019-04-07 19:59:11,319 [INFO] Summary:
2019-04-07 19:59:11,319 [INFO] Batch 93000, worst loss 0.087248 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 19:59:11,320 [INFO] Regularization: 1158.477173 * 0.0000000100 = 0.0000115848 loss
2019-04-07 19:59:11,320 [INFO] unfolding 0, single step 93001
2019-04-07 19:59:11,321 [INFO] Sum of grad norms of most recent batch: 0.040535
2019-04-07 19:59:11,322 [INFO] ---------------------------------
2019-04-07 19:59:32,704 [INFO] ---------------------------------
2019-04-07 19:59:32,705 [INFO] Summary:
2019-04-07 19:59:32,705 [INFO] Batch 94000, worst loss 0.009650 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 19:59:32,706 [INFO] Regularization: 1157.914551 * 0.0000000100 = 0.0000115791 loss
2019-04-07 19:59:32,706 [INFO] unfolding 0, single step 94001
2019-04-07 19:59:32,706 [INFO] Sum of grad norms of most recent batch: 0.140666
2019-04-07 19:59:32,707 [INFO] ---------------------------------
2019-04-07 19:59:53,972 [INFO] ---------------------------------
2019-04-07 19:59:53,973 [INFO] Summary:
2019-04-07 19:59:53,974 [INFO] Batch 95000, worst loss 0.046339 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 19:59:53,974 [INFO] Regularization: 1157.708252 * 0.0000000100 = 0.0000115771 loss
2019-04-07 19:59:53,975 [INFO] unfolding 0, single step 95001
2019-04-07 19:59:53,975 [INFO] Sum of grad norms of most recent batch: 0.091142
2019-04-07 19:59:53,976 [INFO] ---------------------------------
2019-04-07 20:00:15,531 [INFO] ---------------------------------
2019-04-07 20:00:15,532 [INFO] Summary:
2019-04-07 20:00:15,533 [INFO] Batch 96000, worst loss 0.036911 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 20:00:15,533 [INFO] Regularization: 1157.364502 * 0.0000000100 = 0.0000115736 loss
2019-04-07 20:00:15,534 [INFO] unfolding 0, single step 96001
2019-04-07 20:00:15,534 [INFO] Sum of grad norms of most recent batch: 0.052182
2019-04-07 20:00:15,535 [INFO] ---------------------------------
2019-04-07 20:00:37,480 [INFO] ---------------------------------
2019-04-07 20:00:37,481 [INFO] Summary:
2019-04-07 20:00:37,482 [INFO] Batch 97000, worst loss 0.072058 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 20:00:37,483 [INFO] Regularization: 1157.259399 * 0.0000000100 = 0.0000115726 loss
2019-04-07 20:00:37,483 [INFO] unfolding 0, single step 97001
2019-04-07 20:00:37,484 [INFO] Sum of grad norms of most recent batch: 0.032124
2019-04-07 20:00:37,485 [INFO] ---------------------------------
2019-04-07 20:00:59,175 [INFO] ---------------------------------
2019-04-07 20:00:59,176 [INFO] Summary:
2019-04-07 20:00:59,177 [INFO] Batch 98000, worst loss 0.033298 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 20:00:59,177 [INFO] Regularization: 1156.691162 * 0.0000000100 = 0.0000115669 loss
2019-04-07 20:00:59,178 [INFO] unfolding 0, single step 98001
2019-04-07 20:00:59,178 [INFO] Sum of grad norms of most recent batch: 4.328405
2019-04-07 20:00:59,179 [INFO] ---------------------------------
2019-04-07 20:01:19,887 [INFO] ---------------------------------
2019-04-07 20:01:19,888 [INFO] Summary:
2019-04-07 20:01:19,889 [INFO] Batch 99000, worst loss 0.036018 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 20:01:19,889 [INFO] Regularization: 1156.468750 * 0.0000000100 = 0.0000115647 loss
2019-04-07 20:01:19,890 [INFO] unfolding 0, single step 99001
2019-04-07 20:01:19,890 [INFO] Sum of grad norms of most recent batch: 1.668988
2019-04-07 20:01:19,891 [INFO] ---------------------------------
2019-04-07 20:01:41,086 [INFO] ---------------------------------
2019-04-07 20:01:41,087 [INFO] Summary:
2019-04-07 20:01:41,088 [INFO] Batch 100000, worst loss 0.038104 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 20:01:41,089 [INFO] Regularization: 1155.979248 * 0.0000000100 = 0.0000115598 loss
2019-04-07 20:01:41,089 [INFO] unfolding 0, single step 100001
2019-04-07 20:01:41,090 [INFO] Sum of grad norms of most recent batch: 0.043121
2019-04-07 20:01:41,090 [INFO] ---------------------------------
2019-04-07 20:02:18,410 [INFO] ---------------------------------
2019-04-07 20:02:18,411 [INFO] Evaluation:
2019-04-07 20:02:18,411 [INFO] Batch 100000, worst loss 0.068692 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:02:18,412 [INFO] ---------------------------------
2019-04-07 20:02:39,878 [INFO] ---------------------------------
2019-04-07 20:02:39,879 [INFO] Summary:
2019-04-07 20:02:39,879 [INFO] Batch 101000, worst loss 0.034360 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 20:02:39,880 [INFO] Regularization: 1155.797485 * 0.0000000100 = 0.0000115580 loss
2019-04-07 20:02:39,880 [INFO] unfolding 0, single step 101001
2019-04-07 20:02:39,881 [INFO] Sum of grad norms of most recent batch: 0.033034
2019-04-07 20:02:39,881 [INFO] ---------------------------------
2019-04-07 20:03:01,209 [INFO] ---------------------------------
2019-04-07 20:03:01,211 [INFO] Summary:
2019-04-07 20:03:01,211 [INFO] Batch 102000, worst loss 0.055697 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 20:03:01,212 [INFO] Regularization: 1155.716797 * 0.0000000100 = 0.0000115572 loss
2019-04-07 20:03:01,212 [INFO] unfolding 0, single step 102001
2019-04-07 20:03:01,213 [INFO] Sum of grad norms of most recent batch: 0.059064
2019-04-07 20:03:01,214 [INFO] ---------------------------------
2019-04-07 20:03:22,314 [INFO] ---------------------------------
2019-04-07 20:03:22,315 [INFO] Summary:
2019-04-07 20:03:22,316 [INFO] Batch 103000, worst loss 0.015183 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 20:03:22,316 [INFO] Regularization: 1155.646118 * 0.0000000100 = 0.0000115565 loss
2019-04-07 20:03:22,317 [INFO] unfolding 0, single step 103001
2019-04-07 20:03:22,318 [INFO] Sum of grad norms of most recent batch: 0.077456
2019-04-07 20:03:22,318 [INFO] ---------------------------------
2019-04-07 20:03:43,560 [INFO] ---------------------------------
2019-04-07 20:03:43,561 [INFO] Summary:
2019-04-07 20:03:43,561 [INFO] Batch 104000, worst loss 0.038910 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 20:03:43,562 [INFO] Regularization: 1155.330688 * 0.0000000100 = 0.0000115533 loss
2019-04-07 20:03:43,563 [INFO] unfolding 0, single step 104001
2019-04-07 20:03:43,564 [INFO] Sum of grad norms of most recent batch: 3.124925
2019-04-07 20:03:43,564 [INFO] ---------------------------------
2019-04-07 20:04:04,955 [INFO] ---------------------------------
2019-04-07 20:04:04,957 [INFO] Summary:
2019-04-07 20:04:04,957 [INFO] Batch 105000, worst loss 0.038901 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 20:04:04,958 [INFO] Regularization: 1155.098267 * 0.0000000100 = 0.0000115510 loss
2019-04-07 20:04:04,958 [INFO] unfolding 0, single step 105001
2019-04-07 20:04:04,959 [INFO] Sum of grad norms of most recent batch: 0.062936
2019-04-07 20:04:04,959 [INFO] ---------------------------------
2019-04-07 20:04:26,232 [INFO] ---------------------------------
2019-04-07 20:04:26,233 [INFO] Summary:
2019-04-07 20:04:26,233 [INFO] Batch 106000, worst loss 0.030124 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 20:04:26,234 [INFO] Regularization: 1155.017822 * 0.0000000100 = 0.0000115502 loss
2019-04-07 20:04:26,234 [INFO] unfolding 0, single step 106001
2019-04-07 20:04:26,235 [INFO] Sum of grad norms of most recent batch: 0.024040
2019-04-07 20:04:26,235 [INFO] ---------------------------------
2019-04-07 20:04:46,917 [INFO] ---------------------------------
2019-04-07 20:04:46,918 [INFO] Summary:
2019-04-07 20:04:46,919 [INFO] Batch 107000, worst loss 0.045362 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 20:04:46,919 [INFO] Regularization: 1154.768555 * 0.0000000100 = 0.0000115477 loss
2019-04-07 20:04:46,920 [INFO] unfolding 0, single step 107001
2019-04-07 20:04:46,920 [INFO] Sum of grad norms of most recent batch: 1.202379
2019-04-07 20:04:46,921 [INFO] ---------------------------------
2019-04-07 20:05:08,235 [INFO] ---------------------------------
2019-04-07 20:05:08,235 [INFO] Summary:
2019-04-07 20:05:08,236 [INFO] Batch 108000, worst loss 0.041492 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 20:05:08,237 [INFO] Regularization: 1154.788940 * 0.0000000100 = 0.0000115479 loss
2019-04-07 20:05:08,237 [INFO] unfolding 0, single step 108001
2019-04-07 20:05:08,237 [INFO] Sum of grad norms of most recent batch: 0.025471
2019-04-07 20:05:08,238 [INFO] ---------------------------------
2019-04-07 20:05:29,404 [INFO] ---------------------------------
2019-04-07 20:05:29,405 [INFO] Summary:
2019-04-07 20:05:29,405 [INFO] Batch 109000, worst loss 0.030735 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 20:05:29,406 [INFO] Regularization: 1154.437378 * 0.0000000100 = 0.0000115444 loss
2019-04-07 20:05:29,406 [INFO] unfolding 0, single step 109001
2019-04-07 20:05:29,407 [INFO] Sum of grad norms of most recent batch: 0.035788
2019-04-07 20:05:29,407 [INFO] ---------------------------------
2019-04-07 20:05:50,407 [INFO] ---------------------------------
2019-04-07 20:05:50,408 [INFO] Summary:
2019-04-07 20:05:50,409 [INFO] Batch 110000, worst loss 0.030659 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 20:05:50,409 [INFO] Regularization: 1154.288818 * 0.0000000100 = 0.0000115429 loss
2019-04-07 20:05:50,409 [INFO] unfolding 0, single step 110001
2019-04-07 20:05:50,410 [INFO] Sum of grad norms of most recent batch: 0.034104
2019-04-07 20:05:50,410 [INFO] ---------------------------------
2019-04-07 20:06:27,785 [INFO] ---------------------------------
2019-04-07 20:06:27,786 [INFO] Evaluation:
2019-04-07 20:06:27,786 [INFO] Batch 110000, worst loss 0.113520 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:06:27,787 [INFO] ---------------------------------
2019-04-07 20:06:49,022 [INFO] ---------------------------------
2019-04-07 20:06:49,023 [INFO] Summary:
2019-04-07 20:06:49,024 [INFO] Batch 111000, worst loss 0.023882 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 20:06:49,024 [INFO] Regularization: 1154.164551 * 0.0000000100 = 0.0000115416 loss
2019-04-07 20:06:49,025 [INFO] unfolding 0, single step 111001
2019-04-07 20:06:49,025 [INFO] Sum of grad norms of most recent batch: 0.045548
2019-04-07 20:06:49,026 [INFO] ---------------------------------
2019-04-07 20:07:10,329 [INFO] ---------------------------------
2019-04-07 20:07:10,330 [INFO] Summary:
2019-04-07 20:07:10,330 [INFO] Batch 112000, worst loss 0.049178 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 20:07:10,331 [INFO] Regularization: 1153.979614 * 0.0000000100 = 0.0000115398 loss
2019-04-07 20:07:10,331 [INFO] unfolding 0, single step 112001
2019-04-07 20:07:10,332 [INFO] Sum of grad norms of most recent batch: 0.027719
2019-04-07 20:07:10,332 [INFO] ---------------------------------
2019-04-07 20:07:31,080 [INFO] ---------------------------------
2019-04-07 20:07:31,081 [INFO] Summary:
2019-04-07 20:07:31,081 [INFO] Batch 113000, worst loss 0.009766 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 20:07:31,082 [INFO] Regularization: 1153.771729 * 0.0000000100 = 0.0000115377 loss
2019-04-07 20:07:31,083 [INFO] unfolding 0, single step 113001
2019-04-07 20:07:31,083 [INFO] Sum of grad norms of most recent batch: 0.037675
2019-04-07 20:07:31,084 [INFO] ---------------------------------
2019-04-07 20:07:52,593 [INFO] ---------------------------------
2019-04-07 20:07:52,595 [INFO] Summary:
2019-04-07 20:07:52,595 [INFO] Batch 114000, worst loss 0.033258 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 20:07:52,596 [INFO] Regularization: 1153.755127 * 0.0000000100 = 0.0000115376 loss
2019-04-07 20:07:52,596 [INFO] unfolding 0, single step 114001
2019-04-07 20:07:52,597 [INFO] Sum of grad norms of most recent batch: 0.015961
2019-04-07 20:07:52,598 [INFO] ---------------------------------
2019-04-07 20:08:13,820 [INFO] ---------------------------------
2019-04-07 20:08:13,821 [INFO] Summary:
2019-04-07 20:08:13,821 [INFO] Batch 115000, worst loss 0.029366 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 20:08:13,822 [INFO] Regularization: 1153.708252 * 0.0000000100 = 0.0000115371 loss
2019-04-07 20:08:13,822 [INFO] unfolding 0, single step 115001
2019-04-07 20:08:13,823 [INFO] Sum of grad norms of most recent batch: 0.030792
2019-04-07 20:08:13,823 [INFO] ---------------------------------
2019-04-07 20:08:35,053 [INFO] ---------------------------------
2019-04-07 20:08:35,054 [INFO] Summary:
2019-04-07 20:08:35,054 [INFO] Batch 116000, worst loss 0.004383 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 20:08:35,055 [INFO] Regularization: 1153.638916 * 0.0000000100 = 0.0000115364 loss
2019-04-07 20:08:35,055 [INFO] unfolding 0, single step 116001
2019-04-07 20:08:35,056 [INFO] Sum of grad norms of most recent batch: 0.041866
2019-04-07 20:08:35,056 [INFO] ---------------------------------
2019-04-07 20:09:12,483 [INFO] ---------------------------------
2019-04-07 20:09:12,484 [INFO] Evaluation:
2019-04-07 20:09:12,485 [INFO] Batch 116000, worst loss 0.074080 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:09:12,485 [INFO] ---------------------------------
2019-04-07 20:09:33,672 [INFO] ---------------------------------
2019-04-07 20:09:33,673 [INFO] Summary:
2019-04-07 20:09:33,673 [INFO] Batch 117000, worst loss 0.075910 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 20:09:33,674 [INFO] Regularization: 1153.541626 * 0.0000000100 = 0.0000115354 loss
2019-04-07 20:09:33,674 [INFO] unfolding 0, single step 117001
2019-04-07 20:09:33,675 [INFO] Sum of grad norms of most recent batch: 0.038418
2019-04-07 20:09:33,676 [INFO] ---------------------------------
2019-04-07 20:09:55,436 [INFO] ---------------------------------
2019-04-07 20:09:55,437 [INFO] Summary:
2019-04-07 20:09:55,438 [INFO] Batch 118000, worst loss 0.031801 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 20:09:55,438 [INFO] Regularization: 1153.403198 * 0.0000000100 = 0.0000115340 loss
2019-04-07 20:09:55,439 [INFO] unfolding 0, single step 118001
2019-04-07 20:09:55,439 [INFO] Sum of grad norms of most recent batch: 0.047768
2019-04-07 20:09:55,440 [INFO] ---------------------------------
2019-04-07 20:10:16,541 [INFO] ---------------------------------
2019-04-07 20:10:16,543 [INFO] Summary:
2019-04-07 20:10:16,544 [INFO] Batch 119000, worst loss 0.068522 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 20:10:16,545 [INFO] Regularization: 1153.457031 * 0.0000000100 = 0.0000115346 loss
2019-04-07 20:10:16,545 [INFO] unfolding 0, single step 119001
2019-04-07 20:10:16,546 [INFO] Sum of grad norms of most recent batch: 0.033451
2019-04-07 20:10:16,547 [INFO] ---------------------------------
2019-04-07 20:10:37,767 [INFO] ---------------------------------
2019-04-07 20:10:37,769 [INFO] Summary:
2019-04-07 20:10:37,770 [INFO] Batch 120000, worst loss 0.043942 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 20:10:37,771 [INFO] Regularization: 1153.481689 * 0.0000000100 = 0.0000115348 loss
2019-04-07 20:10:37,771 [INFO] unfolding 0, single step 120001
2019-04-07 20:10:37,772 [INFO] Sum of grad norms of most recent batch: 0.102875
2019-04-07 20:10:37,773 [INFO] ---------------------------------
2019-04-07 20:11:15,234 [INFO] ---------------------------------
2019-04-07 20:11:15,235 [INFO] Evaluation:
2019-04-07 20:11:15,236 [INFO] Batch 120000, worst loss 0.110516 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:11:15,236 [INFO] ---------------------------------
2019-04-07 20:11:36,667 [INFO] ---------------------------------
2019-04-07 20:11:36,668 [INFO] Summary:
2019-04-07 20:11:36,669 [INFO] Batch 121000, worst loss 0.031112 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 20:11:36,669 [INFO] Regularization: 1153.519775 * 0.0000000100 = 0.0000115352 loss
2019-04-07 20:11:36,669 [INFO] unfolding 0, single step 121001
2019-04-07 20:11:36,670 [INFO] Sum of grad norms of most recent batch: 0.083841
2019-04-07 20:11:36,670 [INFO] ---------------------------------
2019-04-07 20:11:57,496 [INFO] ---------------------------------
2019-04-07 20:11:57,497 [INFO] Summary:
2019-04-07 20:11:57,498 [INFO] Batch 122000, worst loss 0.012244 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 20:11:57,499 [INFO] Regularization: 1153.462158 * 0.0000000100 = 0.0000115346 loss
2019-04-07 20:11:57,499 [INFO] unfolding 0, single step 122001
2019-04-07 20:11:57,500 [INFO] Sum of grad norms of most recent batch: 0.029867
2019-04-07 20:11:57,501 [INFO] ---------------------------------
2019-04-07 20:12:18,709 [INFO] ---------------------------------
2019-04-07 20:12:18,710 [INFO] Summary:
2019-04-07 20:12:18,711 [INFO] Batch 123000, worst loss 0.042037 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 20:12:18,712 [INFO] Regularization: 1153.342529 * 0.0000000100 = 0.0000115334 loss
2019-04-07 20:12:18,712 [INFO] unfolding 0, single step 123001
2019-04-07 20:12:18,713 [INFO] Sum of grad norms of most recent batch: 0.041734
2019-04-07 20:12:18,713 [INFO] ---------------------------------
2019-04-07 20:12:39,424 [INFO] ---------------------------------
2019-04-07 20:12:39,425 [INFO] Summary:
2019-04-07 20:12:39,426 [INFO] Batch 124000, worst loss 0.007800 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 20:12:39,426 [INFO] Regularization: 1153.308472 * 0.0000000100 = 0.0000115331 loss
2019-04-07 20:12:39,427 [INFO] unfolding 0, single step 124001
2019-04-07 20:12:39,427 [INFO] Sum of grad norms of most recent batch: 0.037506
2019-04-07 20:12:39,428 [INFO] ---------------------------------
2019-04-07 20:13:00,283 [INFO] ---------------------------------
2019-04-07 20:13:00,284 [INFO] Summary:
2019-04-07 20:13:00,284 [INFO] Batch 125000, worst loss 0.031483 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 20:13:00,285 [INFO] Regularization: 1153.307129 * 0.0000000100 = 0.0000115331 loss
2019-04-07 20:13:00,285 [INFO] unfolding 0, single step 125001
2019-04-07 20:13:00,286 [INFO] Sum of grad norms of most recent batch: 0.032254
2019-04-07 20:13:00,286 [INFO] ---------------------------------
2019-04-07 20:13:21,847 [INFO] ---------------------------------
2019-04-07 20:13:21,848 [INFO] Summary:
2019-04-07 20:13:21,849 [INFO] Batch 126000, worst loss 0.024879 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 20:13:21,849 [INFO] Regularization: 1153.248535 * 0.0000000100 = 0.0000115325 loss
2019-04-07 20:13:21,849 [INFO] unfolding 0, single step 126001
2019-04-07 20:13:21,850 [INFO] Sum of grad norms of most recent batch: 0.012544
2019-04-07 20:13:21,850 [INFO] ---------------------------------
2019-04-07 20:13:42,942 [INFO] ---------------------------------
2019-04-07 20:13:42,943 [INFO] Summary:
2019-04-07 20:13:42,944 [INFO] Batch 127000, worst loss 0.024803 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 20:13:42,945 [INFO] Regularization: 1153.224731 * 0.0000000100 = 0.0000115322 loss
2019-04-07 20:13:42,946 [INFO] unfolding 0, single step 127001
2019-04-07 20:13:42,947 [INFO] Sum of grad norms of most recent batch: 0.107252
2019-04-07 20:13:42,948 [INFO] ---------------------------------
2019-04-07 20:14:04,529 [INFO] ---------------------------------
2019-04-07 20:14:04,530 [INFO] Summary:
2019-04-07 20:14:04,531 [INFO] Batch 128000, worst loss 0.014200 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 20:14:04,531 [INFO] Regularization: 1153.157349 * 0.0000000100 = 0.0000115316 loss
2019-04-07 20:14:04,531 [INFO] unfolding 0, single step 128001
2019-04-07 20:14:04,532 [INFO] Sum of grad norms of most recent batch: 0.037751
2019-04-07 20:14:04,532 [INFO] ---------------------------------
2019-04-07 20:14:26,059 [INFO] ---------------------------------
2019-04-07 20:14:26,060 [INFO] Summary:
2019-04-07 20:14:26,061 [INFO] Batch 129000, worst loss 0.042840 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 20:14:26,062 [INFO] Regularization: 1153.173706 * 0.0000000100 = 0.0000115317 loss
2019-04-07 20:14:26,062 [INFO] unfolding 0, single step 129001
2019-04-07 20:14:26,063 [INFO] Sum of grad norms of most recent batch: 0.025661
2019-04-07 20:14:26,064 [INFO] ---------------------------------
2019-04-07 20:14:47,168 [INFO] ---------------------------------
2019-04-07 20:14:47,169 [INFO] Summary:
2019-04-07 20:14:47,169 [INFO] Batch 130000, worst loss 0.087284 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 20:14:47,170 [INFO] Regularization: 1153.145020 * 0.0000000100 = 0.0000115314 loss
2019-04-07 20:14:47,170 [INFO] unfolding 0, single step 130001
2019-04-07 20:14:47,171 [INFO] Sum of grad norms of most recent batch: 0.033061
2019-04-07 20:14:47,171 [INFO] ---------------------------------
2019-04-07 20:15:24,553 [INFO] ---------------------------------
2019-04-07 20:15:24,554 [INFO] Evaluation:
2019-04-07 20:15:24,555 [INFO] Batch 130000, worst loss 0.116858 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:15:24,555 [INFO] ---------------------------------
2019-04-07 20:15:45,873 [INFO] ---------------------------------
2019-04-07 20:15:45,874 [INFO] Summary:
2019-04-07 20:15:45,874 [INFO] Batch 131000, worst loss 0.063152 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:15:45,875 [INFO] Regularization: 1153.118164 * 0.0000000100 = 0.0000115312 loss
2019-04-07 20:15:45,875 [INFO] unfolding 0, single step 131001
2019-04-07 20:15:45,876 [INFO] Sum of grad norms of most recent batch: 0.054418
2019-04-07 20:15:45,876 [INFO] ---------------------------------
2019-04-07 20:16:06,921 [INFO] ---------------------------------
2019-04-07 20:16:06,922 [INFO] Summary:
2019-04-07 20:16:06,922 [INFO] Batch 132000, worst loss 0.159801 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:16:06,923 [INFO] Regularization: 1153.060913 * 0.0000000100 = 0.0000115306 loss
2019-04-07 20:16:06,923 [INFO] unfolding 0, single step 132001
2019-04-07 20:16:06,924 [INFO] Sum of grad norms of most recent batch: 0.013650
2019-04-07 20:16:06,924 [INFO] ---------------------------------
2019-04-07 20:16:28,104 [INFO] ---------------------------------
2019-04-07 20:16:28,105 [INFO] Summary:
2019-04-07 20:16:28,105 [INFO] Batch 133000, worst loss 0.036918 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:16:28,106 [INFO] Regularization: 1153.067017 * 0.0000000100 = 0.0000115307 loss
2019-04-07 20:16:28,106 [INFO] unfolding 0, single step 133001
2019-04-07 20:16:28,107 [INFO] Sum of grad norms of most recent batch: 0.040673
2019-04-07 20:16:28,107 [INFO] ---------------------------------
2019-04-07 20:16:48,922 [INFO] ---------------------------------
2019-04-07 20:16:48,923 [INFO] Summary:
2019-04-07 20:16:48,924 [INFO] Batch 134000, worst loss 0.063039 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:16:48,925 [INFO] Regularization: 1153.069580 * 0.0000000100 = 0.0000115307 loss
2019-04-07 20:16:48,926 [INFO] unfolding 0, single step 134001
2019-04-07 20:16:48,927 [INFO] Sum of grad norms of most recent batch: 0.031596
2019-04-07 20:16:48,928 [INFO] ---------------------------------
2019-04-07 20:17:10,072 [INFO] ---------------------------------
2019-04-07 20:17:10,073 [INFO] Summary:
2019-04-07 20:17:10,074 [INFO] Batch 135000, worst loss 0.061234 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:17:10,074 [INFO] Regularization: 1153.026733 * 0.0000000100 = 0.0000115303 loss
2019-04-07 20:17:10,075 [INFO] unfolding 0, single step 135001
2019-04-07 20:17:10,076 [INFO] Sum of grad norms of most recent batch: 0.019516
2019-04-07 20:17:10,076 [INFO] ---------------------------------
2019-04-07 20:17:30,978 [INFO] ---------------------------------
2019-04-07 20:17:30,979 [INFO] Summary:
2019-04-07 20:17:30,979 [INFO] Batch 136000, worst loss 0.061197 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:17:30,980 [INFO] Regularization: 1153.015381 * 0.0000000100 = 0.0000115302 loss
2019-04-07 20:17:30,980 [INFO] unfolding 0, single step 136001
2019-04-07 20:17:30,981 [INFO] Sum of grad norms of most recent batch: 0.054905
2019-04-07 20:17:30,981 [INFO] ---------------------------------
2019-04-07 20:17:52,042 [INFO] ---------------------------------
2019-04-07 20:17:52,043 [INFO] Summary:
2019-04-07 20:17:52,043 [INFO] Batch 137000, worst loss 0.021421 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:17:52,044 [INFO] Regularization: 1153.011108 * 0.0000000100 = 0.0000115301 loss
2019-04-07 20:17:52,044 [INFO] unfolding 0, single step 137001
2019-04-07 20:17:52,045 [INFO] Sum of grad norms of most recent batch: 0.010126
2019-04-07 20:17:52,045 [INFO] ---------------------------------
2019-04-07 20:18:13,372 [INFO] ---------------------------------
2019-04-07 20:18:13,373 [INFO] Summary:
2019-04-07 20:18:13,374 [INFO] Batch 138000, worst loss 0.054013 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:18:13,374 [INFO] Regularization: 1152.989136 * 0.0000000100 = 0.0000115299 loss
2019-04-07 20:18:13,375 [INFO] unfolding 0, single step 138001
2019-04-07 20:18:13,376 [INFO] Sum of grad norms of most recent batch: 0.014392
2019-04-07 20:18:13,376 [INFO] ---------------------------------
2019-04-07 20:18:34,107 [INFO] ---------------------------------
2019-04-07 20:18:34,108 [INFO] Summary:
2019-04-07 20:18:34,109 [INFO] Batch 139000, worst loss 0.004244 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:18:34,109 [INFO] Regularization: 1152.966675 * 0.0000000100 = 0.0000115297 loss
2019-04-07 20:18:34,110 [INFO] unfolding 0, single step 139001
2019-04-07 20:18:34,110 [INFO] Sum of grad norms of most recent batch: 0.019368
2019-04-07 20:18:34,111 [INFO] ---------------------------------
2019-04-07 20:19:11,530 [INFO] ---------------------------------
2019-04-07 20:19:11,531 [INFO] Evaluation:
2019-04-07 20:19:11,532 [INFO] Batch 139000, worst loss 0.056479 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:19:11,532 [INFO] ---------------------------------
2019-04-07 20:19:33,003 [INFO] ---------------------------------
2019-04-07 20:19:33,004 [INFO] Summary:
2019-04-07 20:19:33,004 [INFO] Batch 140000, worst loss 0.064126 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:19:33,005 [INFO] Regularization: 1152.969727 * 0.0000000100 = 0.0000115297 loss
2019-04-07 20:19:33,005 [INFO] unfolding 0, single step 140001
2019-04-07 20:19:33,006 [INFO] Sum of grad norms of most recent batch: 0.025853
2019-04-07 20:19:33,006 [INFO] ---------------------------------
2019-04-07 20:20:10,593 [INFO] ---------------------------------
2019-04-07 20:20:10,594 [INFO] Evaluation:
2019-04-07 20:20:10,595 [INFO] Batch 140000, worst loss 0.115660 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:20:10,595 [INFO] ---------------------------------
2019-04-07 20:20:31,802 [INFO] ---------------------------------
2019-04-07 20:20:31,803 [INFO] Summary:
2019-04-07 20:20:31,804 [INFO] Batch 141000, worst loss 0.030183 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:20:31,805 [INFO] Regularization: 1152.905640 * 0.0000000100 = 0.0000115291 loss
2019-04-07 20:20:31,805 [INFO] unfolding 0, single step 141001
2019-04-07 20:20:31,806 [INFO] Sum of grad norms of most recent batch: 0.557076
2019-04-07 20:20:31,806 [INFO] ---------------------------------
2019-04-07 20:20:53,952 [INFO] ---------------------------------
2019-04-07 20:20:53,953 [INFO] Summary:
2019-04-07 20:20:53,954 [INFO] Batch 142000, worst loss 0.011775 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:20:53,955 [INFO] Regularization: 1152.890137 * 0.0000000100 = 0.0000115289 loss
2019-04-07 20:20:53,955 [INFO] unfolding 0, single step 142001
2019-04-07 20:20:53,956 [INFO] Sum of grad norms of most recent batch: 0.033203
2019-04-07 20:20:53,957 [INFO] ---------------------------------
2019-04-07 20:21:15,004 [INFO] ---------------------------------
2019-04-07 20:21:15,005 [INFO] Summary:
2019-04-07 20:21:15,005 [INFO] Batch 143000, worst loss 0.008837 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:21:15,006 [INFO] Regularization: 1152.868896 * 0.0000000100 = 0.0000115287 loss
2019-04-07 20:21:15,006 [INFO] unfolding 0, single step 143001
2019-04-07 20:21:15,007 [INFO] Sum of grad norms of most recent batch: 0.041266
2019-04-07 20:21:15,007 [INFO] ---------------------------------
2019-04-07 20:21:36,398 [INFO] ---------------------------------
2019-04-07 20:21:36,400 [INFO] Summary:
2019-04-07 20:21:36,401 [INFO] Batch 144000, worst loss 0.025376 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:21:36,401 [INFO] Regularization: 1152.905640 * 0.0000000100 = 0.0000115291 loss
2019-04-07 20:21:36,402 [INFO] unfolding 0, single step 144001
2019-04-07 20:21:36,403 [INFO] Sum of grad norms of most recent batch: 0.022337
2019-04-07 20:21:36,404 [INFO] ---------------------------------
2019-04-07 20:21:57,526 [INFO] ---------------------------------
2019-04-07 20:21:57,527 [INFO] Summary:
2019-04-07 20:21:57,528 [INFO] Batch 145000, worst loss 0.022579 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:21:57,528 [INFO] Regularization: 1152.881348 * 0.0000000100 = 0.0000115288 loss
2019-04-07 20:21:57,529 [INFO] unfolding 0, single step 145001
2019-04-07 20:21:57,529 [INFO] Sum of grad norms of most recent batch: 0.035122
2019-04-07 20:21:57,530 [INFO] ---------------------------------
2019-04-07 20:22:18,898 [INFO] ---------------------------------
2019-04-07 20:22:18,899 [INFO] Summary:
2019-04-07 20:22:18,900 [INFO] Batch 146000, worst loss 0.022538 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:22:18,901 [INFO] Regularization: 1152.885498 * 0.0000000100 = 0.0000115289 loss
2019-04-07 20:22:18,901 [INFO] unfolding 0, single step 146001
2019-04-07 20:22:18,902 [INFO] Sum of grad norms of most recent batch: 0.025506
2019-04-07 20:22:18,903 [INFO] ---------------------------------
2019-04-07 20:22:40,027 [INFO] ---------------------------------
2019-04-07 20:22:40,028 [INFO] Summary:
2019-04-07 20:22:40,028 [INFO] Batch 147000, worst loss 0.029399 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:22:40,029 [INFO] Regularization: 1152.879028 * 0.0000000100 = 0.0000115288 loss
2019-04-07 20:22:40,029 [INFO] unfolding 0, single step 147001
2019-04-07 20:22:40,030 [INFO] Sum of grad norms of most recent batch: 0.026528
2019-04-07 20:22:40,030 [INFO] ---------------------------------
2019-04-07 20:23:00,674 [INFO] ---------------------------------
2019-04-07 20:23:00,675 [INFO] Summary:
2019-04-07 20:23:00,676 [INFO] Batch 148000, worst loss 0.032159 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:23:00,677 [INFO] Regularization: 1152.849121 * 0.0000000100 = 0.0000115285 loss
2019-04-07 20:23:00,678 [INFO] unfolding 0, single step 148001
2019-04-07 20:23:00,679 [INFO] Sum of grad norms of most recent batch: 0.027427
2019-04-07 20:23:00,680 [INFO] ---------------------------------
2019-04-07 20:23:22,334 [INFO] ---------------------------------
2019-04-07 20:23:22,335 [INFO] Summary:
2019-04-07 20:23:22,336 [INFO] Batch 149000, worst loss 0.012085 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:23:22,336 [INFO] Regularization: 1152.834106 * 0.0000000100 = 0.0000115283 loss
2019-04-07 20:23:22,337 [INFO] unfolding 0, single step 149001
2019-04-07 20:23:22,337 [INFO] Sum of grad norms of most recent batch: 0.013775
2019-04-07 20:23:22,338 [INFO] ---------------------------------
2019-04-07 20:23:43,937 [INFO] ---------------------------------
2019-04-07 20:23:43,938 [INFO] Summary:
2019-04-07 20:23:43,939 [INFO] Batch 150000, worst loss 0.100135 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 20:23:43,939 [INFO] Regularization: 1152.818359 * 0.0000000100 = 0.0000115282 loss
2019-04-07 20:23:43,939 [INFO] unfolding 0, single step 150001
2019-04-07 20:23:43,940 [INFO] Sum of grad norms of most recent batch: 0.040613
2019-04-07 20:23:43,940 [INFO] ---------------------------------
2019-04-07 20:24:21,254 [INFO] ---------------------------------
2019-04-07 20:24:21,255 [INFO] Evaluation:
2019-04-07 20:24:21,256 [INFO] Batch 150000, worst loss 0.081744 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:24:21,257 [INFO] ---------------------------------
2019-04-07 20:24:21,257 [INFO] Finished training, saved to file transition/1554653688/1554661461_1_transition_final.pth
2019-04-07 20:24:21,453 [INFO] ---------------------------------
2019-04-07 20:24:21,455 [INFO] Training model #2: (11, 64, 8) @ 3
2019-04-07 20:24:43,479 [INFO] ---------------------------------
2019-04-07 20:24:43,481 [INFO] Summary:
2019-04-07 20:24:43,482 [INFO] Batch 1000, worst loss 92.641533 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:24:43,483 [INFO] Regularization: 6105.650879 * 0.0000000100 = 0.0000610565 loss
2019-04-07 20:24:43,484 [INFO] unfolding 0, single step 1001
2019-04-07 20:24:43,485 [INFO] Sum of grad norms of most recent batch: 12.794330
2019-04-07 20:24:43,486 [INFO] ---------------------------------
2019-04-07 20:25:04,535 [INFO] ---------------------------------
2019-04-07 20:25:04,536 [INFO] Summary:
2019-04-07 20:25:04,537 [INFO] Batch 2000, worst loss 0.079383 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:25:04,537 [INFO] Regularization: 3708.889160 * 0.0000000100 = 0.0000370889 loss
2019-04-07 20:25:04,537 [INFO] unfolding 0, single step 2001
2019-04-07 20:25:04,538 [INFO] Sum of grad norms of most recent batch: 1.700271
2019-04-07 20:25:04,539 [INFO] ---------------------------------
2019-04-07 20:25:26,611 [INFO] ---------------------------------
2019-04-07 20:25:26,612 [INFO] Summary:
2019-04-07 20:25:26,613 [INFO] Batch 3000, worst loss 0.121717 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:25:26,613 [INFO] Regularization: 3365.698242 * 0.0000000100 = 0.0000336570 loss
2019-04-07 20:25:26,613 [INFO] unfolding 0, single step 3001
2019-04-07 20:25:26,614 [INFO] Sum of grad norms of most recent batch: 2.243452
2019-04-07 20:25:26,615 [INFO] ---------------------------------
2019-04-07 20:25:48,413 [INFO] ---------------------------------
2019-04-07 20:25:48,414 [INFO] Summary:
2019-04-07 20:25:48,415 [INFO] Batch 4000, worst loss 0.121597 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:25:48,415 [INFO] Regularization: 3072.494873 * 0.0000000100 = 0.0000307250 loss
2019-04-07 20:25:48,416 [INFO] unfolding 0, single step 4001
2019-04-07 20:25:48,416 [INFO] Sum of grad norms of most recent batch: 3.539027
2019-04-07 20:25:48,417 [INFO] ---------------------------------
2019-04-07 20:26:10,046 [INFO] ---------------------------------
2019-04-07 20:26:10,047 [INFO] Summary:
2019-04-07 20:26:10,047 [INFO] Batch 5000, worst loss 0.166968 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:26:10,048 [INFO] Regularization: 2809.106201 * 0.0000000100 = 0.0000280911 loss
2019-04-07 20:26:10,048 [INFO] unfolding 0, single step 5001
2019-04-07 20:26:10,049 [INFO] Sum of grad norms of most recent batch: 4.018388
2019-04-07 20:26:10,049 [INFO] ---------------------------------
2019-04-07 20:26:31,264 [INFO] ---------------------------------
2019-04-07 20:26:31,266 [INFO] Summary:
2019-04-07 20:26:31,266 [INFO] Batch 6000, worst loss 0.083305 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:26:31,267 [INFO] Regularization: 2592.474854 * 0.0000000100 = 0.0000259247 loss
2019-04-07 20:26:31,267 [INFO] unfolding 0, single step 6001
2019-04-07 20:26:31,268 [INFO] Sum of grad norms of most recent batch: 1.545184
2019-04-07 20:26:31,268 [INFO] ---------------------------------
2019-04-07 20:26:53,404 [INFO] ---------------------------------
2019-04-07 20:26:53,405 [INFO] Summary:
2019-04-07 20:26:53,405 [INFO] Batch 7000, worst loss 0.025684 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:26:53,406 [INFO] Regularization: 2428.864990 * 0.0000000100 = 0.0000242886 loss
2019-04-07 20:26:53,406 [INFO] unfolding 0, single step 7001
2019-04-07 20:26:53,407 [INFO] Sum of grad norms of most recent batch: 1.689714
2019-04-07 20:26:53,408 [INFO] ---------------------------------
2019-04-07 20:27:14,371 [INFO] ---------------------------------
2019-04-07 20:27:14,372 [INFO] Summary:
2019-04-07 20:27:14,373 [INFO] Batch 8000, worst loss 0.084936 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:27:14,374 [INFO] Regularization: 2313.318359 * 0.0000000100 = 0.0000231332 loss
2019-04-07 20:27:14,374 [INFO] unfolding 0, single step 8001
2019-04-07 20:27:14,375 [INFO] Sum of grad norms of most recent batch: 0.944550
2019-04-07 20:27:14,375 [INFO] ---------------------------------
2019-04-07 20:27:35,397 [INFO] ---------------------------------
2019-04-07 20:27:35,398 [INFO] Summary:
2019-04-07 20:27:35,399 [INFO] Batch 9000, worst loss 0.022491 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:27:35,399 [INFO] Regularization: 2219.970459 * 0.0000000100 = 0.0000221997 loss
2019-04-07 20:27:35,400 [INFO] unfolding 0, single step 9001
2019-04-07 20:27:35,400 [INFO] Sum of grad norms of most recent batch: 1.084022
2019-04-07 20:27:35,401 [INFO] ---------------------------------
2019-04-07 20:27:57,052 [INFO] ---------------------------------
2019-04-07 20:27:57,053 [INFO] Summary:
2019-04-07 20:27:57,054 [INFO] Batch 10000, worst loss 0.051830 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:27:57,054 [INFO] Regularization: 2142.323730 * 0.0000000100 = 0.0000214232 loss
2019-04-07 20:27:57,054 [INFO] unfolding 0, single step 10001
2019-04-07 20:27:57,055 [INFO] Sum of grad norms of most recent batch: 3.628548
2019-04-07 20:27:57,055 [INFO] ---------------------------------
2019-04-07 20:28:34,596 [INFO] ---------------------------------
2019-04-07 20:28:34,597 [INFO] Evaluation:
2019-04-07 20:28:34,598 [INFO] Batch 10000, worst loss 0.165002 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:28:34,598 [INFO] ---------------------------------
2019-04-07 20:28:56,128 [INFO] ---------------------------------
2019-04-07 20:28:56,129 [INFO] Summary:
2019-04-07 20:28:56,130 [INFO] Batch 11000, worst loss 0.101859 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:28:56,131 [INFO] Regularization: 2062.759277 * 0.0000000100 = 0.0000206276 loss
2019-04-07 20:28:56,131 [INFO] unfolding 0, single step 11001
2019-04-07 20:28:56,132 [INFO] Sum of grad norms of most recent batch: 1.735099
2019-04-07 20:28:56,133 [INFO] ---------------------------------
2019-04-07 20:29:18,816 [INFO] ---------------------------------
2019-04-07 20:29:18,817 [INFO] Summary:
2019-04-07 20:29:18,818 [INFO] Batch 12000, worst loss 0.078003 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:29:18,819 [INFO] Regularization: 2046.662842 * 0.0000000100 = 0.0000204666 loss
2019-04-07 20:29:18,819 [INFO] unfolding 0, single step 12001
2019-04-07 20:29:18,820 [INFO] Sum of grad norms of most recent batch: 3.859411
2019-04-07 20:29:18,821 [INFO] ---------------------------------
2019-04-07 20:29:40,816 [INFO] ---------------------------------
2019-04-07 20:29:40,817 [INFO] Summary:
2019-04-07 20:29:40,818 [INFO] Batch 13000, worst loss 0.054862 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:29:40,818 [INFO] Regularization: 2013.282593 * 0.0000000100 = 0.0000201328 loss
2019-04-07 20:29:40,819 [INFO] unfolding 0, single step 13001
2019-04-07 20:29:40,819 [INFO] Sum of grad norms of most recent batch: 1.599369
2019-04-07 20:29:40,820 [INFO] ---------------------------------
2019-04-07 20:30:02,847 [INFO] ---------------------------------
2019-04-07 20:30:02,848 [INFO] Summary:
2019-04-07 20:30:02,849 [INFO] Batch 14000, worst loss 0.082036 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:30:02,850 [INFO] Regularization: 1985.961182 * 0.0000000100 = 0.0000198596 loss
2019-04-07 20:30:02,850 [INFO] unfolding 0, single step 14001
2019-04-07 20:30:02,851 [INFO] Sum of grad norms of most recent batch: 1.176218
2019-04-07 20:30:02,851 [INFO] ---------------------------------
2019-04-07 20:30:24,528 [INFO] ---------------------------------
2019-04-07 20:30:24,529 [INFO] Summary:
2019-04-07 20:30:24,529 [INFO] Batch 15000, worst loss 0.098297 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:30:24,530 [INFO] Regularization: 2021.563843 * 0.0000000100 = 0.0000202156 loss
2019-04-07 20:30:24,530 [INFO] unfolding 0, single step 15001
2019-04-07 20:30:24,531 [INFO] Sum of grad norms of most recent batch: 0.705283
2019-04-07 20:30:24,531 [INFO] ---------------------------------
2019-04-07 20:30:45,547 [INFO] ---------------------------------
2019-04-07 20:30:45,548 [INFO] Summary:
2019-04-07 20:30:45,549 [INFO] Batch 16000, worst loss 0.073031 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:30:45,550 [INFO] Regularization: 1993.390503 * 0.0000000100 = 0.0000199339 loss
2019-04-07 20:30:45,550 [INFO] unfolding 0, single step 16001
2019-04-07 20:30:45,551 [INFO] Sum of grad norms of most recent batch: 1.310583
2019-04-07 20:30:45,552 [INFO] ---------------------------------
2019-04-07 20:31:07,493 [INFO] ---------------------------------
2019-04-07 20:31:07,494 [INFO] Summary:
2019-04-07 20:31:07,495 [INFO] Batch 17000, worst loss 0.123827 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:31:07,496 [INFO] Regularization: 2016.336426 * 0.0000000100 = 0.0000201634 loss
2019-04-07 20:31:07,496 [INFO] unfolding 0, single step 17001
2019-04-07 20:31:07,497 [INFO] Sum of grad norms of most recent batch: 0.829682
2019-04-07 20:31:07,498 [INFO] ---------------------------------
2019-04-07 20:31:29,397 [INFO] ---------------------------------
2019-04-07 20:31:29,398 [INFO] Summary:
2019-04-07 20:31:29,398 [INFO] Batch 18000, worst loss 0.095162 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:31:29,399 [INFO] Regularization: 1973.004028 * 0.0000000100 = 0.0000197300 loss
2019-04-07 20:31:29,399 [INFO] unfolding 0, single step 18001
2019-04-07 20:31:29,400 [INFO] Sum of grad norms of most recent batch: 1.406397
2019-04-07 20:31:29,400 [INFO] ---------------------------------
2019-04-07 20:31:50,466 [INFO] ---------------------------------
2019-04-07 20:31:50,467 [INFO] Summary:
2019-04-07 20:31:50,467 [INFO] Batch 19000, worst loss 0.065444 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:31:50,468 [INFO] Regularization: 1944.653320 * 0.0000000100 = 0.0000194465 loss
2019-04-07 20:31:50,468 [INFO] unfolding 0, single step 19001
2019-04-07 20:31:50,469 [INFO] Sum of grad norms of most recent batch: 1.202843
2019-04-07 20:31:50,470 [INFO] ---------------------------------
2019-04-07 20:32:12,166 [INFO] ---------------------------------
2019-04-07 20:32:12,167 [INFO] Summary:
2019-04-07 20:32:12,168 [INFO] Batch 20000, worst loss 0.073801 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:32:12,168 [INFO] Regularization: 1950.508789 * 0.0000000100 = 0.0000195051 loss
2019-04-07 20:32:12,169 [INFO] unfolding 0, single step 20001
2019-04-07 20:32:12,169 [INFO] Sum of grad norms of most recent batch: 2.578653
2019-04-07 20:32:12,170 [INFO] ---------------------------------
2019-04-07 20:32:49,423 [INFO] ---------------------------------
2019-04-07 20:32:49,424 [INFO] Evaluation:
2019-04-07 20:32:49,425 [INFO] Batch 20000, worst loss 0.150111 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:32:49,425 [INFO] ---------------------------------
2019-04-07 20:33:11,371 [INFO] ---------------------------------
2019-04-07 20:33:11,372 [INFO] Summary:
2019-04-07 20:33:11,372 [INFO] Batch 21000, worst loss 0.110267 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:33:11,373 [INFO] Regularization: 1976.058594 * 0.0000000100 = 0.0000197606 loss
2019-04-07 20:33:11,373 [INFO] unfolding 0, single step 21001
2019-04-07 20:33:11,374 [INFO] Sum of grad norms of most recent batch: 1.086442
2019-04-07 20:33:11,374 [INFO] ---------------------------------
2019-04-07 20:33:33,150 [INFO] ---------------------------------
2019-04-07 20:33:33,151 [INFO] Summary:
2019-04-07 20:33:33,152 [INFO] Batch 22000, worst loss 0.112187 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:33:33,152 [INFO] Regularization: 1968.799194 * 0.0000000100 = 0.0000196880 loss
2019-04-07 20:33:33,152 [INFO] unfolding 0, single step 22001
2019-04-07 20:33:33,153 [INFO] Sum of grad norms of most recent batch: 0.674982
2019-04-07 20:33:33,154 [INFO] ---------------------------------
2019-04-07 20:33:54,395 [INFO] ---------------------------------
2019-04-07 20:33:54,396 [INFO] Summary:
2019-04-07 20:33:54,397 [INFO] Batch 23000, worst loss 0.092855 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:33:54,397 [INFO] Regularization: 1924.034546 * 0.0000000100 = 0.0000192403 loss
2019-04-07 20:33:54,397 [INFO] unfolding 0, single step 23001
2019-04-07 20:33:54,398 [INFO] Sum of grad norms of most recent batch: 0.881158
2019-04-07 20:33:54,399 [INFO] ---------------------------------
2019-04-07 20:34:15,911 [INFO] ---------------------------------
2019-04-07 20:34:15,912 [INFO] Summary:
2019-04-07 20:34:15,913 [INFO] Batch 24000, worst loss 0.068949 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:34:15,913 [INFO] Regularization: 1961.248901 * 0.0000000100 = 0.0000196125 loss
2019-04-07 20:34:15,914 [INFO] unfolding 0, single step 24001
2019-04-07 20:34:15,914 [INFO] Sum of grad norms of most recent batch: 1.051106
2019-04-07 20:34:15,915 [INFO] ---------------------------------
2019-04-07 20:34:37,722 [INFO] ---------------------------------
2019-04-07 20:34:37,723 [INFO] Summary:
2019-04-07 20:34:37,723 [INFO] Batch 25000, worst loss 0.112086 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:34:37,724 [INFO] Regularization: 1943.941406 * 0.0000000100 = 0.0000194394 loss
2019-04-07 20:34:37,724 [INFO] unfolding 0, single step 25001
2019-04-07 20:34:37,725 [INFO] Sum of grad norms of most recent batch: 1.023725
2019-04-07 20:34:37,726 [INFO] ---------------------------------
2019-04-07 20:34:59,447 [INFO] ---------------------------------
2019-04-07 20:34:59,448 [INFO] Summary:
2019-04-07 20:34:59,448 [INFO] Batch 26000, worst loss 0.168058 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:34:59,449 [INFO] Regularization: 1994.965942 * 0.0000000100 = 0.0000199497 loss
2019-04-07 20:34:59,449 [INFO] unfolding 0, single step 26001
2019-04-07 20:34:59,450 [INFO] Sum of grad norms of most recent batch: 1.553219
2019-04-07 20:34:59,450 [INFO] ---------------------------------
2019-04-07 20:35:20,868 [INFO] ---------------------------------
2019-04-07 20:35:20,869 [INFO] Summary:
2019-04-07 20:35:20,870 [INFO] Batch 27000, worst loss 1.028918 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:35:20,870 [INFO] Regularization: 2019.353516 * 0.0000000100 = 0.0000201935 loss
2019-04-07 20:35:20,871 [INFO] unfolding 0, single step 27001
2019-04-07 20:35:20,872 [INFO] Sum of grad norms of most recent batch: 1.147729
2019-04-07 20:35:20,872 [INFO] ---------------------------------
2019-04-07 20:35:42,465 [INFO] ---------------------------------
2019-04-07 20:35:42,466 [INFO] Summary:
2019-04-07 20:35:42,466 [INFO] Batch 28000, worst loss 0.026018 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:35:42,467 [INFO] Regularization: 1973.213867 * 0.0000000100 = 0.0000197321 loss
2019-04-07 20:35:42,467 [INFO] unfolding 0, single step 28001
2019-04-07 20:35:42,468 [INFO] Sum of grad norms of most recent batch: 1.120586
2019-04-07 20:35:42,469 [INFO] ---------------------------------
2019-04-07 20:36:03,773 [INFO] ---------------------------------
2019-04-07 20:36:03,774 [INFO] Summary:
2019-04-07 20:36:03,775 [INFO] Batch 29000, worst loss 0.111753 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:36:03,775 [INFO] Regularization: 1985.446167 * 0.0000000100 = 0.0000198545 loss
2019-04-07 20:36:03,776 [INFO] unfolding 0, single step 29001
2019-04-07 20:36:03,777 [INFO] Sum of grad norms of most recent batch: 0.924657
2019-04-07 20:36:03,777 [INFO] ---------------------------------
2019-04-07 20:36:25,590 [INFO] ---------------------------------
2019-04-07 20:36:25,591 [INFO] Summary:
2019-04-07 20:36:25,591 [INFO] Batch 30000, worst loss 0.115150 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 20:36:25,592 [INFO] Regularization: 1960.837524 * 0.0000000100 = 0.0000196084 loss
2019-04-07 20:36:25,593 [INFO] unfolding 0, single step 30001
2019-04-07 20:36:25,594 [INFO] Sum of grad norms of most recent batch: 0.928124
2019-04-07 20:36:25,595 [INFO] ---------------------------------
2019-04-07 20:37:03,074 [INFO] ---------------------------------
2019-04-07 20:37:03,075 [INFO] Evaluation:
2019-04-07 20:37:03,075 [INFO] Batch 30000, worst loss 0.132720 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:37:03,076 [INFO] ---------------------------------
2019-04-07 20:37:25,168 [INFO] ---------------------------------
2019-04-07 20:37:25,169 [INFO] Summary:
2019-04-07 20:37:25,169 [INFO] Batch 31000, worst loss 0.038224 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 20:37:25,170 [INFO] Regularization: 1883.989624 * 0.0000000100 = 0.0000188399 loss
2019-04-07 20:37:25,170 [INFO] unfolding 0, single step 31001
2019-04-07 20:37:25,171 [INFO] Sum of grad norms of most recent batch: 1.861870
2019-04-07 20:37:25,171 [INFO] ---------------------------------
2019-04-07 20:37:47,294 [INFO] ---------------------------------
2019-04-07 20:37:47,295 [INFO] Summary:
2019-04-07 20:37:47,296 [INFO] Batch 32000, worst loss 0.135827 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 20:37:47,297 [INFO] Regularization: 1823.578735 * 0.0000000100 = 0.0000182358 loss
2019-04-07 20:37:47,297 [INFO] unfolding 0, single step 32001
2019-04-07 20:37:47,298 [INFO] Sum of grad norms of most recent batch: 0.404949
2019-04-07 20:37:47,298 [INFO] ---------------------------------
2019-04-07 20:38:08,956 [INFO] ---------------------------------
2019-04-07 20:38:08,957 [INFO] Summary:
2019-04-07 20:38:08,958 [INFO] Batch 33000, worst loss 0.028043 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 20:38:08,959 [INFO] Regularization: 1742.194824 * 0.0000000100 = 0.0000174219 loss
2019-04-07 20:38:08,959 [INFO] unfolding 0, single step 33001
2019-04-07 20:38:08,960 [INFO] Sum of grad norms of most recent batch: 0.389897
2019-04-07 20:38:08,960 [INFO] ---------------------------------
2019-04-07 20:38:30,743 [INFO] ---------------------------------
2019-04-07 20:38:30,744 [INFO] Summary:
2019-04-07 20:38:30,744 [INFO] Batch 34000, worst loss 0.066453 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 20:38:30,745 [INFO] Regularization: 1726.621948 * 0.0000000100 = 0.0000172662 loss
2019-04-07 20:38:30,746 [INFO] unfolding 0, single step 34001
2019-04-07 20:38:30,746 [INFO] Sum of grad norms of most recent batch: 0.782943
2019-04-07 20:38:30,747 [INFO] ---------------------------------
2019-04-07 20:38:52,622 [INFO] ---------------------------------
2019-04-07 20:38:52,623 [INFO] Summary:
2019-04-07 20:38:52,624 [INFO] Batch 35000, worst loss 0.105105 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 20:38:52,625 [INFO] Regularization: 1660.571899 * 0.0000000100 = 0.0000166057 loss
2019-04-07 20:38:52,626 [INFO] unfolding 0, single step 35001
2019-04-07 20:38:52,627 [INFO] Sum of grad norms of most recent batch: 1.078106
2019-04-07 20:38:52,627 [INFO] ---------------------------------
2019-04-07 20:39:14,291 [INFO] ---------------------------------
2019-04-07 20:39:14,292 [INFO] Summary:
2019-04-07 20:39:14,293 [INFO] Batch 36000, worst loss 0.058087 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 20:39:14,293 [INFO] Regularization: 1660.550293 * 0.0000000100 = 0.0000166055 loss
2019-04-07 20:39:14,294 [INFO] unfolding 0, single step 36001
2019-04-07 20:39:14,294 [INFO] Sum of grad norms of most recent batch: 0.761777
2019-04-07 20:39:14,295 [INFO] ---------------------------------
2019-04-07 20:39:36,072 [INFO] ---------------------------------
2019-04-07 20:39:36,072 [INFO] Summary:
2019-04-07 20:39:36,073 [INFO] Batch 37000, worst loss 0.138098 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 20:39:36,073 [INFO] Regularization: 1720.903809 * 0.0000000100 = 0.0000172090 loss
2019-04-07 20:39:36,074 [INFO] unfolding 0, single step 37001
2019-04-07 20:39:36,074 [INFO] Sum of grad norms of most recent batch: 1.132831
2019-04-07 20:39:36,075 [INFO] ---------------------------------
2019-04-07 20:39:58,416 [INFO] ---------------------------------
2019-04-07 20:39:58,417 [INFO] Summary:
2019-04-07 20:39:58,418 [INFO] Batch 38000, worst loss 0.043825 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 20:39:58,418 [INFO] Regularization: 1661.885986 * 0.0000000100 = 0.0000166189 loss
2019-04-07 20:39:58,419 [INFO] unfolding 0, single step 38001
2019-04-07 20:39:58,419 [INFO] Sum of grad norms of most recent batch: 0.469978
2019-04-07 20:39:58,420 [INFO] ---------------------------------
2019-04-07 20:40:19,930 [INFO] ---------------------------------
2019-04-07 20:40:19,931 [INFO] Summary:
2019-04-07 20:40:19,931 [INFO] Batch 39000, worst loss 0.069862 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 20:40:19,932 [INFO] Regularization: 1642.303833 * 0.0000000100 = 0.0000164230 loss
2019-04-07 20:40:19,932 [INFO] unfolding 0, single step 39001
2019-04-07 20:40:19,933 [INFO] Sum of grad norms of most recent batch: 0.552083
2019-04-07 20:40:19,933 [INFO] ---------------------------------
2019-04-07 20:40:42,046 [INFO] ---------------------------------
2019-04-07 20:40:42,047 [INFO] Summary:
2019-04-07 20:40:42,048 [INFO] Batch 40000, worst loss 0.100482 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 20:40:42,049 [INFO] Regularization: 1658.787354 * 0.0000000100 = 0.0000165879 loss
2019-04-07 20:40:42,049 [INFO] unfolding 0, single step 40001
2019-04-07 20:40:42,050 [INFO] Sum of grad norms of most recent batch: 0.382573
2019-04-07 20:40:42,050 [INFO] ---------------------------------
2019-04-07 20:41:19,338 [INFO] ---------------------------------
2019-04-07 20:41:19,339 [INFO] Evaluation:
2019-04-07 20:41:19,339 [INFO] Batch 40000, worst loss 0.109496 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:41:19,340 [INFO] ---------------------------------
2019-04-07 20:41:40,741 [INFO] ---------------------------------
2019-04-07 20:41:40,742 [INFO] Summary:
2019-04-07 20:41:40,742 [INFO] Batch 41000, worst loss 0.152950 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 20:41:40,743 [INFO] Regularization: 1697.169922 * 0.0000000100 = 0.0000169717 loss
2019-04-07 20:41:40,743 [INFO] unfolding 0, single step 41001
2019-04-07 20:41:40,744 [INFO] Sum of grad norms of most recent batch: 0.984671
2019-04-07 20:41:40,744 [INFO] ---------------------------------
2019-04-07 20:42:02,490 [INFO] ---------------------------------
2019-04-07 20:42:02,492 [INFO] Summary:
2019-04-07 20:42:02,492 [INFO] Batch 42000, worst loss 0.088569 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 20:42:02,493 [INFO] Regularization: 1628.458618 * 0.0000000100 = 0.0000162846 loss
2019-04-07 20:42:02,493 [INFO] unfolding 0, single step 42001
2019-04-07 20:42:02,494 [INFO] Sum of grad norms of most recent batch: 0.427037
2019-04-07 20:42:02,494 [INFO] ---------------------------------
2019-04-07 20:42:24,216 [INFO] ---------------------------------
2019-04-07 20:42:24,217 [INFO] Summary:
2019-04-07 20:42:24,219 [INFO] Batch 43000, worst loss 0.104746 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 20:42:24,219 [INFO] Regularization: 1630.604858 * 0.0000000100 = 0.0000163060 loss
2019-04-07 20:42:24,220 [INFO] unfolding 0, single step 43001
2019-04-07 20:42:24,221 [INFO] Sum of grad norms of most recent batch: 0.114163
2019-04-07 20:42:24,222 [INFO] ---------------------------------
2019-04-07 20:42:46,016 [INFO] ---------------------------------
2019-04-07 20:42:46,017 [INFO] Summary:
2019-04-07 20:42:46,018 [INFO] Batch 44000, worst loss 0.064214 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 20:42:46,018 [INFO] Regularization: 1592.990601 * 0.0000000100 = 0.0000159299 loss
2019-04-07 20:42:46,019 [INFO] unfolding 0, single step 44001
2019-04-07 20:42:46,019 [INFO] Sum of grad norms of most recent batch: 0.461260
2019-04-07 20:42:46,020 [INFO] ---------------------------------
2019-04-07 20:43:07,970 [INFO] ---------------------------------
2019-04-07 20:43:07,970 [INFO] Summary:
2019-04-07 20:43:07,971 [INFO] Batch 45000, worst loss 0.062461 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 20:43:07,972 [INFO] Regularization: 1562.895142 * 0.0000000100 = 0.0000156290 loss
2019-04-07 20:43:07,972 [INFO] unfolding 0, single step 45001
2019-04-07 20:43:07,973 [INFO] Sum of grad norms of most recent batch: 0.631147
2019-04-07 20:43:07,973 [INFO] ---------------------------------
2019-04-07 20:43:29,846 [INFO] ---------------------------------
2019-04-07 20:43:29,847 [INFO] Summary:
2019-04-07 20:43:29,848 [INFO] Batch 46000, worst loss 0.044732 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 20:43:29,848 [INFO] Regularization: 1532.624146 * 0.0000000100 = 0.0000153262 loss
2019-04-07 20:43:29,849 [INFO] unfolding 0, single step 46001
2019-04-07 20:43:29,849 [INFO] Sum of grad norms of most recent batch: 0.635864
2019-04-07 20:43:29,850 [INFO] ---------------------------------
2019-04-07 20:43:51,318 [INFO] ---------------------------------
2019-04-07 20:43:51,319 [INFO] Summary:
2019-04-07 20:43:51,319 [INFO] Batch 47000, worst loss 0.064468 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 20:43:51,320 [INFO] Regularization: 1516.948364 * 0.0000000100 = 0.0000151695 loss
2019-04-07 20:43:51,320 [INFO] unfolding 0, single step 47001
2019-04-07 20:43:51,321 [INFO] Sum of grad norms of most recent batch: 0.665127
2019-04-07 20:43:51,322 [INFO] ---------------------------------
2019-04-07 20:44:13,020 [INFO] ---------------------------------
2019-04-07 20:44:13,021 [INFO] Summary:
2019-04-07 20:44:13,022 [INFO] Batch 48000, worst loss 0.084776 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 20:44:13,022 [INFO] Regularization: 1495.394287 * 0.0000000100 = 0.0000149539 loss
2019-04-07 20:44:13,023 [INFO] unfolding 0, single step 48001
2019-04-07 20:44:13,024 [INFO] Sum of grad norms of most recent batch: 0.212018
2019-04-07 20:44:13,024 [INFO] ---------------------------------
2019-04-07 20:44:33,822 [INFO] ---------------------------------
2019-04-07 20:44:33,823 [INFO] Summary:
2019-04-07 20:44:33,823 [INFO] Batch 49000, worst loss 0.021595 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 20:44:33,824 [INFO] Regularization: 1483.367920 * 0.0000000100 = 0.0000148337 loss
2019-04-07 20:44:33,824 [INFO] unfolding 0, single step 49001
2019-04-07 20:44:33,825 [INFO] Sum of grad norms of most recent batch: 0.357622
2019-04-07 20:44:33,825 [INFO] ---------------------------------
2019-04-07 20:44:54,693 [INFO] ---------------------------------
2019-04-07 20:44:54,694 [INFO] Summary:
2019-04-07 20:44:54,695 [INFO] Batch 50000, worst loss 0.229236 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 20:44:54,695 [INFO] Regularization: 1530.664917 * 0.0000000100 = 0.0000153066 loss
2019-04-07 20:44:54,696 [INFO] unfolding 0, single step 50001
2019-04-07 20:44:54,696 [INFO] Sum of grad norms of most recent batch: 0.588299
2019-04-07 20:44:54,697 [INFO] ---------------------------------
2019-04-07 20:45:32,169 [INFO] ---------------------------------
2019-04-07 20:45:32,170 [INFO] Evaluation:
2019-04-07 20:45:32,171 [INFO] Batch 50000, worst loss 0.128607 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:45:32,171 [INFO] ---------------------------------
2019-04-07 20:45:54,112 [INFO] ---------------------------------
2019-04-07 20:45:54,113 [INFO] Summary:
2019-04-07 20:45:54,113 [INFO] Batch 51000, worst loss 0.102370 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 20:45:54,114 [INFO] Regularization: 1533.755005 * 0.0000000100 = 0.0000153375 loss
2019-04-07 20:45:54,114 [INFO] unfolding 0, single step 51001
2019-04-07 20:45:54,115 [INFO] Sum of grad norms of most recent batch: 0.200036
2019-04-07 20:45:54,116 [INFO] ---------------------------------
2019-04-07 20:46:15,464 [INFO] ---------------------------------
2019-04-07 20:46:15,465 [INFO] Summary:
2019-04-07 20:46:15,466 [INFO] Batch 52000, worst loss 0.050890 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 20:46:15,466 [INFO] Regularization: 1509.502441 * 0.0000000100 = 0.0000150950 loss
2019-04-07 20:46:15,467 [INFO] unfolding 0, single step 52001
2019-04-07 20:46:15,468 [INFO] Sum of grad norms of most recent batch: 0.245265
2019-04-07 20:46:15,469 [INFO] ---------------------------------
2019-04-07 20:46:36,592 [INFO] ---------------------------------
2019-04-07 20:46:36,593 [INFO] Summary:
2019-04-07 20:46:36,593 [INFO] Batch 53000, worst loss 0.079371 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 20:46:36,594 [INFO] Regularization: 1495.261719 * 0.0000000100 = 0.0000149526 loss
2019-04-07 20:46:36,594 [INFO] unfolding 0, single step 53001
2019-04-07 20:46:36,595 [INFO] Sum of grad norms of most recent batch: 0.212190
2019-04-07 20:46:36,595 [INFO] ---------------------------------
2019-04-07 20:46:58,106 [INFO] ---------------------------------
2019-04-07 20:46:58,107 [INFO] Summary:
2019-04-07 20:46:58,108 [INFO] Batch 54000, worst loss 0.067316 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 20:46:58,109 [INFO] Regularization: 1482.158447 * 0.0000000100 = 0.0000148216 loss
2019-04-07 20:46:58,110 [INFO] unfolding 0, single step 54001
2019-04-07 20:46:58,111 [INFO] Sum of grad norms of most recent batch: 0.032988
2019-04-07 20:46:58,112 [INFO] ---------------------------------
2019-04-07 20:47:19,306 [INFO] ---------------------------------
2019-04-07 20:47:19,307 [INFO] Summary:
2019-04-07 20:47:19,307 [INFO] Batch 55000, worst loss 0.098894 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 20:47:19,308 [INFO] Regularization: 1476.191406 * 0.0000000100 = 0.0000147619 loss
2019-04-07 20:47:19,308 [INFO] unfolding 0, single step 55001
2019-04-07 20:47:19,309 [INFO] Sum of grad norms of most recent batch: 0.083758
2019-04-07 20:47:19,309 [INFO] ---------------------------------
2019-04-07 20:47:41,302 [INFO] ---------------------------------
2019-04-07 20:47:41,304 [INFO] Summary:
2019-04-07 20:47:41,304 [INFO] Batch 56000, worst loss 0.078650 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 20:47:41,305 [INFO] Regularization: 1475.021118 * 0.0000000100 = 0.0000147502 loss
2019-04-07 20:47:41,305 [INFO] unfolding 0, single step 56001
2019-04-07 20:47:41,305 [INFO] Sum of grad norms of most recent batch: 0.204901
2019-04-07 20:47:41,306 [INFO] ---------------------------------
2019-04-07 20:48:02,986 [INFO] ---------------------------------
2019-04-07 20:48:02,987 [INFO] Summary:
2019-04-07 20:48:02,988 [INFO] Batch 57000, worst loss 0.049698 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 20:48:02,988 [INFO] Regularization: 1472.875122 * 0.0000000100 = 0.0000147288 loss
2019-04-07 20:48:02,989 [INFO] unfolding 0, single step 57001
2019-04-07 20:48:02,989 [INFO] Sum of grad norms of most recent batch: 0.348559
2019-04-07 20:48:02,990 [INFO] ---------------------------------
2019-04-07 20:48:24,690 [INFO] ---------------------------------
2019-04-07 20:48:24,691 [INFO] Summary:
2019-04-07 20:48:24,691 [INFO] Batch 58000, worst loss 0.016399 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 20:48:24,692 [INFO] Regularization: 1460.294678 * 0.0000000100 = 0.0000146029 loss
2019-04-07 20:48:24,692 [INFO] unfolding 0, single step 58001
2019-04-07 20:48:24,693 [INFO] Sum of grad norms of most recent batch: 0.123239
2019-04-07 20:48:24,693 [INFO] ---------------------------------
2019-04-07 20:48:45,802 [INFO] ---------------------------------
2019-04-07 20:48:45,803 [INFO] Summary:
2019-04-07 20:48:45,803 [INFO] Batch 59000, worst loss 0.030402 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 20:48:45,804 [INFO] Regularization: 1462.352295 * 0.0000000100 = 0.0000146235 loss
2019-04-07 20:48:45,804 [INFO] unfolding 0, single step 59001
2019-04-07 20:48:45,805 [INFO] Sum of grad norms of most recent batch: 0.101037
2019-04-07 20:48:45,805 [INFO] ---------------------------------
2019-04-07 20:49:07,266 [INFO] ---------------------------------
2019-04-07 20:49:07,267 [INFO] Summary:
2019-04-07 20:49:07,268 [INFO] Batch 60000, worst loss 0.043272 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 20:49:07,268 [INFO] Regularization: 1460.567505 * 0.0000000100 = 0.0000146057 loss
2019-04-07 20:49:07,268 [INFO] unfolding 0, single step 60001
2019-04-07 20:49:07,269 [INFO] Sum of grad norms of most recent batch: 0.159654
2019-04-07 20:49:07,270 [INFO] ---------------------------------
2019-04-07 20:49:44,590 [INFO] ---------------------------------
2019-04-07 20:49:44,591 [INFO] Evaluation:
2019-04-07 20:49:44,592 [INFO] Batch 60000, worst loss 0.086441 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:49:44,592 [INFO] ---------------------------------
2019-04-07 20:50:06,168 [INFO] ---------------------------------
2019-04-07 20:50:06,169 [INFO] Summary:
2019-04-07 20:50:06,170 [INFO] Batch 61000, worst loss 0.042438 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 20:50:06,171 [INFO] Regularization: 1457.970581 * 0.0000000100 = 0.0000145797 loss
2019-04-07 20:50:06,171 [INFO] unfolding 0, single step 61001
2019-04-07 20:50:06,172 [INFO] Sum of grad norms of most recent batch: 0.292520
2019-04-07 20:50:06,173 [INFO] ---------------------------------
2019-04-07 20:50:27,645 [INFO] ---------------------------------
2019-04-07 20:50:27,646 [INFO] Summary:
2019-04-07 20:50:27,647 [INFO] Batch 62000, worst loss 0.046288 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 20:50:27,647 [INFO] Regularization: 1453.119019 * 0.0000000100 = 0.0000145312 loss
2019-04-07 20:50:27,647 [INFO] unfolding 0, single step 62001
2019-04-07 20:50:27,648 [INFO] Sum of grad norms of most recent batch: 0.258008
2019-04-07 20:50:27,648 [INFO] ---------------------------------
2019-04-07 20:50:48,060 [INFO] ---------------------------------
2019-04-07 20:50:48,061 [INFO] Summary:
2019-04-07 20:50:48,061 [INFO] Batch 63000, worst loss 0.039681 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 20:50:48,062 [INFO] Regularization: 1448.182129 * 0.0000000100 = 0.0000144818 loss
2019-04-07 20:50:48,062 [INFO] unfolding 0, single step 63001
2019-04-07 20:50:48,063 [INFO] Sum of grad norms of most recent batch: 0.058001
2019-04-07 20:50:48,063 [INFO] ---------------------------------
2019-04-07 20:51:09,451 [INFO] ---------------------------------
2019-04-07 20:51:09,452 [INFO] Summary:
2019-04-07 20:51:09,453 [INFO] Batch 64000, worst loss 0.054607 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 20:51:09,453 [INFO] Regularization: 1445.399414 * 0.0000000100 = 0.0000144540 loss
2019-04-07 20:51:09,454 [INFO] unfolding 0, single step 64001
2019-04-07 20:51:09,454 [INFO] Sum of grad norms of most recent batch: 0.122770
2019-04-07 20:51:09,455 [INFO] ---------------------------------
2019-04-07 20:51:31,110 [INFO] ---------------------------------
2019-04-07 20:51:31,111 [INFO] Summary:
2019-04-07 20:51:31,112 [INFO] Batch 65000, worst loss 0.047967 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 20:51:31,113 [INFO] Regularization: 1448.000366 * 0.0000000100 = 0.0000144800 loss
2019-04-07 20:51:31,113 [INFO] unfolding 0, single step 65001
2019-04-07 20:51:31,113 [INFO] Sum of grad norms of most recent batch: 0.164026
2019-04-07 20:51:31,114 [INFO] ---------------------------------
2019-04-07 20:51:51,589 [INFO] ---------------------------------
2019-04-07 20:51:51,590 [INFO] Summary:
2019-04-07 20:51:51,591 [INFO] Batch 66000, worst loss 0.036964 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 20:51:51,591 [INFO] Regularization: 1450.917847 * 0.0000000100 = 0.0000145092 loss
2019-04-07 20:51:51,592 [INFO] unfolding 0, single step 66001
2019-04-07 20:51:51,592 [INFO] Sum of grad norms of most recent batch: 0.209898
2019-04-07 20:51:51,593 [INFO] ---------------------------------
2019-04-07 20:52:13,100 [INFO] ---------------------------------
2019-04-07 20:52:13,101 [INFO] Summary:
2019-04-07 20:52:13,102 [INFO] Batch 67000, worst loss 0.047210 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 20:52:13,102 [INFO] Regularization: 1448.074097 * 0.0000000100 = 0.0000144807 loss
2019-04-07 20:52:13,103 [INFO] unfolding 0, single step 67001
2019-04-07 20:52:13,103 [INFO] Sum of grad norms of most recent batch: 0.194845
2019-04-07 20:52:13,104 [INFO] ---------------------------------
2019-04-07 20:52:34,524 [INFO] ---------------------------------
2019-04-07 20:52:34,525 [INFO] Summary:
2019-04-07 20:52:34,526 [INFO] Batch 68000, worst loss 0.021536 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 20:52:34,526 [INFO] Regularization: 1441.313354 * 0.0000000100 = 0.0000144131 loss
2019-04-07 20:52:34,526 [INFO] unfolding 0, single step 68001
2019-04-07 20:52:34,527 [INFO] Sum of grad norms of most recent batch: 0.039075
2019-04-07 20:52:34,527 [INFO] ---------------------------------
2019-04-07 20:52:55,821 [INFO] ---------------------------------
2019-04-07 20:52:55,822 [INFO] Summary:
2019-04-07 20:52:55,823 [INFO] Batch 69000, worst loss 0.041535 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 20:52:55,823 [INFO] Regularization: 1442.311035 * 0.0000000100 = 0.0000144231 loss
2019-04-07 20:52:55,823 [INFO] unfolding 0, single step 69001
2019-04-07 20:52:55,824 [INFO] Sum of grad norms of most recent batch: 3.135458
2019-04-07 20:52:55,824 [INFO] ---------------------------------
2019-04-07 20:53:16,733 [INFO] ---------------------------------
2019-04-07 20:53:16,734 [INFO] Summary:
2019-04-07 20:53:16,735 [INFO] Batch 70000, worst loss 0.046064 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 20:53:16,736 [INFO] Regularization: 1441.871216 * 0.0000000100 = 0.0000144187 loss
2019-04-07 20:53:16,736 [INFO] unfolding 0, single step 70001
2019-04-07 20:53:16,736 [INFO] Sum of grad norms of most recent batch: 0.063434
2019-04-07 20:53:16,737 [INFO] ---------------------------------
2019-04-07 20:53:53,979 [INFO] ---------------------------------
2019-04-07 20:53:53,980 [INFO] Evaluation:
2019-04-07 20:53:53,980 [INFO] Batch 70000, worst loss 0.052046 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:53:53,981 [INFO] ---------------------------------
2019-04-07 20:54:14,630 [INFO] ---------------------------------
2019-04-07 20:54:14,631 [INFO] Summary:
2019-04-07 20:54:14,632 [INFO] Batch 71000, worst loss 0.011020 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 20:54:14,632 [INFO] Regularization: 1435.332153 * 0.0000000100 = 0.0000143533 loss
2019-04-07 20:54:14,633 [INFO] unfolding 0, single step 71001
2019-04-07 20:54:14,633 [INFO] Sum of grad norms of most recent batch: 0.052334
2019-04-07 20:54:14,634 [INFO] ---------------------------------
2019-04-07 20:54:35,470 [INFO] ---------------------------------
2019-04-07 20:54:35,471 [INFO] Summary:
2019-04-07 20:54:35,471 [INFO] Batch 72000, worst loss 0.007837 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 20:54:35,472 [INFO] Regularization: 1434.986572 * 0.0000000100 = 0.0000143499 loss
2019-04-07 20:54:35,472 [INFO] unfolding 0, single step 72001
2019-04-07 20:54:35,472 [INFO] Sum of grad norms of most recent batch: 0.729960
2019-04-07 20:54:35,473 [INFO] ---------------------------------
2019-04-07 20:54:56,788 [INFO] ---------------------------------
2019-04-07 20:54:56,789 [INFO] Summary:
2019-04-07 20:54:56,790 [INFO] Batch 73000, worst loss 0.014835 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 20:54:56,790 [INFO] Regularization: 1433.991943 * 0.0000000100 = 0.0000143399 loss
2019-04-07 20:54:56,791 [INFO] unfolding 0, single step 73001
2019-04-07 20:54:56,791 [INFO] Sum of grad norms of most recent batch: 0.066504
2019-04-07 20:54:56,792 [INFO] ---------------------------------
2019-04-07 20:55:18,125 [INFO] ---------------------------------
2019-04-07 20:55:18,126 [INFO] Summary:
2019-04-07 20:55:18,127 [INFO] Batch 74000, worst loss 0.019365 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 20:55:18,128 [INFO] Regularization: 1431.970337 * 0.0000000100 = 0.0000143197 loss
2019-04-07 20:55:18,129 [INFO] unfolding 0, single step 74001
2019-04-07 20:55:18,130 [INFO] Sum of grad norms of most recent batch: 1.470490
2019-04-07 20:55:18,130 [INFO] ---------------------------------
2019-04-07 20:55:39,845 [INFO] ---------------------------------
2019-04-07 20:55:39,847 [INFO] Summary:
2019-04-07 20:55:39,847 [INFO] Batch 75000, worst loss 0.017634 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 20:55:39,848 [INFO] Regularization: 1429.647583 * 0.0000000100 = 0.0000142965 loss
2019-04-07 20:55:39,848 [INFO] unfolding 0, single step 75001
2019-04-07 20:55:39,848 [INFO] Sum of grad norms of most recent batch: 0.057642
2019-04-07 20:55:39,849 [INFO] ---------------------------------
2019-04-07 20:56:00,866 [INFO] ---------------------------------
2019-04-07 20:56:00,867 [INFO] Summary:
2019-04-07 20:56:00,868 [INFO] Batch 76000, worst loss 0.018885 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 20:56:00,869 [INFO] Regularization: 1429.809204 * 0.0000000100 = 0.0000142981 loss
2019-04-07 20:56:00,869 [INFO] unfolding 0, single step 76001
2019-04-07 20:56:00,870 [INFO] Sum of grad norms of most recent batch: 8.512876
2019-04-07 20:56:00,871 [INFO] ---------------------------------
2019-04-07 20:56:22,404 [INFO] ---------------------------------
2019-04-07 20:56:22,405 [INFO] Summary:
2019-04-07 20:56:22,406 [INFO] Batch 77000, worst loss 0.022564 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 20:56:22,406 [INFO] Regularization: 1430.198730 * 0.0000000100 = 0.0000143020 loss
2019-04-07 20:56:22,406 [INFO] unfolding 0, single step 77001
2019-04-07 20:56:22,407 [INFO] Sum of grad norms of most recent batch: 0.032033
2019-04-07 20:56:22,408 [INFO] ---------------------------------
2019-04-07 20:56:43,421 [INFO] ---------------------------------
2019-04-07 20:56:43,422 [INFO] Summary:
2019-04-07 20:56:43,422 [INFO] Batch 78000, worst loss 0.029790 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 20:56:43,423 [INFO] Regularization: 1429.553467 * 0.0000000100 = 0.0000142955 loss
2019-04-07 20:56:43,424 [INFO] unfolding 0, single step 78001
2019-04-07 20:56:43,425 [INFO] Sum of grad norms of most recent batch: 0.056027
2019-04-07 20:56:43,425 [INFO] ---------------------------------
2019-04-07 20:57:04,642 [INFO] ---------------------------------
2019-04-07 20:57:04,643 [INFO] Summary:
2019-04-07 20:57:04,644 [INFO] Batch 79000, worst loss 0.009522 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 20:57:04,644 [INFO] Regularization: 1428.284180 * 0.0000000100 = 0.0000142828 loss
2019-04-07 20:57:04,644 [INFO] unfolding 0, single step 79001
2019-04-07 20:57:04,645 [INFO] Sum of grad norms of most recent batch: 0.042855
2019-04-07 20:57:04,645 [INFO] ---------------------------------
2019-04-07 20:57:25,640 [INFO] ---------------------------------
2019-04-07 20:57:25,641 [INFO] Summary:
2019-04-07 20:57:25,642 [INFO] Batch 80000, worst loss 0.012606 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 20:57:25,643 [INFO] Regularization: 1426.994019 * 0.0000000100 = 0.0000142699 loss
2019-04-07 20:57:25,643 [INFO] unfolding 0, single step 80001
2019-04-07 20:57:25,644 [INFO] Sum of grad norms of most recent batch: 0.093763
2019-04-07 20:57:25,644 [INFO] ---------------------------------
2019-04-07 20:58:03,194 [INFO] ---------------------------------
2019-04-07 20:58:03,194 [INFO] Evaluation:
2019-04-07 20:58:03,195 [INFO] Batch 80000, worst loss 0.081874 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 20:58:03,195 [INFO] ---------------------------------
2019-04-07 20:58:23,927 [INFO] ---------------------------------
2019-04-07 20:58:23,928 [INFO] Summary:
2019-04-07 20:58:23,929 [INFO] Batch 81000, worst loss 0.009012 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 20:58:23,929 [INFO] Regularization: 1426.141357 * 0.0000000100 = 0.0000142614 loss
2019-04-07 20:58:23,929 [INFO] unfolding 0, single step 81001
2019-04-07 20:58:23,930 [INFO] Sum of grad norms of most recent batch: 0.099721
2019-04-07 20:58:23,930 [INFO] ---------------------------------
2019-04-07 20:58:45,452 [INFO] ---------------------------------
2019-04-07 20:58:45,453 [INFO] Summary:
2019-04-07 20:58:45,454 [INFO] Batch 82000, worst loss 0.009042 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 20:58:45,454 [INFO] Regularization: 1425.123047 * 0.0000000100 = 0.0000142512 loss
2019-04-07 20:58:45,455 [INFO] unfolding 0, single step 82001
2019-04-07 20:58:45,455 [INFO] Sum of grad norms of most recent batch: 0.023040
2019-04-07 20:58:45,456 [INFO] ---------------------------------
2019-04-07 20:59:07,200 [INFO] ---------------------------------
2019-04-07 20:59:07,200 [INFO] Summary:
2019-04-07 20:59:07,201 [INFO] Batch 83000, worst loss 0.008449 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 20:59:07,201 [INFO] Regularization: 1424.911011 * 0.0000000100 = 0.0000142491 loss
2019-04-07 20:59:07,202 [INFO] unfolding 0, single step 83001
2019-04-07 20:59:07,202 [INFO] Sum of grad norms of most recent batch: 0.039452
2019-04-07 20:59:07,203 [INFO] ---------------------------------
2019-04-07 20:59:28,047 [INFO] ---------------------------------
2019-04-07 20:59:28,049 [INFO] Summary:
2019-04-07 20:59:28,050 [INFO] Batch 84000, worst loss 0.020385 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 20:59:28,051 [INFO] Regularization: 1424.020996 * 0.0000000100 = 0.0000142402 loss
2019-04-07 20:59:28,051 [INFO] unfolding 0, single step 84001
2019-04-07 20:59:28,052 [INFO] Sum of grad norms of most recent batch: 0.029226
2019-04-07 20:59:28,053 [INFO] ---------------------------------
2019-04-07 20:59:49,865 [INFO] ---------------------------------
2019-04-07 20:59:49,866 [INFO] Summary:
2019-04-07 20:59:49,867 [INFO] Batch 85000, worst loss 0.014783 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 20:59:49,868 [INFO] Regularization: 1423.300293 * 0.0000000100 = 0.0000142330 loss
2019-04-07 20:59:49,868 [INFO] unfolding 0, single step 85001
2019-04-07 20:59:49,868 [INFO] Sum of grad norms of most recent batch: 0.198722
2019-04-07 20:59:49,869 [INFO] ---------------------------------
2019-04-07 21:00:11,525 [INFO] ---------------------------------
2019-04-07 21:00:11,526 [INFO] Summary:
2019-04-07 21:00:11,527 [INFO] Batch 86000, worst loss 0.009611 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 21:00:11,527 [INFO] Regularization: 1423.130249 * 0.0000000100 = 0.0000142313 loss
2019-04-07 21:00:11,528 [INFO] unfolding 0, single step 86001
2019-04-07 21:00:11,528 [INFO] Sum of grad norms of most recent batch: 0.061508
2019-04-07 21:00:11,529 [INFO] ---------------------------------
2019-04-07 21:00:32,510 [INFO] ---------------------------------
2019-04-07 21:00:32,511 [INFO] Summary:
2019-04-07 21:00:32,512 [INFO] Batch 87000, worst loss 0.002411 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 21:00:32,512 [INFO] Regularization: 1421.789185 * 0.0000000100 = 0.0000142179 loss
2019-04-07 21:00:32,513 [INFO] unfolding 0, single step 87001
2019-04-07 21:00:32,513 [INFO] Sum of grad norms of most recent batch: 0.067940
2019-04-07 21:00:32,514 [INFO] ---------------------------------
2019-04-07 21:01:10,147 [INFO] ---------------------------------
2019-04-07 21:01:10,148 [INFO] Evaluation:
2019-04-07 21:01:10,149 [INFO] Batch 87000, worst loss 0.020487 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:01:10,150 [INFO] ---------------------------------
2019-04-07 21:01:31,225 [INFO] ---------------------------------
2019-04-07 21:01:31,226 [INFO] Summary:
2019-04-07 21:01:31,226 [INFO] Batch 88000, worst loss 0.019319 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 21:01:31,227 [INFO] Regularization: 1421.247192 * 0.0000000100 = 0.0000142125 loss
2019-04-07 21:01:31,227 [INFO] unfolding 0, single step 88001
2019-04-07 21:01:31,228 [INFO] Sum of grad norms of most recent batch: 0.113167
2019-04-07 21:01:31,228 [INFO] ---------------------------------
2019-04-07 21:01:52,518 [INFO] ---------------------------------
2019-04-07 21:01:52,519 [INFO] Summary:
2019-04-07 21:01:52,519 [INFO] Batch 89000, worst loss 0.013650 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 21:01:52,520 [INFO] Regularization: 1420.542480 * 0.0000000100 = 0.0000142054 loss
2019-04-07 21:01:52,520 [INFO] unfolding 0, single step 89001
2019-04-07 21:01:52,521 [INFO] Sum of grad norms of most recent batch: 0.063745
2019-04-07 21:01:52,521 [INFO] ---------------------------------
2019-04-07 21:02:14,060 [INFO] ---------------------------------
2019-04-07 21:02:14,061 [INFO] Summary:
2019-04-07 21:02:14,061 [INFO] Batch 90000, worst loss 0.016727 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 21:02:14,062 [INFO] Regularization: 1420.258545 * 0.0000000100 = 0.0000142026 loss
2019-04-07 21:02:14,062 [INFO] unfolding 0, single step 90001
2019-04-07 21:02:14,063 [INFO] Sum of grad norms of most recent batch: 0.031520
2019-04-07 21:02:14,063 [INFO] ---------------------------------
2019-04-07 21:02:51,554 [INFO] ---------------------------------
2019-04-07 21:02:51,555 [INFO] Evaluation:
2019-04-07 21:02:51,555 [INFO] Batch 90000, worst loss 0.030806 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:02:51,556 [INFO] ---------------------------------
2019-04-07 21:03:12,374 [INFO] ---------------------------------
2019-04-07 21:03:12,375 [INFO] Summary:
2019-04-07 21:03:12,375 [INFO] Batch 91000, worst loss 0.007533 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 21:03:12,376 [INFO] Regularization: 1419.798218 * 0.0000000100 = 0.0000141980 loss
2019-04-07 21:03:12,376 [INFO] unfolding 0, single step 91001
2019-04-07 21:03:12,377 [INFO] Sum of grad norms of most recent batch: 0.035314
2019-04-07 21:03:12,378 [INFO] ---------------------------------
2019-04-07 21:03:33,399 [INFO] ---------------------------------
2019-04-07 21:03:33,400 [INFO] Summary:
2019-04-07 21:03:33,400 [INFO] Batch 92000, worst loss 0.007778 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 21:03:33,401 [INFO] Regularization: 1419.401123 * 0.0000000100 = 0.0000141940 loss
2019-04-07 21:03:33,401 [INFO] unfolding 0, single step 92001
2019-04-07 21:03:33,402 [INFO] Sum of grad norms of most recent batch: 0.064241
2019-04-07 21:03:33,402 [INFO] ---------------------------------
2019-04-07 21:03:54,613 [INFO] ---------------------------------
2019-04-07 21:03:54,614 [INFO] Summary:
2019-04-07 21:03:54,614 [INFO] Batch 93000, worst loss 0.005501 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 21:03:54,615 [INFO] Regularization: 1418.990112 * 0.0000000100 = 0.0000141899 loss
2019-04-07 21:03:54,615 [INFO] unfolding 0, single step 93001
2019-04-07 21:03:54,616 [INFO] Sum of grad norms of most recent batch: 0.061334
2019-04-07 21:03:54,616 [INFO] ---------------------------------
2019-04-07 21:04:16,107 [INFO] ---------------------------------
2019-04-07 21:04:16,108 [INFO] Summary:
2019-04-07 21:04:16,108 [INFO] Batch 94000, worst loss 0.006158 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 21:04:16,109 [INFO] Regularization: 1418.696533 * 0.0000000100 = 0.0000141870 loss
2019-04-07 21:04:16,109 [INFO] unfolding 0, single step 94001
2019-04-07 21:04:16,110 [INFO] Sum of grad norms of most recent batch: 0.068696
2019-04-07 21:04:16,110 [INFO] ---------------------------------
2019-04-07 21:04:36,646 [INFO] ---------------------------------
2019-04-07 21:04:36,647 [INFO] Summary:
2019-04-07 21:04:36,648 [INFO] Batch 95000, worst loss 0.006174 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 21:04:36,648 [INFO] Regularization: 1418.389160 * 0.0000000100 = 0.0000141839 loss
2019-04-07 21:04:36,649 [INFO] unfolding 0, single step 95001
2019-04-07 21:04:36,650 [INFO] Sum of grad norms of most recent batch: 0.157131
2019-04-07 21:04:36,650 [INFO] ---------------------------------
2019-04-07 21:04:57,018 [INFO] ---------------------------------
2019-04-07 21:04:57,019 [INFO] Summary:
2019-04-07 21:04:57,019 [INFO] Batch 96000, worst loss 0.031600 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 21:04:57,020 [INFO] Regularization: 1419.338989 * 0.0000000100 = 0.0000141934 loss
2019-04-07 21:04:57,020 [INFO] unfolding 0, single step 96001
2019-04-07 21:04:57,021 [INFO] Sum of grad norms of most recent batch: 0.027065
2019-04-07 21:04:57,022 [INFO] ---------------------------------
2019-04-07 21:05:17,411 [INFO] ---------------------------------
2019-04-07 21:05:17,412 [INFO] Summary:
2019-04-07 21:05:17,413 [INFO] Batch 97000, worst loss 0.008279 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 21:05:17,414 [INFO] Regularization: 1418.802979 * 0.0000000100 = 0.0000141880 loss
2019-04-07 21:05:17,414 [INFO] unfolding 0, single step 97001
2019-04-07 21:05:17,415 [INFO] Sum of grad norms of most recent batch: 0.082288
2019-04-07 21:05:17,416 [INFO] ---------------------------------
2019-04-07 21:05:38,470 [INFO] ---------------------------------
2019-04-07 21:05:38,471 [INFO] Summary:
2019-04-07 21:05:38,472 [INFO] Batch 98000, worst loss 0.014803 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 21:05:38,472 [INFO] Regularization: 1418.315796 * 0.0000000100 = 0.0000141832 loss
2019-04-07 21:05:38,473 [INFO] unfolding 0, single step 98001
2019-04-07 21:05:38,473 [INFO] Sum of grad norms of most recent batch: 0.042205
2019-04-07 21:05:38,474 [INFO] ---------------------------------
2019-04-07 21:05:59,172 [INFO] ---------------------------------
2019-04-07 21:05:59,173 [INFO] Summary:
2019-04-07 21:05:59,173 [INFO] Batch 99000, worst loss 0.006377 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 21:05:59,174 [INFO] Regularization: 1417.790527 * 0.0000000100 = 0.0000141779 loss
2019-04-07 21:05:59,174 [INFO] unfolding 0, single step 99001
2019-04-07 21:05:59,175 [INFO] Sum of grad norms of most recent batch: 0.026228
2019-04-07 21:05:59,175 [INFO] ---------------------------------
2019-04-07 21:06:20,427 [INFO] ---------------------------------
2019-04-07 21:06:20,428 [INFO] Summary:
2019-04-07 21:06:20,428 [INFO] Batch 100000, worst loss 0.010691 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 21:06:20,429 [INFO] Regularization: 1417.794067 * 0.0000000100 = 0.0000141779 loss
2019-04-07 21:06:20,429 [INFO] unfolding 0, single step 100001
2019-04-07 21:06:20,430 [INFO] Sum of grad norms of most recent batch: 0.214682
2019-04-07 21:06:20,430 [INFO] ---------------------------------
2019-04-07 21:06:57,800 [INFO] ---------------------------------
2019-04-07 21:06:57,801 [INFO] Evaluation:
2019-04-07 21:06:57,802 [INFO] Batch 100000, worst loss 0.018092 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:06:57,802 [INFO] ---------------------------------
2019-04-07 21:07:18,788 [INFO] ---------------------------------
2019-04-07 21:07:18,789 [INFO] Summary:
2019-04-07 21:07:18,789 [INFO] Batch 101000, worst loss 0.006218 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 21:07:18,790 [INFO] Regularization: 1417.307739 * 0.0000000100 = 0.0000141731 loss
2019-04-07 21:07:18,791 [INFO] unfolding 0, single step 101001
2019-04-07 21:07:18,792 [INFO] Sum of grad norms of most recent batch: 0.016820
2019-04-07 21:07:18,793 [INFO] ---------------------------------
2019-04-07 21:07:40,107 [INFO] ---------------------------------
2019-04-07 21:07:40,108 [INFO] Summary:
2019-04-07 21:07:40,109 [INFO] Batch 102000, worst loss 0.008413 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 21:07:40,109 [INFO] Regularization: 1416.942261 * 0.0000000100 = 0.0000141694 loss
2019-04-07 21:07:40,109 [INFO] unfolding 0, single step 102001
2019-04-07 21:07:40,110 [INFO] Sum of grad norms of most recent batch: 0.084821
2019-04-07 21:07:40,110 [INFO] ---------------------------------
2019-04-07 21:08:01,166 [INFO] ---------------------------------
2019-04-07 21:08:01,167 [INFO] Summary:
2019-04-07 21:08:01,168 [INFO] Batch 103000, worst loss 0.007194 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 21:08:01,168 [INFO] Regularization: 1416.887085 * 0.0000000100 = 0.0000141689 loss
2019-04-07 21:08:01,169 [INFO] unfolding 0, single step 103001
2019-04-07 21:08:01,169 [INFO] Sum of grad norms of most recent batch: 0.071583
2019-04-07 21:08:01,170 [INFO] ---------------------------------
2019-04-07 21:08:22,037 [INFO] ---------------------------------
2019-04-07 21:08:22,038 [INFO] Summary:
2019-04-07 21:08:22,039 [INFO] Batch 104000, worst loss 0.007492 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 21:08:22,040 [INFO] Regularization: 1416.641846 * 0.0000000100 = 0.0000141664 loss
2019-04-07 21:08:22,040 [INFO] unfolding 0, single step 104001
2019-04-07 21:08:22,041 [INFO] Sum of grad norms of most recent batch: 0.046405
2019-04-07 21:08:22,041 [INFO] ---------------------------------
2019-04-07 21:08:43,747 [INFO] ---------------------------------
2019-04-07 21:08:43,748 [INFO] Summary:
2019-04-07 21:08:43,749 [INFO] Batch 105000, worst loss 0.006928 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 21:08:43,749 [INFO] Regularization: 1416.789307 * 0.0000000100 = 0.0000141679 loss
2019-04-07 21:08:43,750 [INFO] unfolding 0, single step 105001
2019-04-07 21:08:43,752 [INFO] Sum of grad norms of most recent batch: 0.054592
2019-04-07 21:08:43,752 [INFO] ---------------------------------
2019-04-07 21:09:04,377 [INFO] ---------------------------------
2019-04-07 21:09:04,379 [INFO] Summary:
2019-04-07 21:09:04,380 [INFO] Batch 106000, worst loss 0.002137 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 21:09:04,380 [INFO] Regularization: 1416.436646 * 0.0000000100 = 0.0000141644 loss
2019-04-07 21:09:04,381 [INFO] unfolding 0, single step 106001
2019-04-07 21:09:04,382 [INFO] Sum of grad norms of most recent batch: 1.499763
2019-04-07 21:09:04,382 [INFO] ---------------------------------
2019-04-07 21:09:41,957 [INFO] ---------------------------------
2019-04-07 21:09:41,958 [INFO] Evaluation:
2019-04-07 21:09:41,959 [INFO] Batch 106000, worst loss 0.010783 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:09:41,959 [INFO] ---------------------------------
2019-04-07 21:10:03,493 [INFO] ---------------------------------
2019-04-07 21:10:03,494 [INFO] Summary:
2019-04-07 21:10:03,495 [INFO] Batch 107000, worst loss 0.004321 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 21:10:03,495 [INFO] Regularization: 1416.304932 * 0.0000000100 = 0.0000141630 loss
2019-04-07 21:10:03,495 [INFO] unfolding 0, single step 107001
2019-04-07 21:10:03,496 [INFO] Sum of grad norms of most recent batch: 0.845628
2019-04-07 21:10:03,497 [INFO] ---------------------------------
2019-04-07 21:10:41,090 [INFO] ---------------------------------
2019-04-07 21:10:41,091 [INFO] Evaluation:
2019-04-07 21:10:41,092 [INFO] Batch 107000, worst loss 0.014366 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:10:41,092 [INFO] ---------------------------------
2019-04-07 21:11:02,373 [INFO] ---------------------------------
2019-04-07 21:11:02,374 [INFO] Summary:
2019-04-07 21:11:02,374 [INFO] Batch 108000, worst loss 0.009494 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 21:11:02,375 [INFO] Regularization: 1416.219482 * 0.0000000100 = 0.0000141622 loss
2019-04-07 21:11:02,375 [INFO] unfolding 0, single step 108001
2019-04-07 21:11:02,376 [INFO] Sum of grad norms of most recent batch: 0.017777
2019-04-07 21:11:02,377 [INFO] ---------------------------------
2019-04-07 21:11:23,157 [INFO] ---------------------------------
2019-04-07 21:11:23,158 [INFO] Summary:
2019-04-07 21:11:23,158 [INFO] Batch 109000, worst loss 0.008769 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 21:11:23,159 [INFO] Regularization: 1416.406494 * 0.0000000100 = 0.0000141641 loss
2019-04-07 21:11:23,159 [INFO] unfolding 0, single step 109001
2019-04-07 21:11:23,160 [INFO] Sum of grad norms of most recent batch: 0.032706
2019-04-07 21:11:23,160 [INFO] ---------------------------------
2019-04-07 21:11:44,386 [INFO] ---------------------------------
2019-04-07 21:11:44,387 [INFO] Summary:
2019-04-07 21:11:44,387 [INFO] Batch 110000, worst loss 0.011478 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 21:11:44,388 [INFO] Regularization: 1416.256958 * 0.0000000100 = 0.0000141626 loss
2019-04-07 21:11:44,388 [INFO] unfolding 0, single step 110001
2019-04-07 21:11:44,389 [INFO] Sum of grad norms of most recent batch: 0.075671
2019-04-07 21:11:44,389 [INFO] ---------------------------------
2019-04-07 21:12:21,663 [INFO] ---------------------------------
2019-04-07 21:12:21,665 [INFO] Evaluation:
2019-04-07 21:12:21,665 [INFO] Batch 110000, worst loss 0.014781 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:12:21,666 [INFO] ---------------------------------
2019-04-07 21:12:42,413 [INFO] ---------------------------------
2019-04-07 21:12:42,413 [INFO] Summary:
2019-04-07 21:12:42,414 [INFO] Batch 111000, worst loss 0.002596 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 21:12:42,414 [INFO] Regularization: 1416.256348 * 0.0000000100 = 0.0000141626 loss
2019-04-07 21:12:42,415 [INFO] unfolding 0, single step 111001
2019-04-07 21:12:42,415 [INFO] Sum of grad norms of most recent batch: 0.122537
2019-04-07 21:12:42,416 [INFO] ---------------------------------
2019-04-07 21:13:19,695 [INFO] ---------------------------------
2019-04-07 21:13:19,696 [INFO] Evaluation:
2019-04-07 21:13:19,697 [INFO] Batch 111000, worst loss 0.020898 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:13:19,697 [INFO] ---------------------------------
2019-04-07 21:13:40,399 [INFO] ---------------------------------
2019-04-07 21:13:40,400 [INFO] Summary:
2019-04-07 21:13:40,401 [INFO] Batch 112000, worst loss 0.008558 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 21:13:40,401 [INFO] Regularization: 1416.139282 * 0.0000000100 = 0.0000141614 loss
2019-04-07 21:13:40,402 [INFO] unfolding 0, single step 112001
2019-04-07 21:13:40,402 [INFO] Sum of grad norms of most recent batch: 0.065580
2019-04-07 21:13:40,403 [INFO] ---------------------------------
2019-04-07 21:14:01,317 [INFO] ---------------------------------
2019-04-07 21:14:01,318 [INFO] Summary:
2019-04-07 21:14:01,319 [INFO] Batch 113000, worst loss 0.018475 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 21:14:01,319 [INFO] Regularization: 1415.974243 * 0.0000000100 = 0.0000141597 loss
2019-04-07 21:14:01,320 [INFO] unfolding 0, single step 113001
2019-04-07 21:14:01,320 [INFO] Sum of grad norms of most recent batch: 0.281100
2019-04-07 21:14:01,321 [INFO] ---------------------------------
2019-04-07 21:14:23,609 [INFO] ---------------------------------
2019-04-07 21:14:23,610 [INFO] Summary:
2019-04-07 21:14:23,611 [INFO] Batch 114000, worst loss 0.005326 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 21:14:23,611 [INFO] Regularization: 1415.937744 * 0.0000000100 = 0.0000141594 loss
2019-04-07 21:14:23,611 [INFO] unfolding 0, single step 114001
2019-04-07 21:14:23,612 [INFO] Sum of grad norms of most recent batch: 0.033152
2019-04-07 21:14:23,613 [INFO] ---------------------------------
2019-04-07 21:14:44,672 [INFO] ---------------------------------
2019-04-07 21:14:44,673 [INFO] Summary:
2019-04-07 21:14:44,674 [INFO] Batch 115000, worst loss 0.006218 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 21:14:44,675 [INFO] Regularization: 1415.884155 * 0.0000000100 = 0.0000141588 loss
2019-04-07 21:14:44,675 [INFO] unfolding 0, single step 115001
2019-04-07 21:14:44,676 [INFO] Sum of grad norms of most recent batch: 0.022755
2019-04-07 21:14:44,677 [INFO] ---------------------------------
2019-04-07 21:15:05,778 [INFO] ---------------------------------
2019-04-07 21:15:05,780 [INFO] Summary:
2019-04-07 21:15:05,780 [INFO] Batch 116000, worst loss 0.015810 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 21:15:05,781 [INFO] Regularization: 1415.843262 * 0.0000000100 = 0.0000141584 loss
2019-04-07 21:15:05,781 [INFO] unfolding 0, single step 116001
2019-04-07 21:15:05,781 [INFO] Sum of grad norms of most recent batch: 0.031600
2019-04-07 21:15:05,782 [INFO] ---------------------------------
2019-04-07 21:15:26,607 [INFO] ---------------------------------
2019-04-07 21:15:26,608 [INFO] Summary:
2019-04-07 21:15:26,609 [INFO] Batch 117000, worst loss 0.005166 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 21:15:26,609 [INFO] Regularization: 1415.843506 * 0.0000000100 = 0.0000141584 loss
2019-04-07 21:15:26,609 [INFO] unfolding 0, single step 117001
2019-04-07 21:15:26,610 [INFO] Sum of grad norms of most recent batch: 0.023794
2019-04-07 21:15:26,610 [INFO] ---------------------------------
2019-04-07 21:15:48,242 [INFO] ---------------------------------
2019-04-07 21:15:48,243 [INFO] Summary:
2019-04-07 21:15:48,244 [INFO] Batch 118000, worst loss 0.008227 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 21:15:48,244 [INFO] Regularization: 1415.876099 * 0.0000000100 = 0.0000141588 loss
2019-04-07 21:15:48,245 [INFO] unfolding 0, single step 118001
2019-04-07 21:15:48,246 [INFO] Sum of grad norms of most recent batch: 0.021101
2019-04-07 21:15:48,246 [INFO] ---------------------------------
2019-04-07 21:16:09,345 [INFO] ---------------------------------
2019-04-07 21:16:09,346 [INFO] Summary:
2019-04-07 21:16:09,346 [INFO] Batch 119000, worst loss 0.006068 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 21:16:09,347 [INFO] Regularization: 1415.887573 * 0.0000000100 = 0.0000141589 loss
2019-04-07 21:16:09,347 [INFO] unfolding 0, single step 119001
2019-04-07 21:16:09,348 [INFO] Sum of grad norms of most recent batch: 0.015470
2019-04-07 21:16:09,348 [INFO] ---------------------------------
2019-04-07 21:16:31,078 [INFO] ---------------------------------
2019-04-07 21:16:31,078 [INFO] Summary:
2019-04-07 21:16:31,080 [INFO] Batch 120000, worst loss 0.007221 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 21:16:31,080 [INFO] Regularization: 1415.669067 * 0.0000000100 = 0.0000141567 loss
2019-04-07 21:16:31,081 [INFO] unfolding 0, single step 120001
2019-04-07 21:16:31,081 [INFO] Sum of grad norms of most recent batch: 0.014514
2019-04-07 21:16:31,082 [INFO] ---------------------------------
2019-04-07 21:17:08,424 [INFO] ---------------------------------
2019-04-07 21:17:08,425 [INFO] Evaluation:
2019-04-07 21:17:08,426 [INFO] Batch 120000, worst loss 0.017192 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:17:08,426 [INFO] ---------------------------------
2019-04-07 21:17:29,484 [INFO] ---------------------------------
2019-04-07 21:17:29,485 [INFO] Summary:
2019-04-07 21:17:29,486 [INFO] Batch 121000, worst loss 0.007108 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 21:17:29,486 [INFO] Regularization: 1415.633667 * 0.0000000100 = 0.0000141563 loss
2019-04-07 21:17:29,487 [INFO] unfolding 0, single step 121001
2019-04-07 21:17:29,487 [INFO] Sum of grad norms of most recent batch: 0.029714
2019-04-07 21:17:29,488 [INFO] ---------------------------------
2019-04-07 21:17:50,321 [INFO] ---------------------------------
2019-04-07 21:17:50,322 [INFO] Summary:
2019-04-07 21:17:50,322 [INFO] Batch 122000, worst loss 0.006979 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 21:17:50,323 [INFO] Regularization: 1415.573853 * 0.0000000100 = 0.0000141557 loss
2019-04-07 21:17:50,323 [INFO] unfolding 0, single step 122001
2019-04-07 21:17:50,324 [INFO] Sum of grad norms of most recent batch: 0.026888
2019-04-07 21:17:50,324 [INFO] ---------------------------------
2019-04-07 21:18:10,909 [INFO] ---------------------------------
2019-04-07 21:18:10,909 [INFO] Summary:
2019-04-07 21:18:10,910 [INFO] Batch 123000, worst loss 0.016198 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 21:18:10,910 [INFO] Regularization: 1415.492920 * 0.0000000100 = 0.0000141549 loss
2019-04-07 21:18:10,911 [INFO] unfolding 0, single step 123001
2019-04-07 21:18:10,911 [INFO] Sum of grad norms of most recent batch: 0.019216
2019-04-07 21:18:10,912 [INFO] ---------------------------------
2019-04-07 21:18:32,713 [INFO] ---------------------------------
2019-04-07 21:18:32,714 [INFO] Summary:
2019-04-07 21:18:32,715 [INFO] Batch 124000, worst loss 0.009928 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 21:18:32,716 [INFO] Regularization: 1415.478149 * 0.0000000100 = 0.0000141548 loss
2019-04-07 21:18:32,716 [INFO] unfolding 0, single step 124001
2019-04-07 21:18:32,717 [INFO] Sum of grad norms of most recent batch: 0.035320
2019-04-07 21:18:32,718 [INFO] ---------------------------------
2019-04-07 21:18:54,099 [INFO] ---------------------------------
2019-04-07 21:18:54,100 [INFO] Summary:
2019-04-07 21:18:54,101 [INFO] Batch 125000, worst loss 0.008852 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 21:18:54,101 [INFO] Regularization: 1415.433838 * 0.0000000100 = 0.0000141543 loss
2019-04-07 21:18:54,101 [INFO] unfolding 0, single step 125001
2019-04-07 21:18:54,102 [INFO] Sum of grad norms of most recent batch: 0.041885
2019-04-07 21:18:54,102 [INFO] ---------------------------------
2019-04-07 21:19:15,142 [INFO] ---------------------------------
2019-04-07 21:19:15,143 [INFO] Summary:
2019-04-07 21:19:15,143 [INFO] Batch 126000, worst loss 0.003966 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 21:19:15,144 [INFO] Regularization: 1415.391479 * 0.0000000100 = 0.0000141539 loss
2019-04-07 21:19:15,144 [INFO] unfolding 0, single step 126001
2019-04-07 21:19:15,145 [INFO] Sum of grad norms of most recent batch: 0.024698
2019-04-07 21:19:15,145 [INFO] ---------------------------------
2019-04-07 21:19:52,417 [INFO] ---------------------------------
2019-04-07 21:19:52,418 [INFO] Evaluation:
2019-04-07 21:19:52,418 [INFO] Batch 126000, worst loss 0.013729 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:19:52,419 [INFO] ---------------------------------
2019-04-07 21:20:13,462 [INFO] ---------------------------------
2019-04-07 21:20:13,464 [INFO] Summary:
2019-04-07 21:20:13,464 [INFO] Batch 127000, worst loss 0.007282 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 21:20:13,465 [INFO] Regularization: 1415.305786 * 0.0000000100 = 0.0000141531 loss
2019-04-07 21:20:13,465 [INFO] unfolding 0, single step 127001
2019-04-07 21:20:13,465 [INFO] Sum of grad norms of most recent batch: 0.031778
2019-04-07 21:20:13,466 [INFO] ---------------------------------
2019-04-07 21:20:35,472 [INFO] ---------------------------------
2019-04-07 21:20:35,473 [INFO] Summary:
2019-04-07 21:20:35,474 [INFO] Batch 128000, worst loss 0.002474 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 21:20:35,475 [INFO] Regularization: 1415.324219 * 0.0000000100 = 0.0000141532 loss
2019-04-07 21:20:35,476 [INFO] unfolding 0, single step 128001
2019-04-07 21:20:35,477 [INFO] Sum of grad norms of most recent batch: 0.063024
2019-04-07 21:20:35,477 [INFO] ---------------------------------
2019-04-07 21:21:12,783 [INFO] ---------------------------------
2019-04-07 21:21:12,784 [INFO] Evaluation:
2019-04-07 21:21:12,784 [INFO] Batch 128000, worst loss 0.016249 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:21:12,785 [INFO] ---------------------------------
2019-04-07 21:21:33,904 [INFO] ---------------------------------
2019-04-07 21:21:33,905 [INFO] Summary:
2019-04-07 21:21:33,906 [INFO] Batch 129000, worst loss 0.002809 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 21:21:33,906 [INFO] Regularization: 1415.211670 * 0.0000000100 = 0.0000141521 loss
2019-04-07 21:21:33,906 [INFO] unfolding 0, single step 129001
2019-04-07 21:21:33,907 [INFO] Sum of grad norms of most recent batch: 0.031714
2019-04-07 21:21:33,907 [INFO] ---------------------------------
2019-04-07 21:22:11,508 [INFO] ---------------------------------
2019-04-07 21:22:11,509 [INFO] Evaluation:
2019-04-07 21:22:11,509 [INFO] Batch 129000, worst loss 0.029809 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:22:11,510 [INFO] ---------------------------------
2019-04-07 21:22:32,642 [INFO] ---------------------------------
2019-04-07 21:22:32,643 [INFO] Summary:
2019-04-07 21:22:32,644 [INFO] Batch 130000, worst loss 0.003934 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 21:22:32,644 [INFO] Regularization: 1415.148193 * 0.0000000100 = 0.0000141515 loss
2019-04-07 21:22:32,645 [INFO] unfolding 0, single step 130001
2019-04-07 21:22:32,645 [INFO] Sum of grad norms of most recent batch: 0.027800
2019-04-07 21:22:32,646 [INFO] ---------------------------------
2019-04-07 21:23:10,058 [INFO] ---------------------------------
2019-04-07 21:23:10,059 [INFO] Evaluation:
2019-04-07 21:23:10,059 [INFO] Batch 130000, worst loss 0.011117 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:23:10,060 [INFO] ---------------------------------
2019-04-07 21:23:31,534 [INFO] ---------------------------------
2019-04-07 21:23:31,535 [INFO] Summary:
2019-04-07 21:23:31,535 [INFO] Batch 131000, worst loss 0.006333 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:23:31,536 [INFO] Regularization: 1415.117065 * 0.0000000100 = 0.0000141512 loss
2019-04-07 21:23:31,537 [INFO] unfolding 0, single step 131001
2019-04-07 21:23:31,537 [INFO] Sum of grad norms of most recent batch: 1.533693
2019-04-07 21:23:31,538 [INFO] ---------------------------------
2019-04-07 21:23:52,896 [INFO] ---------------------------------
2019-04-07 21:23:52,897 [INFO] Summary:
2019-04-07 21:23:52,897 [INFO] Batch 132000, worst loss 0.012877 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:23:52,898 [INFO] Regularization: 1415.088623 * 0.0000000100 = 0.0000141509 loss
2019-04-07 21:23:52,898 [INFO] unfolding 0, single step 132001
2019-04-07 21:23:52,899 [INFO] Sum of grad norms of most recent batch: 0.021103
2019-04-07 21:23:52,900 [INFO] ---------------------------------
2019-04-07 21:24:14,039 [INFO] ---------------------------------
2019-04-07 21:24:14,040 [INFO] Summary:
2019-04-07 21:24:14,040 [INFO] Batch 133000, worst loss 0.012614 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:24:14,041 [INFO] Regularization: 1415.098267 * 0.0000000100 = 0.0000141510 loss
2019-04-07 21:24:14,041 [INFO] unfolding 0, single step 133001
2019-04-07 21:24:14,042 [INFO] Sum of grad norms of most recent batch: 0.022197
2019-04-07 21:24:14,042 [INFO] ---------------------------------
2019-04-07 21:24:35,189 [INFO] ---------------------------------
2019-04-07 21:24:35,189 [INFO] Summary:
2019-04-07 21:24:35,190 [INFO] Batch 134000, worst loss 0.004800 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:24:35,191 [INFO] Regularization: 1415.091919 * 0.0000000100 = 0.0000141509 loss
2019-04-07 21:24:35,191 [INFO] unfolding 0, single step 134001
2019-04-07 21:24:35,192 [INFO] Sum of grad norms of most recent batch: 0.018392
2019-04-07 21:24:35,192 [INFO] ---------------------------------
2019-04-07 21:25:12,547 [INFO] ---------------------------------
2019-04-07 21:25:12,548 [INFO] Evaluation:
2019-04-07 21:25:12,549 [INFO] Batch 134000, worst loss 0.013414 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:25:12,550 [INFO] ---------------------------------
2019-04-07 21:25:34,268 [INFO] ---------------------------------
2019-04-07 21:25:34,269 [INFO] Summary:
2019-04-07 21:25:34,269 [INFO] Batch 135000, worst loss 0.013887 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:25:34,270 [INFO] Regularization: 1415.094604 * 0.0000000100 = 0.0000141509 loss
2019-04-07 21:25:34,270 [INFO] unfolding 0, single step 135001
2019-04-07 21:25:34,271 [INFO] Sum of grad norms of most recent batch: 0.014652
2019-04-07 21:25:34,271 [INFO] ---------------------------------
2019-04-07 21:25:55,802 [INFO] ---------------------------------
2019-04-07 21:25:55,803 [INFO] Summary:
2019-04-07 21:25:55,803 [INFO] Batch 136000, worst loss 0.003095 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:25:55,804 [INFO] Regularization: 1415.035522 * 0.0000000100 = 0.0000141504 loss
2019-04-07 21:25:55,804 [INFO] unfolding 0, single step 136001
2019-04-07 21:25:55,805 [INFO] Sum of grad norms of most recent batch: 0.022027
2019-04-07 21:25:55,805 [INFO] ---------------------------------
2019-04-07 21:26:33,246 [INFO] ---------------------------------
2019-04-07 21:26:33,247 [INFO] Evaluation:
2019-04-07 21:26:33,248 [INFO] Batch 136000, worst loss 0.016272 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:26:33,248 [INFO] ---------------------------------
2019-04-07 21:26:53,835 [INFO] ---------------------------------
2019-04-07 21:26:53,836 [INFO] Summary:
2019-04-07 21:26:53,837 [INFO] Batch 137000, worst loss 0.003163 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:26:53,837 [INFO] Regularization: 1415.013916 * 0.0000000100 = 0.0000141501 loss
2019-04-07 21:26:53,838 [INFO] unfolding 0, single step 137001
2019-04-07 21:26:53,838 [INFO] Sum of grad norms of most recent batch: 0.024982
2019-04-07 21:26:53,839 [INFO] ---------------------------------
2019-04-07 21:27:31,211 [INFO] ---------------------------------
2019-04-07 21:27:31,212 [INFO] Evaluation:
2019-04-07 21:27:31,213 [INFO] Batch 137000, worst loss 0.010914 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:27:31,213 [INFO] ---------------------------------
2019-04-07 21:27:51,824 [INFO] ---------------------------------
2019-04-07 21:27:51,825 [INFO] Summary:
2019-04-07 21:27:51,826 [INFO] Batch 138000, worst loss 0.006300 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:27:51,826 [INFO] Regularization: 1414.977905 * 0.0000000100 = 0.0000141498 loss
2019-04-07 21:27:51,827 [INFO] unfolding 0, single step 138001
2019-04-07 21:27:51,828 [INFO] Sum of grad norms of most recent batch: 0.020171
2019-04-07 21:27:51,829 [INFO] ---------------------------------
2019-04-07 21:28:12,638 [INFO] ---------------------------------
2019-04-07 21:28:12,639 [INFO] Summary:
2019-04-07 21:28:12,639 [INFO] Batch 139000, worst loss 0.006024 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:28:12,640 [INFO] Regularization: 1414.962769 * 0.0000000100 = 0.0000141496 loss
2019-04-07 21:28:12,640 [INFO] unfolding 0, single step 139001
2019-04-07 21:28:12,640 [INFO] Sum of grad norms of most recent batch: 0.016455
2019-04-07 21:28:12,641 [INFO] ---------------------------------
2019-04-07 21:28:34,386 [INFO] ---------------------------------
2019-04-07 21:28:34,387 [INFO] Summary:
2019-04-07 21:28:34,388 [INFO] Batch 140000, worst loss 0.002314 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:28:34,388 [INFO] Regularization: 1414.932983 * 0.0000000100 = 0.0000141493 loss
2019-04-07 21:28:34,388 [INFO] unfolding 0, single step 140001
2019-04-07 21:28:34,389 [INFO] Sum of grad norms of most recent batch: 0.036578
2019-04-07 21:28:34,390 [INFO] ---------------------------------
2019-04-07 21:29:11,831 [INFO] ---------------------------------
2019-04-07 21:29:11,832 [INFO] Evaluation:
2019-04-07 21:29:11,832 [INFO] Batch 140000, worst loss 0.016280 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:29:11,833 [INFO] ---------------------------------
2019-04-07 21:29:32,608 [INFO] ---------------------------------
2019-04-07 21:29:32,609 [INFO] Summary:
2019-04-07 21:29:32,609 [INFO] Batch 141000, worst loss 0.005187 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:29:32,610 [INFO] Regularization: 1414.928101 * 0.0000000100 = 0.0000141493 loss
2019-04-07 21:29:32,610 [INFO] unfolding 0, single step 141001
2019-04-07 21:29:32,611 [INFO] Sum of grad norms of most recent batch: 0.022757
2019-04-07 21:29:32,611 [INFO] ---------------------------------
2019-04-07 21:29:53,647 [INFO] ---------------------------------
2019-04-07 21:29:53,648 [INFO] Summary:
2019-04-07 21:29:53,649 [INFO] Batch 142000, worst loss 0.007859 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:29:53,649 [INFO] Regularization: 1414.943726 * 0.0000000100 = 0.0000141494 loss
2019-04-07 21:29:53,650 [INFO] unfolding 0, single step 142001
2019-04-07 21:29:53,651 [INFO] Sum of grad norms of most recent batch: 0.028254
2019-04-07 21:29:53,651 [INFO] ---------------------------------
2019-04-07 21:30:15,074 [INFO] ---------------------------------
2019-04-07 21:30:15,075 [INFO] Summary:
2019-04-07 21:30:15,076 [INFO] Batch 143000, worst loss 0.004476 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:30:15,077 [INFO] Regularization: 1414.937378 * 0.0000000100 = 0.0000141494 loss
2019-04-07 21:30:15,077 [INFO] unfolding 0, single step 143001
2019-04-07 21:30:15,078 [INFO] Sum of grad norms of most recent batch: 0.082328
2019-04-07 21:30:15,079 [INFO] ---------------------------------
2019-04-07 21:30:52,571 [INFO] ---------------------------------
2019-04-07 21:30:52,572 [INFO] Evaluation:
2019-04-07 21:30:52,572 [INFO] Batch 143000, worst loss 0.032975 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:30:52,573 [INFO] ---------------------------------
2019-04-07 21:31:13,820 [INFO] ---------------------------------
2019-04-07 21:31:13,821 [INFO] Summary:
2019-04-07 21:31:13,821 [INFO] Batch 144000, worst loss 0.007383 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:31:13,822 [INFO] Regularization: 1414.921875 * 0.0000000100 = 0.0000141492 loss
2019-04-07 21:31:13,822 [INFO] unfolding 0, single step 144001
2019-04-07 21:31:13,823 [INFO] Sum of grad norms of most recent batch: 0.033990
2019-04-07 21:31:13,823 [INFO] ---------------------------------
2019-04-07 21:31:35,440 [INFO] ---------------------------------
2019-04-07 21:31:35,441 [INFO] Summary:
2019-04-07 21:31:35,442 [INFO] Batch 145000, worst loss 0.007265 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:31:35,443 [INFO] Regularization: 1414.977051 * 0.0000000100 = 0.0000141498 loss
2019-04-07 21:31:35,443 [INFO] unfolding 0, single step 145001
2019-04-07 21:31:35,444 [INFO] Sum of grad norms of most recent batch: 0.023572
2019-04-07 21:31:35,445 [INFO] ---------------------------------
2019-04-07 21:31:56,371 [INFO] ---------------------------------
2019-04-07 21:31:56,372 [INFO] Summary:
2019-04-07 21:31:56,372 [INFO] Batch 146000, worst loss 0.016869 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:31:56,373 [INFO] Regularization: 1414.946045 * 0.0000000100 = 0.0000141495 loss
2019-04-07 21:31:56,374 [INFO] unfolding 0, single step 146001
2019-04-07 21:31:56,375 [INFO] Sum of grad norms of most recent batch: 0.038034
2019-04-07 21:31:56,375 [INFO] ---------------------------------
2019-04-07 21:32:17,481 [INFO] ---------------------------------
2019-04-07 21:32:17,482 [INFO] Summary:
2019-04-07 21:32:17,482 [INFO] Batch 147000, worst loss 0.004130 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:32:17,483 [INFO] Regularization: 1414.931030 * 0.0000000100 = 0.0000141493 loss
2019-04-07 21:32:17,483 [INFO] unfolding 0, single step 147001
2019-04-07 21:32:17,484 [INFO] Sum of grad norms of most recent batch: 0.016351
2019-04-07 21:32:17,484 [INFO] ---------------------------------
2019-04-07 21:32:54,824 [INFO] ---------------------------------
2019-04-07 21:32:54,825 [INFO] Evaluation:
2019-04-07 21:32:54,825 [INFO] Batch 147000, worst loss 0.040042 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:32:54,826 [INFO] ---------------------------------
2019-04-07 21:33:15,878 [INFO] ---------------------------------
2019-04-07 21:33:15,879 [INFO] Summary:
2019-04-07 21:33:15,879 [INFO] Batch 148000, worst loss 0.002970 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:33:15,880 [INFO] Regularization: 1414.909180 * 0.0000000100 = 0.0000141491 loss
2019-04-07 21:33:15,880 [INFO] unfolding 0, single step 148001
2019-04-07 21:33:15,881 [INFO] Sum of grad norms of most recent batch: 0.023567
2019-04-07 21:33:15,881 [INFO] ---------------------------------
2019-04-07 21:33:53,463 [INFO] ---------------------------------
2019-04-07 21:33:53,464 [INFO] Evaluation:
2019-04-07 21:33:53,465 [INFO] Batch 148000, worst loss 0.014056 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:33:53,465 [INFO] ---------------------------------
2019-04-07 21:34:13,870 [INFO] ---------------------------------
2019-04-07 21:34:13,871 [INFO] Summary:
2019-04-07 21:34:13,871 [INFO] Batch 149000, worst loss 0.005650 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:34:13,872 [INFO] Regularization: 1414.882202 * 0.0000000100 = 0.0000141488 loss
2019-04-07 21:34:13,872 [INFO] unfolding 0, single step 149001
2019-04-07 21:34:13,873 [INFO] Sum of grad norms of most recent batch: 0.023081
2019-04-07 21:34:13,873 [INFO] ---------------------------------
2019-04-07 21:34:34,811 [INFO] ---------------------------------
2019-04-07 21:34:34,812 [INFO] Summary:
2019-04-07 21:34:34,812 [INFO] Batch 150000, worst loss 0.011483 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 21:34:34,813 [INFO] Regularization: 1414.894653 * 0.0000000100 = 0.0000141489 loss
2019-04-07 21:34:34,814 [INFO] unfolding 0, single step 150001
2019-04-07 21:34:34,814 [INFO] Sum of grad norms of most recent batch: 0.023579
2019-04-07 21:34:34,815 [INFO] ---------------------------------
2019-04-07 21:35:12,116 [INFO] ---------------------------------
2019-04-07 21:35:12,117 [INFO] Evaluation:
2019-04-07 21:35:12,117 [INFO] Batch 150000, worst loss 0.012855 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:35:12,118 [INFO] ---------------------------------
2019-04-07 21:35:12,119 [INFO] Finished training, saved to file transition/1554653688/1554665712_2_transition_final.pth
2019-04-07 21:35:12,303 [INFO] ---------------------------------
2019-04-07 21:35:12,305 [INFO] Training model #3: (11, 64, 8) @ 3
2019-04-07 21:35:33,621 [INFO] ---------------------------------
2019-04-07 21:35:33,623 [INFO] Summary:
2019-04-07 21:35:33,623 [INFO] Batch 1000, worst loss 103.698814 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:35:33,624 [INFO] Regularization: 6655.849121 * 0.0000000100 = 0.0000665585 loss
2019-04-07 21:35:33,624 [INFO] unfolding 0, single step 1001
2019-04-07 21:35:33,625 [INFO] Sum of grad norms of most recent batch: 14.232091
2019-04-07 21:35:33,625 [INFO] ---------------------------------
2019-04-07 21:35:55,098 [INFO] ---------------------------------
2019-04-07 21:35:55,099 [INFO] Summary:
2019-04-07 21:35:55,100 [INFO] Batch 2000, worst loss 0.065911 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:35:55,101 [INFO] Regularization: 3644.331787 * 0.0000000100 = 0.0000364433 loss
2019-04-07 21:35:55,101 [INFO] unfolding 0, single step 2001
2019-04-07 21:35:55,102 [INFO] Sum of grad norms of most recent batch: 3.585784
2019-04-07 21:35:55,103 [INFO] ---------------------------------
2019-04-07 21:36:17,078 [INFO] ---------------------------------
2019-04-07 21:36:17,079 [INFO] Summary:
2019-04-07 21:36:17,079 [INFO] Batch 3000, worst loss 0.086549 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:36:17,080 [INFO] Regularization: 2930.079590 * 0.0000000100 = 0.0000293008 loss
2019-04-07 21:36:17,080 [INFO] unfolding 0, single step 3001
2019-04-07 21:36:17,081 [INFO] Sum of grad norms of most recent batch: 1.628704
2019-04-07 21:36:17,082 [INFO] ---------------------------------
2019-04-07 21:36:38,608 [INFO] ---------------------------------
2019-04-07 21:36:38,609 [INFO] Summary:
2019-04-07 21:36:38,610 [INFO] Batch 4000, worst loss 0.053671 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:36:38,610 [INFO] Regularization: 2620.375732 * 0.0000000100 = 0.0000262038 loss
2019-04-07 21:36:38,610 [INFO] unfolding 0, single step 4001
2019-04-07 21:36:38,611 [INFO] Sum of grad norms of most recent batch: 3.828857
2019-04-07 21:36:38,612 [INFO] ---------------------------------
2019-04-07 21:37:00,254 [INFO] ---------------------------------
2019-04-07 21:37:00,255 [INFO] Summary:
2019-04-07 21:37:00,255 [INFO] Batch 5000, worst loss 0.097252 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:37:00,256 [INFO] Regularization: 2386.834717 * 0.0000000100 = 0.0000238683 loss
2019-04-07 21:37:00,257 [INFO] unfolding 0, single step 5001
2019-04-07 21:37:00,258 [INFO] Sum of grad norms of most recent batch: 1.690879
2019-04-07 21:37:00,258 [INFO] ---------------------------------
2019-04-07 21:37:21,832 [INFO] ---------------------------------
2019-04-07 21:37:21,833 [INFO] Summary:
2019-04-07 21:37:21,834 [INFO] Batch 6000, worst loss 0.161929 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:37:21,834 [INFO] Regularization: 2184.316162 * 0.0000000100 = 0.0000218432 loss
2019-04-07 21:37:21,835 [INFO] unfolding 0, single step 6001
2019-04-07 21:37:21,836 [INFO] Sum of grad norms of most recent batch: 1.768890
2019-04-07 21:37:21,837 [INFO] ---------------------------------
2019-04-07 21:37:42,901 [INFO] ---------------------------------
2019-04-07 21:37:42,901 [INFO] Summary:
2019-04-07 21:37:42,902 [INFO] Batch 7000, worst loss 0.107274 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:37:42,903 [INFO] Regularization: 2002.330811 * 0.0000000100 = 0.0000200233 loss
2019-04-07 21:37:42,903 [INFO] unfolding 0, single step 7001
2019-04-07 21:37:42,904 [INFO] Sum of grad norms of most recent batch: 3.333149
2019-04-07 21:37:42,905 [INFO] ---------------------------------
2019-04-07 21:38:04,002 [INFO] ---------------------------------
2019-04-07 21:38:04,003 [INFO] Summary:
2019-04-07 21:38:04,004 [INFO] Batch 8000, worst loss 0.133151 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:38:04,005 [INFO] Regularization: 1899.423706 * 0.0000000100 = 0.0000189942 loss
2019-04-07 21:38:04,005 [INFO] unfolding 0, single step 8001
2019-04-07 21:38:04,006 [INFO] Sum of grad norms of most recent batch: 2.955586
2019-04-07 21:38:04,007 [INFO] ---------------------------------
2019-04-07 21:38:26,042 [INFO] ---------------------------------
2019-04-07 21:38:26,043 [INFO] Summary:
2019-04-07 21:38:26,044 [INFO] Batch 9000, worst loss 0.143822 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:38:26,044 [INFO] Regularization: 1807.747803 * 0.0000000100 = 0.0000180775 loss
2019-04-07 21:38:26,045 [INFO] unfolding 0, single step 9001
2019-04-07 21:38:26,046 [INFO] Sum of grad norms of most recent batch: 1.737806
2019-04-07 21:38:26,046 [INFO] ---------------------------------
2019-04-07 21:38:47,618 [INFO] ---------------------------------
2019-04-07 21:38:47,619 [INFO] Summary:
2019-04-07 21:38:47,620 [INFO] Batch 10000, worst loss 0.083695 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:38:47,621 [INFO] Regularization: 1747.931519 * 0.0000000100 = 0.0000174793 loss
2019-04-07 21:38:47,621 [INFO] unfolding 0, single step 10001
2019-04-07 21:38:47,622 [INFO] Sum of grad norms of most recent batch: 2.845718
2019-04-07 21:38:47,623 [INFO] ---------------------------------
2019-04-07 21:39:24,890 [INFO] ---------------------------------
2019-04-07 21:39:24,891 [INFO] Evaluation:
2019-04-07 21:39:24,892 [INFO] Batch 10000, worst loss 0.153443 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:39:24,892 [INFO] ---------------------------------
2019-04-07 21:39:47,168 [INFO] ---------------------------------
2019-04-07 21:39:47,169 [INFO] Summary:
2019-04-07 21:39:47,170 [INFO] Batch 11000, worst loss 0.112095 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:39:47,171 [INFO] Regularization: 1711.615723 * 0.0000000100 = 0.0000171162 loss
2019-04-07 21:39:47,171 [INFO] unfolding 0, single step 11001
2019-04-07 21:39:47,173 [INFO] Sum of grad norms of most recent batch: 2.050806
2019-04-07 21:39:47,174 [INFO] ---------------------------------
2019-04-07 21:40:08,401 [INFO] ---------------------------------
2019-04-07 21:40:08,402 [INFO] Summary:
2019-04-07 21:40:08,403 [INFO] Batch 12000, worst loss 0.130165 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:40:08,403 [INFO] Regularization: 1649.923096 * 0.0000000100 = 0.0000164992 loss
2019-04-07 21:40:08,404 [INFO] unfolding 0, single step 12001
2019-04-07 21:40:08,404 [INFO] Sum of grad norms of most recent batch: 2.043522
2019-04-07 21:40:08,405 [INFO] ---------------------------------
2019-04-07 21:40:31,199 [INFO] ---------------------------------
2019-04-07 21:40:31,200 [INFO] Summary:
2019-04-07 21:40:31,201 [INFO] Batch 13000, worst loss 0.066148 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:40:31,201 [INFO] Regularization: 1619.342163 * 0.0000000100 = 0.0000161934 loss
2019-04-07 21:40:31,201 [INFO] unfolding 0, single step 13001
2019-04-07 21:40:31,202 [INFO] Sum of grad norms of most recent batch: 1.974495
2019-04-07 21:40:31,202 [INFO] ---------------------------------
2019-04-07 21:40:52,775 [INFO] ---------------------------------
2019-04-07 21:40:52,776 [INFO] Summary:
2019-04-07 21:40:52,776 [INFO] Batch 14000, worst loss 0.048334 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:40:52,777 [INFO] Regularization: 1603.808716 * 0.0000000100 = 0.0000160381 loss
2019-04-07 21:40:52,777 [INFO] unfolding 0, single step 14001
2019-04-07 21:40:52,778 [INFO] Sum of grad norms of most recent batch: 1.588023
2019-04-07 21:40:52,778 [INFO] ---------------------------------
2019-04-07 21:41:14,114 [INFO] ---------------------------------
2019-04-07 21:41:14,115 [INFO] Summary:
2019-04-07 21:41:14,115 [INFO] Batch 15000, worst loss 0.100536 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:41:14,116 [INFO] Regularization: 1602.066162 * 0.0000000100 = 0.0000160207 loss
2019-04-07 21:41:14,116 [INFO] unfolding 0, single step 15001
2019-04-07 21:41:14,117 [INFO] Sum of grad norms of most recent batch: 3.881076
2019-04-07 21:41:14,117 [INFO] ---------------------------------
2019-04-07 21:41:36,040 [INFO] ---------------------------------
2019-04-07 21:41:36,041 [INFO] Summary:
2019-04-07 21:41:36,041 [INFO] Batch 16000, worst loss 0.041228 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:41:36,042 [INFO] Regularization: 1586.330078 * 0.0000000100 = 0.0000158633 loss
2019-04-07 21:41:36,042 [INFO] unfolding 0, single step 16001
2019-04-07 21:41:36,043 [INFO] Sum of grad norms of most recent batch: 1.205964
2019-04-07 21:41:36,043 [INFO] ---------------------------------
2019-04-07 21:41:57,527 [INFO] ---------------------------------
2019-04-07 21:41:57,528 [INFO] Summary:
2019-04-07 21:41:57,529 [INFO] Batch 17000, worst loss 0.116805 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:41:57,530 [INFO] Regularization: 1544.216553 * 0.0000000100 = 0.0000154422 loss
2019-04-07 21:41:57,530 [INFO] unfolding 0, single step 17001
2019-04-07 21:41:57,531 [INFO] Sum of grad norms of most recent batch: 1.628685
2019-04-07 21:41:57,532 [INFO] ---------------------------------
2019-04-07 21:42:19,238 [INFO] ---------------------------------
2019-04-07 21:42:19,239 [INFO] Summary:
2019-04-07 21:42:19,240 [INFO] Batch 18000, worst loss 0.140309 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:42:19,240 [INFO] Regularization: 1511.156494 * 0.0000000100 = 0.0000151116 loss
2019-04-07 21:42:19,241 [INFO] unfolding 0, single step 18001
2019-04-07 21:42:19,241 [INFO] Sum of grad norms of most recent batch: 1.251206
2019-04-07 21:42:19,242 [INFO] ---------------------------------
2019-04-07 21:42:41,297 [INFO] ---------------------------------
2019-04-07 21:42:41,298 [INFO] Summary:
2019-04-07 21:42:41,299 [INFO] Batch 19000, worst loss 0.112973 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:42:41,299 [INFO] Regularization: 1479.607422 * 0.0000000100 = 0.0000147961 loss
2019-04-07 21:42:41,299 [INFO] unfolding 0, single step 19001
2019-04-07 21:42:41,300 [INFO] Sum of grad norms of most recent batch: 0.995234
2019-04-07 21:42:41,301 [INFO] ---------------------------------
2019-04-07 21:43:03,408 [INFO] ---------------------------------
2019-04-07 21:43:03,409 [INFO] Summary:
2019-04-07 21:43:03,410 [INFO] Batch 20000, worst loss 0.152172 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:43:03,410 [INFO] Regularization: 1451.256226 * 0.0000000100 = 0.0000145126 loss
2019-04-07 21:43:03,411 [INFO] unfolding 0, single step 20001
2019-04-07 21:43:03,411 [INFO] Sum of grad norms of most recent batch: 1.520500
2019-04-07 21:43:03,412 [INFO] ---------------------------------
2019-04-07 21:43:40,794 [INFO] ---------------------------------
2019-04-07 21:43:40,795 [INFO] Evaluation:
2019-04-07 21:43:40,796 [INFO] Batch 20000, worst loss 0.145652 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:43:40,796 [INFO] ---------------------------------
2019-04-07 21:44:02,462 [INFO] ---------------------------------
2019-04-07 21:44:02,463 [INFO] Summary:
2019-04-07 21:44:02,464 [INFO] Batch 21000, worst loss 0.082229 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:44:02,464 [INFO] Regularization: 1441.102295 * 0.0000000100 = 0.0000144110 loss
2019-04-07 21:44:02,465 [INFO] unfolding 0, single step 21001
2019-04-07 21:44:02,465 [INFO] Sum of grad norms of most recent batch: 0.977433
2019-04-07 21:44:02,466 [INFO] ---------------------------------
2019-04-07 21:44:23,953 [INFO] ---------------------------------
2019-04-07 21:44:23,954 [INFO] Summary:
2019-04-07 21:44:23,954 [INFO] Batch 22000, worst loss 0.092596 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:44:23,955 [INFO] Regularization: 1441.367676 * 0.0000000100 = 0.0000144137 loss
2019-04-07 21:44:23,955 [INFO] unfolding 0, single step 22001
2019-04-07 21:44:23,956 [INFO] Sum of grad norms of most recent batch: 1.163067
2019-04-07 21:44:23,956 [INFO] ---------------------------------
2019-04-07 21:44:46,158 [INFO] ---------------------------------
2019-04-07 21:44:46,159 [INFO] Summary:
2019-04-07 21:44:46,160 [INFO] Batch 23000, worst loss 0.078682 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:44:46,160 [INFO] Regularization: 1414.360962 * 0.0000000100 = 0.0000141436 loss
2019-04-07 21:44:46,161 [INFO] unfolding 0, single step 23001
2019-04-07 21:44:46,162 [INFO] Sum of grad norms of most recent batch: 1.332469
2019-04-07 21:44:46,162 [INFO] ---------------------------------
2019-04-07 21:45:07,569 [INFO] ---------------------------------
2019-04-07 21:45:07,571 [INFO] Summary:
2019-04-07 21:45:07,571 [INFO] Batch 24000, worst loss 0.115606 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:45:07,572 [INFO] Regularization: 1384.157471 * 0.0000000100 = 0.0000138416 loss
2019-04-07 21:45:07,572 [INFO] unfolding 0, single step 24001
2019-04-07 21:45:07,572 [INFO] Sum of grad norms of most recent batch: 6.724314
2019-04-07 21:45:07,573 [INFO] ---------------------------------
2019-04-07 21:45:29,159 [INFO] ---------------------------------
2019-04-07 21:45:29,160 [INFO] Summary:
2019-04-07 21:45:29,160 [INFO] Batch 25000, worst loss 0.122002 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:45:29,161 [INFO] Regularization: 1371.420532 * 0.0000000100 = 0.0000137142 loss
2019-04-07 21:45:29,161 [INFO] unfolding 0, single step 25001
2019-04-07 21:45:29,162 [INFO] Sum of grad norms of most recent batch: 1.142974
2019-04-07 21:45:29,162 [INFO] ---------------------------------
2019-04-07 21:45:51,132 [INFO] ---------------------------------
2019-04-07 21:45:51,133 [INFO] Summary:
2019-04-07 21:45:51,134 [INFO] Batch 26000, worst loss 0.047139 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:45:51,134 [INFO] Regularization: 1368.118408 * 0.0000000100 = 0.0000136812 loss
2019-04-07 21:45:51,135 [INFO] unfolding 0, single step 26001
2019-04-07 21:45:51,135 [INFO] Sum of grad norms of most recent batch: 1.205993
2019-04-07 21:45:51,136 [INFO] ---------------------------------
2019-04-07 21:46:12,628 [INFO] ---------------------------------
2019-04-07 21:46:12,629 [INFO] Summary:
2019-04-07 21:46:12,630 [INFO] Batch 27000, worst loss 0.074347 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:46:12,631 [INFO] Regularization: 1328.105591 * 0.0000000100 = 0.0000132811 loss
2019-04-07 21:46:12,631 [INFO] unfolding 0, single step 27001
2019-04-07 21:46:12,632 [INFO] Sum of grad norms of most recent batch: 0.621793
2019-04-07 21:46:12,632 [INFO] ---------------------------------
2019-04-07 21:46:34,014 [INFO] ---------------------------------
2019-04-07 21:46:34,015 [INFO] Summary:
2019-04-07 21:46:34,015 [INFO] Batch 28000, worst loss 0.127402 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:46:34,016 [INFO] Regularization: 1324.505249 * 0.0000000100 = 0.0000132451 loss
2019-04-07 21:46:34,016 [INFO] unfolding 0, single step 28001
2019-04-07 21:46:34,017 [INFO] Sum of grad norms of most recent batch: 1.565155
2019-04-07 21:46:34,017 [INFO] ---------------------------------
2019-04-07 21:46:56,191 [INFO] ---------------------------------
2019-04-07 21:46:56,192 [INFO] Summary:
2019-04-07 21:46:56,193 [INFO] Batch 29000, worst loss 0.138871 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:46:56,193 [INFO] Regularization: 1322.101807 * 0.0000000100 = 0.0000132210 loss
2019-04-07 21:46:56,194 [INFO] unfolding 0, single step 29001
2019-04-07 21:46:56,194 [INFO] Sum of grad norms of most recent batch: 7.220603
2019-04-07 21:46:56,195 [INFO] ---------------------------------
2019-04-07 21:47:18,797 [INFO] ---------------------------------
2019-04-07 21:47:18,798 [INFO] Summary:
2019-04-07 21:47:18,798 [INFO] Batch 30000, worst loss 0.127246 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 21:47:18,799 [INFO] Regularization: 1342.609741 * 0.0000000100 = 0.0000134261 loss
2019-04-07 21:47:18,799 [INFO] unfolding 0, single step 30001
2019-04-07 21:47:18,800 [INFO] Sum of grad norms of most recent batch: 0.396906
2019-04-07 21:47:18,800 [INFO] ---------------------------------
2019-04-07 21:47:56,179 [INFO] ---------------------------------
2019-04-07 21:47:56,180 [INFO] Evaluation:
2019-04-07 21:47:56,181 [INFO] Batch 30000, worst loss 0.148534 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:47:56,181 [INFO] ---------------------------------
2019-04-07 21:48:18,212 [INFO] ---------------------------------
2019-04-07 21:48:18,213 [INFO] Summary:
2019-04-07 21:48:18,214 [INFO] Batch 31000, worst loss 0.135191 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 21:48:18,214 [INFO] Regularization: 1330.150024 * 0.0000000100 = 0.0000133015 loss
2019-04-07 21:48:18,215 [INFO] unfolding 0, single step 31001
2019-04-07 21:48:18,216 [INFO] Sum of grad norms of most recent batch: 0.742956
2019-04-07 21:48:18,216 [INFO] ---------------------------------
2019-04-07 21:48:41,006 [INFO] ---------------------------------
2019-04-07 21:48:41,008 [INFO] Summary:
2019-04-07 21:48:41,008 [INFO] Batch 32000, worst loss 0.079788 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 21:48:41,009 [INFO] Regularization: 1294.733765 * 0.0000000100 = 0.0000129473 loss
2019-04-07 21:48:41,010 [INFO] unfolding 0, single step 32001
2019-04-07 21:48:41,011 [INFO] Sum of grad norms of most recent batch: 0.754720
2019-04-07 21:48:41,011 [INFO] ---------------------------------
2019-04-07 21:49:02,689 [INFO] ---------------------------------
2019-04-07 21:49:02,690 [INFO] Summary:
2019-04-07 21:49:02,691 [INFO] Batch 33000, worst loss 0.061170 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 21:49:02,691 [INFO] Regularization: 1264.602661 * 0.0000000100 = 0.0000126460 loss
2019-04-07 21:49:02,692 [INFO] unfolding 0, single step 33001
2019-04-07 21:49:02,692 [INFO] Sum of grad norms of most recent batch: 5.791475
2019-04-07 21:49:02,693 [INFO] ---------------------------------
2019-04-07 21:49:24,455 [INFO] ---------------------------------
2019-04-07 21:49:24,457 [INFO] Summary:
2019-04-07 21:49:24,457 [INFO] Batch 34000, worst loss 0.083263 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 21:49:24,458 [INFO] Regularization: 1228.151611 * 0.0000000100 = 0.0000122815 loss
2019-04-07 21:49:24,458 [INFO] unfolding 0, single step 34001
2019-04-07 21:49:24,459 [INFO] Sum of grad norms of most recent batch: 0.713858
2019-04-07 21:49:24,459 [INFO] ---------------------------------
2019-04-07 21:49:45,771 [INFO] ---------------------------------
2019-04-07 21:49:45,772 [INFO] Summary:
2019-04-07 21:49:45,773 [INFO] Batch 35000, worst loss 0.132791 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 21:49:45,774 [INFO] Regularization: 1198.508423 * 0.0000000100 = 0.0000119851 loss
2019-04-07 21:49:45,775 [INFO] unfolding 0, single step 35001
2019-04-07 21:49:45,776 [INFO] Sum of grad norms of most recent batch: 0.585546
2019-04-07 21:49:45,777 [INFO] ---------------------------------
2019-04-07 21:50:07,243 [INFO] ---------------------------------
2019-04-07 21:50:07,244 [INFO] Summary:
2019-04-07 21:50:07,245 [INFO] Batch 36000, worst loss 0.080414 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 21:50:07,246 [INFO] Regularization: 1185.661377 * 0.0000000100 = 0.0000118566 loss
2019-04-07 21:50:07,246 [INFO] unfolding 0, single step 36001
2019-04-07 21:50:07,247 [INFO] Sum of grad norms of most recent batch: 0.199429
2019-04-07 21:50:07,247 [INFO] ---------------------------------
2019-04-07 21:50:28,724 [INFO] ---------------------------------
2019-04-07 21:50:28,725 [INFO] Summary:
2019-04-07 21:50:28,726 [INFO] Batch 37000, worst loss 0.115883 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 21:50:28,727 [INFO] Regularization: 1179.121216 * 0.0000000100 = 0.0000117912 loss
2019-04-07 21:50:28,727 [INFO] unfolding 0, single step 37001
2019-04-07 21:50:28,727 [INFO] Sum of grad norms of most recent batch: 0.475352
2019-04-07 21:50:28,728 [INFO] ---------------------------------
2019-04-07 21:50:49,960 [INFO] ---------------------------------
2019-04-07 21:50:49,961 [INFO] Summary:
2019-04-07 21:50:49,962 [INFO] Batch 38000, worst loss 0.056859 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 21:50:49,962 [INFO] Regularization: 1158.776978 * 0.0000000100 = 0.0000115878 loss
2019-04-07 21:50:49,963 [INFO] unfolding 0, single step 38001
2019-04-07 21:50:49,963 [INFO] Sum of grad norms of most recent batch: 2.272822
2019-04-07 21:50:49,964 [INFO] ---------------------------------
2019-04-07 21:51:11,302 [INFO] ---------------------------------
2019-04-07 21:51:11,303 [INFO] Summary:
2019-04-07 21:51:11,304 [INFO] Batch 39000, worst loss 0.081222 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 21:51:11,304 [INFO] Regularization: 1189.084961 * 0.0000000100 = 0.0000118908 loss
2019-04-07 21:51:11,304 [INFO] unfolding 0, single step 39001
2019-04-07 21:51:11,305 [INFO] Sum of grad norms of most recent batch: 1.667379
2019-04-07 21:51:11,306 [INFO] ---------------------------------
2019-04-07 21:51:33,497 [INFO] ---------------------------------
2019-04-07 21:51:33,498 [INFO] Summary:
2019-04-07 21:51:33,499 [INFO] Batch 40000, worst loss 0.111994 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 21:51:33,499 [INFO] Regularization: 1162.136841 * 0.0000000100 = 0.0000116214 loss
2019-04-07 21:51:33,499 [INFO] unfolding 0, single step 40001
2019-04-07 21:51:33,500 [INFO] Sum of grad norms of most recent batch: 1.018140
2019-04-07 21:51:33,500 [INFO] ---------------------------------
2019-04-07 21:52:10,838 [INFO] ---------------------------------
2019-04-07 21:52:10,839 [INFO] Evaluation:
2019-04-07 21:52:10,839 [INFO] Batch 40000, worst loss 0.150056 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:52:10,840 [INFO] ---------------------------------
2019-04-07 21:52:32,670 [INFO] ---------------------------------
2019-04-07 21:52:32,671 [INFO] Summary:
2019-04-07 21:52:32,671 [INFO] Batch 41000, worst loss 0.038616 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 21:52:32,672 [INFO] Regularization: 1132.343018 * 0.0000000100 = 0.0000113234 loss
2019-04-07 21:52:32,672 [INFO] unfolding 0, single step 41001
2019-04-07 21:52:32,673 [INFO] Sum of grad norms of most recent batch: 0.898298
2019-04-07 21:52:32,674 [INFO] ---------------------------------
2019-04-07 21:52:54,052 [INFO] ---------------------------------
2019-04-07 21:52:54,054 [INFO] Summary:
2019-04-07 21:52:54,054 [INFO] Batch 42000, worst loss 0.084912 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 21:52:54,055 [INFO] Regularization: 1106.998291 * 0.0000000100 = 0.0000110700 loss
2019-04-07 21:52:54,056 [INFO] unfolding 0, single step 42001
2019-04-07 21:52:54,056 [INFO] Sum of grad norms of most recent batch: 0.099798
2019-04-07 21:52:54,057 [INFO] ---------------------------------
2019-04-07 21:53:16,325 [INFO] ---------------------------------
2019-04-07 21:53:16,326 [INFO] Summary:
2019-04-07 21:53:16,327 [INFO] Batch 43000, worst loss 0.028030 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 21:53:16,327 [INFO] Regularization: 1084.259888 * 0.0000000100 = 0.0000108426 loss
2019-04-07 21:53:16,328 [INFO] unfolding 0, single step 43001
2019-04-07 21:53:16,328 [INFO] Sum of grad norms of most recent batch: 0.258531
2019-04-07 21:53:16,329 [INFO] ---------------------------------
2019-04-07 21:53:38,285 [INFO] ---------------------------------
2019-04-07 21:53:38,286 [INFO] Summary:
2019-04-07 21:53:38,288 [INFO] Batch 44000, worst loss 0.109328 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 21:53:38,289 [INFO] Regularization: 1080.520508 * 0.0000000100 = 0.0000108052 loss
2019-04-07 21:53:38,290 [INFO] unfolding 0, single step 44001
2019-04-07 21:53:38,292 [INFO] Sum of grad norms of most recent batch: 1.615303
2019-04-07 21:53:38,293 [INFO] ---------------------------------
2019-04-07 21:53:59,568 [INFO] ---------------------------------
2019-04-07 21:53:59,569 [INFO] Summary:
2019-04-07 21:53:59,570 [INFO] Batch 45000, worst loss 0.141295 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 21:53:59,571 [INFO] Regularization: 1064.698853 * 0.0000000100 = 0.0000106470 loss
2019-04-07 21:53:59,571 [INFO] unfolding 0, single step 45001
2019-04-07 21:53:59,572 [INFO] Sum of grad norms of most recent batch: 0.135908
2019-04-07 21:53:59,573 [INFO] ---------------------------------
2019-04-07 21:54:20,661 [INFO] ---------------------------------
2019-04-07 21:54:20,662 [INFO] Summary:
2019-04-07 21:54:20,663 [INFO] Batch 46000, worst loss 0.102307 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 21:54:20,663 [INFO] Regularization: 1046.977905 * 0.0000000100 = 0.0000104698 loss
2019-04-07 21:54:20,663 [INFO] unfolding 0, single step 46001
2019-04-07 21:54:20,664 [INFO] Sum of grad norms of most recent batch: 0.119942
2019-04-07 21:54:20,664 [INFO] ---------------------------------
2019-04-07 21:54:42,093 [INFO] ---------------------------------
2019-04-07 21:54:42,094 [INFO] Summary:
2019-04-07 21:54:42,095 [INFO] Batch 47000, worst loss 0.142266 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 21:54:42,095 [INFO] Regularization: 1053.831421 * 0.0000000100 = 0.0000105383 loss
2019-04-07 21:54:42,096 [INFO] unfolding 0, single step 47001
2019-04-07 21:54:42,096 [INFO] Sum of grad norms of most recent batch: 0.546576
2019-04-07 21:54:42,097 [INFO] ---------------------------------
2019-04-07 21:55:03,689 [INFO] ---------------------------------
2019-04-07 21:55:03,690 [INFO] Summary:
2019-04-07 21:55:03,691 [INFO] Batch 48000, worst loss 0.133983 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 21:55:03,691 [INFO] Regularization: 1053.496216 * 0.0000000100 = 0.0000105350 loss
2019-04-07 21:55:03,692 [INFO] unfolding 0, single step 48001
2019-04-07 21:55:03,692 [INFO] Sum of grad norms of most recent batch: 0.180503
2019-04-07 21:55:03,693 [INFO] ---------------------------------
2019-04-07 21:55:24,717 [INFO] ---------------------------------
2019-04-07 21:55:24,718 [INFO] Summary:
2019-04-07 21:55:24,719 [INFO] Batch 49000, worst loss 0.062146 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 21:55:24,719 [INFO] Regularization: 1036.474487 * 0.0000000100 = 0.0000103647 loss
2019-04-07 21:55:24,719 [INFO] unfolding 0, single step 49001
2019-04-07 21:55:24,720 [INFO] Sum of grad norms of most recent batch: 0.582408
2019-04-07 21:55:24,721 [INFO] ---------------------------------
2019-04-07 21:55:46,335 [INFO] ---------------------------------
2019-04-07 21:55:46,337 [INFO] Summary:
2019-04-07 21:55:46,337 [INFO] Batch 50000, worst loss 0.043141 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 21:55:46,338 [INFO] Regularization: 1023.682678 * 0.0000000100 = 0.0000102368 loss
2019-04-07 21:55:46,338 [INFO] unfolding 0, single step 50001
2019-04-07 21:55:46,339 [INFO] Sum of grad norms of most recent batch: 0.081953
2019-04-07 21:55:46,339 [INFO] ---------------------------------
2019-04-07 21:56:23,613 [INFO] ---------------------------------
2019-04-07 21:56:23,614 [INFO] Evaluation:
2019-04-07 21:56:23,615 [INFO] Batch 50000, worst loss 0.124751 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 21:56:23,615 [INFO] ---------------------------------
2019-04-07 21:56:44,924 [INFO] ---------------------------------
2019-04-07 21:56:44,925 [INFO] Summary:
2019-04-07 21:56:44,926 [INFO] Batch 51000, worst loss 0.143943 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 21:56:44,926 [INFO] Regularization: 1014.434204 * 0.0000000100 = 0.0000101443 loss
2019-04-07 21:56:44,927 [INFO] unfolding 0, single step 51001
2019-04-07 21:56:44,927 [INFO] Sum of grad norms of most recent batch: 0.169799
2019-04-07 21:56:44,928 [INFO] ---------------------------------
2019-04-07 21:57:06,266 [INFO] ---------------------------------
2019-04-07 21:57:06,267 [INFO] Summary:
2019-04-07 21:57:06,268 [INFO] Batch 52000, worst loss 0.104229 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 21:57:06,269 [INFO] Regularization: 1004.226990 * 0.0000000100 = 0.0000100423 loss
2019-04-07 21:57:06,269 [INFO] unfolding 0, single step 52001
2019-04-07 21:57:06,269 [INFO] Sum of grad norms of most recent batch: 3.094245
2019-04-07 21:57:06,270 [INFO] ---------------------------------
2019-04-07 21:57:28,025 [INFO] ---------------------------------
2019-04-07 21:57:28,026 [INFO] Summary:
2019-04-07 21:57:28,027 [INFO] Batch 53000, worst loss 0.054471 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 21:57:28,027 [INFO] Regularization: 995.529541 * 0.0000000100 = 0.0000099553 loss
2019-04-07 21:57:28,027 [INFO] unfolding 0, single step 53001
2019-04-07 21:57:28,028 [INFO] Sum of grad norms of most recent batch: 0.105459
2019-04-07 21:57:28,028 [INFO] ---------------------------------
2019-04-07 21:57:49,673 [INFO] ---------------------------------
2019-04-07 21:57:49,674 [INFO] Summary:
2019-04-07 21:57:49,675 [INFO] Batch 54000, worst loss 0.094591 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 21:57:49,675 [INFO] Regularization: 985.706665 * 0.0000000100 = 0.0000098571 loss
2019-04-07 21:57:49,676 [INFO] unfolding 0, single step 54001
2019-04-07 21:57:49,676 [INFO] Sum of grad norms of most recent batch: 0.075372
2019-04-07 21:57:49,677 [INFO] ---------------------------------
2019-04-07 21:58:10,730 [INFO] ---------------------------------
2019-04-07 21:58:10,731 [INFO] Summary:
2019-04-07 21:58:10,732 [INFO] Batch 55000, worst loss 0.059416 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 21:58:10,732 [INFO] Regularization: 978.316528 * 0.0000000100 = 0.0000097832 loss
2019-04-07 21:58:10,733 [INFO] unfolding 0, single step 55001
2019-04-07 21:58:10,734 [INFO] Sum of grad norms of most recent batch: 0.103193
2019-04-07 21:58:10,734 [INFO] ---------------------------------
2019-04-07 21:58:32,055 [INFO] ---------------------------------
2019-04-07 21:58:32,056 [INFO] Summary:
2019-04-07 21:58:32,057 [INFO] Batch 56000, worst loss 0.054539 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 21:58:32,057 [INFO] Regularization: 973.085632 * 0.0000000100 = 0.0000097309 loss
2019-04-07 21:58:32,057 [INFO] unfolding 0, single step 56001
2019-04-07 21:58:32,058 [INFO] Sum of grad norms of most recent batch: 0.208235
2019-04-07 21:58:32,058 [INFO] ---------------------------------
2019-04-07 21:58:53,665 [INFO] ---------------------------------
2019-04-07 21:58:53,666 [INFO] Summary:
2019-04-07 21:58:53,667 [INFO] Batch 57000, worst loss 0.139547 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 21:58:53,667 [INFO] Regularization: 971.720215 * 0.0000000100 = 0.0000097172 loss
2019-04-07 21:58:53,668 [INFO] unfolding 0, single step 57001
2019-04-07 21:58:53,668 [INFO] Sum of grad norms of most recent batch: 0.396349
2019-04-07 21:58:53,669 [INFO] ---------------------------------
2019-04-07 21:59:14,227 [INFO] ---------------------------------
2019-04-07 21:59:14,229 [INFO] Summary:
2019-04-07 21:59:14,229 [INFO] Batch 58000, worst loss 0.094861 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 21:59:14,230 [INFO] Regularization: 965.529297 * 0.0000000100 = 0.0000096553 loss
2019-04-07 21:59:14,231 [INFO] unfolding 0, single step 58001
2019-04-07 21:59:14,231 [INFO] Sum of grad norms of most recent batch: 0.091423
2019-04-07 21:59:14,232 [INFO] ---------------------------------
2019-04-07 21:59:34,987 [INFO] ---------------------------------
2019-04-07 21:59:34,988 [INFO] Summary:
2019-04-07 21:59:34,988 [INFO] Batch 59000, worst loss 0.052161 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 21:59:34,989 [INFO] Regularization: 964.560242 * 0.0000000100 = 0.0000096456 loss
2019-04-07 21:59:34,990 [INFO] unfolding 0, single step 59001
2019-04-07 21:59:34,990 [INFO] Sum of grad norms of most recent batch: 0.173151
2019-04-07 21:59:34,991 [INFO] ---------------------------------
2019-04-07 21:59:57,302 [INFO] ---------------------------------
2019-04-07 21:59:57,303 [INFO] Summary:
2019-04-07 21:59:57,304 [INFO] Batch 60000, worst loss 0.054984 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 21:59:57,304 [INFO] Regularization: 968.658630 * 0.0000000100 = 0.0000096866 loss
2019-04-07 21:59:57,304 [INFO] unfolding 0, single step 60001
2019-04-07 21:59:57,305 [INFO] Sum of grad norms of most recent batch: 0.192069
2019-04-07 21:59:57,305 [INFO] ---------------------------------
2019-04-07 22:00:34,641 [INFO] ---------------------------------
2019-04-07 22:00:34,642 [INFO] Evaluation:
2019-04-07 22:00:34,643 [INFO] Batch 60000, worst loss 0.141028 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:00:34,643 [INFO] ---------------------------------
2019-04-07 22:00:55,638 [INFO] ---------------------------------
2019-04-07 22:00:55,639 [INFO] Summary:
2019-04-07 22:00:55,639 [INFO] Batch 61000, worst loss 0.154904 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 22:00:55,640 [INFO] Regularization: 959.829895 * 0.0000000100 = 0.0000095983 loss
2019-04-07 22:00:55,640 [INFO] unfolding 0, single step 61001
2019-04-07 22:00:55,641 [INFO] Sum of grad norms of most recent batch: 0.087438
2019-04-07 22:00:55,641 [INFO] ---------------------------------
2019-04-07 22:01:16,530 [INFO] ---------------------------------
2019-04-07 22:01:16,531 [INFO] Summary:
2019-04-07 22:01:16,532 [INFO] Batch 62000, worst loss 0.085651 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 22:01:16,532 [INFO] Regularization: 955.061279 * 0.0000000100 = 0.0000095506 loss
2019-04-07 22:01:16,533 [INFO] unfolding 0, single step 62001
2019-04-07 22:01:16,533 [INFO] Sum of grad norms of most recent batch: 0.078635
2019-04-07 22:01:16,534 [INFO] ---------------------------------
2019-04-07 22:01:37,659 [INFO] ---------------------------------
2019-04-07 22:01:37,660 [INFO] Summary:
2019-04-07 22:01:37,660 [INFO] Batch 63000, worst loss 0.126131 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 22:01:37,661 [INFO] Regularization: 950.556091 * 0.0000000100 = 0.0000095056 loss
2019-04-07 22:01:37,661 [INFO] unfolding 0, single step 63001
2019-04-07 22:01:37,662 [INFO] Sum of grad norms of most recent batch: 0.055951
2019-04-07 22:01:37,662 [INFO] ---------------------------------
2019-04-07 22:01:58,365 [INFO] ---------------------------------
2019-04-07 22:01:58,366 [INFO] Summary:
2019-04-07 22:01:58,367 [INFO] Batch 64000, worst loss 0.158482 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 22:01:58,368 [INFO] Regularization: 946.868286 * 0.0000000100 = 0.0000094687 loss
2019-04-07 22:01:58,368 [INFO] unfolding 0, single step 64001
2019-04-07 22:01:58,369 [INFO] Sum of grad norms of most recent batch: 0.213581
2019-04-07 22:01:58,369 [INFO] ---------------------------------
2019-04-07 22:02:20,523 [INFO] ---------------------------------
2019-04-07 22:02:20,524 [INFO] Summary:
2019-04-07 22:02:20,525 [INFO] Batch 65000, worst loss 0.155156 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 22:02:20,525 [INFO] Regularization: 941.643921 * 0.0000000100 = 0.0000094164 loss
2019-04-07 22:02:20,526 [INFO] unfolding 0, single step 65001
2019-04-07 22:02:20,527 [INFO] Sum of grad norms of most recent batch: 0.065662
2019-04-07 22:02:20,527 [INFO] ---------------------------------
2019-04-07 22:02:41,999 [INFO] ---------------------------------
2019-04-07 22:02:42,000 [INFO] Summary:
2019-04-07 22:02:42,001 [INFO] Batch 66000, worst loss 0.046315 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 22:02:42,001 [INFO] Regularization: 936.149292 * 0.0000000100 = 0.0000093615 loss
2019-04-07 22:02:42,002 [INFO] unfolding 0, single step 66001
2019-04-07 22:02:42,002 [INFO] Sum of grad norms of most recent batch: 0.049497
2019-04-07 22:02:42,003 [INFO] ---------------------------------
2019-04-07 22:03:03,068 [INFO] ---------------------------------
2019-04-07 22:03:03,069 [INFO] Summary:
2019-04-07 22:03:03,070 [INFO] Batch 67000, worst loss 0.122784 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 22:03:03,071 [INFO] Regularization: 932.760132 * 0.0000000100 = 0.0000093276 loss
2019-04-07 22:03:03,072 [INFO] unfolding 0, single step 67001
2019-04-07 22:03:03,073 [INFO] Sum of grad norms of most recent batch: 0.188612
2019-04-07 22:03:03,074 [INFO] ---------------------------------
2019-04-07 22:03:24,113 [INFO] ---------------------------------
2019-04-07 22:03:24,114 [INFO] Summary:
2019-04-07 22:03:24,115 [INFO] Batch 68000, worst loss 0.070505 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 22:03:24,115 [INFO] Regularization: 934.997070 * 0.0000000100 = 0.0000093500 loss
2019-04-07 22:03:24,116 [INFO] unfolding 0, single step 68001
2019-04-07 22:03:24,116 [INFO] Sum of grad norms of most recent batch: 0.122380
2019-04-07 22:03:24,117 [INFO] ---------------------------------
2019-04-07 22:03:45,373 [INFO] ---------------------------------
2019-04-07 22:03:45,374 [INFO] Summary:
2019-04-07 22:03:45,375 [INFO] Batch 69000, worst loss 0.052533 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 22:03:45,376 [INFO] Regularization: 937.432007 * 0.0000000100 = 0.0000093743 loss
2019-04-07 22:03:45,376 [INFO] unfolding 0, single step 69001
2019-04-07 22:03:45,376 [INFO] Sum of grad norms of most recent batch: 0.095459
2019-04-07 22:03:45,377 [INFO] ---------------------------------
2019-04-07 22:04:06,547 [INFO] ---------------------------------
2019-04-07 22:04:06,548 [INFO] Summary:
2019-04-07 22:04:06,548 [INFO] Batch 70000, worst loss 0.036768 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 22:04:06,550 [INFO] Regularization: 932.780457 * 0.0000000100 = 0.0000093278 loss
2019-04-07 22:04:06,550 [INFO] unfolding 0, single step 70001
2019-04-07 22:04:06,551 [INFO] Sum of grad norms of most recent batch: 0.052341
2019-04-07 22:04:06,552 [INFO] ---------------------------------
2019-04-07 22:04:43,895 [INFO] ---------------------------------
2019-04-07 22:04:43,896 [INFO] Evaluation:
2019-04-07 22:04:43,896 [INFO] Batch 70000, worst loss 0.118780 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:04:43,897 [INFO] ---------------------------------
2019-04-07 22:05:05,034 [INFO] ---------------------------------
2019-04-07 22:05:05,035 [INFO] Summary:
2019-04-07 22:05:05,036 [INFO] Batch 71000, worst loss 0.072987 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 22:05:05,036 [INFO] Regularization: 929.728638 * 0.0000000100 = 0.0000092973 loss
2019-04-07 22:05:05,036 [INFO] unfolding 0, single step 71001
2019-04-07 22:05:05,037 [INFO] Sum of grad norms of most recent batch: 0.090929
2019-04-07 22:05:05,037 [INFO] ---------------------------------
2019-04-07 22:05:26,278 [INFO] ---------------------------------
2019-04-07 22:05:26,279 [INFO] Summary:
2019-04-07 22:05:26,280 [INFO] Batch 72000, worst loss 0.059423 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 22:05:26,280 [INFO] Regularization: 927.240234 * 0.0000000100 = 0.0000092724 loss
2019-04-07 22:05:26,281 [INFO] unfolding 0, single step 72001
2019-04-07 22:05:26,281 [INFO] Sum of grad norms of most recent batch: 0.040676
2019-04-07 22:05:26,282 [INFO] ---------------------------------
2019-04-07 22:05:47,085 [INFO] ---------------------------------
2019-04-07 22:05:47,086 [INFO] Summary:
2019-04-07 22:05:47,087 [INFO] Batch 73000, worst loss 0.075480 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 22:05:47,088 [INFO] Regularization: 924.890015 * 0.0000000100 = 0.0000092489 loss
2019-04-07 22:05:47,088 [INFO] unfolding 0, single step 73001
2019-04-07 22:05:47,089 [INFO] Sum of grad norms of most recent batch: 0.044303
2019-04-07 22:05:47,090 [INFO] ---------------------------------
2019-04-07 22:06:07,977 [INFO] ---------------------------------
2019-04-07 22:06:07,978 [INFO] Summary:
2019-04-07 22:06:07,978 [INFO] Batch 74000, worst loss 0.069410 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 22:06:07,979 [INFO] Regularization: 922.872314 * 0.0000000100 = 0.0000092287 loss
2019-04-07 22:06:07,979 [INFO] unfolding 0, single step 74001
2019-04-07 22:06:07,980 [INFO] Sum of grad norms of most recent batch: 0.154092
2019-04-07 22:06:07,980 [INFO] ---------------------------------
2019-04-07 22:06:29,838 [INFO] ---------------------------------
2019-04-07 22:06:29,839 [INFO] Summary:
2019-04-07 22:06:29,840 [INFO] Batch 75000, worst loss 0.113892 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 22:06:29,841 [INFO] Regularization: 922.167786 * 0.0000000100 = 0.0000092217 loss
2019-04-07 22:06:29,842 [INFO] unfolding 0, single step 75001
2019-04-07 22:06:29,843 [INFO] Sum of grad norms of most recent batch: 0.100000
2019-04-07 22:06:29,844 [INFO] ---------------------------------
2019-04-07 22:06:51,550 [INFO] ---------------------------------
2019-04-07 22:06:51,551 [INFO] Summary:
2019-04-07 22:06:51,551 [INFO] Batch 76000, worst loss 0.062569 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 22:06:51,552 [INFO] Regularization: 919.950500 * 0.0000000100 = 0.0000091995 loss
2019-04-07 22:06:51,552 [INFO] unfolding 0, single step 76001
2019-04-07 22:06:51,553 [INFO] Sum of grad norms of most recent batch: 0.105673
2019-04-07 22:06:51,553 [INFO] ---------------------------------
2019-04-07 22:07:12,721 [INFO] ---------------------------------
2019-04-07 22:07:12,722 [INFO] Summary:
2019-04-07 22:07:12,722 [INFO] Batch 77000, worst loss 0.030045 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 22:07:12,723 [INFO] Regularization: 918.024780 * 0.0000000100 = 0.0000091802 loss
2019-04-07 22:07:12,723 [INFO] unfolding 0, single step 77001
2019-04-07 22:07:12,724 [INFO] Sum of grad norms of most recent batch: 0.031972
2019-04-07 22:07:12,724 [INFO] ---------------------------------
2019-04-07 22:07:33,856 [INFO] ---------------------------------
2019-04-07 22:07:33,857 [INFO] Summary:
2019-04-07 22:07:33,857 [INFO] Batch 78000, worst loss 0.036372 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 22:07:33,858 [INFO] Regularization: 916.525146 * 0.0000000100 = 0.0000091653 loss
2019-04-07 22:07:33,858 [INFO] unfolding 0, single step 78001
2019-04-07 22:07:33,859 [INFO] Sum of grad norms of most recent batch: 0.077303
2019-04-07 22:07:33,859 [INFO] ---------------------------------
2019-04-07 22:07:55,045 [INFO] ---------------------------------
2019-04-07 22:07:55,046 [INFO] Summary:
2019-04-07 22:07:55,047 [INFO] Batch 79000, worst loss 0.066604 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 22:07:55,047 [INFO] Regularization: 914.129456 * 0.0000000100 = 0.0000091413 loss
2019-04-07 22:07:55,047 [INFO] unfolding 0, single step 79001
2019-04-07 22:07:55,048 [INFO] Sum of grad norms of most recent batch: 0.069267
2019-04-07 22:07:55,048 [INFO] ---------------------------------
2019-04-07 22:08:15,577 [INFO] ---------------------------------
2019-04-07 22:08:15,578 [INFO] Summary:
2019-04-07 22:08:15,578 [INFO] Batch 80000, worst loss 0.221016 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 22:08:15,579 [INFO] Regularization: 912.480042 * 0.0000000100 = 0.0000091248 loss
2019-04-07 22:08:15,579 [INFO] unfolding 0, single step 80001
2019-04-07 22:08:15,580 [INFO] Sum of grad norms of most recent batch: 0.165569
2019-04-07 22:08:15,580 [INFO] ---------------------------------
2019-04-07 22:08:52,902 [INFO] ---------------------------------
2019-04-07 22:08:52,903 [INFO] Evaluation:
2019-04-07 22:08:52,904 [INFO] Batch 80000, worst loss 0.095963 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:08:52,904 [INFO] ---------------------------------
2019-04-07 22:09:14,417 [INFO] ---------------------------------
2019-04-07 22:09:14,417 [INFO] Summary:
2019-04-07 22:09:14,418 [INFO] Batch 81000, worst loss 0.045759 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 22:09:14,419 [INFO] Regularization: 912.004150 * 0.0000000100 = 0.0000091200 loss
2019-04-07 22:09:14,419 [INFO] unfolding 0, single step 81001
2019-04-07 22:09:14,419 [INFO] Sum of grad norms of most recent batch: 0.182570
2019-04-07 22:09:14,420 [INFO] ---------------------------------
2019-04-07 22:09:35,610 [INFO] ---------------------------------
2019-04-07 22:09:35,611 [INFO] Summary:
2019-04-07 22:09:35,612 [INFO] Batch 82000, worst loss 0.012468 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 22:09:35,612 [INFO] Regularization: 910.788940 * 0.0000000100 = 0.0000091079 loss
2019-04-07 22:09:35,613 [INFO] unfolding 0, single step 82001
2019-04-07 22:09:35,613 [INFO] Sum of grad norms of most recent batch: 0.254951
2019-04-07 22:09:35,614 [INFO] ---------------------------------
2019-04-07 22:09:57,486 [INFO] ---------------------------------
2019-04-07 22:09:57,486 [INFO] Summary:
2019-04-07 22:09:57,487 [INFO] Batch 83000, worst loss 0.037494 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 22:09:57,487 [INFO] Regularization: 910.097656 * 0.0000000100 = 0.0000091010 loss
2019-04-07 22:09:57,488 [INFO] unfolding 0, single step 83001
2019-04-07 22:09:57,488 [INFO] Sum of grad norms of most recent batch: 0.048274
2019-04-07 22:09:57,489 [INFO] ---------------------------------
2019-04-07 22:10:18,979 [INFO] ---------------------------------
2019-04-07 22:10:18,980 [INFO] Summary:
2019-04-07 22:10:18,981 [INFO] Batch 84000, worst loss 0.036877 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 22:10:18,981 [INFO] Regularization: 909.023376 * 0.0000000100 = 0.0000090902 loss
2019-04-07 22:10:18,981 [INFO] unfolding 0, single step 84001
2019-04-07 22:10:18,982 [INFO] Sum of grad norms of most recent batch: 0.028278
2019-04-07 22:10:18,982 [INFO] ---------------------------------
2019-04-07 22:10:40,446 [INFO] ---------------------------------
2019-04-07 22:10:40,447 [INFO] Summary:
2019-04-07 22:10:40,447 [INFO] Batch 85000, worst loss 0.068489 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 22:10:40,448 [INFO] Regularization: 907.882690 * 0.0000000100 = 0.0000090788 loss
2019-04-07 22:10:40,448 [INFO] unfolding 0, single step 85001
2019-04-07 22:10:40,449 [INFO] Sum of grad norms of most recent batch: 0.082430
2019-04-07 22:10:40,449 [INFO] ---------------------------------
2019-04-07 22:11:01,627 [INFO] ---------------------------------
2019-04-07 22:11:01,628 [INFO] Summary:
2019-04-07 22:11:01,629 [INFO] Batch 86000, worst loss 0.024750 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 22:11:01,629 [INFO] Regularization: 906.759644 * 0.0000000100 = 0.0000090676 loss
2019-04-07 22:11:01,630 [INFO] unfolding 0, single step 86001
2019-04-07 22:11:01,630 [INFO] Sum of grad norms of most recent batch: 0.041601
2019-04-07 22:11:01,631 [INFO] ---------------------------------
2019-04-07 22:11:22,355 [INFO] ---------------------------------
2019-04-07 22:11:22,356 [INFO] Summary:
2019-04-07 22:11:22,356 [INFO] Batch 87000, worst loss 0.050620 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 22:11:22,357 [INFO] Regularization: 906.373718 * 0.0000000100 = 0.0000090637 loss
2019-04-07 22:11:22,357 [INFO] unfolding 0, single step 87001
2019-04-07 22:11:22,358 [INFO] Sum of grad norms of most recent batch: 0.042726
2019-04-07 22:11:22,358 [INFO] ---------------------------------
2019-04-07 22:11:43,332 [INFO] ---------------------------------
2019-04-07 22:11:43,334 [INFO] Summary:
2019-04-07 22:11:43,334 [INFO] Batch 88000, worst loss 0.099556 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 22:11:43,335 [INFO] Regularization: 905.398804 * 0.0000000100 = 0.0000090540 loss
2019-04-07 22:11:43,335 [INFO] unfolding 0, single step 88001
2019-04-07 22:11:43,336 [INFO] Sum of grad norms of most recent batch: 0.054598
2019-04-07 22:11:43,337 [INFO] ---------------------------------
2019-04-07 22:12:04,529 [INFO] ---------------------------------
2019-04-07 22:12:04,529 [INFO] Summary:
2019-04-07 22:12:04,530 [INFO] Batch 89000, worst loss 0.029066 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 22:12:04,531 [INFO] Regularization: 904.380554 * 0.0000000100 = 0.0000090438 loss
2019-04-07 22:12:04,531 [INFO] unfolding 0, single step 89001
2019-04-07 22:12:04,531 [INFO] Sum of grad norms of most recent batch: 0.161861
2019-04-07 22:12:04,532 [INFO] ---------------------------------
2019-04-07 22:12:25,946 [INFO] ---------------------------------
2019-04-07 22:12:25,947 [INFO] Summary:
2019-04-07 22:12:25,948 [INFO] Batch 90000, worst loss 0.066672 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 22:12:25,948 [INFO] Regularization: 903.441284 * 0.0000000100 = 0.0000090344 loss
2019-04-07 22:12:25,948 [INFO] unfolding 0, single step 90001
2019-04-07 22:12:25,949 [INFO] Sum of grad norms of most recent batch: 0.018903
2019-04-07 22:12:25,949 [INFO] ---------------------------------
2019-04-07 22:13:03,240 [INFO] ---------------------------------
2019-04-07 22:13:03,241 [INFO] Evaluation:
2019-04-07 22:13:03,241 [INFO] Batch 90000, worst loss 0.103818 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:13:03,242 [INFO] ---------------------------------
2019-04-07 22:13:24,053 [INFO] ---------------------------------
2019-04-07 22:13:24,054 [INFO] Summary:
2019-04-07 22:13:24,055 [INFO] Batch 91000, worst loss 0.074214 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 22:13:24,056 [INFO] Regularization: 902.499695 * 0.0000000100 = 0.0000090250 loss
2019-04-07 22:13:24,057 [INFO] unfolding 0, single step 91001
2019-04-07 22:13:24,057 [INFO] Sum of grad norms of most recent batch: 0.143987
2019-04-07 22:13:24,058 [INFO] ---------------------------------
2019-04-07 22:13:44,872 [INFO] ---------------------------------
2019-04-07 22:13:44,873 [INFO] Summary:
2019-04-07 22:13:44,873 [INFO] Batch 92000, worst loss 0.036560 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 22:13:44,874 [INFO] Regularization: 901.943542 * 0.0000000100 = 0.0000090194 loss
2019-04-07 22:13:44,874 [INFO] unfolding 0, single step 92001
2019-04-07 22:13:44,875 [INFO] Sum of grad norms of most recent batch: 0.042509
2019-04-07 22:13:44,876 [INFO] ---------------------------------
2019-04-07 22:14:05,476 [INFO] ---------------------------------
2019-04-07 22:14:05,477 [INFO] Summary:
2019-04-07 22:14:05,477 [INFO] Batch 93000, worst loss 0.071588 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 22:14:05,478 [INFO] Regularization: 901.362732 * 0.0000000100 = 0.0000090136 loss
2019-04-07 22:14:05,478 [INFO] unfolding 0, single step 93001
2019-04-07 22:14:05,479 [INFO] Sum of grad norms of most recent batch: 0.055138
2019-04-07 22:14:05,479 [INFO] ---------------------------------
2019-04-07 22:14:26,178 [INFO] ---------------------------------
2019-04-07 22:14:26,179 [INFO] Summary:
2019-04-07 22:14:26,179 [INFO] Batch 94000, worst loss 0.062308 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 22:14:26,180 [INFO] Regularization: 900.816711 * 0.0000000100 = 0.0000090082 loss
2019-04-07 22:14:26,180 [INFO] unfolding 0, single step 94001
2019-04-07 22:14:26,181 [INFO] Sum of grad norms of most recent batch: 0.028683
2019-04-07 22:14:26,181 [INFO] ---------------------------------
2019-04-07 22:14:48,072 [INFO] ---------------------------------
2019-04-07 22:14:48,073 [INFO] Summary:
2019-04-07 22:14:48,074 [INFO] Batch 95000, worst loss 0.062191 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 22:14:48,075 [INFO] Regularization: 900.453552 * 0.0000000100 = 0.0000090045 loss
2019-04-07 22:14:48,076 [INFO] unfolding 0, single step 95001
2019-04-07 22:14:48,077 [INFO] Sum of grad norms of most recent batch: 0.040471
2019-04-07 22:14:48,078 [INFO] ---------------------------------
2019-04-07 22:15:09,573 [INFO] ---------------------------------
2019-04-07 22:15:09,574 [INFO] Summary:
2019-04-07 22:15:09,575 [INFO] Batch 96000, worst loss 0.041437 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 22:15:09,575 [INFO] Regularization: 899.979248 * 0.0000000100 = 0.0000089998 loss
2019-04-07 22:15:09,576 [INFO] unfolding 0, single step 96001
2019-04-07 22:15:09,576 [INFO] Sum of grad norms of most recent batch: 0.026905
2019-04-07 22:15:09,577 [INFO] ---------------------------------
2019-04-07 22:15:30,927 [INFO] ---------------------------------
2019-04-07 22:15:30,928 [INFO] Summary:
2019-04-07 22:15:30,928 [INFO] Batch 97000, worst loss 0.081795 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 22:15:30,929 [INFO] Regularization: 899.343567 * 0.0000000100 = 0.0000089934 loss
2019-04-07 22:15:30,929 [INFO] unfolding 0, single step 97001
2019-04-07 22:15:30,930 [INFO] Sum of grad norms of most recent batch: 0.038665
2019-04-07 22:15:30,930 [INFO] ---------------------------------
2019-04-07 22:15:52,433 [INFO] ---------------------------------
2019-04-07 22:15:52,434 [INFO] Summary:
2019-04-07 22:15:52,435 [INFO] Batch 98000, worst loss 0.053564 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 22:15:52,435 [INFO] Regularization: 898.726318 * 0.0000000100 = 0.0000089873 loss
2019-04-07 22:15:52,436 [INFO] unfolding 0, single step 98001
2019-04-07 22:15:52,436 [INFO] Sum of grad norms of most recent batch: 0.063476
2019-04-07 22:15:52,437 [INFO] ---------------------------------
2019-04-07 22:16:14,014 [INFO] ---------------------------------
2019-04-07 22:16:14,015 [INFO] Summary:
2019-04-07 22:16:14,015 [INFO] Batch 99000, worst loss 0.025198 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 22:16:14,016 [INFO] Regularization: 898.234680 * 0.0000000100 = 0.0000089823 loss
2019-04-07 22:16:14,016 [INFO] unfolding 0, single step 99001
2019-04-07 22:16:14,017 [INFO] Sum of grad norms of most recent batch: 0.049588
2019-04-07 22:16:14,018 [INFO] ---------------------------------
2019-04-07 22:16:35,804 [INFO] ---------------------------------
2019-04-07 22:16:35,805 [INFO] Summary:
2019-04-07 22:16:35,806 [INFO] Batch 100000, worst loss 0.070324 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 22:16:35,807 [INFO] Regularization: 897.948364 * 0.0000000100 = 0.0000089795 loss
2019-04-07 22:16:35,807 [INFO] unfolding 0, single step 100001
2019-04-07 22:16:35,808 [INFO] Sum of grad norms of most recent batch: 0.020567
2019-04-07 22:16:35,808 [INFO] ---------------------------------
2019-04-07 22:17:13,178 [INFO] ---------------------------------
2019-04-07 22:17:13,179 [INFO] Evaluation:
2019-04-07 22:17:13,180 [INFO] Batch 100000, worst loss 0.070247 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:17:13,181 [INFO] ---------------------------------
2019-04-07 22:17:34,223 [INFO] ---------------------------------
2019-04-07 22:17:34,224 [INFO] Summary:
2019-04-07 22:17:34,224 [INFO] Batch 101000, worst loss 0.082696 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 22:17:34,225 [INFO] Regularization: 897.509277 * 0.0000000100 = 0.0000089751 loss
2019-04-07 22:17:34,225 [INFO] unfolding 0, single step 101001
2019-04-07 22:17:34,226 [INFO] Sum of grad norms of most recent batch: 0.028289
2019-04-07 22:17:34,226 [INFO] ---------------------------------
2019-04-07 22:17:55,885 [INFO] ---------------------------------
2019-04-07 22:17:55,886 [INFO] Summary:
2019-04-07 22:17:55,886 [INFO] Batch 102000, worst loss 0.090596 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 22:17:55,887 [INFO] Regularization: 897.238892 * 0.0000000100 = 0.0000089724 loss
2019-04-07 22:17:55,887 [INFO] unfolding 0, single step 102001
2019-04-07 22:17:55,888 [INFO] Sum of grad norms of most recent batch: 0.030189
2019-04-07 22:17:55,888 [INFO] ---------------------------------
2019-04-07 22:18:16,431 [INFO] ---------------------------------
2019-04-07 22:18:16,432 [INFO] Summary:
2019-04-07 22:18:16,433 [INFO] Batch 103000, worst loss 0.032888 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 22:18:16,433 [INFO] Regularization: 897.006531 * 0.0000000100 = 0.0000089701 loss
2019-04-07 22:18:16,433 [INFO] unfolding 0, single step 103001
2019-04-07 22:18:16,434 [INFO] Sum of grad norms of most recent batch: 0.062231
2019-04-07 22:18:16,435 [INFO] ---------------------------------
2019-04-07 22:18:37,142 [INFO] ---------------------------------
2019-04-07 22:18:37,143 [INFO] Summary:
2019-04-07 22:18:37,144 [INFO] Batch 104000, worst loss 0.100119 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 22:18:37,144 [INFO] Regularization: 896.745056 * 0.0000000100 = 0.0000089675 loss
2019-04-07 22:18:37,144 [INFO] unfolding 0, single step 104001
2019-04-07 22:18:37,145 [INFO] Sum of grad norms of most recent batch: 0.064786
2019-04-07 22:18:37,145 [INFO] ---------------------------------
2019-04-07 22:18:58,705 [INFO] ---------------------------------
2019-04-07 22:18:58,706 [INFO] Summary:
2019-04-07 22:18:58,707 [INFO] Batch 105000, worst loss 0.046579 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 22:18:58,708 [INFO] Regularization: 896.464294 * 0.0000000100 = 0.0000089646 loss
2019-04-07 22:18:58,708 [INFO] unfolding 0, single step 105001
2019-04-07 22:18:58,709 [INFO] Sum of grad norms of most recent batch: 0.035875
2019-04-07 22:18:58,709 [INFO] ---------------------------------
2019-04-07 22:19:20,046 [INFO] ---------------------------------
2019-04-07 22:19:20,047 [INFO] Summary:
2019-04-07 22:19:20,047 [INFO] Batch 106000, worst loss 0.071199 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 22:19:20,048 [INFO] Regularization: 896.161987 * 0.0000000100 = 0.0000089616 loss
2019-04-07 22:19:20,048 [INFO] unfolding 0, single step 106001
2019-04-07 22:19:20,049 [INFO] Sum of grad norms of most recent batch: 0.027840
2019-04-07 22:19:20,050 [INFO] ---------------------------------
2019-04-07 22:19:41,060 [INFO] ---------------------------------
2019-04-07 22:19:41,061 [INFO] Summary:
2019-04-07 22:19:41,062 [INFO] Batch 107000, worst loss 0.059669 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 22:19:41,062 [INFO] Regularization: 895.875916 * 0.0000000100 = 0.0000089588 loss
2019-04-07 22:19:41,062 [INFO] unfolding 0, single step 107001
2019-04-07 22:19:41,063 [INFO] Sum of grad norms of most recent batch: 0.029828
2019-04-07 22:19:41,063 [INFO] ---------------------------------
2019-04-07 22:20:02,455 [INFO] ---------------------------------
2019-04-07 22:20:02,456 [INFO] Summary:
2019-04-07 22:20:02,456 [INFO] Batch 108000, worst loss 0.034768 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 22:20:02,457 [INFO] Regularization: 895.701599 * 0.0000000100 = 0.0000089570 loss
2019-04-07 22:20:02,457 [INFO] unfolding 0, single step 108001
2019-04-07 22:20:02,458 [INFO] Sum of grad norms of most recent batch: 0.030756
2019-04-07 22:20:02,458 [INFO] ---------------------------------
2019-04-07 22:20:23,128 [INFO] ---------------------------------
2019-04-07 22:20:23,128 [INFO] Summary:
2019-04-07 22:20:23,129 [INFO] Batch 109000, worst loss 0.134932 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 22:20:23,130 [INFO] Regularization: 895.541687 * 0.0000000100 = 0.0000089554 loss
2019-04-07 22:20:23,130 [INFO] unfolding 0, single step 109001
2019-04-07 22:20:23,131 [INFO] Sum of grad norms of most recent batch: 0.023301
2019-04-07 22:20:23,131 [INFO] ---------------------------------
2019-04-07 22:20:44,717 [INFO] ---------------------------------
2019-04-07 22:20:44,718 [INFO] Summary:
2019-04-07 22:20:44,719 [INFO] Batch 110000, worst loss 0.058103 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 22:20:44,719 [INFO] Regularization: 895.313416 * 0.0000000100 = 0.0000089531 loss
2019-04-07 22:20:44,719 [INFO] unfolding 0, single step 110001
2019-04-07 22:20:44,720 [INFO] Sum of grad norms of most recent batch: 0.023099
2019-04-07 22:20:44,720 [INFO] ---------------------------------
2019-04-07 22:21:22,090 [INFO] ---------------------------------
2019-04-07 22:21:22,092 [INFO] Evaluation:
2019-04-07 22:21:22,092 [INFO] Batch 110000, worst loss 0.062648 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:21:22,093 [INFO] ---------------------------------
2019-04-07 22:21:43,418 [INFO] ---------------------------------
2019-04-07 22:21:43,420 [INFO] Summary:
2019-04-07 22:21:43,421 [INFO] Batch 111000, worst loss 0.088792 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 22:21:43,421 [INFO] Regularization: 894.994263 * 0.0000000100 = 0.0000089499 loss
2019-04-07 22:21:43,422 [INFO] unfolding 0, single step 111001
2019-04-07 22:21:43,423 [INFO] Sum of grad norms of most recent batch: 1.711420
2019-04-07 22:21:43,423 [INFO] ---------------------------------
2019-04-07 22:22:04,843 [INFO] ---------------------------------
2019-04-07 22:22:04,844 [INFO] Summary:
2019-04-07 22:22:04,845 [INFO] Batch 112000, worst loss 0.013752 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 22:22:04,845 [INFO] Regularization: 894.831726 * 0.0000000100 = 0.0000089483 loss
2019-04-07 22:22:04,846 [INFO] unfolding 0, single step 112001
2019-04-07 22:22:04,846 [INFO] Sum of grad norms of most recent batch: 0.042379
2019-04-07 22:22:04,847 [INFO] ---------------------------------
2019-04-07 22:22:25,953 [INFO] ---------------------------------
2019-04-07 22:22:25,954 [INFO] Summary:
2019-04-07 22:22:25,955 [INFO] Batch 113000, worst loss 0.079140 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 22:22:25,955 [INFO] Regularization: 894.749939 * 0.0000000100 = 0.0000089475 loss
2019-04-07 22:22:25,955 [INFO] unfolding 0, single step 113001
2019-04-07 22:22:25,956 [INFO] Sum of grad norms of most recent batch: 0.029065
2019-04-07 22:22:25,957 [INFO] ---------------------------------
2019-04-07 22:22:46,861 [INFO] ---------------------------------
2019-04-07 22:22:46,862 [INFO] Summary:
2019-04-07 22:22:46,862 [INFO] Batch 114000, worst loss 0.040671 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 22:22:46,863 [INFO] Regularization: 894.646484 * 0.0000000100 = 0.0000089465 loss
2019-04-07 22:22:46,863 [INFO] unfolding 0, single step 114001
2019-04-07 22:22:46,864 [INFO] Sum of grad norms of most recent batch: 0.018717
2019-04-07 22:22:46,864 [INFO] ---------------------------------
2019-04-07 22:23:07,941 [INFO] ---------------------------------
2019-04-07 22:23:07,942 [INFO] Summary:
2019-04-07 22:23:07,944 [INFO] Batch 115000, worst loss 0.093417 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 22:23:07,944 [INFO] Regularization: 894.582947 * 0.0000000100 = 0.0000089458 loss
2019-04-07 22:23:07,945 [INFO] unfolding 0, single step 115001
2019-04-07 22:23:07,946 [INFO] Sum of grad norms of most recent batch: 0.024640
2019-04-07 22:23:07,947 [INFO] ---------------------------------
2019-04-07 22:23:28,927 [INFO] ---------------------------------
2019-04-07 22:23:28,928 [INFO] Summary:
2019-04-07 22:23:28,928 [INFO] Batch 116000, worst loss 0.093550 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 22:23:28,929 [INFO] Regularization: 894.415222 * 0.0000000100 = 0.0000089442 loss
2019-04-07 22:23:28,929 [INFO] unfolding 0, single step 116001
2019-04-07 22:23:28,930 [INFO] Sum of grad norms of most recent batch: 0.033403
2019-04-07 22:23:28,930 [INFO] ---------------------------------
2019-04-07 22:23:50,122 [INFO] ---------------------------------
2019-04-07 22:23:50,123 [INFO] Summary:
2019-04-07 22:23:50,123 [INFO] Batch 117000, worst loss 0.062632 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 22:23:50,124 [INFO] Regularization: 894.297607 * 0.0000000100 = 0.0000089430 loss
2019-04-07 22:23:50,124 [INFO] unfolding 0, single step 117001
2019-04-07 22:23:50,125 [INFO] Sum of grad norms of most recent batch: 0.039425
2019-04-07 22:23:50,126 [INFO] ---------------------------------
2019-04-07 22:24:11,456 [INFO] ---------------------------------
2019-04-07 22:24:11,457 [INFO] Summary:
2019-04-07 22:24:11,458 [INFO] Batch 118000, worst loss 0.080368 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 22:24:11,459 [INFO] Regularization: 894.160645 * 0.0000000100 = 0.0000089416 loss
2019-04-07 22:24:11,459 [INFO] unfolding 0, single step 118001
2019-04-07 22:24:11,460 [INFO] Sum of grad norms of most recent batch: 0.045519
2019-04-07 22:24:11,461 [INFO] ---------------------------------
2019-04-07 22:24:33,065 [INFO] ---------------------------------
2019-04-07 22:24:33,066 [INFO] Summary:
2019-04-07 22:24:33,067 [INFO] Batch 119000, worst loss 0.051419 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 22:24:33,067 [INFO] Regularization: 894.052185 * 0.0000000100 = 0.0000089405 loss
2019-04-07 22:24:33,068 [INFO] unfolding 0, single step 119001
2019-04-07 22:24:33,068 [INFO] Sum of grad norms of most recent batch: 0.029788
2019-04-07 22:24:33,069 [INFO] ---------------------------------
2019-04-07 22:24:53,875 [INFO] ---------------------------------
2019-04-07 22:24:53,876 [INFO] Summary:
2019-04-07 22:24:53,877 [INFO] Batch 120000, worst loss 0.077197 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 22:24:53,877 [INFO] Regularization: 893.913696 * 0.0000000100 = 0.0000089391 loss
2019-04-07 22:24:53,878 [INFO] unfolding 0, single step 120001
2019-04-07 22:24:53,878 [INFO] Sum of grad norms of most recent batch: 0.018083
2019-04-07 22:24:53,879 [INFO] ---------------------------------
2019-04-07 22:25:31,388 [INFO] ---------------------------------
2019-04-07 22:25:31,389 [INFO] Evaluation:
2019-04-07 22:25:31,390 [INFO] Batch 120000, worst loss 0.128800 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:25:31,390 [INFO] ---------------------------------
2019-04-07 22:25:52,512 [INFO] ---------------------------------
2019-04-07 22:25:52,513 [INFO] Summary:
2019-04-07 22:25:52,513 [INFO] Batch 121000, worst loss 0.031320 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 22:25:52,514 [INFO] Regularization: 893.787964 * 0.0000000100 = 0.0000089379 loss
2019-04-07 22:25:52,514 [INFO] unfolding 0, single step 121001
2019-04-07 22:25:52,515 [INFO] Sum of grad norms of most recent batch: 0.038998
2019-04-07 22:25:52,516 [INFO] ---------------------------------
2019-04-07 22:26:13,752 [INFO] ---------------------------------
2019-04-07 22:26:13,753 [INFO] Summary:
2019-04-07 22:26:13,753 [INFO] Batch 122000, worst loss 0.050370 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 22:26:13,754 [INFO] Regularization: 893.727844 * 0.0000000100 = 0.0000089373 loss
2019-04-07 22:26:13,754 [INFO] unfolding 0, single step 122001
2019-04-07 22:26:13,755 [INFO] Sum of grad norms of most recent batch: 0.023171
2019-04-07 22:26:13,755 [INFO] ---------------------------------
2019-04-07 22:26:35,053 [INFO] ---------------------------------
2019-04-07 22:26:35,054 [INFO] Summary:
2019-04-07 22:26:35,055 [INFO] Batch 123000, worst loss 0.028482 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 22:26:35,056 [INFO] Regularization: 893.647217 * 0.0000000100 = 0.0000089365 loss
2019-04-07 22:26:35,056 [INFO] unfolding 0, single step 123001
2019-04-07 22:26:35,056 [INFO] Sum of grad norms of most recent batch: 0.033349
2019-04-07 22:26:35,057 [INFO] ---------------------------------
2019-04-07 22:26:56,435 [INFO] ---------------------------------
2019-04-07 22:26:56,435 [INFO] Summary:
2019-04-07 22:26:56,436 [INFO] Batch 124000, worst loss 0.037846 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 22:26:56,436 [INFO] Regularization: 893.600891 * 0.0000000100 = 0.0000089360 loss
2019-04-07 22:26:56,437 [INFO] unfolding 0, single step 124001
2019-04-07 22:26:56,437 [INFO] Sum of grad norms of most recent batch: 0.026819
2019-04-07 22:26:56,438 [INFO] ---------------------------------
2019-04-07 22:27:17,598 [INFO] ---------------------------------
2019-04-07 22:27:17,599 [INFO] Summary:
2019-04-07 22:27:17,600 [INFO] Batch 125000, worst loss 0.030365 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 22:27:17,600 [INFO] Regularization: 893.531311 * 0.0000000100 = 0.0000089353 loss
2019-04-07 22:27:17,601 [INFO] unfolding 0, single step 125001
2019-04-07 22:27:17,601 [INFO] Sum of grad norms of most recent batch: 0.030104
2019-04-07 22:27:17,602 [INFO] ---------------------------------
2019-04-07 22:27:39,136 [INFO] ---------------------------------
2019-04-07 22:27:39,138 [INFO] Summary:
2019-04-07 22:27:39,138 [INFO] Batch 126000, worst loss 0.020622 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 22:27:39,139 [INFO] Regularization: 893.479553 * 0.0000000100 = 0.0000089348 loss
2019-04-07 22:27:39,139 [INFO] unfolding 0, single step 126001
2019-04-07 22:27:39,140 [INFO] Sum of grad norms of most recent batch: 0.021814
2019-04-07 22:27:39,140 [INFO] ---------------------------------
2019-04-07 22:27:59,959 [INFO] ---------------------------------
2019-04-07 22:27:59,961 [INFO] Summary:
2019-04-07 22:27:59,961 [INFO] Batch 127000, worst loss 0.027460 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 22:27:59,962 [INFO] Regularization: 893.419617 * 0.0000000100 = 0.0000089342 loss
2019-04-07 22:27:59,962 [INFO] unfolding 0, single step 127001
2019-04-07 22:27:59,963 [INFO] Sum of grad norms of most recent batch: 0.027112
2019-04-07 22:27:59,963 [INFO] ---------------------------------
2019-04-07 22:28:20,922 [INFO] ---------------------------------
2019-04-07 22:28:20,923 [INFO] Summary:
2019-04-07 22:28:20,924 [INFO] Batch 128000, worst loss 0.051628 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 22:28:20,924 [INFO] Regularization: 893.371765 * 0.0000000100 = 0.0000089337 loss
2019-04-07 22:28:20,924 [INFO] unfolding 0, single step 128001
2019-04-07 22:28:20,925 [INFO] Sum of grad norms of most recent batch: 0.058731
2019-04-07 22:28:20,925 [INFO] ---------------------------------
2019-04-07 22:28:41,721 [INFO] ---------------------------------
2019-04-07 22:28:41,722 [INFO] Summary:
2019-04-07 22:28:41,723 [INFO] Batch 129000, worst loss 0.110303 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 22:28:41,724 [INFO] Regularization: 893.318359 * 0.0000000100 = 0.0000089332 loss
2019-04-07 22:28:41,724 [INFO] unfolding 0, single step 129001
2019-04-07 22:28:41,725 [INFO] Sum of grad norms of most recent batch: 0.022928
2019-04-07 22:28:41,726 [INFO] ---------------------------------
2019-04-07 22:29:02,914 [INFO] ---------------------------------
2019-04-07 22:29:02,914 [INFO] Summary:
2019-04-07 22:29:02,915 [INFO] Batch 130000, worst loss 0.110312 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 22:29:02,916 [INFO] Regularization: 893.256226 * 0.0000000100 = 0.0000089326 loss
2019-04-07 22:29:02,916 [INFO] unfolding 0, single step 130001
2019-04-07 22:29:02,917 [INFO] Sum of grad norms of most recent batch: 0.198730
2019-04-07 22:29:02,917 [INFO] ---------------------------------
2019-04-07 22:29:40,376 [INFO] ---------------------------------
2019-04-07 22:29:40,378 [INFO] Evaluation:
2019-04-07 22:29:40,378 [INFO] Batch 130000, worst loss 0.138183 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:29:40,379 [INFO] ---------------------------------
2019-04-07 22:30:01,838 [INFO] ---------------------------------
2019-04-07 22:30:01,838 [INFO] Summary:
2019-04-07 22:30:01,839 [INFO] Batch 131000, worst loss 0.184866 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:30:01,840 [INFO] Regularization: 893.184998 * 0.0000000100 = 0.0000089319 loss
2019-04-07 22:30:01,840 [INFO] unfolding 0, single step 131001
2019-04-07 22:30:01,841 [INFO] Sum of grad norms of most recent batch: 0.011231
2019-04-07 22:30:01,841 [INFO] ---------------------------------
2019-04-07 22:30:22,484 [INFO] ---------------------------------
2019-04-07 22:30:22,485 [INFO] Summary:
2019-04-07 22:30:22,486 [INFO] Batch 132000, worst loss 0.047935 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:30:22,486 [INFO] Regularization: 893.155884 * 0.0000000100 = 0.0000089316 loss
2019-04-07 22:30:22,486 [INFO] unfolding 0, single step 132001
2019-04-07 22:30:22,487 [INFO] Sum of grad norms of most recent batch: 0.040243
2019-04-07 22:30:22,487 [INFO] ---------------------------------
2019-04-07 22:30:42,986 [INFO] ---------------------------------
2019-04-07 22:30:42,987 [INFO] Summary:
2019-04-07 22:30:42,988 [INFO] Batch 133000, worst loss 0.079556 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:30:42,988 [INFO] Regularization: 893.143311 * 0.0000000100 = 0.0000089314 loss
2019-04-07 22:30:42,989 [INFO] unfolding 0, single step 133001
2019-04-07 22:30:42,990 [INFO] Sum of grad norms of most recent batch: 0.025039
2019-04-07 22:30:42,990 [INFO] ---------------------------------
2019-04-07 22:31:03,756 [INFO] ---------------------------------
2019-04-07 22:31:03,758 [INFO] Summary:
2019-04-07 22:31:03,758 [INFO] Batch 134000, worst loss 0.072795 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:31:03,759 [INFO] Regularization: 893.096619 * 0.0000000100 = 0.0000089310 loss
2019-04-07 22:31:03,759 [INFO] unfolding 0, single step 134001
2019-04-07 22:31:03,760 [INFO] Sum of grad norms of most recent batch: 0.031202
2019-04-07 22:31:03,760 [INFO] ---------------------------------
2019-04-07 22:31:25,254 [INFO] ---------------------------------
2019-04-07 22:31:25,255 [INFO] Summary:
2019-04-07 22:31:25,256 [INFO] Batch 135000, worst loss 0.050427 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:31:25,256 [INFO] Regularization: 893.070068 * 0.0000000100 = 0.0000089307 loss
2019-04-07 22:31:25,257 [INFO] unfolding 0, single step 135001
2019-04-07 22:31:25,257 [INFO] Sum of grad norms of most recent batch: 0.021626
2019-04-07 22:31:25,258 [INFO] ---------------------------------
2019-04-07 22:31:46,800 [INFO] ---------------------------------
2019-04-07 22:31:46,801 [INFO] Summary:
2019-04-07 22:31:46,801 [INFO] Batch 136000, worst loss 0.037004 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:31:46,802 [INFO] Regularization: 893.032288 * 0.0000000100 = 0.0000089303 loss
2019-04-07 22:31:46,802 [INFO] unfolding 0, single step 136001
2019-04-07 22:31:46,803 [INFO] Sum of grad norms of most recent batch: 0.068882
2019-04-07 22:31:46,803 [INFO] ---------------------------------
2019-04-07 22:32:08,026 [INFO] ---------------------------------
2019-04-07 22:32:08,027 [INFO] Summary:
2019-04-07 22:32:08,027 [INFO] Batch 137000, worst loss 0.062829 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:32:08,028 [INFO] Regularization: 892.997192 * 0.0000000100 = 0.0000089300 loss
2019-04-07 22:32:08,028 [INFO] unfolding 0, single step 137001
2019-04-07 22:32:08,029 [INFO] Sum of grad norms of most recent batch: 0.025297
2019-04-07 22:32:08,029 [INFO] ---------------------------------
2019-04-07 22:32:29,024 [INFO] ---------------------------------
2019-04-07 22:32:29,025 [INFO] Summary:
2019-04-07 22:32:29,026 [INFO] Batch 138000, worst loss 0.020561 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:32:29,027 [INFO] Regularization: 892.971069 * 0.0000000100 = 0.0000089297 loss
2019-04-07 22:32:29,027 [INFO] unfolding 0, single step 138001
2019-04-07 22:32:29,028 [INFO] Sum of grad norms of most recent batch: 4.141538
2019-04-07 22:32:29,028 [INFO] ---------------------------------
2019-04-07 22:32:50,398 [INFO] ---------------------------------
2019-04-07 22:32:50,399 [INFO] Summary:
2019-04-07 22:32:50,400 [INFO] Batch 139000, worst loss 0.060177 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:32:50,401 [INFO] Regularization: 892.934937 * 0.0000000100 = 0.0000089293 loss
2019-04-07 22:32:50,401 [INFO] unfolding 0, single step 139001
2019-04-07 22:32:50,402 [INFO] Sum of grad norms of most recent batch: 0.015378
2019-04-07 22:32:50,402 [INFO] ---------------------------------
2019-04-07 22:33:11,932 [INFO] ---------------------------------
2019-04-07 22:33:11,933 [INFO] Summary:
2019-04-07 22:33:11,933 [INFO] Batch 140000, worst loss 0.067264 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:33:11,934 [INFO] Regularization: 892.915283 * 0.0000000100 = 0.0000089292 loss
2019-04-07 22:33:11,934 [INFO] unfolding 0, single step 140001
2019-04-07 22:33:11,935 [INFO] Sum of grad norms of most recent batch: 0.062023
2019-04-07 22:33:11,936 [INFO] ---------------------------------
2019-04-07 22:33:49,476 [INFO] ---------------------------------
2019-04-07 22:33:49,477 [INFO] Evaluation:
2019-04-07 22:33:49,477 [INFO] Batch 140000, worst loss 0.078316 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:33:49,478 [INFO] ---------------------------------
2019-04-07 22:34:10,703 [INFO] ---------------------------------
2019-04-07 22:34:10,704 [INFO] Summary:
2019-04-07 22:34:10,705 [INFO] Batch 141000, worst loss 0.041144 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:34:10,706 [INFO] Regularization: 892.871948 * 0.0000000100 = 0.0000089287 loss
2019-04-07 22:34:10,707 [INFO] unfolding 0, single step 141001
2019-04-07 22:34:10,708 [INFO] Sum of grad norms of most recent batch: 0.024072
2019-04-07 22:34:10,708 [INFO] ---------------------------------
2019-04-07 22:34:31,762 [INFO] ---------------------------------
2019-04-07 22:34:31,763 [INFO] Summary:
2019-04-07 22:34:31,764 [INFO] Batch 142000, worst loss 0.108891 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:34:31,765 [INFO] Regularization: 892.856201 * 0.0000000100 = 0.0000089286 loss
2019-04-07 22:34:31,766 [INFO] unfolding 0, single step 142001
2019-04-07 22:34:31,769 [INFO] Sum of grad norms of most recent batch: 0.031492
2019-04-07 22:34:31,770 [INFO] ---------------------------------
2019-04-07 22:34:53,165 [INFO] ---------------------------------
2019-04-07 22:34:53,166 [INFO] Summary:
2019-04-07 22:34:53,167 [INFO] Batch 143000, worst loss 0.039804 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:34:53,167 [INFO] Regularization: 892.834167 * 0.0000000100 = 0.0000089283 loss
2019-04-07 22:34:53,168 [INFO] unfolding 0, single step 143001
2019-04-07 22:34:53,169 [INFO] Sum of grad norms of most recent batch: 0.087604
2019-04-07 22:34:53,169 [INFO] ---------------------------------
2019-04-07 22:35:13,944 [INFO] ---------------------------------
2019-04-07 22:35:13,945 [INFO] Summary:
2019-04-07 22:35:13,946 [INFO] Batch 144000, worst loss 0.046714 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:35:13,946 [INFO] Regularization: 892.819092 * 0.0000000100 = 0.0000089282 loss
2019-04-07 22:35:13,947 [INFO] unfolding 0, single step 144001
2019-04-07 22:35:13,948 [INFO] Sum of grad norms of most recent batch: 0.014973
2019-04-07 22:35:13,948 [INFO] ---------------------------------
2019-04-07 22:35:34,942 [INFO] ---------------------------------
2019-04-07 22:35:34,943 [INFO] Summary:
2019-04-07 22:35:34,944 [INFO] Batch 145000, worst loss 0.030318 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:35:34,944 [INFO] Regularization: 892.804504 * 0.0000000100 = 0.0000089280 loss
2019-04-07 22:35:34,945 [INFO] unfolding 0, single step 145001
2019-04-07 22:35:34,946 [INFO] Sum of grad norms of most recent batch: 0.043115
2019-04-07 22:35:34,947 [INFO] ---------------------------------
2019-04-07 22:35:56,254 [INFO] ---------------------------------
2019-04-07 22:35:56,255 [INFO] Summary:
2019-04-07 22:35:56,255 [INFO] Batch 146000, worst loss 0.050081 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:35:56,256 [INFO] Regularization: 892.790405 * 0.0000000100 = 0.0000089279 loss
2019-04-07 22:35:56,256 [INFO] unfolding 0, single step 146001
2019-04-07 22:35:56,257 [INFO] Sum of grad norms of most recent batch: 0.025490
2019-04-07 22:35:56,257 [INFO] ---------------------------------
2019-04-07 22:36:17,439 [INFO] ---------------------------------
2019-04-07 22:36:17,440 [INFO] Summary:
2019-04-07 22:36:17,441 [INFO] Batch 147000, worst loss 0.078945 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:36:17,441 [INFO] Regularization: 892.776855 * 0.0000000100 = 0.0000089278 loss
2019-04-07 22:36:17,442 [INFO] unfolding 0, single step 147001
2019-04-07 22:36:17,442 [INFO] Sum of grad norms of most recent batch: 0.014438
2019-04-07 22:36:17,443 [INFO] ---------------------------------
2019-04-07 22:36:37,956 [INFO] ---------------------------------
2019-04-07 22:36:37,957 [INFO] Summary:
2019-04-07 22:36:37,957 [INFO] Batch 148000, worst loss 0.136362 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:36:37,958 [INFO] Regularization: 892.767151 * 0.0000000100 = 0.0000089277 loss
2019-04-07 22:36:37,958 [INFO] unfolding 0, single step 148001
2019-04-07 22:36:37,959 [INFO] Sum of grad norms of most recent batch: 0.022831
2019-04-07 22:36:37,959 [INFO] ---------------------------------
2019-04-07 22:36:59,712 [INFO] ---------------------------------
2019-04-07 22:36:59,713 [INFO] Summary:
2019-04-07 22:36:59,714 [INFO] Batch 149000, worst loss 0.038751 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:36:59,715 [INFO] Regularization: 892.747437 * 0.0000000100 = 0.0000089275 loss
2019-04-07 22:36:59,715 [INFO] unfolding 0, single step 149001
2019-04-07 22:36:59,716 [INFO] Sum of grad norms of most recent batch: 0.019729
2019-04-07 22:36:59,717 [INFO] ---------------------------------
2019-04-07 22:37:21,276 [INFO] ---------------------------------
2019-04-07 22:37:21,277 [INFO] Summary:
2019-04-07 22:37:21,278 [INFO] Batch 150000, worst loss 0.007205 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 22:37:21,279 [INFO] Regularization: 892.727112 * 0.0000000100 = 0.0000089273 loss
2019-04-07 22:37:21,280 [INFO] unfolding 0, single step 150001
2019-04-07 22:37:21,281 [INFO] Sum of grad norms of most recent batch: 0.018185
2019-04-07 22:37:21,282 [INFO] ---------------------------------
2019-04-07 22:37:58,753 [INFO] ---------------------------------
2019-04-07 22:37:58,754 [INFO] Evaluation:
2019-04-07 22:37:58,754 [INFO] Batch 150000, worst loss 0.115988 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:37:58,755 [INFO] ---------------------------------
2019-04-07 22:37:58,756 [INFO] Finished training, saved to file transition/1554653688/1554669478_3_transition_final.pth
2019-04-07 22:37:58,942 [INFO] ---------------------------------
2019-04-07 22:37:58,943 [INFO] Training model #4: (11, 64, 8) @ 3
2019-04-07 22:38:20,758 [INFO] ---------------------------------
2019-04-07 22:38:20,760 [INFO] Summary:
2019-04-07 22:38:20,760 [INFO] Batch 1000, worst loss 34.128628 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:38:20,761 [INFO] Regularization: 7187.667969 * 0.0000000100 = 0.0000718767 loss
2019-04-07 22:38:20,761 [INFO] unfolding 0, single step 1001
2019-04-07 22:38:20,762 [INFO] Sum of grad norms of most recent batch: 16.171562
2019-04-07 22:38:20,762 [INFO] ---------------------------------
2019-04-07 22:38:42,572 [INFO] ---------------------------------
2019-04-07 22:38:42,573 [INFO] Summary:
2019-04-07 22:38:42,574 [INFO] Batch 2000, worst loss 0.131966 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:38:42,575 [INFO] Regularization: 4726.939941 * 0.0000000100 = 0.0000472694 loss
2019-04-07 22:38:42,575 [INFO] unfolding 0, single step 2001
2019-04-07 22:38:42,576 [INFO] Sum of grad norms of most recent batch: 1.812487
2019-04-07 22:38:42,577 [INFO] ---------------------------------
2019-04-07 22:39:04,017 [INFO] ---------------------------------
2019-04-07 22:39:04,018 [INFO] Summary:
2019-04-07 22:39:04,018 [INFO] Batch 3000, worst loss 0.147051 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:39:04,019 [INFO] Regularization: 3295.780029 * 0.0000000100 = 0.0000329578 loss
2019-04-07 22:39:04,019 [INFO] unfolding 0, single step 3001
2019-04-07 22:39:04,020 [INFO] Sum of grad norms of most recent batch: 4.433410
2019-04-07 22:39:04,021 [INFO] ---------------------------------
2019-04-07 22:39:25,496 [INFO] ---------------------------------
2019-04-07 22:39:25,497 [INFO] Summary:
2019-04-07 22:39:25,498 [INFO] Batch 4000, worst loss 0.040460 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:39:25,498 [INFO] Regularization: 3018.444824 * 0.0000000100 = 0.0000301844 loss
2019-04-07 22:39:25,499 [INFO] unfolding 0, single step 4001
2019-04-07 22:39:25,499 [INFO] Sum of grad norms of most recent batch: 3.151237
2019-04-07 22:39:25,500 [INFO] ---------------------------------
2019-04-07 22:39:47,240 [INFO] ---------------------------------
2019-04-07 22:39:47,241 [INFO] Summary:
2019-04-07 22:39:47,242 [INFO] Batch 5000, worst loss 0.148283 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:39:47,242 [INFO] Regularization: 2806.041992 * 0.0000000100 = 0.0000280604 loss
2019-04-07 22:39:47,243 [INFO] unfolding 0, single step 5001
2019-04-07 22:39:47,243 [INFO] Sum of grad norms of most recent batch: 1.258315
2019-04-07 22:39:47,244 [INFO] ---------------------------------
2019-04-07 22:40:09,165 [INFO] ---------------------------------
2019-04-07 22:40:09,166 [INFO] Summary:
2019-04-07 22:40:09,166 [INFO] Batch 6000, worst loss 0.068497 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:40:09,167 [INFO] Regularization: 2621.132812 * 0.0000000100 = 0.0000262113 loss
2019-04-07 22:40:09,167 [INFO] unfolding 0, single step 6001
2019-04-07 22:40:09,168 [INFO] Sum of grad norms of most recent batch: 1.476023
2019-04-07 22:40:09,168 [INFO] ---------------------------------
2019-04-07 22:40:31,409 [INFO] ---------------------------------
2019-04-07 22:40:31,410 [INFO] Summary:
2019-04-07 22:40:31,411 [INFO] Batch 7000, worst loss 0.109652 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:40:31,411 [INFO] Regularization: 2433.703857 * 0.0000000100 = 0.0000243370 loss
2019-04-07 22:40:31,412 [INFO] unfolding 0, single step 7001
2019-04-07 22:40:31,412 [INFO] Sum of grad norms of most recent batch: 2.059021
2019-04-07 22:40:31,413 [INFO] ---------------------------------
2019-04-07 22:40:53,578 [INFO] ---------------------------------
2019-04-07 22:40:53,579 [INFO] Summary:
2019-04-07 22:40:53,579 [INFO] Batch 8000, worst loss 0.067580 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:40:53,580 [INFO] Regularization: 2316.368164 * 0.0000000100 = 0.0000231637 loss
2019-04-07 22:40:53,580 [INFO] unfolding 0, single step 8001
2019-04-07 22:40:53,581 [INFO] Sum of grad norms of most recent batch: 3.244570
2019-04-07 22:40:53,581 [INFO] ---------------------------------
2019-04-07 22:41:15,802 [INFO] ---------------------------------
2019-04-07 22:41:15,803 [INFO] Summary:
2019-04-07 22:41:15,804 [INFO] Batch 9000, worst loss 0.161626 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:41:15,804 [INFO] Regularization: 2221.903564 * 0.0000000100 = 0.0000222190 loss
2019-04-07 22:41:15,805 [INFO] unfolding 0, single step 9001
2019-04-07 22:41:15,805 [INFO] Sum of grad norms of most recent batch: 0.670164
2019-04-07 22:41:15,806 [INFO] ---------------------------------
2019-04-07 22:41:38,013 [INFO] ---------------------------------
2019-04-07 22:41:38,014 [INFO] Summary:
2019-04-07 22:41:38,014 [INFO] Batch 10000, worst loss 0.090011 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:41:38,015 [INFO] Regularization: 2186.034424 * 0.0000000100 = 0.0000218603 loss
2019-04-07 22:41:38,015 [INFO] unfolding 0, single step 10001
2019-04-07 22:41:38,016 [INFO] Sum of grad norms of most recent batch: 3.128097
2019-04-07 22:41:38,016 [INFO] ---------------------------------
2019-04-07 22:42:15,414 [INFO] ---------------------------------
2019-04-07 22:42:15,415 [INFO] Evaluation:
2019-04-07 22:42:15,416 [INFO] Batch 10000, worst loss 0.095545 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:42:15,416 [INFO] ---------------------------------
2019-04-07 22:42:36,668 [INFO] ---------------------------------
2019-04-07 22:42:36,669 [INFO] Summary:
2019-04-07 22:42:36,670 [INFO] Batch 11000, worst loss 0.132497 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:42:36,671 [INFO] Regularization: 2212.075928 * 0.0000000100 = 0.0000221208 loss
2019-04-07 22:42:36,671 [INFO] unfolding 0, single step 11001
2019-04-07 22:42:36,672 [INFO] Sum of grad norms of most recent batch: 1.751827
2019-04-07 22:42:36,673 [INFO] ---------------------------------
2019-04-07 22:42:58,727 [INFO] ---------------------------------
2019-04-07 22:42:58,728 [INFO] Summary:
2019-04-07 22:42:58,729 [INFO] Batch 12000, worst loss 0.144836 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:42:58,729 [INFO] Regularization: 2171.037354 * 0.0000000100 = 0.0000217104 loss
2019-04-07 22:42:58,729 [INFO] unfolding 0, single step 12001
2019-04-07 22:42:58,730 [INFO] Sum of grad norms of most recent batch: 1.925792
2019-04-07 22:42:58,730 [INFO] ---------------------------------
2019-04-07 22:43:20,440 [INFO] ---------------------------------
2019-04-07 22:43:20,441 [INFO] Summary:
2019-04-07 22:43:20,441 [INFO] Batch 13000, worst loss 0.112212 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:43:20,442 [INFO] Regularization: 2104.414062 * 0.0000000100 = 0.0000210441 loss
2019-04-07 22:43:20,442 [INFO] unfolding 0, single step 13001
2019-04-07 22:43:20,443 [INFO] Sum of grad norms of most recent batch: 0.883342
2019-04-07 22:43:20,443 [INFO] ---------------------------------
2019-04-07 22:43:42,771 [INFO] ---------------------------------
2019-04-07 22:43:42,772 [INFO] Summary:
2019-04-07 22:43:42,773 [INFO] Batch 14000, worst loss 0.154302 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:43:42,774 [INFO] Regularization: 2055.536377 * 0.0000000100 = 0.0000205554 loss
2019-04-07 22:43:42,774 [INFO] unfolding 0, single step 14001
2019-04-07 22:43:42,775 [INFO] Sum of grad norms of most recent batch: 1.792503
2019-04-07 22:43:42,776 [INFO] ---------------------------------
2019-04-07 22:44:04,282 [INFO] ---------------------------------
2019-04-07 22:44:04,283 [INFO] Summary:
2019-04-07 22:44:04,285 [INFO] Batch 15000, worst loss 0.152386 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:44:04,285 [INFO] Regularization: 2076.371338 * 0.0000000100 = 0.0000207637 loss
2019-04-07 22:44:04,286 [INFO] unfolding 0, single step 15001
2019-04-07 22:44:04,287 [INFO] Sum of grad norms of most recent batch: 1.187894
2019-04-07 22:44:04,288 [INFO] ---------------------------------
2019-04-07 22:44:25,642 [INFO] ---------------------------------
2019-04-07 22:44:25,643 [INFO] Summary:
2019-04-07 22:44:25,644 [INFO] Batch 16000, worst loss 0.032258 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:44:25,645 [INFO] Regularization: 2122.013184 * 0.0000000100 = 0.0000212201 loss
2019-04-07 22:44:25,646 [INFO] unfolding 0, single step 16001
2019-04-07 22:44:25,647 [INFO] Sum of grad norms of most recent batch: 0.654521
2019-04-07 22:44:25,648 [INFO] ---------------------------------
2019-04-07 22:44:47,018 [INFO] ---------------------------------
2019-04-07 22:44:47,019 [INFO] Summary:
2019-04-07 22:44:47,020 [INFO] Batch 17000, worst loss 0.104986 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:44:47,020 [INFO] Regularization: 2050.521973 * 0.0000000100 = 0.0000205052 loss
2019-04-07 22:44:47,021 [INFO] unfolding 0, single step 17001
2019-04-07 22:44:47,022 [INFO] Sum of grad norms of most recent batch: 1.337330
2019-04-07 22:44:47,022 [INFO] ---------------------------------
2019-04-07 22:45:08,865 [INFO] ---------------------------------
2019-04-07 22:45:08,866 [INFO] Summary:
2019-04-07 22:45:08,867 [INFO] Batch 18000, worst loss 0.110347 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:45:08,868 [INFO] Regularization: 2072.535400 * 0.0000000100 = 0.0000207254 loss
2019-04-07 22:45:08,868 [INFO] unfolding 0, single step 18001
2019-04-07 22:45:08,868 [INFO] Sum of grad norms of most recent batch: 1.243172
2019-04-07 22:45:08,869 [INFO] ---------------------------------
2019-04-07 22:45:30,166 [INFO] ---------------------------------
2019-04-07 22:45:30,167 [INFO] Summary:
2019-04-07 22:45:30,168 [INFO] Batch 19000, worst loss 0.102066 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:45:30,168 [INFO] Regularization: 2023.300537 * 0.0000000100 = 0.0000202330 loss
2019-04-07 22:45:30,169 [INFO] unfolding 0, single step 19001
2019-04-07 22:45:30,169 [INFO] Sum of grad norms of most recent batch: 0.794354
2019-04-07 22:45:30,170 [INFO] ---------------------------------
2019-04-07 22:45:51,656 [INFO] ---------------------------------
2019-04-07 22:45:51,657 [INFO] Summary:
2019-04-07 22:45:51,657 [INFO] Batch 20000, worst loss 0.110128 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:45:51,658 [INFO] Regularization: 2046.432861 * 0.0000000100 = 0.0000204643 loss
2019-04-07 22:45:51,658 [INFO] unfolding 0, single step 20001
2019-04-07 22:45:51,659 [INFO] Sum of grad norms of most recent batch: 1.794414
2019-04-07 22:45:51,660 [INFO] ---------------------------------
2019-04-07 22:46:29,073 [INFO] ---------------------------------
2019-04-07 22:46:29,074 [INFO] Evaluation:
2019-04-07 22:46:29,074 [INFO] Batch 20000, worst loss 0.161524 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:46:29,075 [INFO] ---------------------------------
2019-04-07 22:46:51,746 [INFO] ---------------------------------
2019-04-07 22:46:51,747 [INFO] Summary:
2019-04-07 22:46:51,748 [INFO] Batch 21000, worst loss 0.072786 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:46:51,748 [INFO] Regularization: 1983.276123 * 0.0000000100 = 0.0000198328 loss
2019-04-07 22:46:51,748 [INFO] unfolding 0, single step 21001
2019-04-07 22:46:51,749 [INFO] Sum of grad norms of most recent batch: 2.919303
2019-04-07 22:46:51,750 [INFO] ---------------------------------
2019-04-07 22:47:13,155 [INFO] ---------------------------------
2019-04-07 22:47:13,156 [INFO] Summary:
2019-04-07 22:47:13,156 [INFO] Batch 22000, worst loss 0.118692 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:47:13,157 [INFO] Regularization: 1989.720459 * 0.0000000100 = 0.0000198972 loss
2019-04-07 22:47:13,157 [INFO] unfolding 0, single step 22001
2019-04-07 22:47:13,158 [INFO] Sum of grad norms of most recent batch: 1.323637
2019-04-07 22:47:13,158 [INFO] ---------------------------------
2019-04-07 22:47:34,625 [INFO] ---------------------------------
2019-04-07 22:47:34,626 [INFO] Summary:
2019-04-07 22:47:34,626 [INFO] Batch 23000, worst loss 0.073905 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:47:34,627 [INFO] Regularization: 2006.112915 * 0.0000000100 = 0.0000200611 loss
2019-04-07 22:47:34,627 [INFO] unfolding 0, single step 23001
2019-04-07 22:47:34,628 [INFO] Sum of grad norms of most recent batch: 0.706566
2019-04-07 22:47:34,628 [INFO] ---------------------------------
2019-04-07 22:47:55,945 [INFO] ---------------------------------
2019-04-07 22:47:55,946 [INFO] Summary:
2019-04-07 22:47:55,947 [INFO] Batch 24000, worst loss 0.144122 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:47:55,947 [INFO] Regularization: 1988.227173 * 0.0000000100 = 0.0000198823 loss
2019-04-07 22:47:55,948 [INFO] unfolding 0, single step 24001
2019-04-07 22:47:55,948 [INFO] Sum of grad norms of most recent batch: 2.128574
2019-04-07 22:47:55,949 [INFO] ---------------------------------
2019-04-07 22:48:18,067 [INFO] ---------------------------------
2019-04-07 22:48:18,068 [INFO] Summary:
2019-04-07 22:48:18,069 [INFO] Batch 25000, worst loss 0.046148 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:48:18,070 [INFO] Regularization: 1931.182251 * 0.0000000100 = 0.0000193118 loss
2019-04-07 22:48:18,070 [INFO] unfolding 0, single step 25001
2019-04-07 22:48:18,071 [INFO] Sum of grad norms of most recent batch: 0.584459
2019-04-07 22:48:18,071 [INFO] ---------------------------------
2019-04-07 22:48:40,191 [INFO] ---------------------------------
2019-04-07 22:48:40,192 [INFO] Summary:
2019-04-07 22:48:40,193 [INFO] Batch 26000, worst loss 0.060192 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:48:40,193 [INFO] Regularization: 1932.557739 * 0.0000000100 = 0.0000193256 loss
2019-04-07 22:48:40,194 [INFO] unfolding 0, single step 26001
2019-04-07 22:48:40,194 [INFO] Sum of grad norms of most recent batch: 1.805077
2019-04-07 22:48:40,195 [INFO] ---------------------------------
2019-04-07 22:49:02,486 [INFO] ---------------------------------
2019-04-07 22:49:02,487 [INFO] Summary:
2019-04-07 22:49:02,487 [INFO] Batch 27000, worst loss 0.096520 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:49:02,488 [INFO] Regularization: 1939.052979 * 0.0000000100 = 0.0000193905 loss
2019-04-07 22:49:02,488 [INFO] unfolding 0, single step 27001
2019-04-07 22:49:02,489 [INFO] Sum of grad norms of most recent batch: 1.007119
2019-04-07 22:49:02,489 [INFO] ---------------------------------
2019-04-07 22:49:24,989 [INFO] ---------------------------------
2019-04-07 22:49:24,990 [INFO] Summary:
2019-04-07 22:49:24,991 [INFO] Batch 28000, worst loss 0.099948 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:49:24,991 [INFO] Regularization: 1939.980347 * 0.0000000100 = 0.0000193998 loss
2019-04-07 22:49:24,992 [INFO] unfolding 0, single step 28001
2019-04-07 22:49:24,992 [INFO] Sum of grad norms of most recent batch: 0.774719
2019-04-07 22:49:24,993 [INFO] ---------------------------------
2019-04-07 22:49:47,087 [INFO] ---------------------------------
2019-04-07 22:49:47,088 [INFO] Summary:
2019-04-07 22:49:47,089 [INFO] Batch 29000, worst loss 0.100558 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:49:47,090 [INFO] Regularization: 1955.918213 * 0.0000000100 = 0.0000195592 loss
2019-04-07 22:49:47,090 [INFO] unfolding 0, single step 29001
2019-04-07 22:49:47,091 [INFO] Sum of grad norms of most recent batch: 3.117370
2019-04-07 22:49:47,091 [INFO] ---------------------------------
2019-04-07 22:50:09,118 [INFO] ---------------------------------
2019-04-07 22:50:09,119 [INFO] Summary:
2019-04-07 22:50:09,119 [INFO] Batch 30000, worst loss 0.041588 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 22:50:09,120 [INFO] Regularization: 1868.594604 * 0.0000000100 = 0.0000186859 loss
2019-04-07 22:50:09,120 [INFO] unfolding 0, single step 30001
2019-04-07 22:50:09,121 [INFO] Sum of grad norms of most recent batch: 0.518106
2019-04-07 22:50:09,121 [INFO] ---------------------------------
2019-04-07 22:50:46,525 [INFO] ---------------------------------
2019-04-07 22:50:46,526 [INFO] Evaluation:
2019-04-07 22:50:46,527 [INFO] Batch 30000, worst loss 0.147295 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:50:46,527 [INFO] ---------------------------------
2019-04-07 22:51:08,476 [INFO] ---------------------------------
2019-04-07 22:51:08,477 [INFO] Summary:
2019-04-07 22:51:08,478 [INFO] Batch 31000, worst loss 0.114560 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 22:51:08,479 [INFO] Regularization: 1796.121704 * 0.0000000100 = 0.0000179612 loss
2019-04-07 22:51:08,479 [INFO] unfolding 0, single step 31001
2019-04-07 22:51:08,480 [INFO] Sum of grad norms of most recent batch: 1.731266
2019-04-07 22:51:08,480 [INFO] ---------------------------------
2019-04-07 22:51:30,526 [INFO] ---------------------------------
2019-04-07 22:51:30,527 [INFO] Summary:
2019-04-07 22:51:30,528 [INFO] Batch 32000, worst loss 0.113391 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 22:51:30,528 [INFO] Regularization: 1711.915283 * 0.0000000100 = 0.0000171192 loss
2019-04-07 22:51:30,528 [INFO] unfolding 0, single step 32001
2019-04-07 22:51:30,529 [INFO] Sum of grad norms of most recent batch: 0.264321
2019-04-07 22:51:30,529 [INFO] ---------------------------------
2019-04-07 22:51:51,371 [INFO] ---------------------------------
2019-04-07 22:51:51,372 [INFO] Summary:
2019-04-07 22:51:51,373 [INFO] Batch 33000, worst loss 0.068449 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 22:51:51,374 [INFO] Regularization: 1693.696777 * 0.0000000100 = 0.0000169370 loss
2019-04-07 22:51:51,375 [INFO] unfolding 0, single step 33001
2019-04-07 22:51:51,376 [INFO] Sum of grad norms of most recent batch: 1.238829
2019-04-07 22:51:51,377 [INFO] ---------------------------------
2019-04-07 22:52:13,344 [INFO] ---------------------------------
2019-04-07 22:52:13,345 [INFO] Summary:
2019-04-07 22:52:13,345 [INFO] Batch 34000, worst loss 0.040359 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 22:52:13,346 [INFO] Regularization: 1622.744385 * 0.0000000100 = 0.0000162274 loss
2019-04-07 22:52:13,346 [INFO] unfolding 0, single step 34001
2019-04-07 22:52:13,347 [INFO] Sum of grad norms of most recent batch: 1.528176
2019-04-07 22:52:13,347 [INFO] ---------------------------------
2019-04-07 22:52:34,957 [INFO] ---------------------------------
2019-04-07 22:52:34,958 [INFO] Summary:
2019-04-07 22:52:34,959 [INFO] Batch 35000, worst loss 0.123392 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 22:52:34,959 [INFO] Regularization: 1598.115845 * 0.0000000100 = 0.0000159812 loss
2019-04-07 22:52:34,959 [INFO] unfolding 0, single step 35001
2019-04-07 22:52:34,960 [INFO] Sum of grad norms of most recent batch: 2.251327
2019-04-07 22:52:34,960 [INFO] ---------------------------------
2019-04-07 22:52:57,139 [INFO] ---------------------------------
2019-04-07 22:52:57,140 [INFO] Summary:
2019-04-07 22:52:57,141 [INFO] Batch 36000, worst loss 0.072086 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 22:52:57,141 [INFO] Regularization: 1579.820557 * 0.0000000100 = 0.0000157982 loss
2019-04-07 22:52:57,142 [INFO] unfolding 0, single step 36001
2019-04-07 22:52:57,142 [INFO] Sum of grad norms of most recent batch: 0.910454
2019-04-07 22:52:57,143 [INFO] ---------------------------------
2019-04-07 22:53:19,287 [INFO] ---------------------------------
2019-04-07 22:53:19,288 [INFO] Summary:
2019-04-07 22:53:19,289 [INFO] Batch 37000, worst loss 0.084328 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 22:53:19,290 [INFO] Regularization: 1598.824707 * 0.0000000100 = 0.0000159882 loss
2019-04-07 22:53:19,291 [INFO] unfolding 0, single step 37001
2019-04-07 22:53:19,292 [INFO] Sum of grad norms of most recent batch: 0.819315
2019-04-07 22:53:19,292 [INFO] ---------------------------------
2019-04-07 22:53:41,619 [INFO] ---------------------------------
2019-04-07 22:53:41,620 [INFO] Summary:
2019-04-07 22:53:41,621 [INFO] Batch 38000, worst loss 0.114091 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 22:53:41,622 [INFO] Regularization: 1559.366089 * 0.0000000100 = 0.0000155937 loss
2019-04-07 22:53:41,622 [INFO] unfolding 0, single step 38001
2019-04-07 22:53:41,623 [INFO] Sum of grad norms of most recent batch: 0.757392
2019-04-07 22:53:41,624 [INFO] ---------------------------------
2019-04-07 22:54:03,867 [INFO] ---------------------------------
2019-04-07 22:54:03,868 [INFO] Summary:
2019-04-07 22:54:03,868 [INFO] Batch 39000, worst loss 0.074326 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 22:54:03,869 [INFO] Regularization: 1527.836792 * 0.0000000100 = 0.0000152784 loss
2019-04-07 22:54:03,869 [INFO] unfolding 0, single step 39001
2019-04-07 22:54:03,870 [INFO] Sum of grad norms of most recent batch: 0.180464
2019-04-07 22:54:03,870 [INFO] ---------------------------------
2019-04-07 22:54:25,763 [INFO] ---------------------------------
2019-04-07 22:54:25,764 [INFO] Summary:
2019-04-07 22:54:25,764 [INFO] Batch 40000, worst loss 0.157567 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 22:54:25,765 [INFO] Regularization: 1491.424561 * 0.0000000100 = 0.0000149142 loss
2019-04-07 22:54:25,765 [INFO] unfolding 0, single step 40001
2019-04-07 22:54:25,766 [INFO] Sum of grad norms of most recent batch: 0.488490
2019-04-07 22:54:25,766 [INFO] ---------------------------------
2019-04-07 22:55:03,254 [INFO] ---------------------------------
2019-04-07 22:55:03,255 [INFO] Evaluation:
2019-04-07 22:55:03,256 [INFO] Batch 40000, worst loss 0.106007 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:55:03,257 [INFO] ---------------------------------
2019-04-07 22:55:24,901 [INFO] ---------------------------------
2019-04-07 22:55:24,902 [INFO] Summary:
2019-04-07 22:55:24,902 [INFO] Batch 41000, worst loss 0.089372 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 22:55:24,903 [INFO] Regularization: 1553.152344 * 0.0000000100 = 0.0000155315 loss
2019-04-07 22:55:24,903 [INFO] unfolding 0, single step 41001
2019-04-07 22:55:24,904 [INFO] Sum of grad norms of most recent batch: 1.322660
2019-04-07 22:55:24,904 [INFO] ---------------------------------
2019-04-07 22:55:46,606 [INFO] ---------------------------------
2019-04-07 22:55:46,607 [INFO] Summary:
2019-04-07 22:55:46,608 [INFO] Batch 42000, worst loss 0.127413 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 22:55:46,608 [INFO] Regularization: 1476.084106 * 0.0000000100 = 0.0000147608 loss
2019-04-07 22:55:46,608 [INFO] unfolding 0, single step 42001
2019-04-07 22:55:46,609 [INFO] Sum of grad norms of most recent batch: 0.862660
2019-04-07 22:55:46,610 [INFO] ---------------------------------
2019-04-07 22:56:08,700 [INFO] ---------------------------------
2019-04-07 22:56:08,701 [INFO] Summary:
2019-04-07 22:56:08,702 [INFO] Batch 43000, worst loss 0.098498 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 22:56:08,702 [INFO] Regularization: 1446.139771 * 0.0000000100 = 0.0000144614 loss
2019-04-07 22:56:08,703 [INFO] unfolding 0, single step 43001
2019-04-07 22:56:08,703 [INFO] Sum of grad norms of most recent batch: 0.452200
2019-04-07 22:56:08,704 [INFO] ---------------------------------
2019-04-07 22:56:30,674 [INFO] ---------------------------------
2019-04-07 22:56:30,675 [INFO] Summary:
2019-04-07 22:56:30,675 [INFO] Batch 44000, worst loss 0.045581 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 22:56:30,676 [INFO] Regularization: 1424.149292 * 0.0000000100 = 0.0000142415 loss
2019-04-07 22:56:30,676 [INFO] unfolding 0, single step 44001
2019-04-07 22:56:30,677 [INFO] Sum of grad norms of most recent batch: 0.357741
2019-04-07 22:56:30,678 [INFO] ---------------------------------
2019-04-07 22:56:52,209 [INFO] ---------------------------------
2019-04-07 22:56:52,210 [INFO] Summary:
2019-04-07 22:56:52,211 [INFO] Batch 45000, worst loss 0.040912 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 22:56:52,211 [INFO] Regularization: 1402.402222 * 0.0000000100 = 0.0000140240 loss
2019-04-07 22:56:52,212 [INFO] unfolding 0, single step 45001
2019-04-07 22:56:52,212 [INFO] Sum of grad norms of most recent batch: 0.432803
2019-04-07 22:56:52,213 [INFO] ---------------------------------
2019-04-07 22:57:13,787 [INFO] ---------------------------------
2019-04-07 22:57:13,788 [INFO] Summary:
2019-04-07 22:57:13,789 [INFO] Batch 46000, worst loss 0.071564 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 22:57:13,791 [INFO] Regularization: 1395.458984 * 0.0000000100 = 0.0000139546 loss
2019-04-07 22:57:13,791 [INFO] unfolding 0, single step 46001
2019-04-07 22:57:13,793 [INFO] Sum of grad norms of most recent batch: 0.071473
2019-04-07 22:57:13,794 [INFO] ---------------------------------
2019-04-07 22:57:35,776 [INFO] ---------------------------------
2019-04-07 22:57:35,776 [INFO] Summary:
2019-04-07 22:57:35,777 [INFO] Batch 47000, worst loss 0.053697 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 22:57:35,777 [INFO] Regularization: 1374.581055 * 0.0000000100 = 0.0000137458 loss
2019-04-07 22:57:35,778 [INFO] unfolding 0, single step 47001
2019-04-07 22:57:35,778 [INFO] Sum of grad norms of most recent batch: 0.398830
2019-04-07 22:57:35,779 [INFO] ---------------------------------
2019-04-07 22:57:57,708 [INFO] ---------------------------------
2019-04-07 22:57:57,709 [INFO] Summary:
2019-04-07 22:57:57,710 [INFO] Batch 48000, worst loss 0.025966 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 22:57:57,710 [INFO] Regularization: 1342.702148 * 0.0000000100 = 0.0000134270 loss
2019-04-07 22:57:57,710 [INFO] unfolding 0, single step 48001
2019-04-07 22:57:57,711 [INFO] Sum of grad norms of most recent batch: 0.258334
2019-04-07 22:57:57,711 [INFO] ---------------------------------
2019-04-07 22:58:19,664 [INFO] ---------------------------------
2019-04-07 22:58:19,665 [INFO] Summary:
2019-04-07 22:58:19,665 [INFO] Batch 49000, worst loss 0.100220 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 22:58:19,666 [INFO] Regularization: 1338.445190 * 0.0000000100 = 0.0000133845 loss
2019-04-07 22:58:19,666 [INFO] unfolding 0, single step 49001
2019-04-07 22:58:19,667 [INFO] Sum of grad norms of most recent batch: 7.427944
2019-04-07 22:58:19,667 [INFO] ---------------------------------
2019-04-07 22:58:41,723 [INFO] ---------------------------------
2019-04-07 22:58:41,724 [INFO] Summary:
2019-04-07 22:58:41,725 [INFO] Batch 50000, worst loss 0.117805 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 22:58:41,726 [INFO] Regularization: 1330.546387 * 0.0000000100 = 0.0000133055 loss
2019-04-07 22:58:41,726 [INFO] unfolding 0, single step 50001
2019-04-07 22:58:41,727 [INFO] Sum of grad norms of most recent batch: 0.424182
2019-04-07 22:58:41,727 [INFO] ---------------------------------
2019-04-07 22:59:18,970 [INFO] ---------------------------------
2019-04-07 22:59:18,971 [INFO] Evaluation:
2019-04-07 22:59:18,972 [INFO] Batch 50000, worst loss 0.128930 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 22:59:18,972 [INFO] ---------------------------------
2019-04-07 22:59:40,361 [INFO] ---------------------------------
2019-04-07 22:59:40,362 [INFO] Summary:
2019-04-07 22:59:40,363 [INFO] Batch 51000, worst loss 0.097547 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 22:59:40,363 [INFO] Regularization: 1315.782227 * 0.0000000100 = 0.0000131578 loss
2019-04-07 22:59:40,364 [INFO] unfolding 0, single step 51001
2019-04-07 22:59:40,364 [INFO] Sum of grad norms of most recent batch: 0.611701
2019-04-07 22:59:40,365 [INFO] ---------------------------------
2019-04-07 23:00:01,956 [INFO] ---------------------------------
2019-04-07 23:00:01,957 [INFO] Summary:
2019-04-07 23:00:01,957 [INFO] Batch 52000, worst loss 0.076297 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 23:00:01,958 [INFO] Regularization: 1287.197510 * 0.0000000100 = 0.0000128720 loss
2019-04-07 23:00:01,958 [INFO] unfolding 0, single step 52001
2019-04-07 23:00:01,959 [INFO] Sum of grad norms of most recent batch: 0.140817
2019-04-07 23:00:01,960 [INFO] ---------------------------------
2019-04-07 23:00:23,413 [INFO] ---------------------------------
2019-04-07 23:00:23,414 [INFO] Summary:
2019-04-07 23:00:23,415 [INFO] Batch 53000, worst loss 0.033931 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 23:00:23,415 [INFO] Regularization: 1264.260376 * 0.0000000100 = 0.0000126426 loss
2019-04-07 23:00:23,416 [INFO] unfolding 0, single step 53001
2019-04-07 23:00:23,416 [INFO] Sum of grad norms of most recent batch: 0.102267
2019-04-07 23:00:23,417 [INFO] ---------------------------------
2019-04-07 23:00:45,502 [INFO] ---------------------------------
2019-04-07 23:00:45,503 [INFO] Summary:
2019-04-07 23:00:45,503 [INFO] Batch 54000, worst loss 0.055375 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 23:00:45,504 [INFO] Regularization: 1251.837891 * 0.0000000100 = 0.0000125184 loss
2019-04-07 23:00:45,504 [INFO] unfolding 0, single step 54001
2019-04-07 23:00:45,505 [INFO] Sum of grad norms of most recent batch: 0.117408
2019-04-07 23:00:45,505 [INFO] ---------------------------------
2019-04-07 23:01:06,958 [INFO] ---------------------------------
2019-04-07 23:01:06,959 [INFO] Summary:
2019-04-07 23:01:06,960 [INFO] Batch 55000, worst loss 0.070363 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 23:01:06,960 [INFO] Regularization: 1231.744141 * 0.0000000100 = 0.0000123174 loss
2019-04-07 23:01:06,960 [INFO] unfolding 0, single step 55001
2019-04-07 23:01:06,961 [INFO] Sum of grad norms of most recent batch: 0.287964
2019-04-07 23:01:06,961 [INFO] ---------------------------------
2019-04-07 23:01:28,513 [INFO] ---------------------------------
2019-04-07 23:01:28,514 [INFO] Summary:
2019-04-07 23:01:28,514 [INFO] Batch 56000, worst loss 0.079694 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 23:01:28,515 [INFO] Regularization: 1216.475220 * 0.0000000100 = 0.0000121648 loss
2019-04-07 23:01:28,515 [INFO] unfolding 0, single step 56001
2019-04-07 23:01:28,516 [INFO] Sum of grad norms of most recent batch: 0.085734
2019-04-07 23:01:28,516 [INFO] ---------------------------------
2019-04-07 23:01:49,419 [INFO] ---------------------------------
2019-04-07 23:01:49,420 [INFO] Summary:
2019-04-07 23:01:49,420 [INFO] Batch 57000, worst loss 0.038730 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 23:01:49,421 [INFO] Regularization: 1200.167603 * 0.0000000100 = 0.0000120017 loss
2019-04-07 23:01:49,421 [INFO] unfolding 0, single step 57001
2019-04-07 23:01:49,422 [INFO] Sum of grad norms of most recent batch: 0.380151
2019-04-07 23:01:49,422 [INFO] ---------------------------------
2019-04-07 23:02:11,565 [INFO] ---------------------------------
2019-04-07 23:02:11,565 [INFO] Summary:
2019-04-07 23:02:11,566 [INFO] Batch 58000, worst loss 0.061585 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 23:02:11,567 [INFO] Regularization: 1177.444214 * 0.0000000100 = 0.0000117744 loss
2019-04-07 23:02:11,567 [INFO] unfolding 0, single step 58001
2019-04-07 23:02:11,568 [INFO] Sum of grad norms of most recent batch: 0.048109
2019-04-07 23:02:11,568 [INFO] ---------------------------------
2019-04-07 23:02:33,334 [INFO] ---------------------------------
2019-04-07 23:02:33,335 [INFO] Summary:
2019-04-07 23:02:33,335 [INFO] Batch 59000, worst loss 0.059413 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 23:02:33,336 [INFO] Regularization: 1160.166260 * 0.0000000100 = 0.0000116017 loss
2019-04-07 23:02:33,336 [INFO] unfolding 0, single step 59001
2019-04-07 23:02:33,337 [INFO] Sum of grad norms of most recent batch: 0.109026
2019-04-07 23:02:33,337 [INFO] ---------------------------------
2019-04-07 23:02:54,777 [INFO] ---------------------------------
2019-04-07 23:02:54,778 [INFO] Summary:
2019-04-07 23:02:54,779 [INFO] Batch 60000, worst loss 0.058070 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-07 23:02:54,779 [INFO] Regularization: 1146.868774 * 0.0000000100 = 0.0000114687 loss
2019-04-07 23:02:54,780 [INFO] unfolding 0, single step 60001
2019-04-07 23:02:54,780 [INFO] Sum of grad norms of most recent batch: 0.209144
2019-04-07 23:02:54,781 [INFO] ---------------------------------
2019-04-07 23:03:32,160 [INFO] ---------------------------------
2019-04-07 23:03:32,161 [INFO] Evaluation:
2019-04-07 23:03:32,162 [INFO] Batch 60000, worst loss 0.108261 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:03:32,163 [INFO] ---------------------------------
2019-04-07 23:03:54,509 [INFO] ---------------------------------
2019-04-07 23:03:54,510 [INFO] Summary:
2019-04-07 23:03:54,510 [INFO] Batch 61000, worst loss 0.043619 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 23:03:54,512 [INFO] Regularization: 1132.469849 * 0.0000000100 = 0.0000113247 loss
2019-04-07 23:03:54,512 [INFO] unfolding 0, single step 61001
2019-04-07 23:03:54,513 [INFO] Sum of grad norms of most recent batch: 0.318326
2019-04-07 23:03:54,514 [INFO] ---------------------------------
2019-04-07 23:04:15,471 [INFO] ---------------------------------
2019-04-07 23:04:15,472 [INFO] Summary:
2019-04-07 23:04:15,473 [INFO] Batch 62000, worst loss 0.117718 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 23:04:15,473 [INFO] Regularization: 1122.450684 * 0.0000000100 = 0.0000112245 loss
2019-04-07 23:04:15,474 [INFO] unfolding 0, single step 62001
2019-04-07 23:04:15,474 [INFO] Sum of grad norms of most recent batch: 0.390573
2019-04-07 23:04:15,475 [INFO] ---------------------------------
2019-04-07 23:04:36,886 [INFO] ---------------------------------
2019-04-07 23:04:36,887 [INFO] Summary:
2019-04-07 23:04:36,887 [INFO] Batch 63000, worst loss 0.041240 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 23:04:36,888 [INFO] Regularization: 1111.881836 * 0.0000000100 = 0.0000111188 loss
2019-04-07 23:04:36,888 [INFO] unfolding 0, single step 63001
2019-04-07 23:04:36,889 [INFO] Sum of grad norms of most recent batch: 0.059270
2019-04-07 23:04:36,890 [INFO] ---------------------------------
2019-04-07 23:04:58,748 [INFO] ---------------------------------
2019-04-07 23:04:58,749 [INFO] Summary:
2019-04-07 23:04:58,749 [INFO] Batch 64000, worst loss 0.040066 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 23:04:58,750 [INFO] Regularization: 1102.062500 * 0.0000000100 = 0.0000110206 loss
2019-04-07 23:04:58,750 [INFO] unfolding 0, single step 64001
2019-04-07 23:04:58,751 [INFO] Sum of grad norms of most recent batch: 0.331521
2019-04-07 23:04:58,751 [INFO] ---------------------------------
2019-04-07 23:05:20,649 [INFO] ---------------------------------
2019-04-07 23:05:20,650 [INFO] Summary:
2019-04-07 23:05:20,651 [INFO] Batch 65000, worst loss 0.029585 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 23:05:20,651 [INFO] Regularization: 1091.162964 * 0.0000000100 = 0.0000109116 loss
2019-04-07 23:05:20,652 [INFO] unfolding 0, single step 65001
2019-04-07 23:05:20,652 [INFO] Sum of grad norms of most recent batch: 1.598870
2019-04-07 23:05:20,653 [INFO] ---------------------------------
2019-04-07 23:05:42,092 [INFO] ---------------------------------
2019-04-07 23:05:42,093 [INFO] Summary:
2019-04-07 23:05:42,094 [INFO] Batch 66000, worst loss 0.082246 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 23:05:42,094 [INFO] Regularization: 1079.841797 * 0.0000000100 = 0.0000107984 loss
2019-04-07 23:05:42,095 [INFO] unfolding 0, single step 66001
2019-04-07 23:05:42,095 [INFO] Sum of grad norms of most recent batch: 0.071535
2019-04-07 23:05:42,096 [INFO] ---------------------------------
2019-04-07 23:06:03,955 [INFO] ---------------------------------
2019-04-07 23:06:03,956 [INFO] Summary:
2019-04-07 23:06:03,956 [INFO] Batch 67000, worst loss 0.121424 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 23:06:03,957 [INFO] Regularization: 1069.734863 * 0.0000000100 = 0.0000106973 loss
2019-04-07 23:06:03,957 [INFO] unfolding 0, single step 67001
2019-04-07 23:06:03,958 [INFO] Sum of grad norms of most recent batch: 0.475346
2019-04-07 23:06:03,958 [INFO] ---------------------------------
2019-04-07 23:06:24,942 [INFO] ---------------------------------
2019-04-07 23:06:24,943 [INFO] Summary:
2019-04-07 23:06:24,943 [INFO] Batch 68000, worst loss 0.116747 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 23:06:24,944 [INFO] Regularization: 1061.900269 * 0.0000000100 = 0.0000106190 loss
2019-04-07 23:06:24,944 [INFO] unfolding 0, single step 68001
2019-04-07 23:06:24,945 [INFO] Sum of grad norms of most recent batch: 0.159385
2019-04-07 23:06:24,946 [INFO] ---------------------------------
2019-04-07 23:06:46,435 [INFO] ---------------------------------
2019-04-07 23:06:46,436 [INFO] Summary:
2019-04-07 23:06:46,437 [INFO] Batch 69000, worst loss 0.042544 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 23:06:46,437 [INFO] Regularization: 1050.813721 * 0.0000000100 = 0.0000105081 loss
2019-04-07 23:06:46,438 [INFO] unfolding 0, single step 69001
2019-04-07 23:06:46,438 [INFO] Sum of grad norms of most recent batch: 0.100467
2019-04-07 23:06:46,439 [INFO] ---------------------------------
2019-04-07 23:07:07,566 [INFO] ---------------------------------
2019-04-07 23:07:07,567 [INFO] Summary:
2019-04-07 23:07:07,568 [INFO] Batch 70000, worst loss 0.012038 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-07 23:07:07,568 [INFO] Regularization: 1040.958130 * 0.0000000100 = 0.0000104096 loss
2019-04-07 23:07:07,569 [INFO] unfolding 0, single step 70001
2019-04-07 23:07:07,569 [INFO] Sum of grad norms of most recent batch: 0.129240
2019-04-07 23:07:07,570 [INFO] ---------------------------------
2019-04-07 23:07:45,100 [INFO] ---------------------------------
2019-04-07 23:07:45,101 [INFO] Evaluation:
2019-04-07 23:07:45,101 [INFO] Batch 70000, worst loss 0.116770 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:07:45,101 [INFO] ---------------------------------
2019-04-07 23:08:06,645 [INFO] ---------------------------------
2019-04-07 23:08:06,646 [INFO] Summary:
2019-04-07 23:08:06,647 [INFO] Batch 71000, worst loss 0.035128 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 23:08:06,647 [INFO] Regularization: 1037.278809 * 0.0000000100 = 0.0000103728 loss
2019-04-07 23:08:06,647 [INFO] unfolding 0, single step 71001
2019-04-07 23:08:06,648 [INFO] Sum of grad norms of most recent batch: 0.042582
2019-04-07 23:08:06,649 [INFO] ---------------------------------
2019-04-07 23:08:28,098 [INFO] ---------------------------------
2019-04-07 23:08:28,099 [INFO] Summary:
2019-04-07 23:08:28,100 [INFO] Batch 72000, worst loss 0.031302 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 23:08:28,100 [INFO] Regularization: 1031.060181 * 0.0000000100 = 0.0000103106 loss
2019-04-07 23:08:28,101 [INFO] unfolding 0, single step 72001
2019-04-07 23:08:28,102 [INFO] Sum of grad norms of most recent batch: 0.037604
2019-04-07 23:08:28,102 [INFO] ---------------------------------
2019-04-07 23:08:49,318 [INFO] ---------------------------------
2019-04-07 23:08:49,320 [INFO] Summary:
2019-04-07 23:08:49,320 [INFO] Batch 73000, worst loss 0.041674 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 23:08:49,321 [INFO] Regularization: 1026.713501 * 0.0000000100 = 0.0000102671 loss
2019-04-07 23:08:49,322 [INFO] unfolding 0, single step 73001
2019-04-07 23:08:49,323 [INFO] Sum of grad norms of most recent batch: 0.108644
2019-04-07 23:08:49,324 [INFO] ---------------------------------
2019-04-07 23:09:10,918 [INFO] ---------------------------------
2019-04-07 23:09:10,919 [INFO] Summary:
2019-04-07 23:09:10,920 [INFO] Batch 74000, worst loss 0.068710 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 23:09:10,920 [INFO] Regularization: 1021.983154 * 0.0000000100 = 0.0000102198 loss
2019-04-07 23:09:10,921 [INFO] unfolding 0, single step 74001
2019-04-07 23:09:10,921 [INFO] Sum of grad norms of most recent batch: 0.610649
2019-04-07 23:09:10,922 [INFO] ---------------------------------
2019-04-07 23:09:32,496 [INFO] ---------------------------------
2019-04-07 23:09:32,497 [INFO] Summary:
2019-04-07 23:09:32,497 [INFO] Batch 75000, worst loss 0.199703 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 23:09:32,498 [INFO] Regularization: 1019.414124 * 0.0000000100 = 0.0000101941 loss
2019-04-07 23:09:32,498 [INFO] unfolding 0, single step 75001
2019-04-07 23:09:32,498 [INFO] Sum of grad norms of most recent batch: 0.119220
2019-04-07 23:09:32,499 [INFO] ---------------------------------
2019-04-07 23:09:53,590 [INFO] ---------------------------------
2019-04-07 23:09:53,591 [INFO] Summary:
2019-04-07 23:09:53,592 [INFO] Batch 76000, worst loss 0.070528 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 23:09:53,592 [INFO] Regularization: 1017.239502 * 0.0000000100 = 0.0000101724 loss
2019-04-07 23:09:53,593 [INFO] unfolding 0, single step 76001
2019-04-07 23:09:53,593 [INFO] Sum of grad norms of most recent batch: 0.070196
2019-04-07 23:09:53,594 [INFO] ---------------------------------
2019-04-07 23:10:15,583 [INFO] ---------------------------------
2019-04-07 23:10:15,584 [INFO] Summary:
2019-04-07 23:10:15,584 [INFO] Batch 77000, worst loss 0.075511 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 23:10:15,585 [INFO] Regularization: 1012.952148 * 0.0000000100 = 0.0000101295 loss
2019-04-07 23:10:15,585 [INFO] unfolding 0, single step 77001
2019-04-07 23:10:15,586 [INFO] Sum of grad norms of most recent batch: 0.052929
2019-04-07 23:10:15,586 [INFO] ---------------------------------
2019-04-07 23:10:36,867 [INFO] ---------------------------------
2019-04-07 23:10:36,868 [INFO] Summary:
2019-04-07 23:10:36,868 [INFO] Batch 78000, worst loss 0.098542 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 23:10:36,869 [INFO] Regularization: 1009.250061 * 0.0000000100 = 0.0000100925 loss
2019-04-07 23:10:36,869 [INFO] unfolding 0, single step 78001
2019-04-07 23:10:36,870 [INFO] Sum of grad norms of most recent batch: 0.022823
2019-04-07 23:10:36,870 [INFO] ---------------------------------
2019-04-07 23:10:58,327 [INFO] ---------------------------------
2019-04-07 23:10:58,328 [INFO] Summary:
2019-04-07 23:10:58,329 [INFO] Batch 79000, worst loss 0.050855 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 23:10:58,329 [INFO] Regularization: 1004.857849 * 0.0000000100 = 0.0000100486 loss
2019-04-07 23:10:58,330 [INFO] unfolding 0, single step 79001
2019-04-07 23:10:58,330 [INFO] Sum of grad norms of most recent batch: 0.053557
2019-04-07 23:10:58,331 [INFO] ---------------------------------
2019-04-07 23:11:19,658 [INFO] ---------------------------------
2019-04-07 23:11:19,659 [INFO] Summary:
2019-04-07 23:11:19,660 [INFO] Batch 80000, worst loss 0.032642 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-07 23:11:19,660 [INFO] Regularization: 1000.717529 * 0.0000000100 = 0.0000100072 loss
2019-04-07 23:11:19,661 [INFO] unfolding 0, single step 80001
2019-04-07 23:11:19,661 [INFO] Sum of grad norms of most recent batch: 0.042357
2019-04-07 23:11:19,662 [INFO] ---------------------------------
2019-04-07 23:11:57,356 [INFO] ---------------------------------
2019-04-07 23:11:57,357 [INFO] Evaluation:
2019-04-07 23:11:57,358 [INFO] Batch 80000, worst loss 0.070930 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:11:57,358 [INFO] ---------------------------------
2019-04-07 23:12:18,655 [INFO] ---------------------------------
2019-04-07 23:12:18,655 [INFO] Summary:
2019-04-07 23:12:18,656 [INFO] Batch 81000, worst loss 0.051520 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 23:12:18,657 [INFO] Regularization: 997.884644 * 0.0000000100 = 0.0000099788 loss
2019-04-07 23:12:18,657 [INFO] unfolding 0, single step 81001
2019-04-07 23:12:18,658 [INFO] Sum of grad norms of most recent batch: 0.060103
2019-04-07 23:12:18,659 [INFO] ---------------------------------
2019-04-07 23:12:40,096 [INFO] ---------------------------------
2019-04-07 23:12:40,097 [INFO] Summary:
2019-04-07 23:12:40,098 [INFO] Batch 82000, worst loss 0.101917 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 23:12:40,098 [INFO] Regularization: 995.385315 * 0.0000000100 = 0.0000099539 loss
2019-04-07 23:12:40,099 [INFO] unfolding 0, single step 82001
2019-04-07 23:12:40,099 [INFO] Sum of grad norms of most recent batch: 0.043075
2019-04-07 23:12:40,100 [INFO] ---------------------------------
2019-04-07 23:13:01,604 [INFO] ---------------------------------
2019-04-07 23:13:01,605 [INFO] Summary:
2019-04-07 23:13:01,606 [INFO] Batch 83000, worst loss 0.057700 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 23:13:01,606 [INFO] Regularization: 992.957520 * 0.0000000100 = 0.0000099296 loss
2019-04-07 23:13:01,607 [INFO] unfolding 0, single step 83001
2019-04-07 23:13:01,607 [INFO] Sum of grad norms of most recent batch: 0.049570
2019-04-07 23:13:01,608 [INFO] ---------------------------------
2019-04-07 23:13:23,008 [INFO] ---------------------------------
2019-04-07 23:13:23,009 [INFO] Summary:
2019-04-07 23:13:23,010 [INFO] Batch 84000, worst loss 0.068646 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 23:13:23,010 [INFO] Regularization: 991.243286 * 0.0000000100 = 0.0000099124 loss
2019-04-07 23:13:23,011 [INFO] unfolding 0, single step 84001
2019-04-07 23:13:23,011 [INFO] Sum of grad norms of most recent batch: 0.072548
2019-04-07 23:13:23,012 [INFO] ---------------------------------
2019-04-07 23:13:44,299 [INFO] ---------------------------------
2019-04-07 23:13:44,300 [INFO] Summary:
2019-04-07 23:13:44,301 [INFO] Batch 85000, worst loss 0.029355 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 23:13:44,302 [INFO] Regularization: 989.282959 * 0.0000000100 = 0.0000098928 loss
2019-04-07 23:13:44,302 [INFO] unfolding 0, single step 85001
2019-04-07 23:13:44,303 [INFO] Sum of grad norms of most recent batch: 0.035541
2019-04-07 23:13:44,303 [INFO] ---------------------------------
2019-04-07 23:14:05,756 [INFO] ---------------------------------
2019-04-07 23:14:05,757 [INFO] Summary:
2019-04-07 23:14:05,757 [INFO] Batch 86000, worst loss 0.069226 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 23:14:05,758 [INFO] Regularization: 987.587219 * 0.0000000100 = 0.0000098759 loss
2019-04-07 23:14:05,758 [INFO] unfolding 0, single step 86001
2019-04-07 23:14:05,759 [INFO] Sum of grad norms of most recent batch: 0.087554
2019-04-07 23:14:05,760 [INFO] ---------------------------------
2019-04-07 23:14:26,848 [INFO] ---------------------------------
2019-04-07 23:14:26,849 [INFO] Summary:
2019-04-07 23:14:26,850 [INFO] Batch 87000, worst loss 0.106245 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 23:14:26,851 [INFO] Regularization: 985.712402 * 0.0000000100 = 0.0000098571 loss
2019-04-07 23:14:26,851 [INFO] unfolding 0, single step 87001
2019-04-07 23:14:26,852 [INFO] Sum of grad norms of most recent batch: 0.030439
2019-04-07 23:14:26,852 [INFO] ---------------------------------
2019-04-07 23:14:48,215 [INFO] ---------------------------------
2019-04-07 23:14:48,216 [INFO] Summary:
2019-04-07 23:14:48,217 [INFO] Batch 88000, worst loss 0.055845 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 23:14:48,218 [INFO] Regularization: 983.639160 * 0.0000000100 = 0.0000098364 loss
2019-04-07 23:14:48,218 [INFO] unfolding 0, single step 88001
2019-04-07 23:14:48,219 [INFO] Sum of grad norms of most recent batch: 0.043757
2019-04-07 23:14:48,220 [INFO] ---------------------------------
2019-04-07 23:15:09,340 [INFO] ---------------------------------
2019-04-07 23:15:09,341 [INFO] Summary:
2019-04-07 23:15:09,342 [INFO] Batch 89000, worst loss 0.055905 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 23:15:09,342 [INFO] Regularization: 981.544006 * 0.0000000100 = 0.0000098154 loss
2019-04-07 23:15:09,342 [INFO] unfolding 0, single step 89001
2019-04-07 23:15:09,343 [INFO] Sum of grad norms of most recent batch: 0.023886
2019-04-07 23:15:09,343 [INFO] ---------------------------------
2019-04-07 23:15:30,531 [INFO] ---------------------------------
2019-04-07 23:15:30,532 [INFO] Summary:
2019-04-07 23:15:30,532 [INFO] Batch 90000, worst loss 0.088020 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-07 23:15:30,533 [INFO] Regularization: 979.405823 * 0.0000000100 = 0.0000097941 loss
2019-04-07 23:15:30,533 [INFO] unfolding 0, single step 90001
2019-04-07 23:15:30,534 [INFO] Sum of grad norms of most recent batch: 0.079354
2019-04-07 23:15:30,534 [INFO] ---------------------------------
2019-04-07 23:16:07,907 [INFO] ---------------------------------
2019-04-07 23:16:07,908 [INFO] Evaluation:
2019-04-07 23:16:07,909 [INFO] Batch 90000, worst loss 0.076441 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:16:07,910 [INFO] ---------------------------------
2019-04-07 23:16:29,378 [INFO] ---------------------------------
2019-04-07 23:16:29,379 [INFO] Summary:
2019-04-07 23:16:29,379 [INFO] Batch 91000, worst loss 0.084469 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 23:16:29,380 [INFO] Regularization: 976.548828 * 0.0000000100 = 0.0000097655 loss
2019-04-07 23:16:29,381 [INFO] unfolding 0, single step 91001
2019-04-07 23:16:29,381 [INFO] Sum of grad norms of most recent batch: 0.046066
2019-04-07 23:16:29,382 [INFO] ---------------------------------
2019-04-07 23:16:50,597 [INFO] ---------------------------------
2019-04-07 23:16:50,598 [INFO] Summary:
2019-04-07 23:16:50,598 [INFO] Batch 92000, worst loss 0.055781 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 23:16:50,599 [INFO] Regularization: 975.003540 * 0.0000000100 = 0.0000097500 loss
2019-04-07 23:16:50,599 [INFO] unfolding 0, single step 92001
2019-04-07 23:16:50,600 [INFO] Sum of grad norms of most recent batch: 0.040052
2019-04-07 23:16:50,600 [INFO] ---------------------------------
2019-04-07 23:17:12,202 [INFO] ---------------------------------
2019-04-07 23:17:12,203 [INFO] Summary:
2019-04-07 23:17:12,204 [INFO] Batch 93000, worst loss 0.026710 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 23:17:12,204 [INFO] Regularization: 974.327454 * 0.0000000100 = 0.0000097433 loss
2019-04-07 23:17:12,205 [INFO] unfolding 0, single step 93001
2019-04-07 23:17:12,205 [INFO] Sum of grad norms of most recent batch: 0.018179
2019-04-07 23:17:12,206 [INFO] ---------------------------------
2019-04-07 23:17:34,316 [INFO] ---------------------------------
2019-04-07 23:17:34,317 [INFO] Summary:
2019-04-07 23:17:34,317 [INFO] Batch 94000, worst loss 0.055753 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 23:17:34,318 [INFO] Regularization: 973.785645 * 0.0000000100 = 0.0000097379 loss
2019-04-07 23:17:34,318 [INFO] unfolding 0, single step 94001
2019-04-07 23:17:34,319 [INFO] Sum of grad norms of most recent batch: 0.043927
2019-04-07 23:17:34,320 [INFO] ---------------------------------
2019-04-07 23:17:55,395 [INFO] ---------------------------------
2019-04-07 23:17:55,396 [INFO] Summary:
2019-04-07 23:17:55,396 [INFO] Batch 95000, worst loss 0.072568 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 23:17:55,397 [INFO] Regularization: 973.038391 * 0.0000000100 = 0.0000097304 loss
2019-04-07 23:17:55,397 [INFO] unfolding 0, single step 95001
2019-04-07 23:17:55,398 [INFO] Sum of grad norms of most recent batch: 0.036552
2019-04-07 23:17:55,398 [INFO] ---------------------------------
2019-04-07 23:18:16,551 [INFO] ---------------------------------
2019-04-07 23:18:16,552 [INFO] Summary:
2019-04-07 23:18:16,553 [INFO] Batch 96000, worst loss 0.038856 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 23:18:16,553 [INFO] Regularization: 972.429932 * 0.0000000100 = 0.0000097243 loss
2019-04-07 23:18:16,554 [INFO] unfolding 0, single step 96001
2019-04-07 23:18:16,554 [INFO] Sum of grad norms of most recent batch: 0.015575
2019-04-07 23:18:16,555 [INFO] ---------------------------------
2019-04-07 23:18:37,574 [INFO] ---------------------------------
2019-04-07 23:18:37,575 [INFO] Summary:
2019-04-07 23:18:37,576 [INFO] Batch 97000, worst loss 0.041777 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 23:18:37,577 [INFO] Regularization: 971.001282 * 0.0000000100 = 0.0000097100 loss
2019-04-07 23:18:37,577 [INFO] unfolding 0, single step 97001
2019-04-07 23:18:37,578 [INFO] Sum of grad norms of most recent batch: 2.437138
2019-04-07 23:18:37,579 [INFO] ---------------------------------
2019-04-07 23:18:58,328 [INFO] ---------------------------------
2019-04-07 23:18:58,328 [INFO] Summary:
2019-04-07 23:18:58,329 [INFO] Batch 98000, worst loss 0.048454 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 23:18:58,330 [INFO] Regularization: 969.768066 * 0.0000000100 = 0.0000096977 loss
2019-04-07 23:18:58,330 [INFO] unfolding 0, single step 98001
2019-04-07 23:18:58,330 [INFO] Sum of grad norms of most recent batch: 0.086392
2019-04-07 23:18:58,331 [INFO] ---------------------------------
2019-04-07 23:19:19,062 [INFO] ---------------------------------
2019-04-07 23:19:19,063 [INFO] Summary:
2019-04-07 23:19:19,063 [INFO] Batch 99000, worst loss 0.039701 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 23:19:19,064 [INFO] Regularization: 968.951355 * 0.0000000100 = 0.0000096895 loss
2019-04-07 23:19:19,064 [INFO] unfolding 0, single step 99001
2019-04-07 23:19:19,065 [INFO] Sum of grad norms of most recent batch: 0.025979
2019-04-07 23:19:19,066 [INFO] ---------------------------------
2019-04-07 23:19:40,520 [INFO] ---------------------------------
2019-04-07 23:19:40,520 [INFO] Summary:
2019-04-07 23:19:40,521 [INFO] Batch 100000, worst loss 0.057184 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-07 23:19:40,522 [INFO] Regularization: 967.678894 * 0.0000000100 = 0.0000096768 loss
2019-04-07 23:19:40,522 [INFO] unfolding 0, single step 100001
2019-04-07 23:19:40,523 [INFO] Sum of grad norms of most recent batch: 0.056293
2019-04-07 23:19:40,524 [INFO] ---------------------------------
2019-04-07 23:20:17,884 [INFO] ---------------------------------
2019-04-07 23:20:17,885 [INFO] Evaluation:
2019-04-07 23:20:17,885 [INFO] Batch 100000, worst loss 0.103545 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:20:17,886 [INFO] ---------------------------------
2019-04-07 23:20:38,947 [INFO] ---------------------------------
2019-04-07 23:20:38,947 [INFO] Summary:
2019-04-07 23:20:38,948 [INFO] Batch 101000, worst loss 0.062234 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 23:20:38,949 [INFO] Regularization: 966.331970 * 0.0000000100 = 0.0000096633 loss
2019-04-07 23:20:38,949 [INFO] unfolding 0, single step 101001
2019-04-07 23:20:38,950 [INFO] Sum of grad norms of most recent batch: 0.054671
2019-04-07 23:20:38,950 [INFO] ---------------------------------
2019-04-07 23:21:00,095 [INFO] ---------------------------------
2019-04-07 23:21:00,096 [INFO] Summary:
2019-04-07 23:21:00,097 [INFO] Batch 102000, worst loss 0.036084 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 23:21:00,097 [INFO] Regularization: 965.793457 * 0.0000000100 = 0.0000096579 loss
2019-04-07 23:21:00,097 [INFO] unfolding 0, single step 102001
2019-04-07 23:21:00,098 [INFO] Sum of grad norms of most recent batch: 0.064688
2019-04-07 23:21:00,099 [INFO] ---------------------------------
2019-04-07 23:21:21,459 [INFO] ---------------------------------
2019-04-07 23:21:21,461 [INFO] Summary:
2019-04-07 23:21:21,461 [INFO] Batch 103000, worst loss 0.125452 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 23:21:21,462 [INFO] Regularization: 965.345581 * 0.0000000100 = 0.0000096535 loss
2019-04-07 23:21:21,462 [INFO] unfolding 0, single step 103001
2019-04-07 23:21:21,463 [INFO] Sum of grad norms of most recent batch: 0.038998
2019-04-07 23:21:21,463 [INFO] ---------------------------------
2019-04-07 23:21:41,732 [INFO] ---------------------------------
2019-04-07 23:21:41,733 [INFO] Summary:
2019-04-07 23:21:41,734 [INFO] Batch 104000, worst loss 0.032680 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 23:21:41,735 [INFO] Regularization: 964.614746 * 0.0000000100 = 0.0000096461 loss
2019-04-07 23:21:41,735 [INFO] unfolding 0, single step 104001
2019-04-07 23:21:41,736 [INFO] Sum of grad norms of most recent batch: 0.018842
2019-04-07 23:21:41,737 [INFO] ---------------------------------
2019-04-07 23:22:02,380 [INFO] ---------------------------------
2019-04-07 23:22:02,381 [INFO] Summary:
2019-04-07 23:22:02,382 [INFO] Batch 105000, worst loss 0.060616 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 23:22:02,382 [INFO] Regularization: 964.081299 * 0.0000000100 = 0.0000096408 loss
2019-04-07 23:22:02,383 [INFO] unfolding 0, single step 105001
2019-04-07 23:22:02,383 [INFO] Sum of grad norms of most recent batch: 0.052270
2019-04-07 23:22:02,384 [INFO] ---------------------------------
2019-04-07 23:22:23,347 [INFO] ---------------------------------
2019-04-07 23:22:23,348 [INFO] Summary:
2019-04-07 23:22:23,348 [INFO] Batch 106000, worst loss 0.028948 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 23:22:23,349 [INFO] Regularization: 963.256287 * 0.0000000100 = 0.0000096326 loss
2019-04-07 23:22:23,349 [INFO] unfolding 0, single step 106001
2019-04-07 23:22:23,350 [INFO] Sum of grad norms of most recent batch: 0.017972
2019-04-07 23:22:23,351 [INFO] ---------------------------------
2019-04-07 23:22:43,926 [INFO] ---------------------------------
2019-04-07 23:22:43,927 [INFO] Summary:
2019-04-07 23:22:43,928 [INFO] Batch 107000, worst loss 0.040374 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 23:22:43,928 [INFO] Regularization: 963.114563 * 0.0000000100 = 0.0000096311 loss
2019-04-07 23:22:43,929 [INFO] unfolding 0, single step 107001
2019-04-07 23:22:43,930 [INFO] Sum of grad norms of most recent batch: 0.049015
2019-04-07 23:22:43,930 [INFO] ---------------------------------
2019-04-07 23:23:05,086 [INFO] ---------------------------------
2019-04-07 23:23:05,087 [INFO] Summary:
2019-04-07 23:23:05,087 [INFO] Batch 108000, worst loss 0.040394 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 23:23:05,088 [INFO] Regularization: 962.750793 * 0.0000000100 = 0.0000096275 loss
2019-04-07 23:23:05,088 [INFO] unfolding 0, single step 108001
2019-04-07 23:23:05,089 [INFO] Sum of grad norms of most recent batch: 0.095564
2019-04-07 23:23:05,089 [INFO] ---------------------------------
2019-04-07 23:23:26,692 [INFO] ---------------------------------
2019-04-07 23:23:26,693 [INFO] Summary:
2019-04-07 23:23:26,693 [INFO] Batch 109000, worst loss 0.017248 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 23:23:26,694 [INFO] Regularization: 962.079834 * 0.0000000100 = 0.0000096208 loss
2019-04-07 23:23:26,694 [INFO] unfolding 0, single step 109001
2019-04-07 23:23:26,695 [INFO] Sum of grad norms of most recent batch: 0.008556
2019-04-07 23:23:26,695 [INFO] ---------------------------------
2019-04-07 23:23:47,561 [INFO] ---------------------------------
2019-04-07 23:23:47,562 [INFO] Summary:
2019-04-07 23:23:47,563 [INFO] Batch 110000, worst loss 0.064670 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-07 23:23:47,563 [INFO] Regularization: 961.447388 * 0.0000000100 = 0.0000096145 loss
2019-04-07 23:23:47,564 [INFO] unfolding 0, single step 110001
2019-04-07 23:23:47,564 [INFO] Sum of grad norms of most recent batch: 0.029702
2019-04-07 23:23:47,565 [INFO] ---------------------------------
2019-04-07 23:24:24,800 [INFO] ---------------------------------
2019-04-07 23:24:24,801 [INFO] Evaluation:
2019-04-07 23:24:24,802 [INFO] Batch 110000, worst loss 0.096429 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:24:24,802 [INFO] ---------------------------------
2019-04-07 23:24:45,702 [INFO] ---------------------------------
2019-04-07 23:24:45,703 [INFO] Summary:
2019-04-07 23:24:45,703 [INFO] Batch 111000, worst loss 0.039365 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 23:24:45,704 [INFO] Regularization: 960.772339 * 0.0000000100 = 0.0000096077 loss
2019-04-07 23:24:45,704 [INFO] unfolding 0, single step 111001
2019-04-07 23:24:45,705 [INFO] Sum of grad norms of most recent batch: 0.034620
2019-04-07 23:24:45,705 [INFO] ---------------------------------
2019-04-07 23:25:06,742 [INFO] ---------------------------------
2019-04-07 23:25:06,743 [INFO] Summary:
2019-04-07 23:25:06,744 [INFO] Batch 112000, worst loss 0.083372 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 23:25:06,744 [INFO] Regularization: 960.580994 * 0.0000000100 = 0.0000096058 loss
2019-04-07 23:25:06,744 [INFO] unfolding 0, single step 112001
2019-04-07 23:25:06,745 [INFO] Sum of grad norms of most recent batch: 0.560875
2019-04-07 23:25:06,745 [INFO] ---------------------------------
2019-04-07 23:25:28,321 [INFO] ---------------------------------
2019-04-07 23:25:28,322 [INFO] Summary:
2019-04-07 23:25:28,323 [INFO] Batch 113000, worst loss 0.053257 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 23:25:28,323 [INFO] Regularization: 960.185303 * 0.0000000100 = 0.0000096019 loss
2019-04-07 23:25:28,323 [INFO] unfolding 0, single step 113001
2019-04-07 23:25:28,324 [INFO] Sum of grad norms of most recent batch: 0.034261
2019-04-07 23:25:28,325 [INFO] ---------------------------------
2019-04-07 23:25:49,421 [INFO] ---------------------------------
2019-04-07 23:25:49,422 [INFO] Summary:
2019-04-07 23:25:49,422 [INFO] Batch 114000, worst loss 0.080004 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 23:25:49,423 [INFO] Regularization: 959.850220 * 0.0000000100 = 0.0000095985 loss
2019-04-07 23:25:49,423 [INFO] unfolding 0, single step 114001
2019-04-07 23:25:49,424 [INFO] Sum of grad norms of most recent batch: 0.609127
2019-04-07 23:25:49,425 [INFO] ---------------------------------
2019-04-07 23:26:10,461 [INFO] ---------------------------------
2019-04-07 23:26:10,462 [INFO] Summary:
2019-04-07 23:26:10,463 [INFO] Batch 115000, worst loss 0.009731 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 23:26:10,463 [INFO] Regularization: 959.734009 * 0.0000000100 = 0.0000095973 loss
2019-04-07 23:26:10,463 [INFO] unfolding 0, single step 115001
2019-04-07 23:26:10,464 [INFO] Sum of grad norms of most recent batch: 0.050553
2019-04-07 23:26:10,464 [INFO] ---------------------------------
2019-04-07 23:26:31,583 [INFO] ---------------------------------
2019-04-07 23:26:31,584 [INFO] Summary:
2019-04-07 23:26:31,585 [INFO] Batch 116000, worst loss 0.040415 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 23:26:31,585 [INFO] Regularization: 959.756592 * 0.0000000100 = 0.0000095976 loss
2019-04-07 23:26:31,585 [INFO] unfolding 0, single step 116001
2019-04-07 23:26:31,586 [INFO] Sum of grad norms of most recent batch: 0.041986
2019-04-07 23:26:31,586 [INFO] ---------------------------------
2019-04-07 23:26:53,046 [INFO] ---------------------------------
2019-04-07 23:26:53,047 [INFO] Summary:
2019-04-07 23:26:53,047 [INFO] Batch 117000, worst loss 0.129794 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 23:26:53,048 [INFO] Regularization: 959.440063 * 0.0000000100 = 0.0000095944 loss
2019-04-07 23:26:53,048 [INFO] unfolding 0, single step 117001
2019-04-07 23:26:53,049 [INFO] Sum of grad norms of most recent batch: 0.028260
2019-04-07 23:26:53,050 [INFO] ---------------------------------
2019-04-07 23:27:14,222 [INFO] ---------------------------------
2019-04-07 23:27:14,223 [INFO] Summary:
2019-04-07 23:27:14,225 [INFO] Batch 118000, worst loss 0.069715 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 23:27:14,226 [INFO] Regularization: 959.277222 * 0.0000000100 = 0.0000095928 loss
2019-04-07 23:27:14,226 [INFO] unfolding 0, single step 118001
2019-04-07 23:27:14,227 [INFO] Sum of grad norms of most recent batch: 0.014633
2019-04-07 23:27:14,228 [INFO] ---------------------------------
2019-04-07 23:27:35,391 [INFO] ---------------------------------
2019-04-07 23:27:35,393 [INFO] Summary:
2019-04-07 23:27:35,394 [INFO] Batch 119000, worst loss 0.065606 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 23:27:35,395 [INFO] Regularization: 959.050415 * 0.0000000100 = 0.0000095905 loss
2019-04-07 23:27:35,395 [INFO] unfolding 0, single step 119001
2019-04-07 23:27:35,396 [INFO] Sum of grad norms of most recent batch: 0.013808
2019-04-07 23:27:35,397 [INFO] ---------------------------------
2019-04-07 23:27:57,455 [INFO] ---------------------------------
2019-04-07 23:27:57,456 [INFO] Summary:
2019-04-07 23:27:57,457 [INFO] Batch 120000, worst loss 0.055554 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-07 23:27:57,457 [INFO] Regularization: 958.862427 * 0.0000000100 = 0.0000095886 loss
2019-04-07 23:27:57,457 [INFO] unfolding 0, single step 120001
2019-04-07 23:27:57,458 [INFO] Sum of grad norms of most recent batch: 0.018199
2019-04-07 23:27:57,459 [INFO] ---------------------------------
2019-04-07 23:28:34,917 [INFO] ---------------------------------
2019-04-07 23:28:34,918 [INFO] Evaluation:
2019-04-07 23:28:34,919 [INFO] Batch 120000, worst loss 0.108424 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:28:34,919 [INFO] ---------------------------------
2019-04-07 23:28:56,366 [INFO] ---------------------------------
2019-04-07 23:28:56,367 [INFO] Summary:
2019-04-07 23:28:56,367 [INFO] Batch 121000, worst loss 0.008461 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 23:28:56,368 [INFO] Regularization: 958.557251 * 0.0000000100 = 0.0000095856 loss
2019-04-07 23:28:56,368 [INFO] unfolding 0, single step 121001
2019-04-07 23:28:56,369 [INFO] Sum of grad norms of most recent batch: 0.025309
2019-04-07 23:28:56,369 [INFO] ---------------------------------
2019-04-07 23:29:17,409 [INFO] ---------------------------------
2019-04-07 23:29:17,410 [INFO] Summary:
2019-04-07 23:29:17,410 [INFO] Batch 122000, worst loss 0.052838 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 23:29:17,411 [INFO] Regularization: 958.404724 * 0.0000000100 = 0.0000095840 loss
2019-04-07 23:29:17,411 [INFO] unfolding 0, single step 122001
2019-04-07 23:29:17,412 [INFO] Sum of grad norms of most recent batch: 0.034021
2019-04-07 23:29:17,412 [INFO] ---------------------------------
2019-04-07 23:29:38,960 [INFO] ---------------------------------
2019-04-07 23:29:38,961 [INFO] Summary:
2019-04-07 23:29:38,962 [INFO] Batch 123000, worst loss 0.109672 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 23:29:38,962 [INFO] Regularization: 958.282593 * 0.0000000100 = 0.0000095828 loss
2019-04-07 23:29:38,963 [INFO] unfolding 0, single step 123001
2019-04-07 23:29:38,964 [INFO] Sum of grad norms of most recent batch: 0.042013
2019-04-07 23:29:38,964 [INFO] ---------------------------------
2019-04-07 23:30:00,340 [INFO] ---------------------------------
2019-04-07 23:30:00,341 [INFO] Summary:
2019-04-07 23:30:00,342 [INFO] Batch 124000, worst loss 0.114337 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 23:30:00,343 [INFO] Regularization: 958.143127 * 0.0000000100 = 0.0000095814 loss
2019-04-07 23:30:00,343 [INFO] unfolding 0, single step 124001
2019-04-07 23:30:00,343 [INFO] Sum of grad norms of most recent batch: 0.061932
2019-04-07 23:30:00,344 [INFO] ---------------------------------
2019-04-07 23:30:21,694 [INFO] ---------------------------------
2019-04-07 23:30:21,695 [INFO] Summary:
2019-04-07 23:30:21,695 [INFO] Batch 125000, worst loss 0.054972 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 23:30:21,696 [INFO] Regularization: 957.992188 * 0.0000000100 = 0.0000095799 loss
2019-04-07 23:30:21,696 [INFO] unfolding 0, single step 125001
2019-04-07 23:30:21,697 [INFO] Sum of grad norms of most recent batch: 0.018986
2019-04-07 23:30:21,697 [INFO] ---------------------------------
2019-04-07 23:30:43,242 [INFO] ---------------------------------
2019-04-07 23:30:43,243 [INFO] Summary:
2019-04-07 23:30:43,244 [INFO] Batch 126000, worst loss 0.086554 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 23:30:43,244 [INFO] Regularization: 957.821960 * 0.0000000100 = 0.0000095782 loss
2019-04-07 23:30:43,245 [INFO] unfolding 0, single step 126001
2019-04-07 23:30:43,246 [INFO] Sum of grad norms of most recent batch: 0.023533
2019-04-07 23:30:43,246 [INFO] ---------------------------------
2019-04-07 23:31:05,158 [INFO] ---------------------------------
2019-04-07 23:31:05,159 [INFO] Summary:
2019-04-07 23:31:05,159 [INFO] Batch 127000, worst loss 0.076482 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 23:31:05,160 [INFO] Regularization: 957.636292 * 0.0000000100 = 0.0000095764 loss
2019-04-07 23:31:05,160 [INFO] unfolding 0, single step 127001
2019-04-07 23:31:05,160 [INFO] Sum of grad norms of most recent batch: 0.025149
2019-04-07 23:31:05,161 [INFO] ---------------------------------
2019-04-07 23:31:25,895 [INFO] ---------------------------------
2019-04-07 23:31:25,897 [INFO] Summary:
2019-04-07 23:31:25,897 [INFO] Batch 128000, worst loss 0.115601 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 23:31:25,898 [INFO] Regularization: 957.453247 * 0.0000000100 = 0.0000095745 loss
2019-04-07 23:31:25,898 [INFO] unfolding 0, single step 128001
2019-04-07 23:31:25,899 [INFO] Sum of grad norms of most recent batch: 0.017375
2019-04-07 23:31:25,899 [INFO] ---------------------------------
2019-04-07 23:31:47,597 [INFO] ---------------------------------
2019-04-07 23:31:47,598 [INFO] Summary:
2019-04-07 23:31:47,599 [INFO] Batch 129000, worst loss 0.081538 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 23:31:47,599 [INFO] Regularization: 957.292053 * 0.0000000100 = 0.0000095729 loss
2019-04-07 23:31:47,600 [INFO] unfolding 0, single step 129001
2019-04-07 23:31:47,600 [INFO] Sum of grad norms of most recent batch: 0.026538
2019-04-07 23:31:47,601 [INFO] ---------------------------------
2019-04-07 23:32:08,992 [INFO] ---------------------------------
2019-04-07 23:32:08,993 [INFO] Summary:
2019-04-07 23:32:08,994 [INFO] Batch 130000, worst loss 0.042384 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-07 23:32:08,994 [INFO] Regularization: 957.112732 * 0.0000000100 = 0.0000095711 loss
2019-04-07 23:32:08,995 [INFO] unfolding 0, single step 130001
2019-04-07 23:32:08,995 [INFO] Sum of grad norms of most recent batch: 0.021564
2019-04-07 23:32:08,996 [INFO] ---------------------------------
2019-04-07 23:32:46,529 [INFO] ---------------------------------
2019-04-07 23:32:46,530 [INFO] Evaluation:
2019-04-07 23:32:46,531 [INFO] Batch 130000, worst loss 0.187947 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:32:46,531 [INFO] ---------------------------------
2019-04-07 23:33:08,179 [INFO] ---------------------------------
2019-04-07 23:33:08,180 [INFO] Summary:
2019-04-07 23:33:08,180 [INFO] Batch 131000, worst loss 0.065556 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:33:08,181 [INFO] Regularization: 956.927734 * 0.0000000100 = 0.0000095693 loss
2019-04-07 23:33:08,181 [INFO] unfolding 0, single step 131001
2019-04-07 23:33:08,182 [INFO] Sum of grad norms of most recent batch: 0.023764
2019-04-07 23:33:08,182 [INFO] ---------------------------------
2019-04-07 23:33:29,642 [INFO] ---------------------------------
2019-04-07 23:33:29,644 [INFO] Summary:
2019-04-07 23:33:29,644 [INFO] Batch 132000, worst loss 0.065508 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:33:29,645 [INFO] Regularization: 956.800659 * 0.0000000100 = 0.0000095680 loss
2019-04-07 23:33:29,645 [INFO] unfolding 0, single step 132001
2019-04-07 23:33:29,646 [INFO] Sum of grad norms of most recent batch: 0.024428
2019-04-07 23:33:29,647 [INFO] ---------------------------------
2019-04-07 23:33:51,150 [INFO] ---------------------------------
2019-04-07 23:33:51,151 [INFO] Summary:
2019-04-07 23:33:51,151 [INFO] Batch 133000, worst loss 0.062481 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:33:51,152 [INFO] Regularization: 956.708862 * 0.0000000100 = 0.0000095671 loss
2019-04-07 23:33:51,152 [INFO] unfolding 0, single step 133001
2019-04-07 23:33:51,153 [INFO] Sum of grad norms of most recent batch: 0.034750
2019-04-07 23:33:51,153 [INFO] ---------------------------------
2019-04-07 23:34:12,122 [INFO] ---------------------------------
2019-04-07 23:34:12,123 [INFO] Summary:
2019-04-07 23:34:12,123 [INFO] Batch 134000, worst loss 0.048467 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:34:12,124 [INFO] Regularization: 956.665649 * 0.0000000100 = 0.0000095667 loss
2019-04-07 23:34:12,124 [INFO] unfolding 0, single step 134001
2019-04-07 23:34:12,125 [INFO] Sum of grad norms of most recent batch: 0.331884
2019-04-07 23:34:12,125 [INFO] ---------------------------------
2019-04-07 23:34:33,257 [INFO] ---------------------------------
2019-04-07 23:34:33,258 [INFO] Summary:
2019-04-07 23:34:33,258 [INFO] Batch 135000, worst loss 0.058749 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:34:33,259 [INFO] Regularization: 956.571472 * 0.0000000100 = 0.0000095657 loss
2019-04-07 23:34:33,259 [INFO] unfolding 0, single step 135001
2019-04-07 23:34:33,259 [INFO] Sum of grad norms of most recent batch: 0.013844
2019-04-07 23:34:33,260 [INFO] ---------------------------------
2019-04-07 23:34:54,637 [INFO] ---------------------------------
2019-04-07 23:34:54,638 [INFO] Summary:
2019-04-07 23:34:54,639 [INFO] Batch 136000, worst loss 0.068617 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:34:54,640 [INFO] Regularization: 956.453430 * 0.0000000100 = 0.0000095645 loss
2019-04-07 23:34:54,640 [INFO] unfolding 0, single step 136001
2019-04-07 23:34:54,641 [INFO] Sum of grad norms of most recent batch: 0.022840
2019-04-07 23:34:54,641 [INFO] ---------------------------------
2019-04-07 23:35:15,536 [INFO] ---------------------------------
2019-04-07 23:35:15,538 [INFO] Summary:
2019-04-07 23:35:15,539 [INFO] Batch 137000, worst loss 0.068595 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:35:15,540 [INFO] Regularization: 956.374634 * 0.0000000100 = 0.0000095637 loss
2019-04-07 23:35:15,540 [INFO] unfolding 0, single step 137001
2019-04-07 23:35:15,541 [INFO] Sum of grad norms of most recent batch: 0.011814
2019-04-07 23:35:15,542 [INFO] ---------------------------------
2019-04-07 23:35:37,518 [INFO] ---------------------------------
2019-04-07 23:35:37,519 [INFO] Summary:
2019-04-07 23:35:37,519 [INFO] Batch 138000, worst loss 0.052616 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:35:37,520 [INFO] Regularization: 956.278381 * 0.0000000100 = 0.0000095628 loss
2019-04-07 23:35:37,520 [INFO] unfolding 0, single step 138001
2019-04-07 23:35:37,521 [INFO] Sum of grad norms of most recent batch: 0.015266
2019-04-07 23:35:37,521 [INFO] ---------------------------------
2019-04-07 23:35:59,137 [INFO] ---------------------------------
2019-04-07 23:35:59,139 [INFO] Summary:
2019-04-07 23:35:59,140 [INFO] Batch 139000, worst loss 0.049293 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:35:59,141 [INFO] Regularization: 956.188721 * 0.0000000100 = 0.0000095619 loss
2019-04-07 23:35:59,142 [INFO] unfolding 0, single step 139001
2019-04-07 23:35:59,143 [INFO] Sum of grad norms of most recent batch: 0.017074
2019-04-07 23:35:59,143 [INFO] ---------------------------------
2019-04-07 23:36:20,501 [INFO] ---------------------------------
2019-04-07 23:36:20,502 [INFO] Summary:
2019-04-07 23:36:20,503 [INFO] Batch 140000, worst loss 0.032934 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:36:20,503 [INFO] Regularization: 956.088562 * 0.0000000100 = 0.0000095609 loss
2019-04-07 23:36:20,503 [INFO] unfolding 0, single step 140001
2019-04-07 23:36:20,504 [INFO] Sum of grad norms of most recent batch: 0.011913
2019-04-07 23:36:20,504 [INFO] ---------------------------------
2019-04-07 23:36:58,024 [INFO] ---------------------------------
2019-04-07 23:36:58,025 [INFO] Evaluation:
2019-04-07 23:36:58,026 [INFO] Batch 140000, worst loss 0.112471 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:36:58,026 [INFO] ---------------------------------
2019-04-07 23:37:18,546 [INFO] ---------------------------------
2019-04-07 23:37:18,548 [INFO] Summary:
2019-04-07 23:37:18,548 [INFO] Batch 141000, worst loss 0.045017 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:37:18,549 [INFO] Regularization: 956.014832 * 0.0000000100 = 0.0000095601 loss
2019-04-07 23:37:18,549 [INFO] unfolding 0, single step 141001
2019-04-07 23:37:18,549 [INFO] Sum of grad norms of most recent batch: 0.913086
2019-04-07 23:37:18,550 [INFO] ---------------------------------
2019-04-07 23:37:39,911 [INFO] ---------------------------------
2019-04-07 23:37:39,912 [INFO] Summary:
2019-04-07 23:37:39,913 [INFO] Batch 142000, worst loss 0.081823 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:37:39,914 [INFO] Regularization: 955.944092 * 0.0000000100 = 0.0000095594 loss
2019-04-07 23:37:39,914 [INFO] unfolding 0, single step 142001
2019-04-07 23:37:39,915 [INFO] Sum of grad norms of most recent batch: 0.027476
2019-04-07 23:37:39,916 [INFO] ---------------------------------
2019-04-07 23:38:01,275 [INFO] ---------------------------------
2019-04-07 23:38:01,276 [INFO] Summary:
2019-04-07 23:38:01,277 [INFO] Batch 143000, worst loss 0.134184 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:38:01,277 [INFO] Regularization: 955.938660 * 0.0000000100 = 0.0000095594 loss
2019-04-07 23:38:01,278 [INFO] unfolding 0, single step 143001
2019-04-07 23:38:01,278 [INFO] Sum of grad norms of most recent batch: 0.024560
2019-04-07 23:38:01,279 [INFO] ---------------------------------
2019-04-07 23:38:22,855 [INFO] ---------------------------------
2019-04-07 23:38:22,856 [INFO] Summary:
2019-04-07 23:38:22,857 [INFO] Batch 144000, worst loss 0.040148 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:38:22,857 [INFO] Regularization: 955.890198 * 0.0000000100 = 0.0000095589 loss
2019-04-07 23:38:22,857 [INFO] unfolding 0, single step 144001
2019-04-07 23:38:22,858 [INFO] Sum of grad norms of most recent batch: 0.016351
2019-04-07 23:38:22,859 [INFO] ---------------------------------
2019-04-07 23:38:44,342 [INFO] ---------------------------------
2019-04-07 23:38:44,343 [INFO] Summary:
2019-04-07 23:38:44,344 [INFO] Batch 145000, worst loss 0.014658 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:38:44,344 [INFO] Regularization: 955.888550 * 0.0000000100 = 0.0000095589 loss
2019-04-07 23:38:44,345 [INFO] unfolding 0, single step 145001
2019-04-07 23:38:44,345 [INFO] Sum of grad norms of most recent batch: 0.020515
2019-04-07 23:38:44,346 [INFO] ---------------------------------
2019-04-07 23:39:05,560 [INFO] ---------------------------------
2019-04-07 23:39:05,561 [INFO] Summary:
2019-04-07 23:39:05,561 [INFO] Batch 146000, worst loss 0.007266 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:39:05,562 [INFO] Regularization: 955.844360 * 0.0000000100 = 0.0000095584 loss
2019-04-07 23:39:05,562 [INFO] unfolding 0, single step 146001
2019-04-07 23:39:05,563 [INFO] Sum of grad norms of most recent batch: 0.015544
2019-04-07 23:39:05,563 [INFO] ---------------------------------
2019-04-07 23:39:26,868 [INFO] ---------------------------------
2019-04-07 23:39:26,869 [INFO] Summary:
2019-04-07 23:39:26,869 [INFO] Batch 147000, worst loss 0.032596 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:39:26,870 [INFO] Regularization: 955.828552 * 0.0000000100 = 0.0000095583 loss
2019-04-07 23:39:26,870 [INFO] unfolding 0, single step 147001
2019-04-07 23:39:26,871 [INFO] Sum of grad norms of most recent batch: 0.015245
2019-04-07 23:39:26,871 [INFO] ---------------------------------
2019-04-07 23:39:48,785 [INFO] ---------------------------------
2019-04-07 23:39:48,786 [INFO] Summary:
2019-04-07 23:39:48,787 [INFO] Batch 148000, worst loss 0.032586 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:39:48,787 [INFO] Regularization: 955.786499 * 0.0000000100 = 0.0000095579 loss
2019-04-07 23:39:48,788 [INFO] unfolding 0, single step 148001
2019-04-07 23:39:48,789 [INFO] Sum of grad norms of most recent batch: 0.012349
2019-04-07 23:39:48,789 [INFO] ---------------------------------
2019-04-07 23:40:10,054 [INFO] ---------------------------------
2019-04-07 23:40:10,055 [INFO] Summary:
2019-04-07 23:40:10,055 [INFO] Batch 149000, worst loss 0.030722 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:40:10,056 [INFO] Regularization: 955.771606 * 0.0000000100 = 0.0000095577 loss
2019-04-07 23:40:10,056 [INFO] unfolding 0, single step 149001
2019-04-07 23:40:10,057 [INFO] Sum of grad norms of most recent batch: 0.019920
2019-04-07 23:40:10,057 [INFO] ---------------------------------
2019-04-07 23:40:31,105 [INFO] ---------------------------------
2019-04-07 23:40:31,107 [INFO] Summary:
2019-04-07 23:40:31,108 [INFO] Batch 150000, worst loss 0.036879 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-07 23:40:31,108 [INFO] Regularization: 955.739197 * 0.0000000100 = 0.0000095574 loss
2019-04-07 23:40:31,109 [INFO] unfolding 0, single step 150001
2019-04-07 23:40:31,110 [INFO] Sum of grad norms of most recent batch: 0.038144
2019-04-07 23:40:31,111 [INFO] ---------------------------------
2019-04-07 23:41:08,674 [INFO] ---------------------------------
2019-04-07 23:41:08,675 [INFO] Evaluation:
2019-04-07 23:41:08,675 [INFO] Batch 150000, worst loss 0.117858 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:41:08,676 [INFO] ---------------------------------
2019-04-07 23:41:08,676 [INFO] Finished training, saved to file transition/1554653688/1554673268_4_transition_final.pth
2019-04-07 23:41:08,857 [INFO] ---------------------------------
2019-04-07 23:41:08,858 [INFO] Training model #5: (11, 64, 8) @ 3
2019-04-07 23:41:30,255 [INFO] ---------------------------------
2019-04-07 23:41:30,256 [INFO] Summary:
2019-04-07 23:41:30,257 [INFO] Batch 1000, worst loss 21.506783 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:41:30,257 [INFO] Regularization: 6871.158203 * 0.0000000100 = 0.0000687116 loss
2019-04-07 23:41:30,258 [INFO] unfolding 0, single step 1001
2019-04-07 23:41:30,259 [INFO] Sum of grad norms of most recent batch: 21.880560
2019-04-07 23:41:30,260 [INFO] ---------------------------------
2019-04-07 23:41:51,862 [INFO] ---------------------------------
2019-04-07 23:41:51,863 [INFO] Summary:
2019-04-07 23:41:51,864 [INFO] Batch 2000, worst loss 0.071397 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:41:51,864 [INFO] Regularization: 4209.551758 * 0.0000000100 = 0.0000420955 loss
2019-04-07 23:41:51,865 [INFO] unfolding 0, single step 2001
2019-04-07 23:41:51,865 [INFO] Sum of grad norms of most recent batch: 3.485073
2019-04-07 23:41:51,866 [INFO] ---------------------------------
2019-04-07 23:42:12,873 [INFO] ---------------------------------
2019-04-07 23:42:12,874 [INFO] Summary:
2019-04-07 23:42:12,874 [INFO] Batch 3000, worst loss 0.162601 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:42:12,875 [INFO] Regularization: 3371.863525 * 0.0000000100 = 0.0000337186 loss
2019-04-07 23:42:12,875 [INFO] unfolding 0, single step 3001
2019-04-07 23:42:12,876 [INFO] Sum of grad norms of most recent batch: 1.576443
2019-04-07 23:42:12,876 [INFO] ---------------------------------
2019-04-07 23:42:34,700 [INFO] ---------------------------------
2019-04-07 23:42:34,701 [INFO] Summary:
2019-04-07 23:42:34,702 [INFO] Batch 4000, worst loss 0.115587 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:42:34,702 [INFO] Regularization: 3085.341553 * 0.0000000100 = 0.0000308534 loss
2019-04-07 23:42:34,703 [INFO] unfolding 0, single step 4001
2019-04-07 23:42:34,703 [INFO] Sum of grad norms of most recent batch: 2.029917
2019-04-07 23:42:34,704 [INFO] ---------------------------------
2019-04-07 23:42:56,424 [INFO] ---------------------------------
2019-04-07 23:42:56,425 [INFO] Summary:
2019-04-07 23:42:56,426 [INFO] Batch 5000, worst loss 0.084797 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:42:56,427 [INFO] Regularization: 2817.763916 * 0.0000000100 = 0.0000281776 loss
2019-04-07 23:42:56,427 [INFO] unfolding 0, single step 5001
2019-04-07 23:42:56,428 [INFO] Sum of grad norms of most recent batch: 3.658541
2019-04-07 23:42:56,428 [INFO] ---------------------------------
2019-04-07 23:43:18,282 [INFO] ---------------------------------
2019-04-07 23:43:18,283 [INFO] Summary:
2019-04-07 23:43:18,284 [INFO] Batch 6000, worst loss 0.066475 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:43:18,284 [INFO] Regularization: 2578.959473 * 0.0000000100 = 0.0000257896 loss
2019-04-07 23:43:18,284 [INFO] unfolding 0, single step 6001
2019-04-07 23:43:18,285 [INFO] Sum of grad norms of most recent batch: 3.343879
2019-04-07 23:43:18,286 [INFO] ---------------------------------
2019-04-07 23:43:40,656 [INFO] ---------------------------------
2019-04-07 23:43:40,657 [INFO] Summary:
2019-04-07 23:43:40,658 [INFO] Batch 7000, worst loss 0.084154 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:43:40,658 [INFO] Regularization: 2379.026367 * 0.0000000100 = 0.0000237903 loss
2019-04-07 23:43:40,659 [INFO] unfolding 0, single step 7001
2019-04-07 23:43:40,659 [INFO] Sum of grad norms of most recent batch: 2.309902
2019-04-07 23:43:40,660 [INFO] ---------------------------------
2019-04-07 23:44:01,831 [INFO] ---------------------------------
2019-04-07 23:44:01,832 [INFO] Summary:
2019-04-07 23:44:01,832 [INFO] Batch 8000, worst loss 0.071144 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:44:01,833 [INFO] Regularization: 2224.270996 * 0.0000000100 = 0.0000222427 loss
2019-04-07 23:44:01,833 [INFO] unfolding 0, single step 8001
2019-04-07 23:44:01,833 [INFO] Sum of grad norms of most recent batch: 1.538425
2019-04-07 23:44:01,834 [INFO] ---------------------------------
2019-04-07 23:44:23,626 [INFO] ---------------------------------
2019-04-07 23:44:23,627 [INFO] Summary:
2019-04-07 23:44:23,627 [INFO] Batch 9000, worst loss 0.099561 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:44:23,628 [INFO] Regularization: 2085.829590 * 0.0000000100 = 0.0000208583 loss
2019-04-07 23:44:23,628 [INFO] unfolding 0, single step 9001
2019-04-07 23:44:23,629 [INFO] Sum of grad norms of most recent batch: 0.828303
2019-04-07 23:44:23,629 [INFO] ---------------------------------
2019-04-07 23:44:44,762 [INFO] ---------------------------------
2019-04-07 23:44:44,763 [INFO] Summary:
2019-04-07 23:44:44,764 [INFO] Batch 10000, worst loss 0.126407 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:44:44,764 [INFO] Regularization: 2008.697021 * 0.0000000100 = 0.0000200870 loss
2019-04-07 23:44:44,765 [INFO] unfolding 0, single step 10001
2019-04-07 23:44:44,765 [INFO] Sum of grad norms of most recent batch: 4.387270
2019-04-07 23:44:44,766 [INFO] ---------------------------------
2019-04-07 23:45:22,183 [INFO] ---------------------------------
2019-04-07 23:45:22,184 [INFO] Evaluation:
2019-04-07 23:45:22,185 [INFO] Batch 10000, worst loss 0.158256 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:45:22,186 [INFO] ---------------------------------
2019-04-07 23:45:43,743 [INFO] ---------------------------------
2019-04-07 23:45:43,744 [INFO] Summary:
2019-04-07 23:45:43,745 [INFO] Batch 11000, worst loss 0.112027 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:45:43,745 [INFO] Regularization: 1945.161499 * 0.0000000100 = 0.0000194516 loss
2019-04-07 23:45:43,746 [INFO] unfolding 0, single step 11001
2019-04-07 23:45:43,747 [INFO] Sum of grad norms of most recent batch: 1.335356
2019-04-07 23:45:43,747 [INFO] ---------------------------------
2019-04-07 23:46:05,327 [INFO] ---------------------------------
2019-04-07 23:46:05,328 [INFO] Summary:
2019-04-07 23:46:05,328 [INFO] Batch 12000, worst loss 0.109238 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:46:05,329 [INFO] Regularization: 1922.668945 * 0.0000000100 = 0.0000192267 loss
2019-04-07 23:46:05,329 [INFO] unfolding 0, single step 12001
2019-04-07 23:46:05,330 [INFO] Sum of grad norms of most recent batch: 1.566951
2019-04-07 23:46:05,331 [INFO] ---------------------------------
2019-04-07 23:46:27,376 [INFO] ---------------------------------
2019-04-07 23:46:27,377 [INFO] Summary:
2019-04-07 23:46:27,378 [INFO] Batch 13000, worst loss 0.160062 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:46:27,378 [INFO] Regularization: 1883.699707 * 0.0000000100 = 0.0000188370 loss
2019-04-07 23:46:27,378 [INFO] unfolding 0, single step 13001
2019-04-07 23:46:27,379 [INFO] Sum of grad norms of most recent batch: 0.957716
2019-04-07 23:46:27,380 [INFO] ---------------------------------
2019-04-07 23:46:49,397 [INFO] ---------------------------------
2019-04-07 23:46:49,399 [INFO] Summary:
2019-04-07 23:46:49,399 [INFO] Batch 14000, worst loss 0.061352 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:46:49,400 [INFO] Regularization: 1872.126465 * 0.0000000100 = 0.0000187213 loss
2019-04-07 23:46:49,400 [INFO] unfolding 0, single step 14001
2019-04-07 23:46:49,401 [INFO] Sum of grad norms of most recent batch: 2.662618
2019-04-07 23:46:49,401 [INFO] ---------------------------------
2019-04-07 23:47:10,297 [INFO] ---------------------------------
2019-04-07 23:47:10,298 [INFO] Summary:
2019-04-07 23:47:10,299 [INFO] Batch 15000, worst loss 0.124732 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:47:10,299 [INFO] Regularization: 1833.072021 * 0.0000000100 = 0.0000183307 loss
2019-04-07 23:47:10,300 [INFO] unfolding 0, single step 15001
2019-04-07 23:47:10,300 [INFO] Sum of grad norms of most recent batch: 0.722964
2019-04-07 23:47:10,301 [INFO] ---------------------------------
2019-04-07 23:47:32,141 [INFO] ---------------------------------
2019-04-07 23:47:32,142 [INFO] Summary:
2019-04-07 23:47:32,142 [INFO] Batch 16000, worst loss 0.044564 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:47:32,143 [INFO] Regularization: 1787.626099 * 0.0000000100 = 0.0000178763 loss
2019-04-07 23:47:32,143 [INFO] unfolding 0, single step 16001
2019-04-07 23:47:32,144 [INFO] Sum of grad norms of most recent batch: 2.356302
2019-04-07 23:47:32,144 [INFO] ---------------------------------
2019-04-07 23:47:53,801 [INFO] ---------------------------------
2019-04-07 23:47:53,802 [INFO] Summary:
2019-04-07 23:47:53,803 [INFO] Batch 17000, worst loss 0.087216 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:47:53,803 [INFO] Regularization: 1795.375610 * 0.0000000100 = 0.0000179538 loss
2019-04-07 23:47:53,804 [INFO] unfolding 0, single step 17001
2019-04-07 23:47:53,804 [INFO] Sum of grad norms of most recent batch: 1.399858
2019-04-07 23:47:53,805 [INFO] ---------------------------------
2019-04-07 23:48:15,676 [INFO] ---------------------------------
2019-04-07 23:48:15,677 [INFO] Summary:
2019-04-07 23:48:15,677 [INFO] Batch 18000, worst loss 0.109478 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:48:15,678 [INFO] Regularization: 1807.507446 * 0.0000000100 = 0.0000180751 loss
2019-04-07 23:48:15,678 [INFO] unfolding 0, single step 18001
2019-04-07 23:48:15,679 [INFO] Sum of grad norms of most recent batch: 1.789009
2019-04-07 23:48:15,679 [INFO] ---------------------------------
2019-04-07 23:48:37,879 [INFO] ---------------------------------
2019-04-07 23:48:37,880 [INFO] Summary:
2019-04-07 23:48:37,881 [INFO] Batch 19000, worst loss 0.166286 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:48:37,881 [INFO] Regularization: 1836.164673 * 0.0000000100 = 0.0000183616 loss
2019-04-07 23:48:37,881 [INFO] unfolding 0, single step 19001
2019-04-07 23:48:37,882 [INFO] Sum of grad norms of most recent batch: 0.513561
2019-04-07 23:48:37,882 [INFO] ---------------------------------
2019-04-07 23:48:59,890 [INFO] ---------------------------------
2019-04-07 23:48:59,891 [INFO] Summary:
2019-04-07 23:48:59,892 [INFO] Batch 20000, worst loss 0.076776 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:48:59,892 [INFO] Regularization: 1814.958374 * 0.0000000100 = 0.0000181496 loss
2019-04-07 23:48:59,893 [INFO] unfolding 0, single step 20001
2019-04-07 23:48:59,893 [INFO] Sum of grad norms of most recent batch: 1.099095
2019-04-07 23:48:59,894 [INFO] ---------------------------------
2019-04-07 23:49:36,932 [INFO] ---------------------------------
2019-04-07 23:49:36,933 [INFO] Evaluation:
2019-04-07 23:49:36,933 [INFO] Batch 20000, worst loss 0.141991 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:49:36,934 [INFO] ---------------------------------
2019-04-07 23:49:58,516 [INFO] ---------------------------------
2019-04-07 23:49:58,517 [INFO] Summary:
2019-04-07 23:49:58,518 [INFO] Batch 21000, worst loss 0.112240 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:49:58,518 [INFO] Regularization: 1829.770020 * 0.0000000100 = 0.0000182977 loss
2019-04-07 23:49:58,518 [INFO] unfolding 0, single step 21001
2019-04-07 23:49:58,519 [INFO] Sum of grad norms of most recent batch: 1.562138
2019-04-07 23:49:58,519 [INFO] ---------------------------------
2019-04-07 23:50:19,919 [INFO] ---------------------------------
2019-04-07 23:50:19,920 [INFO] Summary:
2019-04-07 23:50:19,921 [INFO] Batch 22000, worst loss 0.084680 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:50:19,921 [INFO] Regularization: 1838.869507 * 0.0000000100 = 0.0000183887 loss
2019-04-07 23:50:19,922 [INFO] unfolding 0, single step 22001
2019-04-07 23:50:19,922 [INFO] Sum of grad norms of most recent batch: 2.459745
2019-04-07 23:50:19,923 [INFO] ---------------------------------
2019-04-07 23:50:41,740 [INFO] ---------------------------------
2019-04-07 23:50:41,741 [INFO] Summary:
2019-04-07 23:50:41,741 [INFO] Batch 23000, worst loss 0.106729 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:50:41,742 [INFO] Regularization: 1829.767578 * 0.0000000100 = 0.0000182977 loss
2019-04-07 23:50:41,742 [INFO] unfolding 0, single step 23001
2019-04-07 23:50:41,743 [INFO] Sum of grad norms of most recent batch: 0.419050
2019-04-07 23:50:41,743 [INFO] ---------------------------------
2019-04-07 23:51:03,414 [INFO] ---------------------------------
2019-04-07 23:51:03,416 [INFO] Summary:
2019-04-07 23:51:03,416 [INFO] Batch 24000, worst loss 0.035828 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:51:03,417 [INFO] Regularization: 1814.657715 * 0.0000000100 = 0.0000181466 loss
2019-04-07 23:51:03,417 [INFO] unfolding 0, single step 24001
2019-04-07 23:51:03,417 [INFO] Sum of grad norms of most recent batch: 2.671015
2019-04-07 23:51:03,418 [INFO] ---------------------------------
2019-04-07 23:51:25,002 [INFO] ---------------------------------
2019-04-07 23:51:25,003 [INFO] Summary:
2019-04-07 23:51:25,003 [INFO] Batch 25000, worst loss 0.052582 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:51:25,004 [INFO] Regularization: 1822.521729 * 0.0000000100 = 0.0000182252 loss
2019-04-07 23:51:25,004 [INFO] unfolding 0, single step 25001
2019-04-07 23:51:25,005 [INFO] Sum of grad norms of most recent batch: 2.311529
2019-04-07 23:51:25,005 [INFO] ---------------------------------
2019-04-07 23:51:46,876 [INFO] ---------------------------------
2019-04-07 23:51:46,877 [INFO] Summary:
2019-04-07 23:51:46,877 [INFO] Batch 26000, worst loss 0.137557 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:51:46,878 [INFO] Regularization: 1802.105103 * 0.0000000100 = 0.0000180211 loss
2019-04-07 23:51:46,878 [INFO] unfolding 0, single step 26001
2019-04-07 23:51:46,879 [INFO] Sum of grad norms of most recent batch: 0.749797
2019-04-07 23:51:46,879 [INFO] ---------------------------------
2019-04-07 23:52:09,012 [INFO] ---------------------------------
2019-04-07 23:52:09,013 [INFO] Summary:
2019-04-07 23:52:09,013 [INFO] Batch 27000, worst loss 0.059841 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:52:09,014 [INFO] Regularization: 1811.186890 * 0.0000000100 = 0.0000181119 loss
2019-04-07 23:52:09,014 [INFO] unfolding 0, single step 27001
2019-04-07 23:52:09,015 [INFO] Sum of grad norms of most recent batch: 1.056433
2019-04-07 23:52:09,015 [INFO] ---------------------------------
2019-04-07 23:52:30,901 [INFO] ---------------------------------
2019-04-07 23:52:30,901 [INFO] Summary:
2019-04-07 23:52:30,902 [INFO] Batch 28000, worst loss 0.104597 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:52:30,903 [INFO] Regularization: 1801.997192 * 0.0000000100 = 0.0000180200 loss
2019-04-07 23:52:30,903 [INFO] unfolding 0, single step 28001
2019-04-07 23:52:30,903 [INFO] Sum of grad norms of most recent batch: 1.606351
2019-04-07 23:52:30,904 [INFO] ---------------------------------
2019-04-07 23:52:52,953 [INFO] ---------------------------------
2019-04-07 23:52:52,954 [INFO] Summary:
2019-04-07 23:52:52,955 [INFO] Batch 29000, worst loss 0.052648 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:52:52,956 [INFO] Regularization: 1808.112183 * 0.0000000100 = 0.0000180811 loss
2019-04-07 23:52:52,956 [INFO] unfolding 0, single step 29001
2019-04-07 23:52:52,957 [INFO] Sum of grad norms of most recent batch: 1.602314
2019-04-07 23:52:52,957 [INFO] ---------------------------------
2019-04-07 23:53:13,787 [INFO] ---------------------------------
2019-04-07 23:53:13,788 [INFO] Summary:
2019-04-07 23:53:13,789 [INFO] Batch 30000, worst loss 0.143870 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-07 23:53:13,789 [INFO] Regularization: 1790.149048 * 0.0000000100 = 0.0000179015 loss
2019-04-07 23:53:13,790 [INFO] unfolding 0, single step 30001
2019-04-07 23:53:13,790 [INFO] Sum of grad norms of most recent batch: 0.849882
2019-04-07 23:53:13,791 [INFO] ---------------------------------
2019-04-07 23:53:51,092 [INFO] ---------------------------------
2019-04-07 23:53:51,093 [INFO] Evaluation:
2019-04-07 23:53:51,094 [INFO] Batch 30000, worst loss 0.126537 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:53:51,095 [INFO] ---------------------------------
2019-04-07 23:54:12,514 [INFO] ---------------------------------
2019-04-07 23:54:12,515 [INFO] Summary:
2019-04-07 23:54:12,516 [INFO] Batch 31000, worst loss 0.093611 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 23:54:12,516 [INFO] Regularization: 1798.516113 * 0.0000000100 = 0.0000179852 loss
2019-04-07 23:54:12,517 [INFO] unfolding 0, single step 31001
2019-04-07 23:54:12,518 [INFO] Sum of grad norms of most recent batch: 1.886945
2019-04-07 23:54:12,519 [INFO] ---------------------------------
2019-04-07 23:54:35,187 [INFO] ---------------------------------
2019-04-07 23:54:35,188 [INFO] Summary:
2019-04-07 23:54:35,189 [INFO] Batch 32000, worst loss 0.108544 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 23:54:35,190 [INFO] Regularization: 1733.505981 * 0.0000000100 = 0.0000173351 loss
2019-04-07 23:54:35,190 [INFO] unfolding 0, single step 32001
2019-04-07 23:54:35,191 [INFO] Sum of grad norms of most recent batch: 2.134995
2019-04-07 23:54:35,192 [INFO] ---------------------------------
2019-04-07 23:54:57,084 [INFO] ---------------------------------
2019-04-07 23:54:57,086 [INFO] Summary:
2019-04-07 23:54:57,087 [INFO] Batch 33000, worst loss 0.077729 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 23:54:57,088 [INFO] Regularization: 1704.254639 * 0.0000000100 = 0.0000170425 loss
2019-04-07 23:54:57,088 [INFO] unfolding 0, single step 33001
2019-04-07 23:54:57,089 [INFO] Sum of grad norms of most recent batch: 0.673787
2019-04-07 23:54:57,090 [INFO] ---------------------------------
2019-04-07 23:55:18,422 [INFO] ---------------------------------
2019-04-07 23:55:18,423 [INFO] Summary:
2019-04-07 23:55:18,424 [INFO] Batch 34000, worst loss 0.118058 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 23:55:18,424 [INFO] Regularization: 1707.408447 * 0.0000000100 = 0.0000170741 loss
2019-04-07 23:55:18,425 [INFO] unfolding 0, single step 34001
2019-04-07 23:55:18,425 [INFO] Sum of grad norms of most recent batch: 1.065484
2019-04-07 23:55:18,426 [INFO] ---------------------------------
2019-04-07 23:55:40,074 [INFO] ---------------------------------
2019-04-07 23:55:40,075 [INFO] Summary:
2019-04-07 23:55:40,076 [INFO] Batch 35000, worst loss 0.116284 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 23:55:40,076 [INFO] Regularization: 1686.462280 * 0.0000000100 = 0.0000168646 loss
2019-04-07 23:55:40,076 [INFO] unfolding 0, single step 35001
2019-04-07 23:55:40,077 [INFO] Sum of grad norms of most recent batch: 0.728063
2019-04-07 23:55:40,078 [INFO] ---------------------------------
2019-04-07 23:56:00,969 [INFO] ---------------------------------
2019-04-07 23:56:00,970 [INFO] Summary:
2019-04-07 23:56:00,971 [INFO] Batch 36000, worst loss 0.126676 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 23:56:00,971 [INFO] Regularization: 1688.903442 * 0.0000000100 = 0.0000168890 loss
2019-04-07 23:56:00,972 [INFO] unfolding 0, single step 36001
2019-04-07 23:56:00,973 [INFO] Sum of grad norms of most recent batch: 1.002323
2019-04-07 23:56:00,973 [INFO] ---------------------------------
2019-04-07 23:56:22,402 [INFO] ---------------------------------
2019-04-07 23:56:22,403 [INFO] Summary:
2019-04-07 23:56:22,404 [INFO] Batch 37000, worst loss 0.043584 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 23:56:22,404 [INFO] Regularization: 1675.329956 * 0.0000000100 = 0.0000167533 loss
2019-04-07 23:56:22,404 [INFO] unfolding 0, single step 37001
2019-04-07 23:56:22,405 [INFO] Sum of grad norms of most recent batch: 0.946314
2019-04-07 23:56:22,405 [INFO] ---------------------------------
2019-04-07 23:56:43,971 [INFO] ---------------------------------
2019-04-07 23:56:43,971 [INFO] Summary:
2019-04-07 23:56:43,972 [INFO] Batch 38000, worst loss 0.042506 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 23:56:43,973 [INFO] Regularization: 1642.000977 * 0.0000000100 = 0.0000164200 loss
2019-04-07 23:56:43,973 [INFO] unfolding 0, single step 38001
2019-04-07 23:56:43,974 [INFO] Sum of grad norms of most recent batch: 0.478875
2019-04-07 23:56:43,974 [INFO] ---------------------------------
2019-04-07 23:57:04,996 [INFO] ---------------------------------
2019-04-07 23:57:04,997 [INFO] Summary:
2019-04-07 23:57:04,997 [INFO] Batch 39000, worst loss 0.038999 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 23:57:04,998 [INFO] Regularization: 1644.671143 * 0.0000000100 = 0.0000164467 loss
2019-04-07 23:57:04,998 [INFO] unfolding 0, single step 39001
2019-04-07 23:57:04,999 [INFO] Sum of grad norms of most recent batch: 0.649521
2019-04-07 23:57:04,999 [INFO] ---------------------------------
2019-04-07 23:57:26,755 [INFO] ---------------------------------
2019-04-07 23:57:26,756 [INFO] Summary:
2019-04-07 23:57:26,756 [INFO] Batch 40000, worst loss 0.031953 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-07 23:57:26,757 [INFO] Regularization: 1637.775391 * 0.0000000100 = 0.0000163778 loss
2019-04-07 23:57:26,757 [INFO] unfolding 0, single step 40001
2019-04-07 23:57:26,758 [INFO] Sum of grad norms of most recent batch: 0.721543
2019-04-07 23:57:26,758 [INFO] ---------------------------------
2019-04-07 23:58:03,990 [INFO] ---------------------------------
2019-04-07 23:58:03,991 [INFO] Evaluation:
2019-04-07 23:58:03,992 [INFO] Batch 40000, worst loss 0.117942 of 10000 batches (without reg.) @est.-depth 3
2019-04-07 23:58:03,992 [INFO] ---------------------------------
2019-04-07 23:58:25,371 [INFO] ---------------------------------
2019-04-07 23:58:25,372 [INFO] Summary:
2019-04-07 23:58:25,372 [INFO] Batch 41000, worst loss 0.132558 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 23:58:25,373 [INFO] Regularization: 1638.711060 * 0.0000000100 = 0.0000163871 loss
2019-04-07 23:58:25,373 [INFO] unfolding 0, single step 41001
2019-04-07 23:58:25,374 [INFO] Sum of grad norms of most recent batch: 0.590436
2019-04-07 23:58:25,374 [INFO] ---------------------------------
2019-04-07 23:58:46,735 [INFO] ---------------------------------
2019-04-07 23:58:46,736 [INFO] Summary:
2019-04-07 23:58:46,737 [INFO] Batch 42000, worst loss 0.058274 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 23:58:46,737 [INFO] Regularization: 1602.374390 * 0.0000000100 = 0.0000160237 loss
2019-04-07 23:58:46,738 [INFO] unfolding 0, single step 42001
2019-04-07 23:58:46,738 [INFO] Sum of grad norms of most recent batch: 0.220294
2019-04-07 23:58:46,739 [INFO] ---------------------------------
2019-04-07 23:59:08,147 [INFO] ---------------------------------
2019-04-07 23:59:08,148 [INFO] Summary:
2019-04-07 23:59:08,149 [INFO] Batch 43000, worst loss 0.101812 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 23:59:08,150 [INFO] Regularization: 1586.679321 * 0.0000000100 = 0.0000158668 loss
2019-04-07 23:59:08,150 [INFO] unfolding 0, single step 43001
2019-04-07 23:59:08,151 [INFO] Sum of grad norms of most recent batch: 0.436841
2019-04-07 23:59:08,151 [INFO] ---------------------------------
2019-04-07 23:59:29,477 [INFO] ---------------------------------
2019-04-07 23:59:29,478 [INFO] Summary:
2019-04-07 23:59:29,479 [INFO] Batch 44000, worst loss 0.065555 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 23:59:29,479 [INFO] Regularization: 1605.553833 * 0.0000000100 = 0.0000160555 loss
2019-04-07 23:59:29,479 [INFO] unfolding 0, single step 44001
2019-04-07 23:59:29,480 [INFO] Sum of grad norms of most recent batch: 0.657705
2019-04-07 23:59:29,481 [INFO] ---------------------------------
2019-04-07 23:59:50,739 [INFO] ---------------------------------
2019-04-07 23:59:50,740 [INFO] Summary:
2019-04-07 23:59:50,740 [INFO] Batch 45000, worst loss 0.088302 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-07 23:59:50,741 [INFO] Regularization: 1586.685303 * 0.0000000100 = 0.0000158669 loss
2019-04-07 23:59:50,741 [INFO] unfolding 0, single step 45001
2019-04-07 23:59:50,741 [INFO] Sum of grad norms of most recent batch: 0.805933
2019-04-07 23:59:50,742 [INFO] ---------------------------------
2019-04-08 00:00:12,175 [INFO] ---------------------------------
2019-04-08 00:00:12,176 [INFO] Summary:
2019-04-08 00:00:12,177 [INFO] Batch 46000, worst loss 0.043756 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 00:00:12,179 [INFO] Regularization: 1586.979980 * 0.0000000100 = 0.0000158698 loss
2019-04-08 00:00:12,179 [INFO] unfolding 0, single step 46001
2019-04-08 00:00:12,180 [INFO] Sum of grad norms of most recent batch: 0.280558
2019-04-08 00:00:12,181 [INFO] ---------------------------------
2019-04-08 00:00:34,109 [INFO] ---------------------------------
2019-04-08 00:00:34,110 [INFO] Summary:
2019-04-08 00:00:34,110 [INFO] Batch 47000, worst loss 0.061593 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 00:00:34,111 [INFO] Regularization: 1577.138428 * 0.0000000100 = 0.0000157714 loss
2019-04-08 00:00:34,111 [INFO] unfolding 0, single step 47001
2019-04-08 00:00:34,112 [INFO] Sum of grad norms of most recent batch: 0.531376
2019-04-08 00:00:34,112 [INFO] ---------------------------------
2019-04-08 00:00:55,656 [INFO] ---------------------------------
2019-04-08 00:00:55,658 [INFO] Summary:
2019-04-08 00:00:55,658 [INFO] Batch 48000, worst loss 0.108632 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 00:00:55,659 [INFO] Regularization: 1576.846680 * 0.0000000100 = 0.0000157685 loss
2019-04-08 00:00:55,659 [INFO] unfolding 0, single step 48001
2019-04-08 00:00:55,660 [INFO] Sum of grad norms of most recent batch: 0.246007
2019-04-08 00:00:55,661 [INFO] ---------------------------------
2019-04-08 00:01:17,203 [INFO] ---------------------------------
2019-04-08 00:01:17,204 [INFO] Summary:
2019-04-08 00:01:17,205 [INFO] Batch 49000, worst loss 0.041128 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 00:01:17,205 [INFO] Regularization: 1565.063843 * 0.0000000100 = 0.0000156506 loss
2019-04-08 00:01:17,205 [INFO] unfolding 0, single step 49001
2019-04-08 00:01:17,206 [INFO] Sum of grad norms of most recent batch: 0.336080
2019-04-08 00:01:17,207 [INFO] ---------------------------------
2019-04-08 00:01:39,683 [INFO] ---------------------------------
2019-04-08 00:01:39,684 [INFO] Summary:
2019-04-08 00:01:39,685 [INFO] Batch 50000, worst loss 0.067194 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 00:01:39,685 [INFO] Regularization: 1564.465088 * 0.0000000100 = 0.0000156447 loss
2019-04-08 00:01:39,686 [INFO] unfolding 0, single step 50001
2019-04-08 00:01:39,686 [INFO] Sum of grad norms of most recent batch: 0.508598
2019-04-08 00:01:39,687 [INFO] ---------------------------------
2019-04-08 00:02:17,184 [INFO] ---------------------------------
2019-04-08 00:02:17,185 [INFO] Evaluation:
2019-04-08 00:02:17,186 [INFO] Batch 50000, worst loss 0.087933 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:02:17,186 [INFO] ---------------------------------
2019-04-08 00:02:39,151 [INFO] ---------------------------------
2019-04-08 00:02:39,152 [INFO] Summary:
2019-04-08 00:02:39,152 [INFO] Batch 51000, worst loss 0.039550 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 00:02:39,153 [INFO] Regularization: 1554.526123 * 0.0000000100 = 0.0000155453 loss
2019-04-08 00:02:39,153 [INFO] unfolding 0, single step 51001
2019-04-08 00:02:39,154 [INFO] Sum of grad norms of most recent batch: 0.679856
2019-04-08 00:02:39,154 [INFO] ---------------------------------
2019-04-08 00:03:00,150 [INFO] ---------------------------------
2019-04-08 00:03:00,151 [INFO] Summary:
2019-04-08 00:03:00,151 [INFO] Batch 52000, worst loss 0.025627 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 00:03:00,152 [INFO] Regularization: 1543.722900 * 0.0000000100 = 0.0000154372 loss
2019-04-08 00:03:00,152 [INFO] unfolding 0, single step 52001
2019-04-08 00:03:00,153 [INFO] Sum of grad norms of most recent batch: 0.237305
2019-04-08 00:03:00,153 [INFO] ---------------------------------
2019-04-08 00:03:20,904 [INFO] ---------------------------------
2019-04-08 00:03:20,905 [INFO] Summary:
2019-04-08 00:03:20,905 [INFO] Batch 53000, worst loss 0.023335 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 00:03:20,906 [INFO] Regularization: 1541.123047 * 0.0000000100 = 0.0000154112 loss
2019-04-08 00:03:20,906 [INFO] unfolding 0, single step 53001
2019-04-08 00:03:20,907 [INFO] Sum of grad norms of most recent batch: 0.097133
2019-04-08 00:03:20,907 [INFO] ---------------------------------
2019-04-08 00:03:42,652 [INFO] ---------------------------------
2019-04-08 00:03:42,653 [INFO] Summary:
2019-04-08 00:03:42,653 [INFO] Batch 54000, worst loss 0.020117 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 00:03:42,654 [INFO] Regularization: 1539.717041 * 0.0000000100 = 0.0000153972 loss
2019-04-08 00:03:42,654 [INFO] unfolding 0, single step 54001
2019-04-08 00:03:42,655 [INFO] Sum of grad norms of most recent batch: 0.321295
2019-04-08 00:03:42,656 [INFO] ---------------------------------
2019-04-08 00:04:04,653 [INFO] ---------------------------------
2019-04-08 00:04:04,654 [INFO] Summary:
2019-04-08 00:04:04,654 [INFO] Batch 55000, worst loss 0.087767 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 00:04:04,655 [INFO] Regularization: 1533.395264 * 0.0000000100 = 0.0000153340 loss
2019-04-08 00:04:04,655 [INFO] unfolding 0, single step 55001
2019-04-08 00:04:04,656 [INFO] Sum of grad norms of most recent batch: 0.224844
2019-04-08 00:04:04,656 [INFO] ---------------------------------
2019-04-08 00:04:26,701 [INFO] ---------------------------------
2019-04-08 00:04:26,702 [INFO] Summary:
2019-04-08 00:04:26,703 [INFO] Batch 56000, worst loss 0.050148 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 00:04:26,703 [INFO] Regularization: 1531.379272 * 0.0000000100 = 0.0000153138 loss
2019-04-08 00:04:26,704 [INFO] unfolding 0, single step 56001
2019-04-08 00:04:26,704 [INFO] Sum of grad norms of most recent batch: 0.296444
2019-04-08 00:04:26,705 [INFO] ---------------------------------
2019-04-08 00:04:48,129 [INFO] ---------------------------------
2019-04-08 00:04:48,130 [INFO] Summary:
2019-04-08 00:04:48,131 [INFO] Batch 57000, worst loss 0.037349 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 00:04:48,131 [INFO] Regularization: 1526.070557 * 0.0000000100 = 0.0000152607 loss
2019-04-08 00:04:48,132 [INFO] unfolding 0, single step 57001
2019-04-08 00:04:48,132 [INFO] Sum of grad norms of most recent batch: 0.183500
2019-04-08 00:04:48,133 [INFO] ---------------------------------
2019-04-08 00:05:09,003 [INFO] ---------------------------------
2019-04-08 00:05:09,004 [INFO] Summary:
2019-04-08 00:05:09,004 [INFO] Batch 58000, worst loss 0.026502 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 00:05:09,005 [INFO] Regularization: 1520.898438 * 0.0000000100 = 0.0000152090 loss
2019-04-08 00:05:09,005 [INFO] unfolding 0, single step 58001
2019-04-08 00:05:09,006 [INFO] Sum of grad norms of most recent batch: 0.529625
2019-04-08 00:05:09,007 [INFO] ---------------------------------
2019-04-08 00:05:30,153 [INFO] ---------------------------------
2019-04-08 00:05:30,154 [INFO] Summary:
2019-04-08 00:05:30,155 [INFO] Batch 59000, worst loss 0.034955 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 00:05:30,155 [INFO] Regularization: 1521.606567 * 0.0000000100 = 0.0000152161 loss
2019-04-08 00:05:30,156 [INFO] unfolding 0, single step 59001
2019-04-08 00:05:30,156 [INFO] Sum of grad norms of most recent batch: 0.340813
2019-04-08 00:05:30,157 [INFO] ---------------------------------
2019-04-08 00:05:51,712 [INFO] ---------------------------------
2019-04-08 00:05:51,713 [INFO] Summary:
2019-04-08 00:05:51,714 [INFO] Batch 60000, worst loss 0.046070 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 00:05:51,715 [INFO] Regularization: 1521.119995 * 0.0000000100 = 0.0000152112 loss
2019-04-08 00:05:51,715 [INFO] unfolding 0, single step 60001
2019-04-08 00:05:51,716 [INFO] Sum of grad norms of most recent batch: 0.290500
2019-04-08 00:05:51,716 [INFO] ---------------------------------
2019-04-08 00:06:29,173 [INFO] ---------------------------------
2019-04-08 00:06:29,174 [INFO] Evaluation:
2019-04-08 00:06:29,174 [INFO] Batch 60000, worst loss 0.065653 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:06:29,175 [INFO] ---------------------------------
2019-04-08 00:06:50,762 [INFO] ---------------------------------
2019-04-08 00:06:50,763 [INFO] Summary:
2019-04-08 00:06:50,763 [INFO] Batch 61000, worst loss 0.063273 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 00:06:50,764 [INFO] Regularization: 1516.792725 * 0.0000000100 = 0.0000151679 loss
2019-04-08 00:06:50,764 [INFO] unfolding 0, single step 61001
2019-04-08 00:06:50,765 [INFO] Sum of grad norms of most recent batch: 0.211803
2019-04-08 00:06:50,766 [INFO] ---------------------------------
2019-04-08 00:07:12,350 [INFO] ---------------------------------
2019-04-08 00:07:12,351 [INFO] Summary:
2019-04-08 00:07:12,352 [INFO] Batch 62000, worst loss 0.014698 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 00:07:12,352 [INFO] Regularization: 1513.554077 * 0.0000000100 = 0.0000151355 loss
2019-04-08 00:07:12,353 [INFO] unfolding 0, single step 62001
2019-04-08 00:07:12,353 [INFO] Sum of grad norms of most recent batch: 0.115468
2019-04-08 00:07:12,354 [INFO] ---------------------------------
2019-04-08 00:07:33,449 [INFO] ---------------------------------
2019-04-08 00:07:33,450 [INFO] Summary:
2019-04-08 00:07:33,451 [INFO] Batch 63000, worst loss 0.017502 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 00:07:33,451 [INFO] Regularization: 1511.366089 * 0.0000000100 = 0.0000151137 loss
2019-04-08 00:07:33,451 [INFO] unfolding 0, single step 63001
2019-04-08 00:07:33,452 [INFO] Sum of grad norms of most recent batch: 0.198911
2019-04-08 00:07:33,452 [INFO] ---------------------------------
2019-04-08 00:07:54,861 [INFO] ---------------------------------
2019-04-08 00:07:54,862 [INFO] Summary:
2019-04-08 00:07:54,863 [INFO] Batch 64000, worst loss 0.007550 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 00:07:54,863 [INFO] Regularization: 1508.210449 * 0.0000000100 = 0.0000150821 loss
2019-04-08 00:07:54,864 [INFO] unfolding 0, single step 64001
2019-04-08 00:07:54,864 [INFO] Sum of grad norms of most recent batch: 0.081750
2019-04-08 00:07:54,865 [INFO] ---------------------------------
2019-04-08 00:08:16,218 [INFO] ---------------------------------
2019-04-08 00:08:16,220 [INFO] Summary:
2019-04-08 00:08:16,221 [INFO] Batch 65000, worst loss 0.021009 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 00:08:16,222 [INFO] Regularization: 1505.542969 * 0.0000000100 = 0.0000150554 loss
2019-04-08 00:08:16,223 [INFO] unfolding 0, single step 65001
2019-04-08 00:08:16,224 [INFO] Sum of grad norms of most recent batch: 0.067662
2019-04-08 00:08:16,225 [INFO] ---------------------------------
2019-04-08 00:08:38,279 [INFO] ---------------------------------
2019-04-08 00:08:38,280 [INFO] Summary:
2019-04-08 00:08:38,280 [INFO] Batch 66000, worst loss 0.024532 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 00:08:38,281 [INFO] Regularization: 1503.328125 * 0.0000000100 = 0.0000150333 loss
2019-04-08 00:08:38,281 [INFO] unfolding 0, single step 66001
2019-04-08 00:08:38,281 [INFO] Sum of grad norms of most recent batch: 0.123308
2019-04-08 00:08:38,282 [INFO] ---------------------------------
2019-04-08 00:08:59,354 [INFO] ---------------------------------
2019-04-08 00:08:59,355 [INFO] Summary:
2019-04-08 00:08:59,355 [INFO] Batch 67000, worst loss 0.016134 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 00:08:59,356 [INFO] Regularization: 1502.051025 * 0.0000000100 = 0.0000150205 loss
2019-04-08 00:08:59,356 [INFO] unfolding 0, single step 67001
2019-04-08 00:08:59,357 [INFO] Sum of grad norms of most recent batch: 0.057477
2019-04-08 00:08:59,357 [INFO] ---------------------------------
2019-04-08 00:09:20,381 [INFO] ---------------------------------
2019-04-08 00:09:20,382 [INFO] Summary:
2019-04-08 00:09:20,383 [INFO] Batch 68000, worst loss 0.018422 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 00:09:20,383 [INFO] Regularization: 1499.129395 * 0.0000000100 = 0.0000149913 loss
2019-04-08 00:09:20,383 [INFO] unfolding 0, single step 68001
2019-04-08 00:09:20,384 [INFO] Sum of grad norms of most recent batch: 0.107269
2019-04-08 00:09:20,384 [INFO] ---------------------------------
2019-04-08 00:09:41,488 [INFO] ---------------------------------
2019-04-08 00:09:41,489 [INFO] Summary:
2019-04-08 00:09:41,490 [INFO] Batch 69000, worst loss 0.033503 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 00:09:41,490 [INFO] Regularization: 1498.432007 * 0.0000000100 = 0.0000149843 loss
2019-04-08 00:09:41,491 [INFO] unfolding 0, single step 69001
2019-04-08 00:09:41,491 [INFO] Sum of grad norms of most recent batch: 0.222672
2019-04-08 00:09:41,492 [INFO] ---------------------------------
2019-04-08 00:10:02,555 [INFO] ---------------------------------
2019-04-08 00:10:02,556 [INFO] Summary:
2019-04-08 00:10:02,556 [INFO] Batch 70000, worst loss 0.013593 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 00:10:02,557 [INFO] Regularization: 1497.307007 * 0.0000000100 = 0.0000149731 loss
2019-04-08 00:10:02,557 [INFO] unfolding 0, single step 70001
2019-04-08 00:10:02,558 [INFO] Sum of grad norms of most recent batch: 0.153027
2019-04-08 00:10:02,558 [INFO] ---------------------------------
2019-04-08 00:10:39,976 [INFO] ---------------------------------
2019-04-08 00:10:39,977 [INFO] Evaluation:
2019-04-08 00:10:39,977 [INFO] Batch 70000, worst loss 0.031949 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:10:39,978 [INFO] ---------------------------------
2019-04-08 00:11:01,295 [INFO] ---------------------------------
2019-04-08 00:11:01,296 [INFO] Summary:
2019-04-08 00:11:01,296 [INFO] Batch 71000, worst loss 0.014115 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 00:11:01,297 [INFO] Regularization: 1495.161011 * 0.0000000100 = 0.0000149516 loss
2019-04-08 00:11:01,297 [INFO] unfolding 0, single step 71001
2019-04-08 00:11:01,298 [INFO] Sum of grad norms of most recent batch: 0.248759
2019-04-08 00:11:01,298 [INFO] ---------------------------------
2019-04-08 00:11:21,919 [INFO] ---------------------------------
2019-04-08 00:11:21,919 [INFO] Summary:
2019-04-08 00:11:21,920 [INFO] Batch 72000, worst loss 0.020180 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 00:11:21,920 [INFO] Regularization: 1494.262085 * 0.0000000100 = 0.0000149426 loss
2019-04-08 00:11:21,920 [INFO] unfolding 0, single step 72001
2019-04-08 00:11:21,921 [INFO] Sum of grad norms of most recent batch: 0.055188
2019-04-08 00:11:21,921 [INFO] ---------------------------------
2019-04-08 00:11:43,015 [INFO] ---------------------------------
2019-04-08 00:11:43,016 [INFO] Summary:
2019-04-08 00:11:43,017 [INFO] Batch 73000, worst loss 0.015765 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 00:11:43,018 [INFO] Regularization: 1493.158325 * 0.0000000100 = 0.0000149316 loss
2019-04-08 00:11:43,018 [INFO] unfolding 0, single step 73001
2019-04-08 00:11:43,019 [INFO] Sum of grad norms of most recent batch: 0.044384
2019-04-08 00:11:43,019 [INFO] ---------------------------------
2019-04-08 00:12:04,070 [INFO] ---------------------------------
2019-04-08 00:12:04,070 [INFO] Summary:
2019-04-08 00:12:04,071 [INFO] Batch 74000, worst loss 0.022994 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 00:12:04,072 [INFO] Regularization: 1492.104736 * 0.0000000100 = 0.0000149210 loss
2019-04-08 00:12:04,072 [INFO] unfolding 0, single step 74001
2019-04-08 00:12:04,073 [INFO] Sum of grad norms of most recent batch: 0.103775
2019-04-08 00:12:04,073 [INFO] ---------------------------------
2019-04-08 00:12:24,887 [INFO] ---------------------------------
2019-04-08 00:12:24,888 [INFO] Summary:
2019-04-08 00:12:24,888 [INFO] Batch 75000, worst loss 0.015467 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 00:12:24,889 [INFO] Regularization: 1491.467407 * 0.0000000100 = 0.0000149147 loss
2019-04-08 00:12:24,889 [INFO] unfolding 0, single step 75001
2019-04-08 00:12:24,890 [INFO] Sum of grad norms of most recent batch: 0.035591
2019-04-08 00:12:24,890 [INFO] ---------------------------------
2019-04-08 00:12:45,810 [INFO] ---------------------------------
2019-04-08 00:12:45,811 [INFO] Summary:
2019-04-08 00:12:45,812 [INFO] Batch 76000, worst loss 0.012150 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 00:12:45,813 [INFO] Regularization: 1490.022583 * 0.0000000100 = 0.0000149002 loss
2019-04-08 00:12:45,814 [INFO] unfolding 0, single step 76001
2019-04-08 00:12:45,815 [INFO] Sum of grad norms of most recent batch: 0.051050
2019-04-08 00:12:45,816 [INFO] ---------------------------------
2019-04-08 00:13:07,490 [INFO] ---------------------------------
2019-04-08 00:13:07,491 [INFO] Summary:
2019-04-08 00:13:07,492 [INFO] Batch 77000, worst loss 0.011103 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 00:13:07,493 [INFO] Regularization: 1488.521118 * 0.0000000100 = 0.0000148852 loss
2019-04-08 00:13:07,493 [INFO] unfolding 0, single step 77001
2019-04-08 00:13:07,494 [INFO] Sum of grad norms of most recent batch: 0.081453
2019-04-08 00:13:07,495 [INFO] ---------------------------------
2019-04-08 00:13:29,492 [INFO] ---------------------------------
2019-04-08 00:13:29,493 [INFO] Summary:
2019-04-08 00:13:29,494 [INFO] Batch 78000, worst loss 0.010554 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 00:13:29,494 [INFO] Regularization: 1488.150635 * 0.0000000100 = 0.0000148815 loss
2019-04-08 00:13:29,494 [INFO] unfolding 0, single step 78001
2019-04-08 00:13:29,495 [INFO] Sum of grad norms of most recent batch: 0.058602
2019-04-08 00:13:29,496 [INFO] ---------------------------------
2019-04-08 00:13:50,012 [INFO] ---------------------------------
2019-04-08 00:13:50,013 [INFO] Summary:
2019-04-08 00:13:50,013 [INFO] Batch 79000, worst loss 0.021082 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 00:13:50,014 [INFO] Regularization: 1487.097778 * 0.0000000100 = 0.0000148710 loss
2019-04-08 00:13:50,014 [INFO] unfolding 0, single step 79001
2019-04-08 00:13:50,015 [INFO] Sum of grad norms of most recent batch: 0.122366
2019-04-08 00:13:50,015 [INFO] ---------------------------------
2019-04-08 00:14:11,547 [INFO] ---------------------------------
2019-04-08 00:14:11,549 [INFO] Summary:
2019-04-08 00:14:11,549 [INFO] Batch 80000, worst loss 0.007546 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 00:14:11,550 [INFO] Regularization: 1486.218384 * 0.0000000100 = 0.0000148622 loss
2019-04-08 00:14:11,550 [INFO] unfolding 0, single step 80001
2019-04-08 00:14:11,550 [INFO] Sum of grad norms of most recent batch: 0.063088
2019-04-08 00:14:11,551 [INFO] ---------------------------------
2019-04-08 00:14:48,935 [INFO] ---------------------------------
2019-04-08 00:14:48,936 [INFO] Evaluation:
2019-04-08 00:14:48,937 [INFO] Batch 80000, worst loss 0.021204 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:14:48,937 [INFO] ---------------------------------
2019-04-08 00:15:10,071 [INFO] ---------------------------------
2019-04-08 00:15:10,072 [INFO] Summary:
2019-04-08 00:15:10,073 [INFO] Batch 81000, worst loss 0.023410 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 00:15:10,073 [INFO] Regularization: 1485.384888 * 0.0000000100 = 0.0000148538 loss
2019-04-08 00:15:10,074 [INFO] unfolding 0, single step 81001
2019-04-08 00:15:10,074 [INFO] Sum of grad norms of most recent batch: 0.183929
2019-04-08 00:15:10,075 [INFO] ---------------------------------
2019-04-08 00:15:30,880 [INFO] ---------------------------------
2019-04-08 00:15:30,881 [INFO] Summary:
2019-04-08 00:15:30,882 [INFO] Batch 82000, worst loss 0.009698 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 00:15:30,883 [INFO] Regularization: 1484.537842 * 0.0000000100 = 0.0000148454 loss
2019-04-08 00:15:30,883 [INFO] unfolding 0, single step 82001
2019-04-08 00:15:30,884 [INFO] Sum of grad norms of most recent batch: 0.051611
2019-04-08 00:15:30,884 [INFO] ---------------------------------
2019-04-08 00:15:52,295 [INFO] ---------------------------------
2019-04-08 00:15:52,296 [INFO] Summary:
2019-04-08 00:15:52,297 [INFO] Batch 83000, worst loss 0.008710 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 00:15:52,297 [INFO] Regularization: 1484.090576 * 0.0000000100 = 0.0000148409 loss
2019-04-08 00:15:52,298 [INFO] unfolding 0, single step 83001
2019-04-08 00:15:52,299 [INFO] Sum of grad norms of most recent batch: 0.040093
2019-04-08 00:15:52,299 [INFO] ---------------------------------
2019-04-08 00:16:14,008 [INFO] ---------------------------------
2019-04-08 00:16:14,009 [INFO] Summary:
2019-04-08 00:16:14,009 [INFO] Batch 84000, worst loss 0.009401 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 00:16:14,010 [INFO] Regularization: 1483.468994 * 0.0000000100 = 0.0000148347 loss
2019-04-08 00:16:14,010 [INFO] unfolding 0, single step 84001
2019-04-08 00:16:14,011 [INFO] Sum of grad norms of most recent batch: 0.048250
2019-04-08 00:16:14,011 [INFO] ---------------------------------
2019-04-08 00:16:35,343 [INFO] ---------------------------------
2019-04-08 00:16:35,344 [INFO] Summary:
2019-04-08 00:16:35,345 [INFO] Batch 85000, worst loss 0.012884 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 00:16:35,345 [INFO] Regularization: 1483.042847 * 0.0000000100 = 0.0000148304 loss
2019-04-08 00:16:35,346 [INFO] unfolding 0, single step 85001
2019-04-08 00:16:35,346 [INFO] Sum of grad norms of most recent batch: 0.076358
2019-04-08 00:16:35,347 [INFO] ---------------------------------
2019-04-08 00:16:56,611 [INFO] ---------------------------------
2019-04-08 00:16:56,612 [INFO] Summary:
2019-04-08 00:16:56,613 [INFO] Batch 86000, worst loss 0.004452 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 00:16:56,613 [INFO] Regularization: 1482.482422 * 0.0000000100 = 0.0000148248 loss
2019-04-08 00:16:56,613 [INFO] unfolding 0, single step 86001
2019-04-08 00:16:56,614 [INFO] Sum of grad norms of most recent batch: 0.073919
2019-04-08 00:16:56,614 [INFO] ---------------------------------
2019-04-08 00:17:34,028 [INFO] ---------------------------------
2019-04-08 00:17:34,029 [INFO] Evaluation:
2019-04-08 00:17:34,030 [INFO] Batch 86000, worst loss 0.031126 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:17:34,030 [INFO] ---------------------------------
2019-04-08 00:17:54,995 [INFO] ---------------------------------
2019-04-08 00:17:54,996 [INFO] Summary:
2019-04-08 00:17:54,997 [INFO] Batch 87000, worst loss 0.004117 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 00:17:54,997 [INFO] Regularization: 1482.089355 * 0.0000000100 = 0.0000148209 loss
2019-04-08 00:17:54,998 [INFO] unfolding 0, single step 87001
2019-04-08 00:17:54,998 [INFO] Sum of grad norms of most recent batch: 0.308126
2019-04-08 00:17:54,999 [INFO] ---------------------------------
2019-04-08 00:18:32,407 [INFO] ---------------------------------
2019-04-08 00:18:32,408 [INFO] Evaluation:
2019-04-08 00:18:32,409 [INFO] Batch 87000, worst loss 0.015357 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:18:32,409 [INFO] ---------------------------------
2019-04-08 00:18:53,722 [INFO] ---------------------------------
2019-04-08 00:18:53,723 [INFO] Summary:
2019-04-08 00:18:53,723 [INFO] Batch 88000, worst loss 0.008273 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 00:18:53,724 [INFO] Regularization: 1481.588379 * 0.0000000100 = 0.0000148159 loss
2019-04-08 00:18:53,724 [INFO] unfolding 0, single step 88001
2019-04-08 00:18:53,725 [INFO] Sum of grad norms of most recent batch: 0.062379
2019-04-08 00:18:53,725 [INFO] ---------------------------------
2019-04-08 00:19:14,870 [INFO] ---------------------------------
2019-04-08 00:19:14,871 [INFO] Summary:
2019-04-08 00:19:14,871 [INFO] Batch 89000, worst loss 0.009358 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 00:19:14,872 [INFO] Regularization: 1481.137207 * 0.0000000100 = 0.0000148114 loss
2019-04-08 00:19:14,873 [INFO] unfolding 0, single step 89001
2019-04-08 00:19:14,873 [INFO] Sum of grad norms of most recent batch: 1.304141
2019-04-08 00:19:14,874 [INFO] ---------------------------------
2019-04-08 00:19:36,477 [INFO] ---------------------------------
2019-04-08 00:19:36,478 [INFO] Summary:
2019-04-08 00:19:36,478 [INFO] Batch 90000, worst loss 0.011975 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 00:19:36,479 [INFO] Regularization: 1480.684570 * 0.0000000100 = 0.0000148068 loss
2019-04-08 00:19:36,479 [INFO] unfolding 0, single step 90001
2019-04-08 00:19:36,479 [INFO] Sum of grad norms of most recent batch: 0.047603
2019-04-08 00:19:36,480 [INFO] ---------------------------------
2019-04-08 00:20:13,757 [INFO] ---------------------------------
2019-04-08 00:20:13,758 [INFO] Evaluation:
2019-04-08 00:20:13,758 [INFO] Batch 90000, worst loss 0.016266 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:20:13,759 [INFO] ---------------------------------
2019-04-08 00:20:34,558 [INFO] ---------------------------------
2019-04-08 00:20:34,559 [INFO] Summary:
2019-04-08 00:20:34,559 [INFO] Batch 91000, worst loss 0.004334 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 00:20:34,560 [INFO] Regularization: 1480.006226 * 0.0000000100 = 0.0000148001 loss
2019-04-08 00:20:34,560 [INFO] unfolding 0, single step 91001
2019-04-08 00:20:34,561 [INFO] Sum of grad norms of most recent batch: 0.082277
2019-04-08 00:20:34,561 [INFO] ---------------------------------
2019-04-08 00:21:12,141 [INFO] ---------------------------------
2019-04-08 00:21:12,142 [INFO] Evaluation:
2019-04-08 00:21:12,142 [INFO] Batch 91000, worst loss 0.019427 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:21:12,143 [INFO] ---------------------------------
2019-04-08 00:21:33,570 [INFO] ---------------------------------
2019-04-08 00:21:33,571 [INFO] Summary:
2019-04-08 00:21:33,572 [INFO] Batch 92000, worst loss 0.018588 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 00:21:33,572 [INFO] Regularization: 1479.773926 * 0.0000000100 = 0.0000147977 loss
2019-04-08 00:21:33,573 [INFO] unfolding 0, single step 92001
2019-04-08 00:21:33,573 [INFO] Sum of grad norms of most recent batch: 0.088846
2019-04-08 00:21:33,574 [INFO] ---------------------------------
2019-04-08 00:21:54,579 [INFO] ---------------------------------
2019-04-08 00:21:54,580 [INFO] Summary:
2019-04-08 00:21:54,580 [INFO] Batch 93000, worst loss 0.003133 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 00:21:54,581 [INFO] Regularization: 1479.470215 * 0.0000000100 = 0.0000147947 loss
2019-04-08 00:21:54,581 [INFO] unfolding 0, single step 93001
2019-04-08 00:21:54,582 [INFO] Sum of grad norms of most recent batch: 0.050902
2019-04-08 00:21:54,582 [INFO] ---------------------------------
2019-04-08 00:22:31,987 [INFO] ---------------------------------
2019-04-08 00:22:31,989 [INFO] Evaluation:
2019-04-08 00:22:31,989 [INFO] Batch 93000, worst loss 0.009512 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:22:31,990 [INFO] ---------------------------------
2019-04-08 00:22:54,057 [INFO] ---------------------------------
2019-04-08 00:22:54,058 [INFO] Summary:
2019-04-08 00:22:54,059 [INFO] Batch 94000, worst loss 0.002083 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 00:22:54,060 [INFO] Regularization: 1479.262695 * 0.0000000100 = 0.0000147926 loss
2019-04-08 00:22:54,060 [INFO] unfolding 0, single step 94001
2019-04-08 00:22:54,060 [INFO] Sum of grad norms of most recent batch: 0.023291
2019-04-08 00:22:54,061 [INFO] ---------------------------------
2019-04-08 00:23:31,399 [INFO] ---------------------------------
2019-04-08 00:23:31,400 [INFO] Evaluation:
2019-04-08 00:23:31,401 [INFO] Batch 94000, worst loss 0.024538 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:23:31,402 [INFO] ---------------------------------
2019-04-08 00:23:52,380 [INFO] ---------------------------------
2019-04-08 00:23:52,381 [INFO] Summary:
2019-04-08 00:23:52,382 [INFO] Batch 95000, worst loss 0.006554 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 00:23:52,382 [INFO] Regularization: 1478.885620 * 0.0000000100 = 0.0000147889 loss
2019-04-08 00:23:52,383 [INFO] unfolding 0, single step 95001
2019-04-08 00:23:52,384 [INFO] Sum of grad norms of most recent batch: 0.072891
2019-04-08 00:23:52,384 [INFO] ---------------------------------
2019-04-08 00:24:14,321 [INFO] ---------------------------------
2019-04-08 00:24:14,322 [INFO] Summary:
2019-04-08 00:24:14,323 [INFO] Batch 96000, worst loss 0.011464 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 00:24:14,323 [INFO] Regularization: 1478.755005 * 0.0000000100 = 0.0000147875 loss
2019-04-08 00:24:14,323 [INFO] unfolding 0, single step 96001
2019-04-08 00:24:14,324 [INFO] Sum of grad norms of most recent batch: 0.081245
2019-04-08 00:24:14,324 [INFO] ---------------------------------
2019-04-08 00:24:35,560 [INFO] ---------------------------------
2019-04-08 00:24:35,561 [INFO] Summary:
2019-04-08 00:24:35,562 [INFO] Batch 97000, worst loss 0.012376 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 00:24:35,562 [INFO] Regularization: 1478.486084 * 0.0000000100 = 0.0000147849 loss
2019-04-08 00:24:35,563 [INFO] unfolding 0, single step 97001
2019-04-08 00:24:35,563 [INFO] Sum of grad norms of most recent batch: 0.026595
2019-04-08 00:24:35,564 [INFO] ---------------------------------
2019-04-08 00:24:56,780 [INFO] ---------------------------------
2019-04-08 00:24:56,781 [INFO] Summary:
2019-04-08 00:24:56,782 [INFO] Batch 98000, worst loss 0.007334 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 00:24:56,782 [INFO] Regularization: 1478.384644 * 0.0000000100 = 0.0000147838 loss
2019-04-08 00:24:56,782 [INFO] unfolding 0, single step 98001
2019-04-08 00:24:56,783 [INFO] Sum of grad norms of most recent batch: 1.847769
2019-04-08 00:24:56,783 [INFO] ---------------------------------
2019-04-08 00:25:18,061 [INFO] ---------------------------------
2019-04-08 00:25:18,062 [INFO] Summary:
2019-04-08 00:25:18,063 [INFO] Batch 99000, worst loss 0.007248 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 00:25:18,063 [INFO] Regularization: 1478.210327 * 0.0000000100 = 0.0000147821 loss
2019-04-08 00:25:18,064 [INFO] unfolding 0, single step 99001
2019-04-08 00:25:18,064 [INFO] Sum of grad norms of most recent batch: 0.053941
2019-04-08 00:25:18,065 [INFO] ---------------------------------
2019-04-08 00:25:39,619 [INFO] ---------------------------------
2019-04-08 00:25:39,620 [INFO] Summary:
2019-04-08 00:25:39,621 [INFO] Batch 100000, worst loss 0.009202 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 00:25:39,621 [INFO] Regularization: 1477.904785 * 0.0000000100 = 0.0000147790 loss
2019-04-08 00:25:39,622 [INFO] unfolding 0, single step 100001
2019-04-08 00:25:39,622 [INFO] Sum of grad norms of most recent batch: 0.014076
2019-04-08 00:25:39,623 [INFO] ---------------------------------
2019-04-08 00:26:17,039 [INFO] ---------------------------------
2019-04-08 00:26:17,040 [INFO] Evaluation:
2019-04-08 00:26:17,041 [INFO] Batch 100000, worst loss 0.009857 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:26:17,042 [INFO] ---------------------------------
2019-04-08 00:26:38,486 [INFO] ---------------------------------
2019-04-08 00:26:38,487 [INFO] Summary:
2019-04-08 00:26:38,487 [INFO] Batch 101000, worst loss 0.008271 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 00:26:38,488 [INFO] Regularization: 1477.672974 * 0.0000000100 = 0.0000147767 loss
2019-04-08 00:26:38,488 [INFO] unfolding 0, single step 101001
2019-04-08 00:26:38,489 [INFO] Sum of grad norms of most recent batch: 0.042327
2019-04-08 00:26:38,489 [INFO] ---------------------------------
2019-04-08 00:26:59,624 [INFO] ---------------------------------
2019-04-08 00:26:59,625 [INFO] Summary:
2019-04-08 00:26:59,626 [INFO] Batch 102000, worst loss 0.008489 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 00:26:59,626 [INFO] Regularization: 1477.594604 * 0.0000000100 = 0.0000147759 loss
2019-04-08 00:26:59,627 [INFO] unfolding 0, single step 102001
2019-04-08 00:26:59,627 [INFO] Sum of grad norms of most recent batch: 0.158553
2019-04-08 00:26:59,628 [INFO] ---------------------------------
2019-04-08 00:27:21,850 [INFO] ---------------------------------
2019-04-08 00:27:21,852 [INFO] Summary:
2019-04-08 00:27:21,852 [INFO] Batch 103000, worst loss 0.006125 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 00:27:21,853 [INFO] Regularization: 1477.455078 * 0.0000000100 = 0.0000147746 loss
2019-04-08 00:27:21,853 [INFO] unfolding 0, single step 103001
2019-04-08 00:27:21,853 [INFO] Sum of grad norms of most recent batch: 0.038292
2019-04-08 00:27:21,854 [INFO] ---------------------------------
2019-04-08 00:27:43,690 [INFO] ---------------------------------
2019-04-08 00:27:43,691 [INFO] Summary:
2019-04-08 00:27:43,691 [INFO] Batch 104000, worst loss 0.018252 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 00:27:43,692 [INFO] Regularization: 1477.372681 * 0.0000000100 = 0.0000147737 loss
2019-04-08 00:27:43,692 [INFO] unfolding 0, single step 104001
2019-04-08 00:27:43,693 [INFO] Sum of grad norms of most recent batch: 0.065898
2019-04-08 00:27:43,694 [INFO] ---------------------------------
2019-04-08 00:28:05,791 [INFO] ---------------------------------
2019-04-08 00:28:05,792 [INFO] Summary:
2019-04-08 00:28:05,792 [INFO] Batch 105000, worst loss 0.003179 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 00:28:05,793 [INFO] Regularization: 1477.186890 * 0.0000000100 = 0.0000147719 loss
2019-04-08 00:28:05,793 [INFO] unfolding 0, single step 105001
2019-04-08 00:28:05,794 [INFO] Sum of grad norms of most recent batch: 0.105969
2019-04-08 00:28:05,795 [INFO] ---------------------------------
2019-04-08 00:28:43,299 [INFO] ---------------------------------
2019-04-08 00:28:43,300 [INFO] Evaluation:
2019-04-08 00:28:43,300 [INFO] Batch 105000, worst loss 0.013764 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:28:43,300 [INFO] ---------------------------------
2019-04-08 00:29:04,943 [INFO] ---------------------------------
2019-04-08 00:29:04,944 [INFO] Summary:
2019-04-08 00:29:04,945 [INFO] Batch 106000, worst loss 0.016256 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 00:29:04,945 [INFO] Regularization: 1477.032959 * 0.0000000100 = 0.0000147703 loss
2019-04-08 00:29:04,946 [INFO] unfolding 0, single step 106001
2019-04-08 00:29:04,946 [INFO] Sum of grad norms of most recent batch: 0.028623
2019-04-08 00:29:04,947 [INFO] ---------------------------------
2019-04-08 00:29:25,289 [INFO] ---------------------------------
2019-04-08 00:29:25,290 [INFO] Summary:
2019-04-08 00:29:25,291 [INFO] Batch 107000, worst loss 0.004451 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 00:29:25,291 [INFO] Regularization: 1476.929565 * 0.0000000100 = 0.0000147693 loss
2019-04-08 00:29:25,292 [INFO] unfolding 0, single step 107001
2019-04-08 00:29:25,292 [INFO] Sum of grad norms of most recent batch: 0.055379
2019-04-08 00:29:25,293 [INFO] ---------------------------------
2019-04-08 00:30:02,795 [INFO] ---------------------------------
2019-04-08 00:30:02,795 [INFO] Evaluation:
2019-04-08 00:30:02,796 [INFO] Batch 107000, worst loss 0.013768 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:30:02,797 [INFO] ---------------------------------
2019-04-08 00:30:24,353 [INFO] ---------------------------------
2019-04-08 00:30:24,354 [INFO] Summary:
2019-04-08 00:30:24,355 [INFO] Batch 108000, worst loss 0.003290 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 00:30:24,356 [INFO] Regularization: 1476.746948 * 0.0000000100 = 0.0000147675 loss
2019-04-08 00:30:24,356 [INFO] unfolding 0, single step 108001
2019-04-08 00:30:24,357 [INFO] Sum of grad norms of most recent batch: 0.050401
2019-04-08 00:30:24,358 [INFO] ---------------------------------
2019-04-08 00:31:01,666 [INFO] ---------------------------------
2019-04-08 00:31:01,667 [INFO] Evaluation:
2019-04-08 00:31:01,668 [INFO] Batch 108000, worst loss 0.016272 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:31:01,669 [INFO] ---------------------------------
2019-04-08 00:31:22,708 [INFO] ---------------------------------
2019-04-08 00:31:22,709 [INFO] Summary:
2019-04-08 00:31:22,710 [INFO] Batch 109000, worst loss 0.004087 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 00:31:22,710 [INFO] Regularization: 1476.648071 * 0.0000000100 = 0.0000147665 loss
2019-04-08 00:31:22,711 [INFO] unfolding 0, single step 109001
2019-04-08 00:31:22,712 [INFO] Sum of grad norms of most recent batch: 0.089727
2019-04-08 00:31:22,712 [INFO] ---------------------------------
2019-04-08 00:32:00,100 [INFO] ---------------------------------
2019-04-08 00:32:00,101 [INFO] Evaluation:
2019-04-08 00:32:00,102 [INFO] Batch 109000, worst loss 0.051270 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:32:00,102 [INFO] ---------------------------------
2019-04-08 00:32:21,486 [INFO] ---------------------------------
2019-04-08 00:32:21,487 [INFO] Summary:
2019-04-08 00:32:21,488 [INFO] Batch 110000, worst loss 0.003142 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 00:32:21,488 [INFO] Regularization: 1476.538208 * 0.0000000100 = 0.0000147654 loss
2019-04-08 00:32:21,488 [INFO] unfolding 0, single step 110001
2019-04-08 00:32:21,489 [INFO] Sum of grad norms of most recent batch: 0.045541
2019-04-08 00:32:21,489 [INFO] ---------------------------------
2019-04-08 00:32:58,954 [INFO] ---------------------------------
2019-04-08 00:32:58,955 [INFO] Evaluation:
2019-04-08 00:32:58,955 [INFO] Batch 110000, worst loss 0.004822 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:32:58,956 [INFO] New best loss 0.004822, saved to file transition/1554653688/1554676378_5_transition_110000.pth
2019-04-08 00:32:58,965 [INFO] Target
2019-04-08 00:32:58,966 [INFO] [[0.       1.       0.968747 0.0743   0.010417 0.03125  0.15     0.95    ]
 [1.       0.       0.979167 0.8875   0.020833 0.020833 0.35     0.8     ]
 [1.       0.       0.010417 0.9772   0.03125  0.020833 0.05     1.      ]
 [0.       1.       0.458337 0.2169   0.03125  0.020833 0.05     0.8     ]
 [0.       1.       0.083337 0.743    0.03125  0.03125  0.4      0.75    ]
 [1.       0.       0.729167 0.2089   0.03125  0.010417 0.15     0.95    ]
 [1.       0.       0.677087 0.3413   0.020833 0.020833 0.25     0.9     ]
 [0.       1.       0.510417 0.7226   0.010417 0.010417 0.05     0.95    ]
 [1.       0.       0.322917 0.3849   0.       0.       0.2      0.75    ]
 [1.       0.       0.770837 0.1563   0.03125  0.03125  0.       0.75    ]
 [0.       1.       0.999997 0.686    0.020833 0.       0.1      0.7     ]
 [1.       0.       0.427087 0.5317   0.       0.010417 0.2      0.6     ]
 [1.       0.       0.552087 0.8424   0.020833 0.020833 0.2      0.8     ]
 [1.       0.       0.499997 0.7168   0.       0.010417 0.2      0.95    ]
 [1.       0.       0.010417 0.8453   0.020833 0.010417 0.1      0.6     ]
 [1.       0.       0.052087 0.8132   0.       0.010417 0.3      0.6     ]
 [1.       0.       0.302087 0.865    0.010417 0.       0.       0.9     ]
 [1.       0.       0.010417 0.7315   0.       0.020833 0.1      0.6     ]
 [1.       0.       0.010417 0.9057   0.03125  0.       0.       1.      ]
 [0.       1.       0.010417 0.2099   0.020833 0.010417 0.15     0.8     ]
 [1.       0.       0.447917 0.9635   0.010417 0.020833 0.35     0.95    ]
 [1.       0.       0.822917 0.7425   0.03125  0.020833 0.       0.65    ]
 [0.       1.       0.031247 0.2171   0.       0.03125  0.4      0.7     ]
 [0.       1.       0.458337 0.1503   0.010417 0.010417 0.4      0.75    ]
 [0.       1.       0.406247 0.1113   0.03125  0.010417 0.15     0.95    ]
 [0.       1.       0.906247 0.5487   0.020833 0.03125  0.1      0.85    ]
 [1.       0.       0.458337 0.3653   0.03125  0.010417 0.3      0.85    ]
 [1.       0.       0.270837 0.4786   0.03125  0.03125  0.15     0.95    ]
 [0.       1.       0.104167 0.4511   0.010417 0.020833 0.3      1.      ]
 [1.       0.       0.197917 0.       0.010417 0.020833 0.       0.8     ]
 [0.       1.       0.437497 0.2053   0.020833 0.020833 0.2      1.      ]
 [1.       0.       0.010417 0.7704   0.03125  0.03125  0.3      0.85    ]]
2019-04-08 00:32:58,970 [INFO] Estimator output
2019-04-08 00:32:58,970 [INFO] [[ 0.000001  1.000057  0.968723  0.074311  0.010416  0.031254  0.15      0.950007]
 [ 0.999916  0.000092  0.979161  0.887453  0.020834  0.020834  0.35      0.8     ]
 [ 0.999982 -0.000029  0.010488  0.977127  0.031249  0.020833  0.050003  0.999997]
 [ 0.000015  0.999968  0.458151  0.216883  0.03125   0.020835  0.050005  0.799995]
 [-0.000031  0.999941  0.083092  0.743059  0.031252  0.031248  0.400005  0.749991]
 [ 0.999961  0.000007  0.729236  0.208921  0.031249  0.010418  0.150003  0.949996]
 [ 0.99996   0.00002   0.677152  0.341252  0.020833  0.020835  0.250001  0.899999]
 [-0.000038  0.999984  0.510254  0.722542  0.010417  0.010417  0.050001  0.950006]
 [ 1.000032 -0.000059  0.323038  0.384984 -0.        0.000001  0.199995  0.75001 ]
 [ 0.999943  0.000034  0.770868  0.15629   0.031249  0.031252  0.000004  0.749994]
 [-0.000005  0.999994  0.999851  0.685998  0.020833  0.000001  0.1       0.700006]
 [ 0.999963  0.000003  0.427067  0.531701 -0.        0.010417  0.199995  0.60001 ]
 [ 0.999964  0.000001  0.552056  0.842369  0.020834  0.020831  0.200002  0.799997]
 [ 0.999952  0.000017  0.49983   0.716854  0.        0.010415  0.199997  0.950009]
 [ 0.999969  0.000021  0.010527  0.845242  0.020831  0.010419  0.099994  0.600008]
 [ 0.999954  0.000005  0.052073  0.813203  0.000001  0.010415  0.299996  0.600007]
 [ 0.999932  0.000013  0.301943  0.86502   0.010416  0.        0.        0.900004]
 [ 1.000002 -0.        0.010441  0.731519 -0.000001  0.020835  0.099992  0.600013]
 [ 1.000015 -0.000057  0.010617  0.905628  0.031249  0.000001  0.000001  0.999999]
 [-0.000027  0.999899  0.010405  0.209938  0.020834  0.010414  0.150005  0.799993]
 [ 0.999917  0.000059  0.447641  0.963478  0.010417  0.020831  0.35      0.950002]
 [ 0.999919  0.000045  0.822962  0.742478  0.031249  0.020833  0.000003  0.649995]
 [ 0.00003   0.999966  0.03103   0.217034  0.000001  0.031253  0.399997  0.700006]
 [-0.000011  1.000001  0.458325  0.150294  0.010418  0.01042   0.399997  0.750006]
 [ 0.000015  0.999973  0.406221  0.111226  0.03125   0.010419  0.150005  0.949996]
 [-0.000045  1.000042  0.90622   0.548692  0.020832  0.031253  0.100003  0.850001]
 [ 0.99997  -0.000009  0.458311  0.365252  0.03125   0.010417  0.300003  0.849994]
 [ 0.999964  0.000019  0.270832  0.478616  0.031249  0.031251  0.150006  0.94999 ]
 [-0.00003   0.999985  0.104126  0.451148  0.010418  0.020835  0.300001  1.000003]
 [ 0.999203  0.00053   0.199036 -0.013882  0.010414  0.020842 -0.000001  0.800001]
 [ 0.000027  0.999972  0.437476  0.205289  0.020834  0.020836  0.200003  0.999999]
 [ 0.999915  0.000055  0.010502  0.770399  0.031248  0.031251  0.3       0.849998]]
2019-04-08 00:32:58,974 [INFO] ---------------------------------
2019-04-08 00:33:19,620 [INFO] ---------------------------------
2019-04-08 00:33:19,621 [INFO] Summary:
2019-04-08 00:33:19,622 [INFO] Batch 111000, worst loss 0.004567 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 00:33:19,622 [INFO] Regularization: 1476.484375 * 0.0000000100 = 0.0000147648 loss
2019-04-08 00:33:19,623 [INFO] unfolding 0, single step 111001
2019-04-08 00:33:19,624 [INFO] Sum of grad norms of most recent batch: 0.029518
2019-04-08 00:33:19,624 [INFO] ---------------------------------
2019-04-08 00:33:56,830 [INFO] ---------------------------------
2019-04-08 00:33:56,831 [INFO] Evaluation:
2019-04-08 00:33:56,832 [INFO] Batch 111000, worst loss 0.005002 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:33:56,832 [INFO] ---------------------------------
2019-04-08 00:34:18,045 [INFO] ---------------------------------
2019-04-08 00:34:18,046 [INFO] Summary:
2019-04-08 00:34:18,047 [INFO] Batch 112000, worst loss 0.001624 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 00:34:18,047 [INFO] Regularization: 1476.350586 * 0.0000000100 = 0.0000147635 loss
2019-04-08 00:34:18,048 [INFO] unfolding 0, single step 112001
2019-04-08 00:34:18,048 [INFO] Sum of grad norms of most recent batch: 0.030286
2019-04-08 00:34:18,049 [INFO] ---------------------------------
2019-04-08 00:34:55,395 [INFO] ---------------------------------
2019-04-08 00:34:55,397 [INFO] Evaluation:
2019-04-08 00:34:55,398 [INFO] Batch 112000, worst loss 0.016282 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:34:55,398 [INFO] ---------------------------------
2019-04-08 00:35:16,814 [INFO] ---------------------------------
2019-04-08 00:35:16,815 [INFO] Summary:
2019-04-08 00:35:16,815 [INFO] Batch 113000, worst loss 0.002285 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 00:35:16,816 [INFO] Regularization: 1476.344849 * 0.0000000100 = 0.0000147634 loss
2019-04-08 00:35:16,816 [INFO] unfolding 0, single step 113001
2019-04-08 00:35:16,817 [INFO] Sum of grad norms of most recent batch: 0.028412
2019-04-08 00:35:16,817 [INFO] ---------------------------------
2019-04-08 00:35:54,162 [INFO] ---------------------------------
2019-04-08 00:35:54,164 [INFO] Evaluation:
2019-04-08 00:35:54,164 [INFO] Batch 113000, worst loss 0.017429 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:35:54,165 [INFO] ---------------------------------
2019-04-08 00:36:15,410 [INFO] ---------------------------------
2019-04-08 00:36:15,410 [INFO] Summary:
2019-04-08 00:36:15,411 [INFO] Batch 114000, worst loss 0.002762 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 00:36:15,412 [INFO] Regularization: 1476.198486 * 0.0000000100 = 0.0000147620 loss
2019-04-08 00:36:15,412 [INFO] unfolding 0, single step 114001
2019-04-08 00:36:15,413 [INFO] Sum of grad norms of most recent batch: 0.029525
2019-04-08 00:36:15,413 [INFO] ---------------------------------
2019-04-08 00:36:52,817 [INFO] ---------------------------------
2019-04-08 00:36:52,818 [INFO] Evaluation:
2019-04-08 00:36:52,818 [INFO] Batch 114000, worst loss 0.005798 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:36:52,819 [INFO] ---------------------------------
2019-04-08 00:37:15,024 [INFO] ---------------------------------
2019-04-08 00:37:15,026 [INFO] Summary:
2019-04-08 00:37:15,026 [INFO] Batch 115000, worst loss 0.014129 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 00:37:15,028 [INFO] Regularization: 1476.170044 * 0.0000000100 = 0.0000147617 loss
2019-04-08 00:37:15,028 [INFO] unfolding 0, single step 115001
2019-04-08 00:37:15,030 [INFO] Sum of grad norms of most recent batch: 0.049355
2019-04-08 00:37:15,031 [INFO] ---------------------------------
2019-04-08 00:37:36,286 [INFO] ---------------------------------
2019-04-08 00:37:36,287 [INFO] Summary:
2019-04-08 00:37:36,287 [INFO] Batch 116000, worst loss 0.004356 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 00:37:36,288 [INFO] Regularization: 1476.077515 * 0.0000000100 = 0.0000147608 loss
2019-04-08 00:37:36,288 [INFO] unfolding 0, single step 116001
2019-04-08 00:37:36,289 [INFO] Sum of grad norms of most recent batch: 0.051687
2019-04-08 00:37:36,289 [INFO] ---------------------------------
2019-04-08 00:38:13,819 [INFO] ---------------------------------
2019-04-08 00:38:13,820 [INFO] Evaluation:
2019-04-08 00:38:13,821 [INFO] Batch 116000, worst loss 0.016281 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:38:13,821 [INFO] ---------------------------------
2019-04-08 00:38:34,631 [INFO] ---------------------------------
2019-04-08 00:38:34,632 [INFO] Summary:
2019-04-08 00:38:34,633 [INFO] Batch 117000, worst loss 0.007581 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 00:38:34,634 [INFO] Regularization: 1475.993042 * 0.0000000100 = 0.0000147599 loss
2019-04-08 00:38:34,635 [INFO] unfolding 0, single step 117001
2019-04-08 00:38:34,637 [INFO] Sum of grad norms of most recent batch: 0.014353
2019-04-08 00:38:34,638 [INFO] ---------------------------------
2019-04-08 00:38:55,707 [INFO] ---------------------------------
2019-04-08 00:38:55,707 [INFO] Summary:
2019-04-08 00:38:55,708 [INFO] Batch 118000, worst loss 0.000902 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 00:38:55,708 [INFO] Regularization: 1475.908203 * 0.0000000100 = 0.0000147591 loss
2019-04-08 00:38:55,709 [INFO] unfolding 0, single step 118001
2019-04-08 00:38:55,709 [INFO] Sum of grad norms of most recent batch: 0.032600
2019-04-08 00:38:55,710 [INFO] ---------------------------------
2019-04-08 00:39:33,179 [INFO] ---------------------------------
2019-04-08 00:39:33,180 [INFO] Evaluation:
2019-04-08 00:39:33,181 [INFO] Batch 118000, worst loss 0.032771 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:39:33,182 [INFO] ---------------------------------
2019-04-08 00:39:54,152 [INFO] ---------------------------------
2019-04-08 00:39:54,153 [INFO] Summary:
2019-04-08 00:39:54,154 [INFO] Batch 119000, worst loss 0.019577 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 00:39:54,154 [INFO] Regularization: 1475.844727 * 0.0000000100 = 0.0000147584 loss
2019-04-08 00:39:54,154 [INFO] unfolding 0, single step 119001
2019-04-08 00:39:54,155 [INFO] Sum of grad norms of most recent batch: 0.027002
2019-04-08 00:39:54,156 [INFO] ---------------------------------
2019-04-08 00:40:15,651 [INFO] ---------------------------------
2019-04-08 00:40:15,652 [INFO] Summary:
2019-04-08 00:40:15,653 [INFO] Batch 120000, worst loss 0.001440 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 00:40:15,654 [INFO] Regularization: 1475.817261 * 0.0000000100 = 0.0000147582 loss
2019-04-08 00:40:15,654 [INFO] unfolding 0, single step 120001
2019-04-08 00:40:15,655 [INFO] Sum of grad norms of most recent batch: 0.030434
2019-04-08 00:40:15,656 [INFO] ---------------------------------
2019-04-08 00:40:53,054 [INFO] ---------------------------------
2019-04-08 00:40:53,055 [INFO] Evaluation:
2019-04-08 00:40:53,056 [INFO] Batch 120000, worst loss 0.039867 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:40:53,056 [INFO] ---------------------------------
2019-04-08 00:41:14,610 [INFO] ---------------------------------
2019-04-08 00:41:14,611 [INFO] Summary:
2019-04-08 00:41:14,612 [INFO] Batch 121000, worst loss 0.002516 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 00:41:14,612 [INFO] Regularization: 1475.759644 * 0.0000000100 = 0.0000147576 loss
2019-04-08 00:41:14,612 [INFO] unfolding 0, single step 121001
2019-04-08 00:41:14,613 [INFO] Sum of grad norms of most recent batch: 0.044899
2019-04-08 00:41:14,613 [INFO] ---------------------------------
2019-04-08 00:41:51,953 [INFO] ---------------------------------
2019-04-08 00:41:51,954 [INFO] Evaluation:
2019-04-08 00:41:51,954 [INFO] Batch 121000, worst loss 0.014081 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:41:51,955 [INFO] ---------------------------------
2019-04-08 00:42:13,986 [INFO] ---------------------------------
2019-04-08 00:42:13,987 [INFO] Summary:
2019-04-08 00:42:13,988 [INFO] Batch 122000, worst loss 0.002346 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 00:42:13,988 [INFO] Regularization: 1475.750122 * 0.0000000100 = 0.0000147575 loss
2019-04-08 00:42:13,989 [INFO] unfolding 0, single step 122001
2019-04-08 00:42:13,989 [INFO] Sum of grad norms of most recent batch: 0.029808
2019-04-08 00:42:13,990 [INFO] ---------------------------------
2019-04-08 00:42:51,069 [INFO] ---------------------------------
2019-04-08 00:42:51,070 [INFO] Evaluation:
2019-04-08 00:42:51,071 [INFO] Batch 122000, worst loss 0.010399 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:42:51,071 [INFO] ---------------------------------
2019-04-08 00:43:12,327 [INFO] ---------------------------------
2019-04-08 00:43:12,328 [INFO] Summary:
2019-04-08 00:43:12,329 [INFO] Batch 123000, worst loss 0.005936 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 00:43:12,329 [INFO] Regularization: 1475.656250 * 0.0000000100 = 0.0000147566 loss
2019-04-08 00:43:12,330 [INFO] unfolding 0, single step 123001
2019-04-08 00:43:12,330 [INFO] Sum of grad norms of most recent batch: 0.053365
2019-04-08 00:43:12,331 [INFO] ---------------------------------
2019-04-08 00:43:33,393 [INFO] ---------------------------------
2019-04-08 00:43:33,393 [INFO] Summary:
2019-04-08 00:43:33,394 [INFO] Batch 124000, worst loss 0.002170 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 00:43:33,395 [INFO] Regularization: 1475.612427 * 0.0000000100 = 0.0000147561 loss
2019-04-08 00:43:33,395 [INFO] unfolding 0, single step 124001
2019-04-08 00:43:33,395 [INFO] Sum of grad norms of most recent batch: 0.029114
2019-04-08 00:43:33,396 [INFO] ---------------------------------
2019-04-08 00:44:10,976 [INFO] ---------------------------------
2019-04-08 00:44:10,977 [INFO] Evaluation:
2019-04-08 00:44:10,978 [INFO] Batch 124000, worst loss 0.016007 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:44:10,979 [INFO] ---------------------------------
2019-04-08 00:44:32,172 [INFO] ---------------------------------
2019-04-08 00:44:32,173 [INFO] Summary:
2019-04-08 00:44:32,173 [INFO] Batch 125000, worst loss 0.002212 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 00:44:32,174 [INFO] Regularization: 1475.580078 * 0.0000000100 = 0.0000147558 loss
2019-04-08 00:44:32,174 [INFO] unfolding 0, single step 125001
2019-04-08 00:44:32,175 [INFO] Sum of grad norms of most recent batch: 0.036630
2019-04-08 00:44:32,176 [INFO] ---------------------------------
2019-04-08 00:45:09,456 [INFO] ---------------------------------
2019-04-08 00:45:09,457 [INFO] Evaluation:
2019-04-08 00:45:09,458 [INFO] Batch 125000, worst loss 0.007198 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:45:09,459 [INFO] ---------------------------------
2019-04-08 00:45:30,618 [INFO] ---------------------------------
2019-04-08 00:45:30,619 [INFO] Summary:
2019-04-08 00:45:30,620 [INFO] Batch 126000, worst loss 0.003343 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 00:45:30,621 [INFO] Regularization: 1475.527466 * 0.0000000100 = 0.0000147553 loss
2019-04-08 00:45:30,621 [INFO] unfolding 0, single step 126001
2019-04-08 00:45:30,622 [INFO] Sum of grad norms of most recent batch: 0.062962
2019-04-08 00:45:30,623 [INFO] ---------------------------------
2019-04-08 00:46:08,189 [INFO] ---------------------------------
2019-04-08 00:46:08,190 [INFO] Evaluation:
2019-04-08 00:46:08,191 [INFO] Batch 126000, worst loss 0.013700 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:46:08,191 [INFO] ---------------------------------
2019-04-08 00:46:29,454 [INFO] ---------------------------------
2019-04-08 00:46:29,455 [INFO] Summary:
2019-04-08 00:46:29,456 [INFO] Batch 127000, worst loss 0.001442 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 00:46:29,456 [INFO] Regularization: 1475.460815 * 0.0000000100 = 0.0000147546 loss
2019-04-08 00:46:29,456 [INFO] unfolding 0, single step 127001
2019-04-08 00:46:29,457 [INFO] Sum of grad norms of most recent batch: 0.037860
2019-04-08 00:46:29,458 [INFO] ---------------------------------
2019-04-08 00:47:06,841 [INFO] ---------------------------------
2019-04-08 00:47:06,842 [INFO] Evaluation:
2019-04-08 00:47:06,842 [INFO] Batch 127000, worst loss 0.018484 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:47:06,843 [INFO] ---------------------------------
2019-04-08 00:47:28,533 [INFO] ---------------------------------
2019-04-08 00:47:28,534 [INFO] Summary:
2019-04-08 00:47:28,535 [INFO] Batch 128000, worst loss 0.002998 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 00:47:28,536 [INFO] Regularization: 1475.418579 * 0.0000000100 = 0.0000147542 loss
2019-04-08 00:47:28,536 [INFO] unfolding 0, single step 128001
2019-04-08 00:47:28,537 [INFO] Sum of grad norms of most recent batch: 0.043033
2019-04-08 00:47:28,538 [INFO] ---------------------------------
2019-04-08 00:48:05,958 [INFO] ---------------------------------
2019-04-08 00:48:05,959 [INFO] Evaluation:
2019-04-08 00:48:05,960 [INFO] Batch 128000, worst loss 0.036841 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:48:05,961 [INFO] ---------------------------------
2019-04-08 00:48:27,040 [INFO] ---------------------------------
2019-04-08 00:48:27,041 [INFO] Summary:
2019-04-08 00:48:27,042 [INFO] Batch 129000, worst loss 0.013769 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 00:48:27,042 [INFO] Regularization: 1475.401978 * 0.0000000100 = 0.0000147540 loss
2019-04-08 00:48:27,043 [INFO] unfolding 0, single step 129001
2019-04-08 00:48:27,043 [INFO] Sum of grad norms of most recent batch: 0.057020
2019-04-08 00:48:27,044 [INFO] ---------------------------------
2019-04-08 00:48:48,346 [INFO] ---------------------------------
2019-04-08 00:48:48,347 [INFO] Summary:
2019-04-08 00:48:48,348 [INFO] Batch 130000, worst loss 0.005554 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 00:48:48,348 [INFO] Regularization: 1475.358765 * 0.0000000100 = 0.0000147536 loss
2019-04-08 00:48:48,349 [INFO] unfolding 0, single step 130001
2019-04-08 00:48:48,350 [INFO] Sum of grad norms of most recent batch: 0.022325
2019-04-08 00:48:48,350 [INFO] ---------------------------------
2019-04-08 00:49:25,845 [INFO] ---------------------------------
2019-04-08 00:49:25,846 [INFO] Evaluation:
2019-04-08 00:49:25,847 [INFO] Batch 130000, worst loss 0.008359 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:49:25,848 [INFO] ---------------------------------
2019-04-08 00:49:47,079 [INFO] ---------------------------------
2019-04-08 00:49:47,080 [INFO] Summary:
2019-04-08 00:49:47,081 [INFO] Batch 131000, worst loss 0.001980 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:49:47,082 [INFO] Regularization: 1475.307251 * 0.0000000100 = 0.0000147531 loss
2019-04-08 00:49:47,082 [INFO] unfolding 0, single step 131001
2019-04-08 00:49:47,083 [INFO] Sum of grad norms of most recent batch: 0.060202
2019-04-08 00:49:47,084 [INFO] ---------------------------------
2019-04-08 00:50:24,501 [INFO] ---------------------------------
2019-04-08 00:50:24,502 [INFO] Evaluation:
2019-04-08 00:50:24,503 [INFO] Batch 131000, worst loss 0.016405 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:50:24,504 [INFO] ---------------------------------
2019-04-08 00:50:45,573 [INFO] ---------------------------------
2019-04-08 00:50:45,574 [INFO] Summary:
2019-04-08 00:50:45,574 [INFO] Batch 132000, worst loss 0.001915 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:50:45,575 [INFO] Regularization: 1475.267334 * 0.0000000100 = 0.0000147527 loss
2019-04-08 00:50:45,575 [INFO] unfolding 0, single step 132001
2019-04-08 00:50:45,576 [INFO] Sum of grad norms of most recent batch: 0.037467
2019-04-08 00:50:45,576 [INFO] ---------------------------------
2019-04-08 00:51:22,807 [INFO] ---------------------------------
2019-04-08 00:51:22,808 [INFO] Evaluation:
2019-04-08 00:51:22,809 [INFO] Batch 132000, worst loss 0.006572 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:51:22,810 [INFO] ---------------------------------
2019-04-08 00:51:43,932 [INFO] ---------------------------------
2019-04-08 00:51:43,933 [INFO] Summary:
2019-04-08 00:51:43,933 [INFO] Batch 133000, worst loss 0.009408 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:51:43,934 [INFO] Regularization: 1475.237671 * 0.0000000100 = 0.0000147524 loss
2019-04-08 00:51:43,934 [INFO] unfolding 0, single step 133001
2019-04-08 00:51:43,935 [INFO] Sum of grad norms of most recent batch: 0.024265
2019-04-08 00:51:43,935 [INFO] ---------------------------------
2019-04-08 00:52:05,490 [INFO] ---------------------------------
2019-04-08 00:52:05,491 [INFO] Summary:
2019-04-08 00:52:05,491 [INFO] Batch 134000, worst loss 0.002516 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:52:05,492 [INFO] Regularization: 1475.223999 * 0.0000000100 = 0.0000147522 loss
2019-04-08 00:52:05,492 [INFO] unfolding 0, single step 134001
2019-04-08 00:52:05,493 [INFO] Sum of grad norms of most recent batch: 0.020511
2019-04-08 00:52:05,493 [INFO] ---------------------------------
2019-04-08 00:52:43,079 [INFO] ---------------------------------
2019-04-08 00:52:43,080 [INFO] Evaluation:
2019-04-08 00:52:43,080 [INFO] Batch 134000, worst loss 0.008923 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:52:43,081 [INFO] ---------------------------------
2019-04-08 00:53:03,747 [INFO] ---------------------------------
2019-04-08 00:53:03,748 [INFO] Summary:
2019-04-08 00:53:03,749 [INFO] Batch 135000, worst loss 0.014143 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:53:03,749 [INFO] Regularization: 1475.230591 * 0.0000000100 = 0.0000147523 loss
2019-04-08 00:53:03,749 [INFO] unfolding 0, single step 135001
2019-04-08 00:53:03,750 [INFO] Sum of grad norms of most recent batch: 0.030590
2019-04-08 00:53:03,751 [INFO] ---------------------------------
2019-04-08 00:53:25,491 [INFO] ---------------------------------
2019-04-08 00:53:25,491 [INFO] Summary:
2019-04-08 00:53:25,492 [INFO] Batch 136000, worst loss 0.004545 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:53:25,492 [INFO] Regularization: 1475.219727 * 0.0000000100 = 0.0000147522 loss
2019-04-08 00:53:25,493 [INFO] unfolding 0, single step 136001
2019-04-08 00:53:25,493 [INFO] Sum of grad norms of most recent batch: 0.042119
2019-04-08 00:53:25,494 [INFO] ---------------------------------
2019-04-08 00:54:03,004 [INFO] ---------------------------------
2019-04-08 00:54:03,005 [INFO] Evaluation:
2019-04-08 00:54:03,005 [INFO] Batch 136000, worst loss 0.040807 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:54:03,006 [INFO] ---------------------------------
2019-04-08 00:54:24,137 [INFO] ---------------------------------
2019-04-08 00:54:24,138 [INFO] Summary:
2019-04-08 00:54:24,138 [INFO] Batch 137000, worst loss 0.002211 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:54:24,139 [INFO] Regularization: 1475.185059 * 0.0000000100 = 0.0000147519 loss
2019-04-08 00:54:24,140 [INFO] unfolding 0, single step 137001
2019-04-08 00:54:24,141 [INFO] Sum of grad norms of most recent batch: 0.045158
2019-04-08 00:54:24,141 [INFO] ---------------------------------
2019-04-08 00:55:01,474 [INFO] ---------------------------------
2019-04-08 00:55:01,474 [INFO] Evaluation:
2019-04-08 00:55:01,475 [INFO] Batch 137000, worst loss 0.012512 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:55:01,476 [INFO] ---------------------------------
2019-04-08 00:55:22,513 [INFO] ---------------------------------
2019-04-08 00:55:22,514 [INFO] Summary:
2019-04-08 00:55:22,515 [INFO] Batch 138000, worst loss 0.003994 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:55:22,516 [INFO] Regularization: 1475.172363 * 0.0000000100 = 0.0000147517 loss
2019-04-08 00:55:22,517 [INFO] unfolding 0, single step 138001
2019-04-08 00:55:22,517 [INFO] Sum of grad norms of most recent batch: 0.017766
2019-04-08 00:55:22,518 [INFO] ---------------------------------
2019-04-08 00:55:59,992 [INFO] ---------------------------------
2019-04-08 00:55:59,993 [INFO] Evaluation:
2019-04-08 00:55:59,993 [INFO] Batch 138000, worst loss 0.012572 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:55:59,994 [INFO] ---------------------------------
2019-04-08 00:56:21,986 [INFO] ---------------------------------
2019-04-08 00:56:21,987 [INFO] Summary:
2019-04-08 00:56:21,988 [INFO] Batch 139000, worst loss 0.004029 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:56:21,988 [INFO] Regularization: 1475.147339 * 0.0000000100 = 0.0000147515 loss
2019-04-08 00:56:21,989 [INFO] unfolding 0, single step 139001
2019-04-08 00:56:21,989 [INFO] Sum of grad norms of most recent batch: 0.025548
2019-04-08 00:56:21,990 [INFO] ---------------------------------
2019-04-08 00:56:59,427 [INFO] ---------------------------------
2019-04-08 00:56:59,428 [INFO] Evaluation:
2019-04-08 00:56:59,428 [INFO] Batch 139000, worst loss 0.021898 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:56:59,429 [INFO] ---------------------------------
2019-04-08 00:57:19,667 [INFO] ---------------------------------
2019-04-08 00:57:19,668 [INFO] Summary:
2019-04-08 00:57:19,669 [INFO] Batch 140000, worst loss 0.002214 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:57:19,670 [INFO] Regularization: 1475.112305 * 0.0000000100 = 0.0000147511 loss
2019-04-08 00:57:19,671 [INFO] unfolding 0, single step 140001
2019-04-08 00:57:19,672 [INFO] Sum of grad norms of most recent batch: 0.026725
2019-04-08 00:57:19,673 [INFO] ---------------------------------
2019-04-08 00:57:57,204 [INFO] ---------------------------------
2019-04-08 00:57:57,205 [INFO] Evaluation:
2019-04-08 00:57:57,206 [INFO] Batch 140000, worst loss 0.009641 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:57:57,206 [INFO] ---------------------------------
2019-04-08 00:58:18,147 [INFO] ---------------------------------
2019-04-08 00:58:18,148 [INFO] Summary:
2019-04-08 00:58:18,148 [INFO] Batch 141000, worst loss 0.003469 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:58:18,149 [INFO] Regularization: 1475.094604 * 0.0000000100 = 0.0000147509 loss
2019-04-08 00:58:18,149 [INFO] unfolding 0, single step 141001
2019-04-08 00:58:18,150 [INFO] Sum of grad norms of most recent batch: 0.016000
2019-04-08 00:58:18,150 [INFO] ---------------------------------
2019-04-08 00:58:55,393 [INFO] ---------------------------------
2019-04-08 00:58:55,394 [INFO] Evaluation:
2019-04-08 00:58:55,395 [INFO] Batch 141000, worst loss 0.011986 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:58:55,396 [INFO] ---------------------------------
2019-04-08 00:59:16,094 [INFO] ---------------------------------
2019-04-08 00:59:16,095 [INFO] Summary:
2019-04-08 00:59:16,095 [INFO] Batch 142000, worst loss 0.003726 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 00:59:16,096 [INFO] Regularization: 1475.070068 * 0.0000000100 = 0.0000147507 loss
2019-04-08 00:59:16,096 [INFO] unfolding 0, single step 142001
2019-04-08 00:59:16,097 [INFO] Sum of grad norms of most recent batch: 0.051822
2019-04-08 00:59:16,097 [INFO] ---------------------------------
2019-04-08 00:59:53,319 [INFO] ---------------------------------
2019-04-08 00:59:53,320 [INFO] Evaluation:
2019-04-08 00:59:53,321 [INFO] Batch 142000, worst loss 0.013748 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 00:59:53,321 [INFO] ---------------------------------
2019-04-08 01:00:14,372 [INFO] ---------------------------------
2019-04-08 01:00:14,373 [INFO] Summary:
2019-04-08 01:00:14,374 [INFO] Batch 143000, worst loss 0.001943 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 01:00:14,374 [INFO] Regularization: 1475.057739 * 0.0000000100 = 0.0000147506 loss
2019-04-08 01:00:14,375 [INFO] unfolding 0, single step 143001
2019-04-08 01:00:14,375 [INFO] Sum of grad norms of most recent batch: 0.020321
2019-04-08 01:00:14,376 [INFO] ---------------------------------
2019-04-08 01:00:51,811 [INFO] ---------------------------------
2019-04-08 01:00:51,812 [INFO] Evaluation:
2019-04-08 01:00:51,813 [INFO] Batch 143000, worst loss 0.018339 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:00:51,813 [INFO] ---------------------------------
2019-04-08 01:01:13,591 [INFO] ---------------------------------
2019-04-08 01:01:13,592 [INFO] Summary:
2019-04-08 01:01:13,592 [INFO] Batch 144000, worst loss 0.008130 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 01:01:13,593 [INFO] Regularization: 1475.052856 * 0.0000000100 = 0.0000147505 loss
2019-04-08 01:01:13,593 [INFO] unfolding 0, single step 144001
2019-04-08 01:01:13,594 [INFO] Sum of grad norms of most recent batch: 0.025846
2019-04-08 01:01:13,594 [INFO] ---------------------------------
2019-04-08 01:01:34,814 [INFO] ---------------------------------
2019-04-08 01:01:34,816 [INFO] Summary:
2019-04-08 01:01:34,816 [INFO] Batch 145000, worst loss 0.009663 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 01:01:34,817 [INFO] Regularization: 1475.038330 * 0.0000000100 = 0.0000147504 loss
2019-04-08 01:01:34,817 [INFO] unfolding 0, single step 145001
2019-04-08 01:01:34,817 [INFO] Sum of grad norms of most recent batch: 0.047962
2019-04-08 01:01:34,818 [INFO] ---------------------------------
2019-04-08 01:01:55,714 [INFO] ---------------------------------
2019-04-08 01:01:55,715 [INFO] Summary:
2019-04-08 01:01:55,715 [INFO] Batch 146000, worst loss 0.009668 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 01:01:55,716 [INFO] Regularization: 1475.051025 * 0.0000000100 = 0.0000147505 loss
2019-04-08 01:01:55,716 [INFO] unfolding 0, single step 146001
2019-04-08 01:01:55,717 [INFO] Sum of grad norms of most recent batch: 0.042446
2019-04-08 01:01:55,717 [INFO] ---------------------------------
2019-04-08 01:02:16,876 [INFO] ---------------------------------
2019-04-08 01:02:16,877 [INFO] Summary:
2019-04-08 01:02:16,877 [INFO] Batch 147000, worst loss 0.001263 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 01:02:16,878 [INFO] Regularization: 1475.037598 * 0.0000000100 = 0.0000147504 loss
2019-04-08 01:02:16,878 [INFO] unfolding 0, single step 147001
2019-04-08 01:02:16,879 [INFO] Sum of grad norms of most recent batch: 1.644813
2019-04-08 01:02:16,879 [INFO] ---------------------------------
2019-04-08 01:02:54,118 [INFO] ---------------------------------
2019-04-08 01:02:54,119 [INFO] Evaluation:
2019-04-08 01:02:54,119 [INFO] Batch 147000, worst loss 0.005218 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:02:54,120 [INFO] ---------------------------------
2019-04-08 01:03:15,242 [INFO] ---------------------------------
2019-04-08 01:03:15,243 [INFO] Summary:
2019-04-08 01:03:15,243 [INFO] Batch 148000, worst loss 0.005000 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 01:03:15,244 [INFO] Regularization: 1475.022339 * 0.0000000100 = 0.0000147502 loss
2019-04-08 01:03:15,244 [INFO] unfolding 0, single step 148001
2019-04-08 01:03:15,245 [INFO] Sum of grad norms of most recent batch: 0.018075
2019-04-08 01:03:15,245 [INFO] ---------------------------------
2019-04-08 01:03:36,031 [INFO] ---------------------------------
2019-04-08 01:03:36,032 [INFO] Summary:
2019-04-08 01:03:36,032 [INFO] Batch 149000, worst loss 0.003371 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 01:03:36,033 [INFO] Regularization: 1475.004395 * 0.0000000100 = 0.0000147500 loss
2019-04-08 01:03:36,033 [INFO] unfolding 0, single step 149001
2019-04-08 01:03:36,034 [INFO] Sum of grad norms of most recent batch: 0.028882
2019-04-08 01:03:36,034 [INFO] ---------------------------------
2019-04-08 01:04:13,462 [INFO] ---------------------------------
2019-04-08 01:04:13,463 [INFO] Evaluation:
2019-04-08 01:04:13,464 [INFO] Batch 149000, worst loss 0.014086 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:04:13,465 [INFO] ---------------------------------
2019-04-08 01:04:34,424 [INFO] ---------------------------------
2019-04-08 01:04:34,425 [INFO] Summary:
2019-04-08 01:04:34,426 [INFO] Batch 150000, worst loss 0.009818 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 01:04:34,427 [INFO] Regularization: 1474.997070 * 0.0000000100 = 0.0000147500 loss
2019-04-08 01:04:34,427 [INFO] unfolding 0, single step 150001
2019-04-08 01:04:34,428 [INFO] Sum of grad norms of most recent batch: 0.024053
2019-04-08 01:04:34,429 [INFO] ---------------------------------
2019-04-08 01:05:11,784 [INFO] ---------------------------------
2019-04-08 01:05:11,785 [INFO] Evaluation:
2019-04-08 01:05:11,785 [INFO] Batch 150000, worst loss 0.007374 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:05:11,786 [INFO] ---------------------------------
2019-04-08 01:05:11,786 [INFO] Finished training, saved to file transition/1554653688/1554678311_5_transition_final.pth
2019-04-08 01:05:11,963 [INFO] ---------------------------------
2019-04-08 01:05:11,964 [INFO] Training model #6: (11, 64, 8) @ 3
2019-04-08 01:05:33,260 [INFO] ---------------------------------
2019-04-08 01:05:33,261 [INFO] Summary:
2019-04-08 01:05:33,262 [INFO] Batch 1000, worst loss 12.905761 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:05:33,262 [INFO] Regularization: 7098.875488 * 0.0000000100 = 0.0000709888 loss
2019-04-08 01:05:33,263 [INFO] unfolding 0, single step 1001
2019-04-08 01:05:33,263 [INFO] Sum of grad norms of most recent batch: 28.237976
2019-04-08 01:05:33,264 [INFO] ---------------------------------
2019-04-08 01:05:54,806 [INFO] ---------------------------------
2019-04-08 01:05:54,807 [INFO] Summary:
2019-04-08 01:05:54,807 [INFO] Batch 2000, worst loss 0.078562 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:05:54,808 [INFO] Regularization: 4504.165039 * 0.0000000100 = 0.0000450417 loss
2019-04-08 01:05:54,809 [INFO] unfolding 0, single step 2001
2019-04-08 01:05:54,809 [INFO] Sum of grad norms of most recent batch: 5.574982
2019-04-08 01:05:54,810 [INFO] ---------------------------------
2019-04-08 01:06:16,867 [INFO] ---------------------------------
2019-04-08 01:06:16,868 [INFO] Summary:
2019-04-08 01:06:16,869 [INFO] Batch 3000, worst loss 0.103756 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:06:16,870 [INFO] Regularization: 3012.485107 * 0.0000000100 = 0.0000301249 loss
2019-04-08 01:06:16,871 [INFO] unfolding 0, single step 3001
2019-04-08 01:06:16,872 [INFO] Sum of grad norms of most recent batch: 4.643640
2019-04-08 01:06:16,873 [INFO] ---------------------------------
2019-04-08 01:06:38,318 [INFO] ---------------------------------
2019-04-08 01:06:38,319 [INFO] Summary:
2019-04-08 01:06:38,319 [INFO] Batch 4000, worst loss 0.125863 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:06:38,320 [INFO] Regularization: 2625.728271 * 0.0000000100 = 0.0000262573 loss
2019-04-08 01:06:38,320 [INFO] unfolding 0, single step 4001
2019-04-08 01:06:38,321 [INFO] Sum of grad norms of most recent batch: 7.147489
2019-04-08 01:06:38,321 [INFO] ---------------------------------
2019-04-08 01:06:59,354 [INFO] ---------------------------------
2019-04-08 01:06:59,355 [INFO] Summary:
2019-04-08 01:06:59,356 [INFO] Batch 5000, worst loss 0.140347 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:06:59,356 [INFO] Regularization: 2366.322510 * 0.0000000100 = 0.0000236632 loss
2019-04-08 01:06:59,357 [INFO] unfolding 0, single step 5001
2019-04-08 01:06:59,358 [INFO] Sum of grad norms of most recent batch: 5.126529
2019-04-08 01:06:59,358 [INFO] ---------------------------------
2019-04-08 01:07:21,319 [INFO] ---------------------------------
2019-04-08 01:07:21,320 [INFO] Summary:
2019-04-08 01:07:21,321 [INFO] Batch 6000, worst loss 0.065943 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:07:21,321 [INFO] Regularization: 2100.933350 * 0.0000000100 = 0.0000210093 loss
2019-04-08 01:07:21,321 [INFO] unfolding 0, single step 6001
2019-04-08 01:07:21,322 [INFO] Sum of grad norms of most recent batch: 2.710127
2019-04-08 01:07:21,323 [INFO] ---------------------------------
2019-04-08 01:07:42,362 [INFO] ---------------------------------
2019-04-08 01:07:42,363 [INFO] Summary:
2019-04-08 01:07:42,364 [INFO] Batch 7000, worst loss 0.053860 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:07:42,364 [INFO] Regularization: 1862.043335 * 0.0000000100 = 0.0000186204 loss
2019-04-08 01:07:42,365 [INFO] unfolding 0, single step 7001
2019-04-08 01:07:42,365 [INFO] Sum of grad norms of most recent batch: 6.440034
2019-04-08 01:07:42,366 [INFO] ---------------------------------
2019-04-08 01:08:04,077 [INFO] ---------------------------------
2019-04-08 01:08:04,078 [INFO] Summary:
2019-04-08 01:08:04,079 [INFO] Batch 8000, worst loss 0.098521 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:08:04,079 [INFO] Regularization: 1681.943115 * 0.0000000100 = 0.0000168194 loss
2019-04-08 01:08:04,080 [INFO] unfolding 0, single step 8001
2019-04-08 01:08:04,080 [INFO] Sum of grad norms of most recent batch: 4.473260
2019-04-08 01:08:04,081 [INFO] ---------------------------------
2019-04-08 01:08:25,447 [INFO] ---------------------------------
2019-04-08 01:08:25,448 [INFO] Summary:
2019-04-08 01:08:25,448 [INFO] Batch 9000, worst loss 0.037245 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:08:25,449 [INFO] Regularization: 1520.755371 * 0.0000000100 = 0.0000152076 loss
2019-04-08 01:08:25,449 [INFO] unfolding 0, single step 9001
2019-04-08 01:08:25,450 [INFO] Sum of grad norms of most recent batch: 3.519855
2019-04-08 01:08:25,450 [INFO] ---------------------------------
2019-04-08 01:08:46,208 [INFO] ---------------------------------
2019-04-08 01:08:46,209 [INFO] Summary:
2019-04-08 01:08:46,209 [INFO] Batch 10000, worst loss 0.132024 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:08:46,210 [INFO] Regularization: 1402.946777 * 0.0000000100 = 0.0000140295 loss
2019-04-08 01:08:46,210 [INFO] unfolding 0, single step 10001
2019-04-08 01:08:46,210 [INFO] Sum of grad norms of most recent batch: 3.116693
2019-04-08 01:08:46,211 [INFO] ---------------------------------
2019-04-08 01:09:23,628 [INFO] ---------------------------------
2019-04-08 01:09:23,629 [INFO] Evaluation:
2019-04-08 01:09:23,629 [INFO] Batch 10000, worst loss 0.160309 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:09:23,630 [INFO] ---------------------------------
2019-04-08 01:09:45,307 [INFO] ---------------------------------
2019-04-08 01:09:45,308 [INFO] Summary:
2019-04-08 01:09:45,309 [INFO] Batch 11000, worst loss 0.086786 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:09:45,310 [INFO] Regularization: 1354.231812 * 0.0000000100 = 0.0000135423 loss
2019-04-08 01:09:45,310 [INFO] unfolding 0, single step 11001
2019-04-08 01:09:45,311 [INFO] Sum of grad norms of most recent batch: 4.383848
2019-04-08 01:09:45,311 [INFO] ---------------------------------
2019-04-08 01:10:06,666 [INFO] ---------------------------------
2019-04-08 01:10:06,667 [INFO] Summary:
2019-04-08 01:10:06,667 [INFO] Batch 12000, worst loss 0.032832 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:10:06,668 [INFO] Regularization: 1299.704590 * 0.0000000100 = 0.0000129970 loss
2019-04-08 01:10:06,668 [INFO] unfolding 0, single step 12001
2019-04-08 01:10:06,669 [INFO] Sum of grad norms of most recent batch: 2.063917
2019-04-08 01:10:06,669 [INFO] ---------------------------------
2019-04-08 01:10:28,536 [INFO] ---------------------------------
2019-04-08 01:10:28,537 [INFO] Summary:
2019-04-08 01:10:28,538 [INFO] Batch 13000, worst loss 0.166493 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:10:28,539 [INFO] Regularization: 1277.534058 * 0.0000000100 = 0.0000127753 loss
2019-04-08 01:10:28,540 [INFO] unfolding 0, single step 13001
2019-04-08 01:10:28,541 [INFO] Sum of grad norms of most recent batch: 1.674300
2019-04-08 01:10:28,542 [INFO] ---------------------------------
2019-04-08 01:10:50,553 [INFO] ---------------------------------
2019-04-08 01:10:50,555 [INFO] Summary:
2019-04-08 01:10:50,556 [INFO] Batch 14000, worst loss 0.288365 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:10:50,556 [INFO] Regularization: 1289.275269 * 0.0000000100 = 0.0000128928 loss
2019-04-08 01:10:50,557 [INFO] unfolding 0, single step 14001
2019-04-08 01:10:50,558 [INFO] Sum of grad norms of most recent batch: 2.594365
2019-04-08 01:10:50,559 [INFO] ---------------------------------
2019-04-08 01:11:11,876 [INFO] ---------------------------------
2019-04-08 01:11:11,877 [INFO] Summary:
2019-04-08 01:11:11,878 [INFO] Batch 15000, worst loss 0.100271 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:11:11,878 [INFO] Regularization: 1264.461548 * 0.0000000100 = 0.0000126446 loss
2019-04-08 01:11:11,879 [INFO] unfolding 0, single step 15001
2019-04-08 01:11:11,880 [INFO] Sum of grad norms of most recent batch: 1.695794
2019-04-08 01:11:11,880 [INFO] ---------------------------------
2019-04-08 01:11:33,564 [INFO] ---------------------------------
2019-04-08 01:11:33,565 [INFO] Summary:
2019-04-08 01:11:33,565 [INFO] Batch 16000, worst loss 0.043640 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:11:33,566 [INFO] Regularization: 1233.640015 * 0.0000000100 = 0.0000123364 loss
2019-04-08 01:11:33,566 [INFO] unfolding 0, single step 16001
2019-04-08 01:11:33,567 [INFO] Sum of grad norms of most recent batch: 1.888778
2019-04-08 01:11:33,567 [INFO] ---------------------------------
2019-04-08 01:11:55,374 [INFO] ---------------------------------
2019-04-08 01:11:55,375 [INFO] Summary:
2019-04-08 01:11:55,376 [INFO] Batch 17000, worst loss 0.114818 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:11:55,376 [INFO] Regularization: 1213.560303 * 0.0000000100 = 0.0000121356 loss
2019-04-08 01:11:55,376 [INFO] unfolding 0, single step 17001
2019-04-08 01:11:55,377 [INFO] Sum of grad norms of most recent batch: 1.244951
2019-04-08 01:11:55,377 [INFO] ---------------------------------
2019-04-08 01:12:16,943 [INFO] ---------------------------------
2019-04-08 01:12:16,944 [INFO] Summary:
2019-04-08 01:12:16,945 [INFO] Batch 18000, worst loss 0.114948 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:12:16,945 [INFO] Regularization: 1211.919922 * 0.0000000100 = 0.0000121192 loss
2019-04-08 01:12:16,946 [INFO] unfolding 0, single step 18001
2019-04-08 01:12:16,946 [INFO] Sum of grad norms of most recent batch: 2.243401
2019-04-08 01:12:16,947 [INFO] ---------------------------------
2019-04-08 01:12:38,609 [INFO] ---------------------------------
2019-04-08 01:12:38,610 [INFO] Summary:
2019-04-08 01:12:38,611 [INFO] Batch 19000, worst loss 0.041785 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:12:38,612 [INFO] Regularization: 1194.840698 * 0.0000000100 = 0.0000119484 loss
2019-04-08 01:12:38,612 [INFO] unfolding 0, single step 19001
2019-04-08 01:12:38,613 [INFO] Sum of grad norms of most recent batch: 1.487556
2019-04-08 01:12:38,613 [INFO] ---------------------------------
2019-04-08 01:12:59,980 [INFO] ---------------------------------
2019-04-08 01:12:59,981 [INFO] Summary:
2019-04-08 01:12:59,981 [INFO] Batch 20000, worst loss 0.068939 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:12:59,982 [INFO] Regularization: 1199.369751 * 0.0000000100 = 0.0000119937 loss
2019-04-08 01:12:59,982 [INFO] unfolding 0, single step 20001
2019-04-08 01:12:59,983 [INFO] Sum of grad norms of most recent batch: 1.369006
2019-04-08 01:12:59,983 [INFO] ---------------------------------
2019-04-08 01:13:37,151 [INFO] ---------------------------------
2019-04-08 01:13:37,152 [INFO] Evaluation:
2019-04-08 01:13:37,153 [INFO] Batch 20000, worst loss 0.138415 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:13:37,153 [INFO] ---------------------------------
2019-04-08 01:13:59,532 [INFO] ---------------------------------
2019-04-08 01:13:59,533 [INFO] Summary:
2019-04-08 01:13:59,534 [INFO] Batch 21000, worst loss 0.062933 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:13:59,535 [INFO] Regularization: 1178.852417 * 0.0000000100 = 0.0000117885 loss
2019-04-08 01:13:59,536 [INFO] unfolding 0, single step 21001
2019-04-08 01:13:59,537 [INFO] Sum of grad norms of most recent batch: 1.296164
2019-04-08 01:13:59,538 [INFO] ---------------------------------
2019-04-08 01:14:21,527 [INFO] ---------------------------------
2019-04-08 01:14:21,528 [INFO] Summary:
2019-04-08 01:14:21,529 [INFO] Batch 22000, worst loss 0.112367 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:14:21,530 [INFO] Regularization: 1181.641724 * 0.0000000100 = 0.0000118164 loss
2019-04-08 01:14:21,530 [INFO] unfolding 0, single step 22001
2019-04-08 01:14:21,530 [INFO] Sum of grad norms of most recent batch: 1.438159
2019-04-08 01:14:21,531 [INFO] ---------------------------------
2019-04-08 01:14:43,530 [INFO] ---------------------------------
2019-04-08 01:14:43,531 [INFO] Summary:
2019-04-08 01:14:43,531 [INFO] Batch 23000, worst loss 0.062533 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:14:43,532 [INFO] Regularization: 1170.855103 * 0.0000000100 = 0.0000117086 loss
2019-04-08 01:14:43,532 [INFO] unfolding 0, single step 23001
2019-04-08 01:14:43,533 [INFO] Sum of grad norms of most recent batch: 2.422031
2019-04-08 01:14:43,533 [INFO] ---------------------------------
2019-04-08 01:15:05,158 [INFO] ---------------------------------
2019-04-08 01:15:05,159 [INFO] Summary:
2019-04-08 01:15:05,160 [INFO] Batch 24000, worst loss 0.067253 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:15:05,160 [INFO] Regularization: 1162.316406 * 0.0000000100 = 0.0000116232 loss
2019-04-08 01:15:05,161 [INFO] unfolding 0, single step 24001
2019-04-08 01:15:05,161 [INFO] Sum of grad norms of most recent batch: 2.490223
2019-04-08 01:15:05,162 [INFO] ---------------------------------
2019-04-08 01:15:26,759 [INFO] ---------------------------------
2019-04-08 01:15:26,760 [INFO] Summary:
2019-04-08 01:15:26,761 [INFO] Batch 25000, worst loss 0.092535 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:15:26,761 [INFO] Regularization: 1163.793091 * 0.0000000100 = 0.0000116379 loss
2019-04-08 01:15:26,762 [INFO] unfolding 0, single step 25001
2019-04-08 01:15:26,762 [INFO] Sum of grad norms of most recent batch: 5.454529
2019-04-08 01:15:26,763 [INFO] ---------------------------------
2019-04-08 01:15:47,611 [INFO] ---------------------------------
2019-04-08 01:15:47,612 [INFO] Summary:
2019-04-08 01:15:47,613 [INFO] Batch 26000, worst loss 0.062701 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:15:47,613 [INFO] Regularization: 1146.520630 * 0.0000000100 = 0.0000114652 loss
2019-04-08 01:15:47,613 [INFO] unfolding 0, single step 26001
2019-04-08 01:15:47,614 [INFO] Sum of grad norms of most recent batch: 2.243099
2019-04-08 01:15:47,614 [INFO] ---------------------------------
2019-04-08 01:16:09,175 [INFO] ---------------------------------
2019-04-08 01:16:09,176 [INFO] Summary:
2019-04-08 01:16:09,177 [INFO] Batch 27000, worst loss 0.084307 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:16:09,177 [INFO] Regularization: 1143.179565 * 0.0000000100 = 0.0000114318 loss
2019-04-08 01:16:09,177 [INFO] unfolding 0, single step 27001
2019-04-08 01:16:09,178 [INFO] Sum of grad norms of most recent batch: 1.172588
2019-04-08 01:16:09,179 [INFO] ---------------------------------
2019-04-08 01:16:30,509 [INFO] ---------------------------------
2019-04-08 01:16:30,510 [INFO] Summary:
2019-04-08 01:16:30,510 [INFO] Batch 28000, worst loss 0.137141 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:16:30,511 [INFO] Regularization: 1124.626831 * 0.0000000100 = 0.0000112463 loss
2019-04-08 01:16:30,511 [INFO] unfolding 0, single step 28001
2019-04-08 01:16:30,512 [INFO] Sum of grad norms of most recent batch: 0.455762
2019-04-08 01:16:30,512 [INFO] ---------------------------------
2019-04-08 01:16:52,253 [INFO] ---------------------------------
2019-04-08 01:16:52,254 [INFO] Summary:
2019-04-08 01:16:52,255 [INFO] Batch 29000, worst loss 0.088011 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:16:52,255 [INFO] Regularization: 1110.551636 * 0.0000000100 = 0.0000111055 loss
2019-04-08 01:16:52,255 [INFO] unfolding 0, single step 29001
2019-04-08 01:16:52,256 [INFO] Sum of grad norms of most recent batch: 4.436434
2019-04-08 01:16:52,256 [INFO] ---------------------------------
2019-04-08 01:17:14,106 [INFO] ---------------------------------
2019-04-08 01:17:14,108 [INFO] Summary:
2019-04-08 01:17:14,108 [INFO] Batch 30000, worst loss 0.117000 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 01:17:14,109 [INFO] Regularization: 1100.412231 * 0.0000000100 = 0.0000110041 loss
2019-04-08 01:17:14,110 [INFO] unfolding 0, single step 30001
2019-04-08 01:17:14,111 [INFO] Sum of grad norms of most recent batch: 1.370141
2019-04-08 01:17:14,111 [INFO] ---------------------------------
2019-04-08 01:17:51,456 [INFO] ---------------------------------
2019-04-08 01:17:51,457 [INFO] Evaluation:
2019-04-08 01:17:51,458 [INFO] Batch 30000, worst loss 0.156703 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:17:51,458 [INFO] ---------------------------------
2019-04-08 01:18:12,774 [INFO] ---------------------------------
2019-04-08 01:18:12,775 [INFO] Summary:
2019-04-08 01:18:12,776 [INFO] Batch 31000, worst loss 0.123113 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 01:18:12,777 [INFO] Regularization: 1114.272827 * 0.0000000100 = 0.0000111427 loss
2019-04-08 01:18:12,777 [INFO] unfolding 0, single step 31001
2019-04-08 01:18:12,778 [INFO] Sum of grad norms of most recent batch: 1.883239
2019-04-08 01:18:12,778 [INFO] ---------------------------------
2019-04-08 01:18:34,473 [INFO] ---------------------------------
2019-04-08 01:18:34,474 [INFO] Summary:
2019-04-08 01:18:34,475 [INFO] Batch 32000, worst loss 0.035373 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 01:18:34,475 [INFO] Regularization: 1070.458862 * 0.0000000100 = 0.0000107046 loss
2019-04-08 01:18:34,476 [INFO] unfolding 0, single step 32001
2019-04-08 01:18:34,476 [INFO] Sum of grad norms of most recent batch: 0.447984
2019-04-08 01:18:34,477 [INFO] ---------------------------------
2019-04-08 01:18:55,878 [INFO] ---------------------------------
2019-04-08 01:18:55,879 [INFO] Summary:
2019-04-08 01:18:55,879 [INFO] Batch 33000, worst loss 0.031730 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 01:18:55,880 [INFO] Regularization: 1042.113403 * 0.0000000100 = 0.0000104211 loss
2019-04-08 01:18:55,880 [INFO] unfolding 0, single step 33001
2019-04-08 01:18:55,881 [INFO] Sum of grad norms of most recent batch: 0.551169
2019-04-08 01:18:55,881 [INFO] ---------------------------------
2019-04-08 01:19:17,025 [INFO] ---------------------------------
2019-04-08 01:19:17,026 [INFO] Summary:
2019-04-08 01:19:17,027 [INFO] Batch 34000, worst loss 0.036076 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 01:19:17,027 [INFO] Regularization: 1027.603760 * 0.0000000100 = 0.0000102760 loss
2019-04-08 01:19:17,027 [INFO] unfolding 0, single step 34001
2019-04-08 01:19:17,028 [INFO] Sum of grad norms of most recent batch: 0.567310
2019-04-08 01:19:17,028 [INFO] ---------------------------------
2019-04-08 01:19:38,670 [INFO] ---------------------------------
2019-04-08 01:19:38,671 [INFO] Summary:
2019-04-08 01:19:38,671 [INFO] Batch 35000, worst loss 0.087494 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 01:19:38,672 [INFO] Regularization: 1005.783752 * 0.0000000100 = 0.0000100578 loss
2019-04-08 01:19:38,672 [INFO] unfolding 0, single step 35001
2019-04-08 01:19:38,673 [INFO] Sum of grad norms of most recent batch: 1.258588
2019-04-08 01:19:38,673 [INFO] ---------------------------------
2019-04-08 01:20:00,529 [INFO] ---------------------------------
2019-04-08 01:20:00,530 [INFO] Summary:
2019-04-08 01:20:00,531 [INFO] Batch 36000, worst loss 0.029879 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 01:20:00,531 [INFO] Regularization: 994.108093 * 0.0000000100 = 0.0000099411 loss
2019-04-08 01:20:00,532 [INFO] unfolding 0, single step 36001
2019-04-08 01:20:00,532 [INFO] Sum of grad norms of most recent batch: 0.359256
2019-04-08 01:20:00,533 [INFO] ---------------------------------
2019-04-08 01:20:21,882 [INFO] ---------------------------------
2019-04-08 01:20:21,883 [INFO] Summary:
2019-04-08 01:20:21,883 [INFO] Batch 37000, worst loss 0.048948 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 01:20:21,884 [INFO] Regularization: 1000.820312 * 0.0000000100 = 0.0000100082 loss
2019-04-08 01:20:21,884 [INFO] unfolding 0, single step 37001
2019-04-08 01:20:21,885 [INFO] Sum of grad norms of most recent batch: 0.521640
2019-04-08 01:20:21,886 [INFO] ---------------------------------
2019-04-08 01:20:43,839 [INFO] ---------------------------------
2019-04-08 01:20:43,840 [INFO] Summary:
2019-04-08 01:20:43,840 [INFO] Batch 38000, worst loss 0.166888 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 01:20:43,841 [INFO] Regularization: 995.490723 * 0.0000000100 = 0.0000099549 loss
2019-04-08 01:20:43,841 [INFO] unfolding 0, single step 38001
2019-04-08 01:20:43,842 [INFO] Sum of grad norms of most recent batch: 0.955032
2019-04-08 01:20:43,842 [INFO] ---------------------------------
2019-04-08 01:21:05,502 [INFO] ---------------------------------
2019-04-08 01:21:05,503 [INFO] Summary:
2019-04-08 01:21:05,503 [INFO] Batch 39000, worst loss 0.103302 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 01:21:05,504 [INFO] Regularization: 985.284058 * 0.0000000100 = 0.0000098528 loss
2019-04-08 01:21:05,505 [INFO] unfolding 0, single step 39001
2019-04-08 01:21:05,505 [INFO] Sum of grad norms of most recent batch: 0.620492
2019-04-08 01:21:05,506 [INFO] ---------------------------------
2019-04-08 01:21:26,962 [INFO] ---------------------------------
2019-04-08 01:21:26,963 [INFO] Summary:
2019-04-08 01:21:26,963 [INFO] Batch 40000, worst loss 0.146761 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 01:21:26,964 [INFO] Regularization: 989.255737 * 0.0000000100 = 0.0000098926 loss
2019-04-08 01:21:26,964 [INFO] unfolding 0, single step 40001
2019-04-08 01:21:26,965 [INFO] Sum of grad norms of most recent batch: 0.509789
2019-04-08 01:21:26,966 [INFO] ---------------------------------
2019-04-08 01:22:04,222 [INFO] ---------------------------------
2019-04-08 01:22:04,223 [INFO] Evaluation:
2019-04-08 01:22:04,224 [INFO] Batch 40000, worst loss 0.133054 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:22:04,225 [INFO] ---------------------------------
2019-04-08 01:22:26,716 [INFO] ---------------------------------
2019-04-08 01:22:26,717 [INFO] Summary:
2019-04-08 01:22:26,717 [INFO] Batch 41000, worst loss 0.068328 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 01:22:26,718 [INFO] Regularization: 980.900085 * 0.0000000100 = 0.0000098090 loss
2019-04-08 01:22:26,718 [INFO] unfolding 0, single step 41001
2019-04-08 01:22:26,718 [INFO] Sum of grad norms of most recent batch: 1.145195
2019-04-08 01:22:26,719 [INFO] ---------------------------------
2019-04-08 01:22:47,532 [INFO] ---------------------------------
2019-04-08 01:22:47,533 [INFO] Summary:
2019-04-08 01:22:47,533 [INFO] Batch 42000, worst loss 0.099539 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 01:22:47,534 [INFO] Regularization: 958.872742 * 0.0000000100 = 0.0000095887 loss
2019-04-08 01:22:47,534 [INFO] unfolding 0, single step 42001
2019-04-08 01:22:47,535 [INFO] Sum of grad norms of most recent batch: 0.884881
2019-04-08 01:22:47,535 [INFO] ---------------------------------
2019-04-08 01:23:09,185 [INFO] ---------------------------------
2019-04-08 01:23:09,186 [INFO] Summary:
2019-04-08 01:23:09,187 [INFO] Batch 43000, worst loss 0.060995 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 01:23:09,188 [INFO] Regularization: 946.348267 * 0.0000000100 = 0.0000094635 loss
2019-04-08 01:23:09,189 [INFO] unfolding 0, single step 43001
2019-04-08 01:23:09,190 [INFO] Sum of grad norms of most recent batch: 0.498015
2019-04-08 01:23:09,191 [INFO] ---------------------------------
2019-04-08 01:23:30,326 [INFO] ---------------------------------
2019-04-08 01:23:30,328 [INFO] Summary:
2019-04-08 01:23:30,328 [INFO] Batch 44000, worst loss 0.081933 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 01:23:30,329 [INFO] Regularization: 942.214661 * 0.0000000100 = 0.0000094221 loss
2019-04-08 01:23:30,329 [INFO] unfolding 0, single step 44001
2019-04-08 01:23:30,330 [INFO] Sum of grad norms of most recent batch: 0.913435
2019-04-08 01:23:30,330 [INFO] ---------------------------------
2019-04-08 01:23:51,597 [INFO] ---------------------------------
2019-04-08 01:23:51,598 [INFO] Summary:
2019-04-08 01:23:51,599 [INFO] Batch 45000, worst loss 0.084946 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 01:23:51,599 [INFO] Regularization: 934.321289 * 0.0000000100 = 0.0000093432 loss
2019-04-08 01:23:51,600 [INFO] unfolding 0, single step 45001
2019-04-08 01:23:51,601 [INFO] Sum of grad norms of most recent batch: 0.343686
2019-04-08 01:23:51,601 [INFO] ---------------------------------
2019-04-08 01:24:12,841 [INFO] ---------------------------------
2019-04-08 01:24:12,842 [INFO] Summary:
2019-04-08 01:24:12,842 [INFO] Batch 46000, worst loss 0.055849 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 01:24:12,843 [INFO] Regularization: 931.336548 * 0.0000000100 = 0.0000093134 loss
2019-04-08 01:24:12,843 [INFO] unfolding 0, single step 46001
2019-04-08 01:24:12,844 [INFO] Sum of grad norms of most recent batch: 1.439133
2019-04-08 01:24:12,844 [INFO] ---------------------------------
2019-04-08 01:24:34,131 [INFO] ---------------------------------
2019-04-08 01:24:34,132 [INFO] Summary:
2019-04-08 01:24:34,133 [INFO] Batch 47000, worst loss 0.096935 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 01:24:34,133 [INFO] Regularization: 925.524231 * 0.0000000100 = 0.0000092552 loss
2019-04-08 01:24:34,134 [INFO] unfolding 0, single step 47001
2019-04-08 01:24:34,134 [INFO] Sum of grad norms of most recent batch: 1.984933
2019-04-08 01:24:34,135 [INFO] ---------------------------------
2019-04-08 01:24:55,835 [INFO] ---------------------------------
2019-04-08 01:24:55,836 [INFO] Summary:
2019-04-08 01:24:55,836 [INFO] Batch 48000, worst loss 0.061026 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 01:24:55,837 [INFO] Regularization: 918.307251 * 0.0000000100 = 0.0000091831 loss
2019-04-08 01:24:55,837 [INFO] unfolding 0, single step 48001
2019-04-08 01:24:55,838 [INFO] Sum of grad norms of most recent batch: 0.329443
2019-04-08 01:24:55,838 [INFO] ---------------------------------
2019-04-08 01:25:17,662 [INFO] ---------------------------------
2019-04-08 01:25:17,663 [INFO] Summary:
2019-04-08 01:25:17,665 [INFO] Batch 49000, worst loss 0.021184 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 01:25:17,666 [INFO] Regularization: 913.693359 * 0.0000000100 = 0.0000091369 loss
2019-04-08 01:25:17,666 [INFO] unfolding 0, single step 49001
2019-04-08 01:25:17,667 [INFO] Sum of grad norms of most recent batch: 1.278733
2019-04-08 01:25:17,668 [INFO] ---------------------------------
2019-04-08 01:25:39,947 [INFO] ---------------------------------
2019-04-08 01:25:39,948 [INFO] Summary:
2019-04-08 01:25:39,948 [INFO] Batch 50000, worst loss 0.143289 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 01:25:39,949 [INFO] Regularization: 910.511169 * 0.0000000100 = 0.0000091051 loss
2019-04-08 01:25:39,949 [INFO] unfolding 0, single step 50001
2019-04-08 01:25:39,950 [INFO] Sum of grad norms of most recent batch: 0.242520
2019-04-08 01:25:39,950 [INFO] ---------------------------------
2019-04-08 01:26:17,371 [INFO] ---------------------------------
2019-04-08 01:26:17,372 [INFO] Evaluation:
2019-04-08 01:26:17,373 [INFO] Batch 50000, worst loss 0.108445 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:26:17,373 [INFO] ---------------------------------
2019-04-08 01:26:39,121 [INFO] ---------------------------------
2019-04-08 01:26:39,122 [INFO] Summary:
2019-04-08 01:26:39,123 [INFO] Batch 51000, worst loss 0.022157 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 01:26:39,124 [INFO] Regularization: 905.893066 * 0.0000000100 = 0.0000090589 loss
2019-04-08 01:26:39,124 [INFO] unfolding 0, single step 51001
2019-04-08 01:26:39,125 [INFO] Sum of grad norms of most recent batch: 0.453229
2019-04-08 01:26:39,126 [INFO] ---------------------------------
2019-04-08 01:27:00,424 [INFO] ---------------------------------
2019-04-08 01:27:00,425 [INFO] Summary:
2019-04-08 01:27:00,426 [INFO] Batch 52000, worst loss 0.054945 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 01:27:00,427 [INFO] Regularization: 905.600952 * 0.0000000100 = 0.0000090560 loss
2019-04-08 01:27:00,427 [INFO] unfolding 0, single step 52001
2019-04-08 01:27:00,428 [INFO] Sum of grad norms of most recent batch: 0.103978
2019-04-08 01:27:00,428 [INFO] ---------------------------------
2019-04-08 01:27:22,131 [INFO] ---------------------------------
2019-04-08 01:27:22,132 [INFO] Summary:
2019-04-08 01:27:22,133 [INFO] Batch 53000, worst loss 0.061253 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 01:27:22,133 [INFO] Regularization: 898.964294 * 0.0000000100 = 0.0000089896 loss
2019-04-08 01:27:22,133 [INFO] unfolding 0, single step 53001
2019-04-08 01:27:22,134 [INFO] Sum of grad norms of most recent batch: 0.170871
2019-04-08 01:27:22,135 [INFO] ---------------------------------
2019-04-08 01:27:42,962 [INFO] ---------------------------------
2019-04-08 01:27:42,963 [INFO] Summary:
2019-04-08 01:27:42,964 [INFO] Batch 54000, worst loss 0.097067 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 01:27:42,965 [INFO] Regularization: 898.039429 * 0.0000000100 = 0.0000089804 loss
2019-04-08 01:27:42,965 [INFO] unfolding 0, single step 54001
2019-04-08 01:27:42,965 [INFO] Sum of grad norms of most recent batch: 0.162217
2019-04-08 01:27:42,966 [INFO] ---------------------------------
2019-04-08 01:28:04,257 [INFO] ---------------------------------
2019-04-08 01:28:04,258 [INFO] Summary:
2019-04-08 01:28:04,258 [INFO] Batch 55000, worst loss 0.097064 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 01:28:04,259 [INFO] Regularization: 895.431885 * 0.0000000100 = 0.0000089543 loss
2019-04-08 01:28:04,259 [INFO] unfolding 0, single step 55001
2019-04-08 01:28:04,260 [INFO] Sum of grad norms of most recent batch: 0.089528
2019-04-08 01:28:04,260 [INFO] ---------------------------------
2019-04-08 01:28:25,540 [INFO] ---------------------------------
2019-04-08 01:28:25,541 [INFO] Summary:
2019-04-08 01:28:25,542 [INFO] Batch 56000, worst loss 0.058154 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 01:28:25,543 [INFO] Regularization: 892.371704 * 0.0000000100 = 0.0000089237 loss
2019-04-08 01:28:25,543 [INFO] unfolding 0, single step 56001
2019-04-08 01:28:25,544 [INFO] Sum of grad norms of most recent batch: 0.268194
2019-04-08 01:28:25,544 [INFO] ---------------------------------
2019-04-08 01:28:47,310 [INFO] ---------------------------------
2019-04-08 01:28:47,311 [INFO] Summary:
2019-04-08 01:28:47,312 [INFO] Batch 57000, worst loss 0.067378 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 01:28:47,312 [INFO] Regularization: 888.320251 * 0.0000000100 = 0.0000088832 loss
2019-04-08 01:28:47,312 [INFO] unfolding 0, single step 57001
2019-04-08 01:28:47,313 [INFO] Sum of grad norms of most recent batch: 0.228366
2019-04-08 01:28:47,314 [INFO] ---------------------------------
2019-04-08 01:29:08,966 [INFO] ---------------------------------
2019-04-08 01:29:08,967 [INFO] Summary:
2019-04-08 01:29:08,967 [INFO] Batch 58000, worst loss 0.130848 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 01:29:08,968 [INFO] Regularization: 886.673828 * 0.0000000100 = 0.0000088667 loss
2019-04-08 01:29:08,968 [INFO] unfolding 0, single step 58001
2019-04-08 01:29:08,969 [INFO] Sum of grad norms of most recent batch: 0.245301
2019-04-08 01:29:08,969 [INFO] ---------------------------------
2019-04-08 01:29:30,480 [INFO] ---------------------------------
2019-04-08 01:29:30,481 [INFO] Summary:
2019-04-08 01:29:30,482 [INFO] Batch 59000, worst loss 0.023002 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 01:29:30,483 [INFO] Regularization: 881.308289 * 0.0000000100 = 0.0000088131 loss
2019-04-08 01:29:30,483 [INFO] unfolding 0, single step 59001
2019-04-08 01:29:30,484 [INFO] Sum of grad norms of most recent batch: 0.196592
2019-04-08 01:29:30,484 [INFO] ---------------------------------
2019-04-08 01:29:52,151 [INFO] ---------------------------------
2019-04-08 01:29:52,152 [INFO] Summary:
2019-04-08 01:29:52,152 [INFO] Batch 60000, worst loss 0.117203 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 01:29:52,153 [INFO] Regularization: 880.440979 * 0.0000000100 = 0.0000088044 loss
2019-04-08 01:29:52,153 [INFO] unfolding 0, single step 60001
2019-04-08 01:29:52,154 [INFO] Sum of grad norms of most recent batch: 0.346402
2019-04-08 01:29:52,154 [INFO] ---------------------------------
2019-04-08 01:30:29,550 [INFO] ---------------------------------
2019-04-08 01:30:29,551 [INFO] Evaluation:
2019-04-08 01:30:29,552 [INFO] Batch 60000, worst loss 0.121304 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:30:29,552 [INFO] ---------------------------------
2019-04-08 01:30:51,033 [INFO] ---------------------------------
2019-04-08 01:30:51,035 [INFO] Summary:
2019-04-08 01:30:51,035 [INFO] Batch 61000, worst loss 0.066042 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 01:30:51,036 [INFO] Regularization: 879.652832 * 0.0000000100 = 0.0000087965 loss
2019-04-08 01:30:51,036 [INFO] unfolding 0, single step 61001
2019-04-08 01:30:51,036 [INFO] Sum of grad norms of most recent batch: 0.361094
2019-04-08 01:30:51,037 [INFO] ---------------------------------
2019-04-08 01:31:12,253 [INFO] ---------------------------------
2019-04-08 01:31:12,254 [INFO] Summary:
2019-04-08 01:31:12,255 [INFO] Batch 62000, worst loss 0.036854 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 01:31:12,256 [INFO] Regularization: 874.794800 * 0.0000000100 = 0.0000087479 loss
2019-04-08 01:31:12,256 [INFO] unfolding 0, single step 62001
2019-04-08 01:31:12,257 [INFO] Sum of grad norms of most recent batch: 0.085805
2019-04-08 01:31:12,258 [INFO] ---------------------------------
2019-04-08 01:31:33,581 [INFO] ---------------------------------
2019-04-08 01:31:33,582 [INFO] Summary:
2019-04-08 01:31:33,583 [INFO] Batch 63000, worst loss 0.134900 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 01:31:33,583 [INFO] Regularization: 874.509644 * 0.0000000100 = 0.0000087451 loss
2019-04-08 01:31:33,584 [INFO] unfolding 0, single step 63001
2019-04-08 01:31:33,584 [INFO] Sum of grad norms of most recent batch: 0.327889
2019-04-08 01:31:33,585 [INFO] ---------------------------------
2019-04-08 01:31:55,029 [INFO] ---------------------------------
2019-04-08 01:31:55,030 [INFO] Summary:
2019-04-08 01:31:55,030 [INFO] Batch 64000, worst loss 0.054355 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 01:31:55,031 [INFO] Regularization: 872.056885 * 0.0000000100 = 0.0000087206 loss
2019-04-08 01:31:55,031 [INFO] unfolding 0, single step 64001
2019-04-08 01:31:55,032 [INFO] Sum of grad norms of most recent batch: 0.419170
2019-04-08 01:31:55,032 [INFO] ---------------------------------
2019-04-08 01:32:16,554 [INFO] ---------------------------------
2019-04-08 01:32:16,555 [INFO] Summary:
2019-04-08 01:32:16,555 [INFO] Batch 65000, worst loss 0.060919 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 01:32:16,557 [INFO] Regularization: 870.674255 * 0.0000000100 = 0.0000087067 loss
2019-04-08 01:32:16,557 [INFO] unfolding 0, single step 65001
2019-04-08 01:32:16,558 [INFO] Sum of grad norms of most recent batch: 0.269770
2019-04-08 01:32:16,559 [INFO] ---------------------------------
2019-04-08 01:32:38,912 [INFO] ---------------------------------
2019-04-08 01:32:38,913 [INFO] Summary:
2019-04-08 01:32:38,913 [INFO] Batch 66000, worst loss 0.077275 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 01:32:38,914 [INFO] Regularization: 869.366211 * 0.0000000100 = 0.0000086937 loss
2019-04-08 01:32:38,914 [INFO] unfolding 0, single step 66001
2019-04-08 01:32:38,915 [INFO] Sum of grad norms of most recent batch: 0.063181
2019-04-08 01:32:38,915 [INFO] ---------------------------------
2019-04-08 01:32:59,902 [INFO] ---------------------------------
2019-04-08 01:32:59,903 [INFO] Summary:
2019-04-08 01:32:59,904 [INFO] Batch 67000, worst loss 0.076757 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 01:32:59,904 [INFO] Regularization: 868.336304 * 0.0000000100 = 0.0000086834 loss
2019-04-08 01:32:59,905 [INFO] unfolding 0, single step 67001
2019-04-08 01:32:59,905 [INFO] Sum of grad norms of most recent batch: 0.104318
2019-04-08 01:32:59,906 [INFO] ---------------------------------
2019-04-08 01:33:21,415 [INFO] ---------------------------------
2019-04-08 01:33:21,416 [INFO] Summary:
2019-04-08 01:33:21,417 [INFO] Batch 68000, worst loss 0.116183 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 01:33:21,417 [INFO] Regularization: 867.051697 * 0.0000000100 = 0.0000086705 loss
2019-04-08 01:33:21,418 [INFO] unfolding 0, single step 68001
2019-04-08 01:33:21,418 [INFO] Sum of grad norms of most recent batch: 0.160993
2019-04-08 01:33:21,419 [INFO] ---------------------------------
2019-04-08 01:33:42,721 [INFO] ---------------------------------
2019-04-08 01:33:42,722 [INFO] Summary:
2019-04-08 01:33:42,722 [INFO] Batch 69000, worst loss 0.064081 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 01:33:42,723 [INFO] Regularization: 866.878662 * 0.0000000100 = 0.0000086688 loss
2019-04-08 01:33:42,723 [INFO] unfolding 0, single step 69001
2019-04-08 01:33:42,724 [INFO] Sum of grad norms of most recent batch: 0.096463
2019-04-08 01:33:42,725 [INFO] ---------------------------------
2019-04-08 01:34:03,499 [INFO] ---------------------------------
2019-04-08 01:34:03,500 [INFO] Summary:
2019-04-08 01:34:03,500 [INFO] Batch 70000, worst loss 0.031173 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 01:34:03,501 [INFO] Regularization: 864.937134 * 0.0000000100 = 0.0000086494 loss
2019-04-08 01:34:03,501 [INFO] unfolding 0, single step 70001
2019-04-08 01:34:03,502 [INFO] Sum of grad norms of most recent batch: 0.069345
2019-04-08 01:34:03,502 [INFO] ---------------------------------
2019-04-08 01:34:40,987 [INFO] ---------------------------------
2019-04-08 01:34:40,988 [INFO] Evaluation:
2019-04-08 01:34:40,989 [INFO] Batch 70000, worst loss 0.225647 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:34:40,989 [INFO] ---------------------------------
2019-04-08 01:35:02,415 [INFO] ---------------------------------
2019-04-08 01:35:02,416 [INFO] Summary:
2019-04-08 01:35:02,416 [INFO] Batch 71000, worst loss 0.050055 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 01:35:02,417 [INFO] Regularization: 863.102722 * 0.0000000100 = 0.0000086310 loss
2019-04-08 01:35:02,417 [INFO] unfolding 0, single step 71001
2019-04-08 01:35:02,418 [INFO] Sum of grad norms of most recent batch: 0.137281
2019-04-08 01:35:02,418 [INFO] ---------------------------------
2019-04-08 01:35:23,384 [INFO] ---------------------------------
2019-04-08 01:35:23,385 [INFO] Summary:
2019-04-08 01:35:23,386 [INFO] Batch 72000, worst loss 0.070788 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 01:35:23,386 [INFO] Regularization: 862.158020 * 0.0000000100 = 0.0000086216 loss
2019-04-08 01:35:23,387 [INFO] unfolding 0, single step 72001
2019-04-08 01:35:23,387 [INFO] Sum of grad norms of most recent batch: 0.074695
2019-04-08 01:35:23,388 [INFO] ---------------------------------
2019-04-08 01:35:44,426 [INFO] ---------------------------------
2019-04-08 01:35:44,428 [INFO] Summary:
2019-04-08 01:35:44,428 [INFO] Batch 73000, worst loss 0.066140 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 01:35:44,429 [INFO] Regularization: 861.759033 * 0.0000000100 = 0.0000086176 loss
2019-04-08 01:35:44,429 [INFO] unfolding 0, single step 73001
2019-04-08 01:35:44,430 [INFO] Sum of grad norms of most recent batch: 0.059663
2019-04-08 01:35:44,431 [INFO] ---------------------------------
2019-04-08 01:36:05,700 [INFO] ---------------------------------
2019-04-08 01:36:05,701 [INFO] Summary:
2019-04-08 01:36:05,702 [INFO] Batch 74000, worst loss 0.029644 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 01:36:05,702 [INFO] Regularization: 861.276245 * 0.0000000100 = 0.0000086128 loss
2019-04-08 01:36:05,703 [INFO] unfolding 0, single step 74001
2019-04-08 01:36:05,703 [INFO] Sum of grad norms of most recent batch: 0.079759
2019-04-08 01:36:05,704 [INFO] ---------------------------------
2019-04-08 01:36:27,448 [INFO] ---------------------------------
2019-04-08 01:36:27,449 [INFO] Summary:
2019-04-08 01:36:27,450 [INFO] Batch 75000, worst loss 0.013752 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 01:36:27,450 [INFO] Regularization: 860.167786 * 0.0000000100 = 0.0000086017 loss
2019-04-08 01:36:27,451 [INFO] unfolding 0, single step 75001
2019-04-08 01:36:27,451 [INFO] Sum of grad norms of most recent batch: 0.071598
2019-04-08 01:36:27,452 [INFO] ---------------------------------
2019-04-08 01:36:48,571 [INFO] ---------------------------------
2019-04-08 01:36:48,572 [INFO] Summary:
2019-04-08 01:36:48,573 [INFO] Batch 76000, worst loss 0.035653 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 01:36:48,574 [INFO] Regularization: 859.654114 * 0.0000000100 = 0.0000085965 loss
2019-04-08 01:36:48,574 [INFO] unfolding 0, single step 76001
2019-04-08 01:36:48,575 [INFO] Sum of grad norms of most recent batch: 0.084110
2019-04-08 01:36:48,576 [INFO] ---------------------------------
2019-04-08 01:37:09,485 [INFO] ---------------------------------
2019-04-08 01:37:09,486 [INFO] Summary:
2019-04-08 01:37:09,487 [INFO] Batch 77000, worst loss 0.023133 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 01:37:09,487 [INFO] Regularization: 859.177734 * 0.0000000100 = 0.0000085918 loss
2019-04-08 01:37:09,488 [INFO] unfolding 0, single step 77001
2019-04-08 01:37:09,488 [INFO] Sum of grad norms of most recent batch: 0.083924
2019-04-08 01:37:09,489 [INFO] ---------------------------------
2019-04-08 01:37:30,974 [INFO] ---------------------------------
2019-04-08 01:37:30,975 [INFO] Summary:
2019-04-08 01:37:30,976 [INFO] Batch 78000, worst loss 0.070381 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 01:37:30,976 [INFO] Regularization: 858.551514 * 0.0000000100 = 0.0000085855 loss
2019-04-08 01:37:30,977 [INFO] unfolding 0, single step 78001
2019-04-08 01:37:30,977 [INFO] Sum of grad norms of most recent batch: 0.030775
2019-04-08 01:37:30,978 [INFO] ---------------------------------
2019-04-08 01:37:52,794 [INFO] ---------------------------------
2019-04-08 01:37:52,795 [INFO] Summary:
2019-04-08 01:37:52,795 [INFO] Batch 79000, worst loss 0.015798 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 01:37:52,796 [INFO] Regularization: 857.755127 * 0.0000000100 = 0.0000085776 loss
2019-04-08 01:37:52,796 [INFO] unfolding 0, single step 79001
2019-04-08 01:37:52,797 [INFO] Sum of grad norms of most recent batch: 0.118444
2019-04-08 01:37:52,797 [INFO] ---------------------------------
2019-04-08 01:38:13,790 [INFO] ---------------------------------
2019-04-08 01:38:13,791 [INFO] Summary:
2019-04-08 01:38:13,791 [INFO] Batch 80000, worst loss 0.051299 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 01:38:13,792 [INFO] Regularization: 857.935608 * 0.0000000100 = 0.0000085794 loss
2019-04-08 01:38:13,792 [INFO] unfolding 0, single step 80001
2019-04-08 01:38:13,793 [INFO] Sum of grad norms of most recent batch: 0.278185
2019-04-08 01:38:13,793 [INFO] ---------------------------------
2019-04-08 01:38:51,308 [INFO] ---------------------------------
2019-04-08 01:38:51,309 [INFO] Evaluation:
2019-04-08 01:38:51,309 [INFO] Batch 80000, worst loss 0.105879 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:38:51,310 [INFO] ---------------------------------
2019-04-08 01:39:12,438 [INFO] ---------------------------------
2019-04-08 01:39:12,439 [INFO] Summary:
2019-04-08 01:39:12,440 [INFO] Batch 81000, worst loss 0.044435 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 01:39:12,440 [INFO] Regularization: 857.077637 * 0.0000000100 = 0.0000085708 loss
2019-04-08 01:39:12,441 [INFO] unfolding 0, single step 81001
2019-04-08 01:39:12,441 [INFO] Sum of grad norms of most recent batch: 0.208226
2019-04-08 01:39:12,442 [INFO] ---------------------------------
2019-04-08 01:39:33,473 [INFO] ---------------------------------
2019-04-08 01:39:33,474 [INFO] Summary:
2019-04-08 01:39:33,475 [INFO] Batch 82000, worst loss 0.048389 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 01:39:33,476 [INFO] Regularization: 856.461365 * 0.0000000100 = 0.0000085646 loss
2019-04-08 01:39:33,476 [INFO] unfolding 0, single step 82001
2019-04-08 01:39:33,477 [INFO] Sum of grad norms of most recent batch: 0.054467
2019-04-08 01:39:33,477 [INFO] ---------------------------------
2019-04-08 01:39:55,096 [INFO] ---------------------------------
2019-04-08 01:39:55,097 [INFO] Summary:
2019-04-08 01:39:55,097 [INFO] Batch 83000, worst loss 0.039194 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 01:39:55,098 [INFO] Regularization: 856.140076 * 0.0000000100 = 0.0000085614 loss
2019-04-08 01:39:55,099 [INFO] unfolding 0, single step 83001
2019-04-08 01:39:55,099 [INFO] Sum of grad norms of most recent batch: 0.033580
2019-04-08 01:39:55,100 [INFO] ---------------------------------
2019-04-08 01:40:16,066 [INFO] ---------------------------------
2019-04-08 01:40:16,067 [INFO] Summary:
2019-04-08 01:40:16,067 [INFO] Batch 84000, worst loss 0.076510 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 01:40:16,068 [INFO] Regularization: 855.588318 * 0.0000000100 = 0.0000085559 loss
2019-04-08 01:40:16,068 [INFO] unfolding 0, single step 84001
2019-04-08 01:40:16,069 [INFO] Sum of grad norms of most recent batch: 0.023308
2019-04-08 01:40:16,069 [INFO] ---------------------------------
2019-04-08 01:40:36,647 [INFO] ---------------------------------
2019-04-08 01:40:36,648 [INFO] Summary:
2019-04-08 01:40:36,648 [INFO] Batch 85000, worst loss 0.022317 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 01:40:36,649 [INFO] Regularization: 855.310547 * 0.0000000100 = 0.0000085531 loss
2019-04-08 01:40:36,649 [INFO] unfolding 0, single step 85001
2019-04-08 01:40:36,650 [INFO] Sum of grad norms of most recent batch: 0.099367
2019-04-08 01:40:36,650 [INFO] ---------------------------------
2019-04-08 01:40:57,640 [INFO] ---------------------------------
2019-04-08 01:40:57,641 [INFO] Summary:
2019-04-08 01:40:57,642 [INFO] Batch 86000, worst loss 0.073796 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 01:40:57,643 [INFO] Regularization: 854.713257 * 0.0000000100 = 0.0000085471 loss
2019-04-08 01:40:57,644 [INFO] unfolding 0, single step 86001
2019-04-08 01:40:57,645 [INFO] Sum of grad norms of most recent batch: 0.070545
2019-04-08 01:40:57,646 [INFO] ---------------------------------
2019-04-08 01:41:19,090 [INFO] ---------------------------------
2019-04-08 01:41:19,091 [INFO] Summary:
2019-04-08 01:41:19,092 [INFO] Batch 87000, worst loss 0.058875 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 01:41:19,092 [INFO] Regularization: 854.494812 * 0.0000000100 = 0.0000085449 loss
2019-04-08 01:41:19,093 [INFO] unfolding 0, single step 87001
2019-04-08 01:41:19,093 [INFO] Sum of grad norms of most recent batch: 0.068091
2019-04-08 01:41:19,094 [INFO] ---------------------------------
2019-04-08 01:41:39,952 [INFO] ---------------------------------
2019-04-08 01:41:39,953 [INFO] Summary:
2019-04-08 01:41:39,953 [INFO] Batch 88000, worst loss 0.064332 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 01:41:39,954 [INFO] Regularization: 854.431335 * 0.0000000100 = 0.0000085443 loss
2019-04-08 01:41:39,954 [INFO] unfolding 0, single step 88001
2019-04-08 01:41:39,955 [INFO] Sum of grad norms of most recent batch: 0.070598
2019-04-08 01:41:39,956 [INFO] ---------------------------------
2019-04-08 01:42:00,868 [INFO] ---------------------------------
2019-04-08 01:42:00,869 [INFO] Summary:
2019-04-08 01:42:00,870 [INFO] Batch 89000, worst loss 0.042793 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 01:42:00,870 [INFO] Regularization: 854.094849 * 0.0000000100 = 0.0000085409 loss
2019-04-08 01:42:00,871 [INFO] unfolding 0, single step 89001
2019-04-08 01:42:00,871 [INFO] Sum of grad norms of most recent batch: 0.049520
2019-04-08 01:42:00,872 [INFO] ---------------------------------
2019-04-08 01:42:21,896 [INFO] ---------------------------------
2019-04-08 01:42:21,897 [INFO] Summary:
2019-04-08 01:42:21,898 [INFO] Batch 90000, worst loss 0.084195 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 01:42:21,898 [INFO] Regularization: 853.580872 * 0.0000000100 = 0.0000085358 loss
2019-04-08 01:42:21,899 [INFO] unfolding 0, single step 90001
2019-04-08 01:42:21,899 [INFO] Sum of grad norms of most recent batch: 1.316281
2019-04-08 01:42:21,900 [INFO] ---------------------------------
2019-04-08 01:42:59,399 [INFO] ---------------------------------
2019-04-08 01:42:59,400 [INFO] Evaluation:
2019-04-08 01:42:59,401 [INFO] Batch 90000, worst loss 0.072219 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:42:59,402 [INFO] ---------------------------------
2019-04-08 01:43:20,727 [INFO] ---------------------------------
2019-04-08 01:43:20,728 [INFO] Summary:
2019-04-08 01:43:20,728 [INFO] Batch 91000, worst loss 0.038572 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 01:43:20,729 [INFO] Regularization: 853.222351 * 0.0000000100 = 0.0000085322 loss
2019-04-08 01:43:20,729 [INFO] unfolding 0, single step 91001
2019-04-08 01:43:20,730 [INFO] Sum of grad norms of most recent batch: 0.159139
2019-04-08 01:43:20,730 [INFO] ---------------------------------
2019-04-08 01:43:41,888 [INFO] ---------------------------------
2019-04-08 01:43:41,889 [INFO] Summary:
2019-04-08 01:43:41,890 [INFO] Batch 92000, worst loss 0.038453 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 01:43:41,890 [INFO] Regularization: 853.065918 * 0.0000000100 = 0.0000085307 loss
2019-04-08 01:43:41,891 [INFO] unfolding 0, single step 92001
2019-04-08 01:43:41,891 [INFO] Sum of grad norms of most recent batch: 0.054850
2019-04-08 01:43:41,892 [INFO] ---------------------------------
2019-04-08 01:44:03,558 [INFO] ---------------------------------
2019-04-08 01:44:03,559 [INFO] Summary:
2019-04-08 01:44:03,560 [INFO] Batch 93000, worst loss 0.041655 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 01:44:03,560 [INFO] Regularization: 852.952209 * 0.0000000100 = 0.0000085295 loss
2019-04-08 01:44:03,560 [INFO] unfolding 0, single step 93001
2019-04-08 01:44:03,561 [INFO] Sum of grad norms of most recent batch: 0.034397
2019-04-08 01:44:03,561 [INFO] ---------------------------------
2019-04-08 01:44:25,123 [INFO] ---------------------------------
2019-04-08 01:44:25,124 [INFO] Summary:
2019-04-08 01:44:25,125 [INFO] Batch 94000, worst loss 0.051748 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 01:44:25,125 [INFO] Regularization: 852.716309 * 0.0000000100 = 0.0000085272 loss
2019-04-08 01:44:25,126 [INFO] unfolding 0, single step 94001
2019-04-08 01:44:25,127 [INFO] Sum of grad norms of most recent batch: 0.126632
2019-04-08 01:44:25,128 [INFO] ---------------------------------
2019-04-08 01:44:46,344 [INFO] ---------------------------------
2019-04-08 01:44:46,345 [INFO] Summary:
2019-04-08 01:44:46,346 [INFO] Batch 95000, worst loss 0.094208 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 01:44:46,346 [INFO] Regularization: 852.406738 * 0.0000000100 = 0.0000085241 loss
2019-04-08 01:44:46,346 [INFO] unfolding 0, single step 95001
2019-04-08 01:44:46,347 [INFO] Sum of grad norms of most recent batch: 0.071160
2019-04-08 01:44:46,347 [INFO] ---------------------------------
2019-04-08 01:45:07,640 [INFO] ---------------------------------
2019-04-08 01:45:07,641 [INFO] Summary:
2019-04-08 01:45:07,641 [INFO] Batch 96000, worst loss 0.094050 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 01:45:07,642 [INFO] Regularization: 852.157043 * 0.0000000100 = 0.0000085216 loss
2019-04-08 01:45:07,642 [INFO] unfolding 0, single step 96001
2019-04-08 01:45:07,643 [INFO] Sum of grad norms of most recent batch: 0.062138
2019-04-08 01:45:07,644 [INFO] ---------------------------------
2019-04-08 01:45:28,933 [INFO] ---------------------------------
2019-04-08 01:45:28,934 [INFO] Summary:
2019-04-08 01:45:28,935 [INFO] Batch 97000, worst loss 0.103170 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 01:45:28,935 [INFO] Regularization: 852.028076 * 0.0000000100 = 0.0000085203 loss
2019-04-08 01:45:28,936 [INFO] unfolding 0, single step 97001
2019-04-08 01:45:28,936 [INFO] Sum of grad norms of most recent batch: 0.076133
2019-04-08 01:45:28,937 [INFO] ---------------------------------
2019-04-08 01:45:50,589 [INFO] ---------------------------------
2019-04-08 01:45:50,590 [INFO] Summary:
2019-04-08 01:45:50,591 [INFO] Batch 98000, worst loss 0.057400 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 01:45:50,591 [INFO] Regularization: 851.747986 * 0.0000000100 = 0.0000085175 loss
2019-04-08 01:45:50,592 [INFO] unfolding 0, single step 98001
2019-04-08 01:45:50,592 [INFO] Sum of grad norms of most recent batch: 0.070711
2019-04-08 01:45:50,593 [INFO] ---------------------------------
2019-04-08 01:46:11,535 [INFO] ---------------------------------
2019-04-08 01:46:11,536 [INFO] Summary:
2019-04-08 01:46:11,537 [INFO] Batch 99000, worst loss 0.073687 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 01:46:11,537 [INFO] Regularization: 851.672485 * 0.0000000100 = 0.0000085167 loss
2019-04-08 01:46:11,538 [INFO] unfolding 0, single step 99001
2019-04-08 01:46:11,538 [INFO] Sum of grad norms of most recent batch: 0.025319
2019-04-08 01:46:11,539 [INFO] ---------------------------------
2019-04-08 01:46:31,880 [INFO] ---------------------------------
2019-04-08 01:46:31,881 [INFO] Summary:
2019-04-08 01:46:31,881 [INFO] Batch 100000, worst loss 0.062001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 01:46:31,882 [INFO] Regularization: 851.334961 * 0.0000000100 = 0.0000085133 loss
2019-04-08 01:46:31,882 [INFO] unfolding 0, single step 100001
2019-04-08 01:46:31,883 [INFO] Sum of grad norms of most recent batch: 0.076994
2019-04-08 01:46:31,883 [INFO] ---------------------------------
2019-04-08 01:47:09,123 [INFO] ---------------------------------
2019-04-08 01:47:09,124 [INFO] Evaluation:
2019-04-08 01:47:09,125 [INFO] Batch 100000, worst loss 0.139003 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:47:09,126 [INFO] ---------------------------------
2019-04-08 01:47:30,255 [INFO] ---------------------------------
2019-04-08 01:47:30,257 [INFO] Summary:
2019-04-08 01:47:30,257 [INFO] Batch 101000, worst loss 0.048926 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 01:47:30,258 [INFO] Regularization: 851.176575 * 0.0000000100 = 0.0000085118 loss
2019-04-08 01:47:30,258 [INFO] unfolding 0, single step 101001
2019-04-08 01:47:30,259 [INFO] Sum of grad norms of most recent batch: 0.038134
2019-04-08 01:47:30,260 [INFO] ---------------------------------
2019-04-08 01:47:51,700 [INFO] ---------------------------------
2019-04-08 01:47:51,701 [INFO] Summary:
2019-04-08 01:47:51,702 [INFO] Batch 102000, worst loss 0.054735 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 01:47:51,702 [INFO] Regularization: 851.023926 * 0.0000000100 = 0.0000085102 loss
2019-04-08 01:47:51,703 [INFO] unfolding 0, single step 102001
2019-04-08 01:47:51,703 [INFO] Sum of grad norms of most recent batch: 0.064294
2019-04-08 01:47:51,704 [INFO] ---------------------------------
2019-04-08 01:48:13,078 [INFO] ---------------------------------
2019-04-08 01:48:13,079 [INFO] Summary:
2019-04-08 01:48:13,080 [INFO] Batch 103000, worst loss 0.060531 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 01:48:13,081 [INFO] Regularization: 850.945618 * 0.0000000100 = 0.0000085095 loss
2019-04-08 01:48:13,081 [INFO] unfolding 0, single step 103001
2019-04-08 01:48:13,082 [INFO] Sum of grad norms of most recent batch: 0.032541
2019-04-08 01:48:13,082 [INFO] ---------------------------------
2019-04-08 01:48:34,537 [INFO] ---------------------------------
2019-04-08 01:48:34,538 [INFO] Summary:
2019-04-08 01:48:34,539 [INFO] Batch 104000, worst loss 0.092047 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 01:48:34,539 [INFO] Regularization: 850.791870 * 0.0000000100 = 0.0000085079 loss
2019-04-08 01:48:34,540 [INFO] unfolding 0, single step 104001
2019-04-08 01:48:34,540 [INFO] Sum of grad norms of most recent batch: 0.041382
2019-04-08 01:48:34,541 [INFO] ---------------------------------
2019-04-08 01:48:56,043 [INFO] ---------------------------------
2019-04-08 01:48:56,044 [INFO] Summary:
2019-04-08 01:48:56,045 [INFO] Batch 105000, worst loss 0.017040 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 01:48:56,046 [INFO] Regularization: 850.622314 * 0.0000000100 = 0.0000085062 loss
2019-04-08 01:48:56,046 [INFO] unfolding 0, single step 105001
2019-04-08 01:48:56,046 [INFO] Sum of grad norms of most recent batch: 0.031250
2019-04-08 01:48:56,047 [INFO] ---------------------------------
2019-04-08 01:49:16,526 [INFO] ---------------------------------
2019-04-08 01:49:16,527 [INFO] Summary:
2019-04-08 01:49:16,527 [INFO] Batch 106000, worst loss 0.081827 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 01:49:16,528 [INFO] Regularization: 850.515259 * 0.0000000100 = 0.0000085052 loss
2019-04-08 01:49:16,528 [INFO] unfolding 0, single step 106001
2019-04-08 01:49:16,529 [INFO] Sum of grad norms of most recent batch: 0.667749
2019-04-08 01:49:16,529 [INFO] ---------------------------------
2019-04-08 01:49:37,252 [INFO] ---------------------------------
2019-04-08 01:49:37,253 [INFO] Summary:
2019-04-08 01:49:37,254 [INFO] Batch 107000, worst loss 0.042493 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 01:49:37,254 [INFO] Regularization: 850.383911 * 0.0000000100 = 0.0000085038 loss
2019-04-08 01:49:37,255 [INFO] unfolding 0, single step 107001
2019-04-08 01:49:37,256 [INFO] Sum of grad norms of most recent batch: 0.029404
2019-04-08 01:49:37,257 [INFO] ---------------------------------
2019-04-08 01:49:58,274 [INFO] ---------------------------------
2019-04-08 01:49:58,275 [INFO] Summary:
2019-04-08 01:49:58,276 [INFO] Batch 108000, worst loss 0.092129 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 01:49:58,276 [INFO] Regularization: 850.445129 * 0.0000000100 = 0.0000085045 loss
2019-04-08 01:49:58,276 [INFO] unfolding 0, single step 108001
2019-04-08 01:49:58,277 [INFO] Sum of grad norms of most recent batch: 0.059029
2019-04-08 01:49:58,278 [INFO] ---------------------------------
2019-04-08 01:50:19,137 [INFO] ---------------------------------
2019-04-08 01:50:19,139 [INFO] Summary:
2019-04-08 01:50:19,139 [INFO] Batch 109000, worst loss 0.096418 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 01:50:19,140 [INFO] Regularization: 850.266052 * 0.0000000100 = 0.0000085027 loss
2019-04-08 01:50:19,140 [INFO] unfolding 0, single step 109001
2019-04-08 01:50:19,141 [INFO] Sum of grad norms of most recent batch: 0.022935
2019-04-08 01:50:19,142 [INFO] ---------------------------------
2019-04-08 01:50:40,111 [INFO] ---------------------------------
2019-04-08 01:50:40,112 [INFO] Summary:
2019-04-08 01:50:40,112 [INFO] Batch 110000, worst loss 0.039335 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 01:50:40,113 [INFO] Regularization: 850.155334 * 0.0000000100 = 0.0000085016 loss
2019-04-08 01:50:40,113 [INFO] unfolding 0, single step 110001
2019-04-08 01:50:40,114 [INFO] Sum of grad norms of most recent batch: 0.085821
2019-04-08 01:50:40,114 [INFO] ---------------------------------
2019-04-08 01:51:17,536 [INFO] ---------------------------------
2019-04-08 01:51:17,537 [INFO] Evaluation:
2019-04-08 01:51:17,538 [INFO] Batch 110000, worst loss 0.132875 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:51:17,539 [INFO] ---------------------------------
2019-04-08 01:51:39,276 [INFO] ---------------------------------
2019-04-08 01:51:39,277 [INFO] Summary:
2019-04-08 01:51:39,278 [INFO] Batch 111000, worst loss 0.041184 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 01:51:39,278 [INFO] Regularization: 850.057861 * 0.0000000100 = 0.0000085006 loss
2019-04-08 01:51:39,279 [INFO] unfolding 0, single step 111001
2019-04-08 01:51:39,280 [INFO] Sum of grad norms of most recent batch: 0.039148
2019-04-08 01:51:39,280 [INFO] ---------------------------------
2019-04-08 01:52:00,326 [INFO] ---------------------------------
2019-04-08 01:52:00,327 [INFO] Summary:
2019-04-08 01:52:00,327 [INFO] Batch 112000, worst loss 0.052650 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 01:52:00,328 [INFO] Regularization: 849.977966 * 0.0000000100 = 0.0000084998 loss
2019-04-08 01:52:00,328 [INFO] unfolding 0, single step 112001
2019-04-08 01:52:00,329 [INFO] Sum of grad norms of most recent batch: 0.020752
2019-04-08 01:52:00,329 [INFO] ---------------------------------
2019-04-08 01:52:21,666 [INFO] ---------------------------------
2019-04-08 01:52:21,667 [INFO] Summary:
2019-04-08 01:52:21,668 [INFO] Batch 113000, worst loss 0.052634 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 01:52:21,668 [INFO] Regularization: 849.935791 * 0.0000000100 = 0.0000084994 loss
2019-04-08 01:52:21,669 [INFO] unfolding 0, single step 113001
2019-04-08 01:52:21,669 [INFO] Sum of grad norms of most recent batch: 0.017903
2019-04-08 01:52:21,670 [INFO] ---------------------------------
2019-04-08 01:52:43,314 [INFO] ---------------------------------
2019-04-08 01:52:43,315 [INFO] Summary:
2019-04-08 01:52:43,315 [INFO] Batch 114000, worst loss 0.016383 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 01:52:43,316 [INFO] Regularization: 849.966064 * 0.0000000100 = 0.0000084997 loss
2019-04-08 01:52:43,316 [INFO] unfolding 0, single step 114001
2019-04-08 01:52:43,317 [INFO] Sum of grad norms of most recent batch: 0.026622
2019-04-08 01:52:43,317 [INFO] ---------------------------------
2019-04-08 01:53:05,340 [INFO] ---------------------------------
2019-04-08 01:53:05,341 [INFO] Summary:
2019-04-08 01:53:05,342 [INFO] Batch 115000, worst loss 0.052438 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 01:53:05,342 [INFO] Regularization: 849.847900 * 0.0000000100 = 0.0000084985 loss
2019-04-08 01:53:05,343 [INFO] unfolding 0, single step 115001
2019-04-08 01:53:05,343 [INFO] Sum of grad norms of most recent batch: 0.029394
2019-04-08 01:53:05,344 [INFO] ---------------------------------
2019-04-08 01:53:27,228 [INFO] ---------------------------------
2019-04-08 01:53:27,229 [INFO] Summary:
2019-04-08 01:53:27,229 [INFO] Batch 116000, worst loss 0.011598 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 01:53:27,230 [INFO] Regularization: 849.830811 * 0.0000000100 = 0.0000084983 loss
2019-04-08 01:53:27,230 [INFO] unfolding 0, single step 116001
2019-04-08 01:53:27,231 [INFO] Sum of grad norms of most recent batch: 0.034089
2019-04-08 01:53:27,231 [INFO] ---------------------------------
2019-04-08 01:53:48,335 [INFO] ---------------------------------
2019-04-08 01:53:48,337 [INFO] Summary:
2019-04-08 01:53:48,337 [INFO] Batch 117000, worst loss 0.039639 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 01:53:48,338 [INFO] Regularization: 849.816589 * 0.0000000100 = 0.0000084982 loss
2019-04-08 01:53:48,338 [INFO] unfolding 0, single step 117001
2019-04-08 01:53:48,338 [INFO] Sum of grad norms of most recent batch: 0.025507
2019-04-08 01:53:48,339 [INFO] ---------------------------------
2019-04-08 01:54:10,127 [INFO] ---------------------------------
2019-04-08 01:54:10,128 [INFO] Summary:
2019-04-08 01:54:10,128 [INFO] Batch 118000, worst loss 0.031625 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 01:54:10,129 [INFO] Regularization: 849.748596 * 0.0000000100 = 0.0000084975 loss
2019-04-08 01:54:10,129 [INFO] unfolding 0, single step 118001
2019-04-08 01:54:10,129 [INFO] Sum of grad norms of most recent batch: 0.042908
2019-04-08 01:54:10,130 [INFO] ---------------------------------
2019-04-08 01:54:31,610 [INFO] ---------------------------------
2019-04-08 01:54:31,611 [INFO] Summary:
2019-04-08 01:54:31,612 [INFO] Batch 119000, worst loss 0.031563 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 01:54:31,613 [INFO] Regularization: 849.668518 * 0.0000000100 = 0.0000084967 loss
2019-04-08 01:54:31,613 [INFO] unfolding 0, single step 119001
2019-04-08 01:54:31,614 [INFO] Sum of grad norms of most recent batch: 0.041167
2019-04-08 01:54:31,614 [INFO] ---------------------------------
2019-04-08 01:54:53,240 [INFO] ---------------------------------
2019-04-08 01:54:53,241 [INFO] Summary:
2019-04-08 01:54:53,242 [INFO] Batch 120000, worst loss 0.018124 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 01:54:53,242 [INFO] Regularization: 849.617310 * 0.0000000100 = 0.0000084962 loss
2019-04-08 01:54:53,242 [INFO] unfolding 0, single step 120001
2019-04-08 01:54:53,243 [INFO] Sum of grad norms of most recent batch: 0.024758
2019-04-08 01:54:53,244 [INFO] ---------------------------------
2019-04-08 01:55:30,622 [INFO] ---------------------------------
2019-04-08 01:55:30,623 [INFO] Evaluation:
2019-04-08 01:55:30,623 [INFO] Batch 120000, worst loss 0.143590 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:55:30,624 [INFO] ---------------------------------
2019-04-08 01:55:51,838 [INFO] ---------------------------------
2019-04-08 01:55:51,839 [INFO] Summary:
2019-04-08 01:55:51,840 [INFO] Batch 121000, worst loss 0.035084 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 01:55:51,840 [INFO] Regularization: 849.585327 * 0.0000000100 = 0.0000084959 loss
2019-04-08 01:55:51,841 [INFO] unfolding 0, single step 121001
2019-04-08 01:55:51,841 [INFO] Sum of grad norms of most recent batch: 0.085888
2019-04-08 01:55:51,842 [INFO] ---------------------------------
2019-04-08 01:56:13,281 [INFO] ---------------------------------
2019-04-08 01:56:13,282 [INFO] Summary:
2019-04-08 01:56:13,283 [INFO] Batch 122000, worst loss 0.054192 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 01:56:13,283 [INFO] Regularization: 849.528870 * 0.0000000100 = 0.0000084953 loss
2019-04-08 01:56:13,283 [INFO] unfolding 0, single step 122001
2019-04-08 01:56:13,284 [INFO] Sum of grad norms of most recent batch: 0.034848
2019-04-08 01:56:13,285 [INFO] ---------------------------------
2019-04-08 01:56:34,258 [INFO] ---------------------------------
2019-04-08 01:56:34,259 [INFO] Summary:
2019-04-08 01:56:34,259 [INFO] Batch 123000, worst loss 0.078951 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 01:56:34,260 [INFO] Regularization: 849.510864 * 0.0000000100 = 0.0000084951 loss
2019-04-08 01:56:34,260 [INFO] unfolding 0, single step 123001
2019-04-08 01:56:34,261 [INFO] Sum of grad norms of most recent batch: 0.042839
2019-04-08 01:56:34,261 [INFO] ---------------------------------
2019-04-08 01:56:55,797 [INFO] ---------------------------------
2019-04-08 01:56:55,798 [INFO] Summary:
2019-04-08 01:56:55,799 [INFO] Batch 124000, worst loss 0.189642 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 01:56:55,799 [INFO] Regularization: 849.472473 * 0.0000000100 = 0.0000084947 loss
2019-04-08 01:56:55,799 [INFO] unfolding 0, single step 124001
2019-04-08 01:56:55,800 [INFO] Sum of grad norms of most recent batch: 0.032746
2019-04-08 01:56:55,800 [INFO] ---------------------------------
2019-04-08 01:57:16,874 [INFO] ---------------------------------
2019-04-08 01:57:16,875 [INFO] Summary:
2019-04-08 01:57:16,876 [INFO] Batch 125000, worst loss 0.049909 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 01:57:16,876 [INFO] Regularization: 849.479675 * 0.0000000100 = 0.0000084948 loss
2019-04-08 01:57:16,877 [INFO] unfolding 0, single step 125001
2019-04-08 01:57:16,878 [INFO] Sum of grad norms of most recent batch: 0.584992
2019-04-08 01:57:16,878 [INFO] ---------------------------------
2019-04-08 01:57:37,880 [INFO] ---------------------------------
2019-04-08 01:57:37,881 [INFO] Summary:
2019-04-08 01:57:37,882 [INFO] Batch 126000, worst loss 0.082525 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 01:57:37,882 [INFO] Regularization: 849.453613 * 0.0000000100 = 0.0000084945 loss
2019-04-08 01:57:37,883 [INFO] unfolding 0, single step 126001
2019-04-08 01:57:37,883 [INFO] Sum of grad norms of most recent batch: 0.036804
2019-04-08 01:57:37,884 [INFO] ---------------------------------
2019-04-08 01:57:59,258 [INFO] ---------------------------------
2019-04-08 01:57:59,259 [INFO] Summary:
2019-04-08 01:57:59,259 [INFO] Batch 127000, worst loss 0.075765 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 01:57:59,260 [INFO] Regularization: 849.410156 * 0.0000000100 = 0.0000084941 loss
2019-04-08 01:57:59,260 [INFO] unfolding 0, single step 127001
2019-04-08 01:57:59,261 [INFO] Sum of grad norms of most recent batch: 0.027603
2019-04-08 01:57:59,261 [INFO] ---------------------------------
2019-04-08 01:58:19,978 [INFO] ---------------------------------
2019-04-08 01:58:19,979 [INFO] Summary:
2019-04-08 01:58:19,980 [INFO] Batch 128000, worst loss 0.075769 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 01:58:19,981 [INFO] Regularization: 849.369385 * 0.0000000100 = 0.0000084937 loss
2019-04-08 01:58:19,981 [INFO] unfolding 0, single step 128001
2019-04-08 01:58:19,982 [INFO] Sum of grad norms of most recent batch: 0.034293
2019-04-08 01:58:19,982 [INFO] ---------------------------------
2019-04-08 01:58:41,206 [INFO] ---------------------------------
2019-04-08 01:58:41,207 [INFO] Summary:
2019-04-08 01:58:41,208 [INFO] Batch 129000, worst loss 0.028121 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 01:58:41,208 [INFO] Regularization: 849.367920 * 0.0000000100 = 0.0000084937 loss
2019-04-08 01:58:41,208 [INFO] unfolding 0, single step 129001
2019-04-08 01:58:41,209 [INFO] Sum of grad norms of most recent batch: 0.036411
2019-04-08 01:58:41,209 [INFO] ---------------------------------
2019-04-08 01:59:02,626 [INFO] ---------------------------------
2019-04-08 01:59:02,627 [INFO] Summary:
2019-04-08 01:59:02,628 [INFO] Batch 130000, worst loss 0.065312 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 01:59:02,628 [INFO] Regularization: 849.333191 * 0.0000000100 = 0.0000084933 loss
2019-04-08 01:59:02,628 [INFO] unfolding 0, single step 130001
2019-04-08 01:59:02,629 [INFO] Sum of grad norms of most recent batch: 0.041175
2019-04-08 01:59:02,630 [INFO] ---------------------------------
2019-04-08 01:59:40,184 [INFO] ---------------------------------
2019-04-08 01:59:40,185 [INFO] Evaluation:
2019-04-08 01:59:40,185 [INFO] Batch 130000, worst loss 0.070808 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 01:59:40,186 [INFO] ---------------------------------
2019-04-08 02:00:01,138 [INFO] ---------------------------------
2019-04-08 02:00:01,139 [INFO] Summary:
2019-04-08 02:00:01,139 [INFO] Batch 131000, worst loss 0.072809 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:00:01,140 [INFO] Regularization: 849.296875 * 0.0000000100 = 0.0000084930 loss
2019-04-08 02:00:01,140 [INFO] unfolding 0, single step 131001
2019-04-08 02:00:01,140 [INFO] Sum of grad norms of most recent batch: 0.034539
2019-04-08 02:00:01,141 [INFO] ---------------------------------
2019-04-08 02:00:22,342 [INFO] ---------------------------------
2019-04-08 02:00:22,343 [INFO] Summary:
2019-04-08 02:00:22,344 [INFO] Batch 132000, worst loss 0.081690 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:00:22,344 [INFO] Regularization: 849.287659 * 0.0000000100 = 0.0000084929 loss
2019-04-08 02:00:22,345 [INFO] unfolding 0, single step 132001
2019-04-08 02:00:22,345 [INFO] Sum of grad norms of most recent batch: 0.031440
2019-04-08 02:00:22,346 [INFO] ---------------------------------
2019-04-08 02:00:43,993 [INFO] ---------------------------------
2019-04-08 02:00:43,993 [INFO] Summary:
2019-04-08 02:00:43,994 [INFO] Batch 133000, worst loss 0.062124 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:00:43,995 [INFO] Regularization: 849.267212 * 0.0000000100 = 0.0000084927 loss
2019-04-08 02:00:43,995 [INFO] unfolding 0, single step 133001
2019-04-08 02:00:43,995 [INFO] Sum of grad norms of most recent batch: 5.114436
2019-04-08 02:00:43,996 [INFO] ---------------------------------
2019-04-08 02:01:04,588 [INFO] ---------------------------------
2019-04-08 02:01:04,589 [INFO] Summary:
2019-04-08 02:01:04,589 [INFO] Batch 134000, worst loss 0.042231 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:01:04,590 [INFO] Regularization: 849.239563 * 0.0000000100 = 0.0000084924 loss
2019-04-08 02:01:04,590 [INFO] unfolding 0, single step 134001
2019-04-08 02:01:04,591 [INFO] Sum of grad norms of most recent batch: 0.039692
2019-04-08 02:01:04,591 [INFO] ---------------------------------
2019-04-08 02:01:25,791 [INFO] ---------------------------------
2019-04-08 02:01:25,793 [INFO] Summary:
2019-04-08 02:01:25,793 [INFO] Batch 135000, worst loss 0.112403 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:01:25,794 [INFO] Regularization: 849.228149 * 0.0000000100 = 0.0000084923 loss
2019-04-08 02:01:25,794 [INFO] unfolding 0, single step 135001
2019-04-08 02:01:25,795 [INFO] Sum of grad norms of most recent batch: 0.879793
2019-04-08 02:01:25,795 [INFO] ---------------------------------
2019-04-08 02:01:46,744 [INFO] ---------------------------------
2019-04-08 02:01:46,745 [INFO] Summary:
2019-04-08 02:01:46,746 [INFO] Batch 136000, worst loss 0.041561 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:01:46,746 [INFO] Regularization: 849.225891 * 0.0000000100 = 0.0000084923 loss
2019-04-08 02:01:46,747 [INFO] unfolding 0, single step 136001
2019-04-08 02:01:46,747 [INFO] Sum of grad norms of most recent batch: 0.036759
2019-04-08 02:01:46,748 [INFO] ---------------------------------
2019-04-08 02:02:07,595 [INFO] ---------------------------------
2019-04-08 02:02:07,596 [INFO] Summary:
2019-04-08 02:02:07,597 [INFO] Batch 137000, worst loss 0.048900 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:02:07,597 [INFO] Regularization: 849.226074 * 0.0000000100 = 0.0000084923 loss
2019-04-08 02:02:07,598 [INFO] unfolding 0, single step 137001
2019-04-08 02:02:07,598 [INFO] Sum of grad norms of most recent batch: 0.022746
2019-04-08 02:02:07,599 [INFO] ---------------------------------
2019-04-08 02:02:28,831 [INFO] ---------------------------------
2019-04-08 02:02:28,832 [INFO] Summary:
2019-04-08 02:02:28,832 [INFO] Batch 138000, worst loss 0.036023 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:02:28,833 [INFO] Regularization: 849.220520 * 0.0000000100 = 0.0000084922 loss
2019-04-08 02:02:28,833 [INFO] unfolding 0, single step 138001
2019-04-08 02:02:28,834 [INFO] Sum of grad norms of most recent batch: 0.052190
2019-04-08 02:02:28,835 [INFO] ---------------------------------
2019-04-08 02:02:50,051 [INFO] ---------------------------------
2019-04-08 02:02:50,052 [INFO] Summary:
2019-04-08 02:02:50,053 [INFO] Batch 139000, worst loss 0.052029 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:02:50,053 [INFO] Regularization: 849.220032 * 0.0000000100 = 0.0000084922 loss
2019-04-08 02:02:50,054 [INFO] unfolding 0, single step 139001
2019-04-08 02:02:50,054 [INFO] Sum of grad norms of most recent batch: 0.055182
2019-04-08 02:02:50,055 [INFO] ---------------------------------
2019-04-08 02:03:11,135 [INFO] ---------------------------------
2019-04-08 02:03:11,136 [INFO] Summary:
2019-04-08 02:03:11,137 [INFO] Batch 140000, worst loss 0.081810 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:03:11,137 [INFO] Regularization: 849.208252 * 0.0000000100 = 0.0000084921 loss
2019-04-08 02:03:11,138 [INFO] unfolding 0, single step 140001
2019-04-08 02:03:11,138 [INFO] Sum of grad norms of most recent batch: 0.024006
2019-04-08 02:03:11,139 [INFO] ---------------------------------
2019-04-08 02:03:48,469 [INFO] ---------------------------------
2019-04-08 02:03:48,470 [INFO] Evaluation:
2019-04-08 02:03:48,471 [INFO] Batch 140000, worst loss 0.141100 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:03:48,471 [INFO] ---------------------------------
2019-04-08 02:04:09,963 [INFO] ---------------------------------
2019-04-08 02:04:09,964 [INFO] Summary:
2019-04-08 02:04:09,965 [INFO] Batch 141000, worst loss 0.016175 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:04:09,965 [INFO] Regularization: 849.193604 * 0.0000000100 = 0.0000084919 loss
2019-04-08 02:04:09,965 [INFO] unfolding 0, single step 141001
2019-04-08 02:04:09,966 [INFO] Sum of grad norms of most recent batch: 3.610159
2019-04-08 02:04:09,967 [INFO] ---------------------------------
2019-04-08 02:04:30,962 [INFO] ---------------------------------
2019-04-08 02:04:30,964 [INFO] Summary:
2019-04-08 02:04:30,964 [INFO] Batch 142000, worst loss 0.119996 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:04:30,965 [INFO] Regularization: 849.176636 * 0.0000000100 = 0.0000084918 loss
2019-04-08 02:04:30,965 [INFO] unfolding 0, single step 142001
2019-04-08 02:04:30,966 [INFO] Sum of grad norms of most recent batch: 0.047878
2019-04-08 02:04:30,967 [INFO] ---------------------------------
2019-04-08 02:04:52,979 [INFO] ---------------------------------
2019-04-08 02:04:52,980 [INFO] Summary:
2019-04-08 02:04:52,980 [INFO] Batch 143000, worst loss 0.062609 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:04:52,981 [INFO] Regularization: 849.171143 * 0.0000000100 = 0.0000084917 loss
2019-04-08 02:04:52,981 [INFO] unfolding 0, single step 143001
2019-04-08 02:04:52,982 [INFO] Sum of grad norms of most recent batch: 0.019468
2019-04-08 02:04:52,982 [INFO] ---------------------------------
2019-04-08 02:05:13,915 [INFO] ---------------------------------
2019-04-08 02:05:13,916 [INFO] Summary:
2019-04-08 02:05:13,917 [INFO] Batch 144000, worst loss 0.033324 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:05:13,918 [INFO] Regularization: 849.158203 * 0.0000000100 = 0.0000084916 loss
2019-04-08 02:05:13,919 [INFO] unfolding 0, single step 144001
2019-04-08 02:05:13,920 [INFO] Sum of grad norms of most recent batch: 0.018139
2019-04-08 02:05:13,921 [INFO] ---------------------------------
2019-04-08 02:05:34,615 [INFO] ---------------------------------
2019-04-08 02:05:34,616 [INFO] Summary:
2019-04-08 02:05:34,617 [INFO] Batch 145000, worst loss 0.015276 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:05:34,617 [INFO] Regularization: 849.142761 * 0.0000000100 = 0.0000084914 loss
2019-04-08 02:05:34,617 [INFO] unfolding 0, single step 145001
2019-04-08 02:05:34,618 [INFO] Sum of grad norms of most recent batch: 0.019854
2019-04-08 02:05:34,619 [INFO] ---------------------------------
2019-04-08 02:05:55,549 [INFO] ---------------------------------
2019-04-08 02:05:55,550 [INFO] Summary:
2019-04-08 02:05:55,550 [INFO] Batch 146000, worst loss 0.065017 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:05:55,551 [INFO] Regularization: 849.131653 * 0.0000000100 = 0.0000084913 loss
2019-04-08 02:05:55,551 [INFO] unfolding 0, single step 146001
2019-04-08 02:05:55,552 [INFO] Sum of grad norms of most recent batch: 0.017175
2019-04-08 02:05:55,553 [INFO] ---------------------------------
2019-04-08 02:06:16,514 [INFO] ---------------------------------
2019-04-08 02:06:16,515 [INFO] Summary:
2019-04-08 02:06:16,515 [INFO] Batch 147000, worst loss 0.030686 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:06:16,516 [INFO] Regularization: 849.122070 * 0.0000000100 = 0.0000084912 loss
2019-04-08 02:06:16,516 [INFO] unfolding 0, single step 147001
2019-04-08 02:06:16,517 [INFO] Sum of grad norms of most recent batch: 4.594108
2019-04-08 02:06:16,517 [INFO] ---------------------------------
2019-04-08 02:06:38,311 [INFO] ---------------------------------
2019-04-08 02:06:38,312 [INFO] Summary:
2019-04-08 02:06:38,312 [INFO] Batch 148000, worst loss 0.082296 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:06:38,313 [INFO] Regularization: 849.112793 * 0.0000000100 = 0.0000084911 loss
2019-04-08 02:06:38,313 [INFO] unfolding 0, single step 148001
2019-04-08 02:06:38,314 [INFO] Sum of grad norms of most recent batch: 0.038121
2019-04-08 02:06:38,314 [INFO] ---------------------------------
2019-04-08 02:06:59,422 [INFO] ---------------------------------
2019-04-08 02:06:59,423 [INFO] Summary:
2019-04-08 02:06:59,423 [INFO] Batch 149000, worst loss 0.097475 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:06:59,424 [INFO] Regularization: 849.100708 * 0.0000000100 = 0.0000084910 loss
2019-04-08 02:06:59,424 [INFO] unfolding 0, single step 149001
2019-04-08 02:06:59,425 [INFO] Sum of grad norms of most recent batch: 0.053434
2019-04-08 02:06:59,426 [INFO] ---------------------------------
2019-04-08 02:07:20,982 [INFO] ---------------------------------
2019-04-08 02:07:20,984 [INFO] Summary:
2019-04-08 02:07:20,985 [INFO] Batch 150000, worst loss 0.022224 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 02:07:20,985 [INFO] Regularization: 849.091553 * 0.0000000100 = 0.0000084909 loss
2019-04-08 02:07:20,986 [INFO] unfolding 0, single step 150001
2019-04-08 02:07:20,987 [INFO] Sum of grad norms of most recent batch: 0.022820
2019-04-08 02:07:20,988 [INFO] ---------------------------------
2019-04-08 02:07:58,309 [INFO] ---------------------------------
2019-04-08 02:07:58,310 [INFO] Evaluation:
2019-04-08 02:07:58,311 [INFO] Batch 150000, worst loss 0.106852 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:07:58,311 [INFO] ---------------------------------
2019-04-08 02:07:58,312 [INFO] Finished training, saved to file transition/1554653688/1554682078_6_transition_final.pth
2019-04-08 02:07:58,497 [INFO] ---------------------------------
2019-04-08 02:07:58,499 [INFO] Training model #7: (11, 64, 8) @ 3
2019-04-08 02:08:20,279 [INFO] ---------------------------------
2019-04-08 02:08:20,280 [INFO] Summary:
2019-04-08 02:08:20,281 [INFO] Batch 1000, worst loss 39.207108 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:08:20,281 [INFO] Regularization: 6746.400391 * 0.0000000100 = 0.0000674640 loss
2019-04-08 02:08:20,282 [INFO] unfolding 0, single step 1001
2019-04-08 02:08:20,282 [INFO] Sum of grad norms of most recent batch: 13.528439
2019-04-08 02:08:20,283 [INFO] ---------------------------------
2019-04-08 02:08:41,648 [INFO] ---------------------------------
2019-04-08 02:08:41,649 [INFO] Summary:
2019-04-08 02:08:41,649 [INFO] Batch 2000, worst loss 0.098718 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:08:41,650 [INFO] Regularization: 4088.692383 * 0.0000000100 = 0.0000408869 loss
2019-04-08 02:08:41,650 [INFO] unfolding 0, single step 2001
2019-04-08 02:08:41,651 [INFO] Sum of grad norms of most recent batch: 2.198810
2019-04-08 02:08:41,652 [INFO] ---------------------------------
2019-04-08 02:09:03,019 [INFO] ---------------------------------
2019-04-08 02:09:03,020 [INFO] Summary:
2019-04-08 02:09:03,021 [INFO] Batch 3000, worst loss 0.096847 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:09:03,022 [INFO] Regularization: 3237.136475 * 0.0000000100 = 0.0000323714 loss
2019-04-08 02:09:03,022 [INFO] unfolding 0, single step 3001
2019-04-08 02:09:03,023 [INFO] Sum of grad norms of most recent batch: 4.014356
2019-04-08 02:09:03,024 [INFO] ---------------------------------
2019-04-08 02:09:25,074 [INFO] ---------------------------------
2019-04-08 02:09:25,075 [INFO] Summary:
2019-04-08 02:09:25,076 [INFO] Batch 4000, worst loss 0.058628 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:09:25,076 [INFO] Regularization: 2945.743896 * 0.0000000100 = 0.0000294574 loss
2019-04-08 02:09:25,077 [INFO] unfolding 0, single step 4001
2019-04-08 02:09:25,077 [INFO] Sum of grad norms of most recent batch: 1.728958
2019-04-08 02:09:25,078 [INFO] ---------------------------------
2019-04-08 02:09:47,128 [INFO] ---------------------------------
2019-04-08 02:09:47,129 [INFO] Summary:
2019-04-08 02:09:47,130 [INFO] Batch 5000, worst loss 0.140759 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:09:47,130 [INFO] Regularization: 2708.087891 * 0.0000000100 = 0.0000270809 loss
2019-04-08 02:09:47,131 [INFO] unfolding 0, single step 5001
2019-04-08 02:09:47,131 [INFO] Sum of grad norms of most recent batch: 10.632431
2019-04-08 02:09:47,132 [INFO] ---------------------------------
2019-04-08 02:10:09,164 [INFO] ---------------------------------
2019-04-08 02:10:09,165 [INFO] Summary:
2019-04-08 02:10:09,166 [INFO] Batch 6000, worst loss 0.110908 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:10:09,167 [INFO] Regularization: 2499.276367 * 0.0000000100 = 0.0000249928 loss
2019-04-08 02:10:09,167 [INFO] unfolding 0, single step 6001
2019-04-08 02:10:09,167 [INFO] Sum of grad norms of most recent batch: 6.894794
2019-04-08 02:10:09,168 [INFO] ---------------------------------
2019-04-08 02:10:31,135 [INFO] ---------------------------------
2019-04-08 02:10:31,135 [INFO] Summary:
2019-04-08 02:10:31,136 [INFO] Batch 7000, worst loss 0.092914 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:10:31,137 [INFO] Regularization: 2322.754883 * 0.0000000100 = 0.0000232275 loss
2019-04-08 02:10:31,137 [INFO] unfolding 0, single step 7001
2019-04-08 02:10:31,137 [INFO] Sum of grad norms of most recent batch: 3.419224
2019-04-08 02:10:31,138 [INFO] ---------------------------------
2019-04-08 02:10:52,871 [INFO] ---------------------------------
2019-04-08 02:10:52,872 [INFO] Summary:
2019-04-08 02:10:52,873 [INFO] Batch 8000, worst loss 0.141800 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:10:52,873 [INFO] Regularization: 2204.238037 * 0.0000000100 = 0.0000220424 loss
2019-04-08 02:10:52,874 [INFO] unfolding 0, single step 8001
2019-04-08 02:10:52,874 [INFO] Sum of grad norms of most recent batch: 3.164931
2019-04-08 02:10:52,875 [INFO] ---------------------------------
2019-04-08 02:11:14,095 [INFO] ---------------------------------
2019-04-08 02:11:14,096 [INFO] Summary:
2019-04-08 02:11:14,096 [INFO] Batch 9000, worst loss 0.159989 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:11:14,097 [INFO] Regularization: 2082.979980 * 0.0000000100 = 0.0000208298 loss
2019-04-08 02:11:14,097 [INFO] unfolding 0, single step 9001
2019-04-08 02:11:14,098 [INFO] Sum of grad norms of most recent batch: 2.472715
2019-04-08 02:11:14,098 [INFO] ---------------------------------
2019-04-08 02:11:35,161 [INFO] ---------------------------------
2019-04-08 02:11:35,162 [INFO] Summary:
2019-04-08 02:11:35,163 [INFO] Batch 10000, worst loss 0.106724 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:11:35,164 [INFO] Regularization: 1998.115601 * 0.0000000100 = 0.0000199812 loss
2019-04-08 02:11:35,164 [INFO] unfolding 0, single step 10001
2019-04-08 02:11:35,165 [INFO] Sum of grad norms of most recent batch: 0.923579
2019-04-08 02:11:35,166 [INFO] ---------------------------------
2019-04-08 02:12:12,665 [INFO] ---------------------------------
2019-04-08 02:12:12,665 [INFO] Evaluation:
2019-04-08 02:12:12,666 [INFO] Batch 10000, worst loss 0.163600 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:12:12,667 [INFO] ---------------------------------
2019-04-08 02:12:34,459 [INFO] ---------------------------------
2019-04-08 02:12:34,460 [INFO] Summary:
2019-04-08 02:12:34,460 [INFO] Batch 11000, worst loss 0.112829 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:12:34,461 [INFO] Regularization: 1981.882690 * 0.0000000100 = 0.0000198188 loss
2019-04-08 02:12:34,461 [INFO] unfolding 0, single step 11001
2019-04-08 02:12:34,462 [INFO] Sum of grad norms of most recent batch: 12.745806
2019-04-08 02:12:34,462 [INFO] ---------------------------------
2019-04-08 02:12:56,308 [INFO] ---------------------------------
2019-04-08 02:12:56,309 [INFO] Summary:
2019-04-08 02:12:56,310 [INFO] Batch 12000, worst loss 0.120264 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:12:56,311 [INFO] Regularization: 1927.874146 * 0.0000000100 = 0.0000192787 loss
2019-04-08 02:12:56,311 [INFO] unfolding 0, single step 12001
2019-04-08 02:12:56,312 [INFO] Sum of grad norms of most recent batch: 1.680554
2019-04-08 02:12:56,312 [INFO] ---------------------------------
2019-04-08 02:13:18,035 [INFO] ---------------------------------
2019-04-08 02:13:18,036 [INFO] Summary:
2019-04-08 02:13:18,036 [INFO] Batch 13000, worst loss 0.122324 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:13:18,037 [INFO] Regularization: 1887.602783 * 0.0000000100 = 0.0000188760 loss
2019-04-08 02:13:18,037 [INFO] unfolding 0, single step 13001
2019-04-08 02:13:18,038 [INFO] Sum of grad norms of most recent batch: 2.618489
2019-04-08 02:13:18,038 [INFO] ---------------------------------
2019-04-08 02:13:39,526 [INFO] ---------------------------------
2019-04-08 02:13:39,527 [INFO] Summary:
2019-04-08 02:13:39,527 [INFO] Batch 14000, worst loss 0.075049 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:13:39,528 [INFO] Regularization: 1856.154053 * 0.0000000100 = 0.0000185615 loss
2019-04-08 02:13:39,528 [INFO] unfolding 0, single step 14001
2019-04-08 02:13:39,528 [INFO] Sum of grad norms of most recent batch: 2.430910
2019-04-08 02:13:39,529 [INFO] ---------------------------------
2019-04-08 02:14:01,018 [INFO] ---------------------------------
2019-04-08 02:14:01,019 [INFO] Summary:
2019-04-08 02:14:01,020 [INFO] Batch 15000, worst loss 0.113518 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:14:01,020 [INFO] Regularization: 1827.460327 * 0.0000000100 = 0.0000182746 loss
2019-04-08 02:14:01,021 [INFO] unfolding 0, single step 15001
2019-04-08 02:14:01,021 [INFO] Sum of grad norms of most recent batch: 3.170673
2019-04-08 02:14:01,022 [INFO] ---------------------------------
2019-04-08 02:14:23,144 [INFO] ---------------------------------
2019-04-08 02:14:23,145 [INFO] Summary:
2019-04-08 02:14:23,146 [INFO] Batch 16000, worst loss 0.063984 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:14:23,146 [INFO] Regularization: 1817.838989 * 0.0000000100 = 0.0000181784 loss
2019-04-08 02:14:23,146 [INFO] unfolding 0, single step 16001
2019-04-08 02:14:23,147 [INFO] Sum of grad norms of most recent batch: 1.195729
2019-04-08 02:14:23,148 [INFO] ---------------------------------
2019-04-08 02:14:44,734 [INFO] ---------------------------------
2019-04-08 02:14:44,735 [INFO] Summary:
2019-04-08 02:14:44,735 [INFO] Batch 17000, worst loss 0.144728 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:14:44,736 [INFO] Regularization: 1843.867310 * 0.0000000100 = 0.0000184387 loss
2019-04-08 02:14:44,736 [INFO] unfolding 0, single step 17001
2019-04-08 02:14:44,737 [INFO] Sum of grad norms of most recent batch: 1.174321
2019-04-08 02:14:44,737 [INFO] ---------------------------------
2019-04-08 02:15:05,631 [INFO] ---------------------------------
2019-04-08 02:15:05,632 [INFO] Summary:
2019-04-08 02:15:05,633 [INFO] Batch 18000, worst loss 0.049785 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:15:05,633 [INFO] Regularization: 1820.706909 * 0.0000000100 = 0.0000182071 loss
2019-04-08 02:15:05,634 [INFO] unfolding 0, single step 18001
2019-04-08 02:15:05,635 [INFO] Sum of grad norms of most recent batch: 2.033371
2019-04-08 02:15:05,635 [INFO] ---------------------------------
2019-04-08 02:15:27,967 [INFO] ---------------------------------
2019-04-08 02:15:27,968 [INFO] Summary:
2019-04-08 02:15:27,969 [INFO] Batch 19000, worst loss 0.096110 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:15:27,970 [INFO] Regularization: 1821.109619 * 0.0000000100 = 0.0000182111 loss
2019-04-08 02:15:27,970 [INFO] unfolding 0, single step 19001
2019-04-08 02:15:27,970 [INFO] Sum of grad norms of most recent batch: 4.892165
2019-04-08 02:15:27,971 [INFO] ---------------------------------
2019-04-08 02:15:49,323 [INFO] ---------------------------------
2019-04-08 02:15:49,324 [INFO] Summary:
2019-04-08 02:15:49,325 [INFO] Batch 20000, worst loss 0.104325 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:15:49,326 [INFO] Regularization: 1841.493774 * 0.0000000100 = 0.0000184149 loss
2019-04-08 02:15:49,326 [INFO] unfolding 0, single step 20001
2019-04-08 02:15:49,327 [INFO] Sum of grad norms of most recent batch: 2.435414
2019-04-08 02:15:49,328 [INFO] ---------------------------------
2019-04-08 02:16:26,905 [INFO] ---------------------------------
2019-04-08 02:16:26,906 [INFO] Evaluation:
2019-04-08 02:16:26,907 [INFO] Batch 20000, worst loss 0.144565 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:16:26,907 [INFO] ---------------------------------
2019-04-08 02:16:48,851 [INFO] ---------------------------------
2019-04-08 02:16:48,852 [INFO] Summary:
2019-04-08 02:16:48,852 [INFO] Batch 21000, worst loss 0.070649 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:16:48,853 [INFO] Regularization: 1819.774170 * 0.0000000100 = 0.0000181977 loss
2019-04-08 02:16:48,853 [INFO] unfolding 0, single step 21001
2019-04-08 02:16:48,854 [INFO] Sum of grad norms of most recent batch: 0.939273
2019-04-08 02:16:48,855 [INFO] ---------------------------------
2019-04-08 02:17:10,464 [INFO] ---------------------------------
2019-04-08 02:17:10,465 [INFO] Summary:
2019-04-08 02:17:10,466 [INFO] Batch 22000, worst loss 0.122906 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:17:10,466 [INFO] Regularization: 1833.955200 * 0.0000000100 = 0.0000183396 loss
2019-04-08 02:17:10,467 [INFO] unfolding 0, single step 22001
2019-04-08 02:17:10,468 [INFO] Sum of grad norms of most recent batch: 1.199295
2019-04-08 02:17:10,468 [INFO] ---------------------------------
2019-04-08 02:17:32,309 [INFO] ---------------------------------
2019-04-08 02:17:32,311 [INFO] Summary:
2019-04-08 02:17:32,311 [INFO] Batch 23000, worst loss 0.101735 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:17:32,312 [INFO] Regularization: 1818.106567 * 0.0000000100 = 0.0000181811 loss
2019-04-08 02:17:32,313 [INFO] unfolding 0, single step 23001
2019-04-08 02:17:32,314 [INFO] Sum of grad norms of most recent batch: 3.054223
2019-04-08 02:17:32,314 [INFO] ---------------------------------
2019-04-08 02:17:53,721 [INFO] ---------------------------------
2019-04-08 02:17:53,722 [INFO] Summary:
2019-04-08 02:17:53,723 [INFO] Batch 24000, worst loss 0.057724 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:17:53,723 [INFO] Regularization: 1772.678833 * 0.0000000100 = 0.0000177268 loss
2019-04-08 02:17:53,723 [INFO] unfolding 0, single step 24001
2019-04-08 02:17:53,724 [INFO] Sum of grad norms of most recent batch: 2.662239
2019-04-08 02:17:53,724 [INFO] ---------------------------------
2019-04-08 02:18:14,804 [INFO] ---------------------------------
2019-04-08 02:18:14,805 [INFO] Summary:
2019-04-08 02:18:14,805 [INFO] Batch 25000, worst loss 0.061772 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:18:14,806 [INFO] Regularization: 1746.012451 * 0.0000000100 = 0.0000174601 loss
2019-04-08 02:18:14,806 [INFO] unfolding 0, single step 25001
2019-04-08 02:18:14,807 [INFO] Sum of grad norms of most recent batch: 1.571051
2019-04-08 02:18:14,807 [INFO] ---------------------------------
2019-04-08 02:18:36,573 [INFO] ---------------------------------
2019-04-08 02:18:36,574 [INFO] Summary:
2019-04-08 02:18:36,575 [INFO] Batch 26000, worst loss 0.036834 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:18:36,575 [INFO] Regularization: 1697.538818 * 0.0000000100 = 0.0000169754 loss
2019-04-08 02:18:36,576 [INFO] unfolding 0, single step 26001
2019-04-08 02:18:36,576 [INFO] Sum of grad norms of most recent batch: 1.455101
2019-04-08 02:18:36,577 [INFO] ---------------------------------
2019-04-08 02:18:57,960 [INFO] ---------------------------------
2019-04-08 02:18:57,961 [INFO] Summary:
2019-04-08 02:18:57,962 [INFO] Batch 27000, worst loss 0.083040 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:18:57,962 [INFO] Regularization: 1660.307251 * 0.0000000100 = 0.0000166031 loss
2019-04-08 02:18:57,963 [INFO] unfolding 0, single step 27001
2019-04-08 02:18:57,963 [INFO] Sum of grad norms of most recent batch: 0.466738
2019-04-08 02:18:57,964 [INFO] ---------------------------------
2019-04-08 02:19:19,577 [INFO] ---------------------------------
2019-04-08 02:19:19,578 [INFO] Summary:
2019-04-08 02:19:19,579 [INFO] Batch 28000, worst loss 0.137103 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:19:19,580 [INFO] Regularization: 1623.916016 * 0.0000000100 = 0.0000162392 loss
2019-04-08 02:19:19,580 [INFO] unfolding 0, single step 28001
2019-04-08 02:19:19,581 [INFO] Sum of grad norms of most recent batch: 1.657185
2019-04-08 02:19:19,581 [INFO] ---------------------------------
2019-04-08 02:19:41,751 [INFO] ---------------------------------
2019-04-08 02:19:41,752 [INFO] Summary:
2019-04-08 02:19:41,752 [INFO] Batch 29000, worst loss 0.077083 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:19:41,753 [INFO] Regularization: 1585.330811 * 0.0000000100 = 0.0000158533 loss
2019-04-08 02:19:41,753 [INFO] unfolding 0, single step 29001
2019-04-08 02:19:41,754 [INFO] Sum of grad norms of most recent batch: 4.711757
2019-04-08 02:19:41,754 [INFO] ---------------------------------
2019-04-08 02:20:03,725 [INFO] ---------------------------------
2019-04-08 02:20:03,726 [INFO] Summary:
2019-04-08 02:20:03,727 [INFO] Batch 30000, worst loss 0.102871 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 02:20:03,728 [INFO] Regularization: 1625.802734 * 0.0000000100 = 0.0000162580 loss
2019-04-08 02:20:03,729 [INFO] unfolding 0, single step 30001
2019-04-08 02:20:03,730 [INFO] Sum of grad norms of most recent batch: 1.384887
2019-04-08 02:20:03,731 [INFO] ---------------------------------
2019-04-08 02:20:41,009 [INFO] ---------------------------------
2019-04-08 02:20:41,010 [INFO] Evaluation:
2019-04-08 02:20:41,011 [INFO] Batch 30000, worst loss 0.146454 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:20:41,011 [INFO] ---------------------------------
2019-04-08 02:21:02,611 [INFO] ---------------------------------
2019-04-08 02:21:02,612 [INFO] Summary:
2019-04-08 02:21:02,612 [INFO] Batch 31000, worst loss 0.026290 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 02:21:02,613 [INFO] Regularization: 1575.536133 * 0.0000000100 = 0.0000157554 loss
2019-04-08 02:21:02,613 [INFO] unfolding 0, single step 31001
2019-04-08 02:21:02,614 [INFO] Sum of grad norms of most recent batch: 2.613630
2019-04-08 02:21:02,614 [INFO] ---------------------------------
2019-04-08 02:21:23,775 [INFO] ---------------------------------
2019-04-08 02:21:23,776 [INFO] Summary:
2019-04-08 02:21:23,777 [INFO] Batch 32000, worst loss 0.177247 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 02:21:23,778 [INFO] Regularization: 1505.707886 * 0.0000000100 = 0.0000150571 loss
2019-04-08 02:21:23,778 [INFO] unfolding 0, single step 32001
2019-04-08 02:21:23,779 [INFO] Sum of grad norms of most recent batch: 2.248151
2019-04-08 02:21:23,779 [INFO] ---------------------------------
2019-04-08 02:21:45,656 [INFO] ---------------------------------
2019-04-08 02:21:45,657 [INFO] Summary:
2019-04-08 02:21:45,658 [INFO] Batch 33000, worst loss 0.165529 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 02:21:45,659 [INFO] Regularization: 1434.275024 * 0.0000000100 = 0.0000143428 loss
2019-04-08 02:21:45,659 [INFO] unfolding 0, single step 33001
2019-04-08 02:21:45,659 [INFO] Sum of grad norms of most recent batch: 0.715294
2019-04-08 02:21:45,660 [INFO] ---------------------------------
2019-04-08 02:22:07,964 [INFO] ---------------------------------
2019-04-08 02:22:07,965 [INFO] Summary:
2019-04-08 02:22:07,966 [INFO] Batch 34000, worst loss 0.161476 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 02:22:07,967 [INFO] Regularization: 1404.047119 * 0.0000000100 = 0.0000140405 loss
2019-04-08 02:22:07,967 [INFO] unfolding 0, single step 34001
2019-04-08 02:22:07,968 [INFO] Sum of grad norms of most recent batch: 0.374621
2019-04-08 02:22:07,969 [INFO] ---------------------------------
2019-04-08 02:22:29,129 [INFO] ---------------------------------
2019-04-08 02:22:29,130 [INFO] Summary:
2019-04-08 02:22:29,131 [INFO] Batch 35000, worst loss 0.147440 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 02:22:29,131 [INFO] Regularization: 1401.543213 * 0.0000000100 = 0.0000140154 loss
2019-04-08 02:22:29,132 [INFO] unfolding 0, single step 35001
2019-04-08 02:22:29,132 [INFO] Sum of grad norms of most recent batch: 0.858744
2019-04-08 02:22:29,133 [INFO] ---------------------------------
2019-04-08 02:22:50,875 [INFO] ---------------------------------
2019-04-08 02:22:50,875 [INFO] Summary:
2019-04-08 02:22:50,876 [INFO] Batch 36000, worst loss 0.131374 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 02:22:50,877 [INFO] Regularization: 1408.152832 * 0.0000000100 = 0.0000140815 loss
2019-04-08 02:22:50,877 [INFO] unfolding 0, single step 36001
2019-04-08 02:22:50,878 [INFO] Sum of grad norms of most recent batch: 1.288294
2019-04-08 02:22:50,878 [INFO] ---------------------------------
2019-04-08 02:23:12,000 [INFO] ---------------------------------
2019-04-08 02:23:12,002 [INFO] Summary:
2019-04-08 02:23:12,002 [INFO] Batch 37000, worst loss 0.062606 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 02:23:12,003 [INFO] Regularization: 1365.156128 * 0.0000000100 = 0.0000136516 loss
2019-04-08 02:23:12,003 [INFO] unfolding 0, single step 37001
2019-04-08 02:23:12,004 [INFO] Sum of grad norms of most recent batch: 0.376672
2019-04-08 02:23:12,005 [INFO] ---------------------------------
2019-04-08 02:23:33,726 [INFO] ---------------------------------
2019-04-08 02:23:33,727 [INFO] Summary:
2019-04-08 02:23:33,727 [INFO] Batch 38000, worst loss 0.042019 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 02:23:33,728 [INFO] Regularization: 1323.098633 * 0.0000000100 = 0.0000132310 loss
2019-04-08 02:23:33,728 [INFO] unfolding 0, single step 38001
2019-04-08 02:23:33,729 [INFO] Sum of grad norms of most recent batch: 1.015276
2019-04-08 02:23:33,729 [INFO] ---------------------------------
2019-04-08 02:23:55,332 [INFO] ---------------------------------
2019-04-08 02:23:55,333 [INFO] Summary:
2019-04-08 02:23:55,334 [INFO] Batch 39000, worst loss 0.087447 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 02:23:55,334 [INFO] Regularization: 1317.826782 * 0.0000000100 = 0.0000131783 loss
2019-04-08 02:23:55,335 [INFO] unfolding 0, single step 39001
2019-04-08 02:23:55,335 [INFO] Sum of grad norms of most recent batch: 0.799440
2019-04-08 02:23:55,336 [INFO] ---------------------------------
2019-04-08 02:24:16,991 [INFO] ---------------------------------
2019-04-08 02:24:16,992 [INFO] Summary:
2019-04-08 02:24:16,993 [INFO] Batch 40000, worst loss 0.092210 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 02:24:16,993 [INFO] Regularization: 1293.071899 * 0.0000000100 = 0.0000129307 loss
2019-04-08 02:24:16,994 [INFO] unfolding 0, single step 40001
2019-04-08 02:24:16,994 [INFO] Sum of grad norms of most recent batch: 1.365324
2019-04-08 02:24:16,995 [INFO] ---------------------------------
2019-04-08 02:24:54,487 [INFO] ---------------------------------
2019-04-08 02:24:54,488 [INFO] Evaluation:
2019-04-08 02:24:54,488 [INFO] Batch 40000, worst loss 0.119566 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:24:54,489 [INFO] ---------------------------------
2019-04-08 02:25:16,331 [INFO] ---------------------------------
2019-04-08 02:25:16,332 [INFO] Summary:
2019-04-08 02:25:16,332 [INFO] Batch 41000, worst loss 0.053090 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 02:25:16,333 [INFO] Regularization: 1274.722656 * 0.0000000100 = 0.0000127472 loss
2019-04-08 02:25:16,333 [INFO] unfolding 0, single step 41001
2019-04-08 02:25:16,334 [INFO] Sum of grad norms of most recent batch: 0.335471
2019-04-08 02:25:16,334 [INFO] ---------------------------------
2019-04-08 02:25:37,449 [INFO] ---------------------------------
2019-04-08 02:25:37,450 [INFO] Summary:
2019-04-08 02:25:37,451 [INFO] Batch 42000, worst loss 0.095518 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 02:25:37,451 [INFO] Regularization: 1224.937744 * 0.0000000100 = 0.0000122494 loss
2019-04-08 02:25:37,452 [INFO] unfolding 0, single step 42001
2019-04-08 02:25:37,452 [INFO] Sum of grad norms of most recent batch: 0.365167
2019-04-08 02:25:37,453 [INFO] ---------------------------------
2019-04-08 02:25:59,183 [INFO] ---------------------------------
2019-04-08 02:25:59,184 [INFO] Summary:
2019-04-08 02:25:59,184 [INFO] Batch 43000, worst loss 0.057708 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 02:25:59,185 [INFO] Regularization: 1188.855835 * 0.0000000100 = 0.0000118886 loss
2019-04-08 02:25:59,185 [INFO] unfolding 0, single step 43001
2019-04-08 02:25:59,186 [INFO] Sum of grad norms of most recent batch: 0.314002
2019-04-08 02:25:59,186 [INFO] ---------------------------------
2019-04-08 02:26:19,862 [INFO] ---------------------------------
2019-04-08 02:26:19,863 [INFO] Summary:
2019-04-08 02:26:19,863 [INFO] Batch 44000, worst loss 0.039704 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 02:26:19,864 [INFO] Regularization: 1154.152588 * 0.0000000100 = 0.0000115415 loss
2019-04-08 02:26:19,864 [INFO] unfolding 0, single step 44001
2019-04-08 02:26:19,865 [INFO] Sum of grad norms of most recent batch: 0.508512
2019-04-08 02:26:19,865 [INFO] ---------------------------------
2019-04-08 02:26:41,574 [INFO] ---------------------------------
2019-04-08 02:26:41,575 [INFO] Summary:
2019-04-08 02:26:41,576 [INFO] Batch 45000, worst loss 0.134072 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 02:26:41,576 [INFO] Regularization: 1142.340454 * 0.0000000100 = 0.0000114234 loss
2019-04-08 02:26:41,577 [INFO] unfolding 0, single step 45001
2019-04-08 02:26:41,577 [INFO] Sum of grad norms of most recent batch: 0.494488
2019-04-08 02:26:41,578 [INFO] ---------------------------------
2019-04-08 02:27:03,166 [INFO] ---------------------------------
2019-04-08 02:27:03,167 [INFO] Summary:
2019-04-08 02:27:03,167 [INFO] Batch 46000, worst loss 0.086807 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 02:27:03,168 [INFO] Regularization: 1122.887695 * 0.0000000100 = 0.0000112289 loss
2019-04-08 02:27:03,168 [INFO] unfolding 0, single step 46001
2019-04-08 02:27:03,169 [INFO] Sum of grad norms of most recent batch: 0.227693
2019-04-08 02:27:03,170 [INFO] ---------------------------------
2019-04-08 02:27:24,667 [INFO] ---------------------------------
2019-04-08 02:27:24,668 [INFO] Summary:
2019-04-08 02:27:24,669 [INFO] Batch 47000, worst loss 0.084101 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 02:27:24,670 [INFO] Regularization: 1120.373901 * 0.0000000100 = 0.0000112037 loss
2019-04-08 02:27:24,670 [INFO] unfolding 0, single step 47001
2019-04-08 02:27:24,671 [INFO] Sum of grad norms of most recent batch: 0.780303
2019-04-08 02:27:24,672 [INFO] ---------------------------------
2019-04-08 02:27:46,937 [INFO] ---------------------------------
2019-04-08 02:27:46,938 [INFO] Summary:
2019-04-08 02:27:46,940 [INFO] Batch 48000, worst loss 0.122904 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 02:27:46,941 [INFO] Regularization: 1112.910889 * 0.0000000100 = 0.0000111291 loss
2019-04-08 02:27:46,941 [INFO] unfolding 0, single step 48001
2019-04-08 02:27:46,942 [INFO] Sum of grad norms of most recent batch: 0.826270
2019-04-08 02:27:46,943 [INFO] ---------------------------------
2019-04-08 02:28:08,850 [INFO] ---------------------------------
2019-04-08 02:28:08,850 [INFO] Summary:
2019-04-08 02:28:08,851 [INFO] Batch 49000, worst loss 0.078426 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 02:28:08,852 [INFO] Regularization: 1113.123535 * 0.0000000100 = 0.0000111312 loss
2019-04-08 02:28:08,852 [INFO] unfolding 0, single step 49001
2019-04-08 02:28:08,852 [INFO] Sum of grad norms of most recent batch: 0.569741
2019-04-08 02:28:08,853 [INFO] ---------------------------------
2019-04-08 02:28:30,279 [INFO] ---------------------------------
2019-04-08 02:28:30,280 [INFO] Summary:
2019-04-08 02:28:30,281 [INFO] Batch 50000, worst loss 0.041075 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 02:28:30,282 [INFO] Regularization: 1103.617188 * 0.0000000100 = 0.0000110362 loss
2019-04-08 02:28:30,282 [INFO] unfolding 0, single step 50001
2019-04-08 02:28:30,283 [INFO] Sum of grad norms of most recent batch: 0.490026
2019-04-08 02:28:30,284 [INFO] ---------------------------------
2019-04-08 02:29:07,723 [INFO] ---------------------------------
2019-04-08 02:29:07,724 [INFO] Evaluation:
2019-04-08 02:29:07,725 [INFO] Batch 50000, worst loss 0.135875 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:29:07,725 [INFO] ---------------------------------
2019-04-08 02:29:29,514 [INFO] ---------------------------------
2019-04-08 02:29:29,515 [INFO] Summary:
2019-04-08 02:29:29,516 [INFO] Batch 51000, worst loss 0.144202 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 02:29:29,517 [INFO] Regularization: 1098.610962 * 0.0000000100 = 0.0000109861 loss
2019-04-08 02:29:29,517 [INFO] unfolding 0, single step 51001
2019-04-08 02:29:29,518 [INFO] Sum of grad norms of most recent batch: 2.844742
2019-04-08 02:29:29,519 [INFO] ---------------------------------
2019-04-08 02:29:50,835 [INFO] ---------------------------------
2019-04-08 02:29:50,837 [INFO] Summary:
2019-04-08 02:29:50,837 [INFO] Batch 52000, worst loss 0.112968 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 02:29:50,838 [INFO] Regularization: 1079.431274 * 0.0000000100 = 0.0000107943 loss
2019-04-08 02:29:50,838 [INFO] unfolding 0, single step 52001
2019-04-08 02:29:50,839 [INFO] Sum of grad norms of most recent batch: 0.141465
2019-04-08 02:29:50,839 [INFO] ---------------------------------
2019-04-08 02:30:11,927 [INFO] ---------------------------------
2019-04-08 02:30:11,928 [INFO] Summary:
2019-04-08 02:30:11,929 [INFO] Batch 53000, worst loss 0.147545 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 02:30:11,929 [INFO] Regularization: 1066.206909 * 0.0000000100 = 0.0000106621 loss
2019-04-08 02:30:11,929 [INFO] unfolding 0, single step 53001
2019-04-08 02:30:11,930 [INFO] Sum of grad norms of most recent batch: 0.212202
2019-04-08 02:30:11,931 [INFO] ---------------------------------
2019-04-08 02:30:33,154 [INFO] ---------------------------------
2019-04-08 02:30:33,155 [INFO] Summary:
2019-04-08 02:30:33,156 [INFO] Batch 54000, worst loss 0.117114 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 02:30:33,156 [INFO] Regularization: 1057.378906 * 0.0000000100 = 0.0000105738 loss
2019-04-08 02:30:33,156 [INFO] unfolding 0, single step 54001
2019-04-08 02:30:33,157 [INFO] Sum of grad norms of most recent batch: 0.262044
2019-04-08 02:30:33,157 [INFO] ---------------------------------
2019-04-08 02:30:54,261 [INFO] ---------------------------------
2019-04-08 02:30:54,262 [INFO] Summary:
2019-04-08 02:30:54,263 [INFO] Batch 55000, worst loss 0.112861 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 02:30:54,263 [INFO] Regularization: 1065.661377 * 0.0000000100 = 0.0000106566 loss
2019-04-08 02:30:54,264 [INFO] unfolding 0, single step 55001
2019-04-08 02:30:54,264 [INFO] Sum of grad norms of most recent batch: 2.111427
2019-04-08 02:30:54,265 [INFO] ---------------------------------
2019-04-08 02:31:16,301 [INFO] ---------------------------------
2019-04-08 02:31:16,302 [INFO] Summary:
2019-04-08 02:31:16,302 [INFO] Batch 56000, worst loss 0.038941 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 02:31:16,303 [INFO] Regularization: 1052.986938 * 0.0000000100 = 0.0000105299 loss
2019-04-08 02:31:16,303 [INFO] unfolding 0, single step 56001
2019-04-08 02:31:16,304 [INFO] Sum of grad norms of most recent batch: 0.133008
2019-04-08 02:31:16,304 [INFO] ---------------------------------
2019-04-08 02:31:37,295 [INFO] ---------------------------------
2019-04-08 02:31:37,296 [INFO] Summary:
2019-04-08 02:31:37,297 [INFO] Batch 57000, worst loss 0.056745 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 02:31:37,297 [INFO] Regularization: 1052.074341 * 0.0000000100 = 0.0000105207 loss
2019-04-08 02:31:37,297 [INFO] unfolding 0, single step 57001
2019-04-08 02:31:37,298 [INFO] Sum of grad norms of most recent batch: 0.304363
2019-04-08 02:31:37,298 [INFO] ---------------------------------
2019-04-08 02:31:58,266 [INFO] ---------------------------------
2019-04-08 02:31:58,267 [INFO] Summary:
2019-04-08 02:31:58,268 [INFO] Batch 58000, worst loss 0.028321 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 02:31:58,268 [INFO] Regularization: 1039.930420 * 0.0000000100 = 0.0000103993 loss
2019-04-08 02:31:58,269 [INFO] unfolding 0, single step 58001
2019-04-08 02:31:58,269 [INFO] Sum of grad norms of most recent batch: 0.136990
2019-04-08 02:31:58,270 [INFO] ---------------------------------
2019-04-08 02:32:19,468 [INFO] ---------------------------------
2019-04-08 02:32:19,469 [INFO] Summary:
2019-04-08 02:32:19,469 [INFO] Batch 59000, worst loss 0.077083 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 02:32:19,470 [INFO] Regularization: 1037.000122 * 0.0000000100 = 0.0000103700 loss
2019-04-08 02:32:19,470 [INFO] unfolding 0, single step 59001
2019-04-08 02:32:19,471 [INFO] Sum of grad norms of most recent batch: 0.157210
2019-04-08 02:32:19,471 [INFO] ---------------------------------
2019-04-08 02:32:41,057 [INFO] ---------------------------------
2019-04-08 02:32:41,058 [INFO] Summary:
2019-04-08 02:32:41,060 [INFO] Batch 60000, worst loss 0.116293 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 02:32:41,060 [INFO] Regularization: 1030.188110 * 0.0000000100 = 0.0000103019 loss
2019-04-08 02:32:41,061 [INFO] unfolding 0, single step 60001
2019-04-08 02:32:41,061 [INFO] Sum of grad norms of most recent batch: 0.120924
2019-04-08 02:32:41,062 [INFO] ---------------------------------
2019-04-08 02:33:18,481 [INFO] ---------------------------------
2019-04-08 02:33:18,482 [INFO] Evaluation:
2019-04-08 02:33:18,483 [INFO] Batch 60000, worst loss 0.122999 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:33:18,484 [INFO] ---------------------------------
2019-04-08 02:33:40,049 [INFO] ---------------------------------
2019-04-08 02:33:40,050 [INFO] Summary:
2019-04-08 02:33:40,051 [INFO] Batch 61000, worst loss 0.074087 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 02:33:40,052 [INFO] Regularization: 1025.831421 * 0.0000000100 = 0.0000102583 loss
2019-04-08 02:33:40,052 [INFO] unfolding 0, single step 61001
2019-04-08 02:33:40,052 [INFO] Sum of grad norms of most recent batch: 0.330741
2019-04-08 02:33:40,053 [INFO] ---------------------------------
2019-04-08 02:34:01,694 [INFO] ---------------------------------
2019-04-08 02:34:01,695 [INFO] Summary:
2019-04-08 02:34:01,696 [INFO] Batch 62000, worst loss 0.092380 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 02:34:01,697 [INFO] Regularization: 1023.029724 * 0.0000000100 = 0.0000102303 loss
2019-04-08 02:34:01,697 [INFO] unfolding 0, single step 62001
2019-04-08 02:34:01,698 [INFO] Sum of grad norms of most recent batch: 0.156394
2019-04-08 02:34:01,698 [INFO] ---------------------------------
2019-04-08 02:34:23,376 [INFO] ---------------------------------
2019-04-08 02:34:23,377 [INFO] Summary:
2019-04-08 02:34:23,377 [INFO] Batch 63000, worst loss 0.112670 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 02:34:23,378 [INFO] Regularization: 1023.027954 * 0.0000000100 = 0.0000102303 loss
2019-04-08 02:34:23,378 [INFO] unfolding 0, single step 63001
2019-04-08 02:34:23,379 [INFO] Sum of grad norms of most recent batch: 0.129397
2019-04-08 02:34:23,379 [INFO] ---------------------------------
2019-04-08 02:34:44,420 [INFO] ---------------------------------
2019-04-08 02:34:44,421 [INFO] Summary:
2019-04-08 02:34:44,421 [INFO] Batch 64000, worst loss 0.080349 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 02:34:44,422 [INFO] Regularization: 1019.287476 * 0.0000000100 = 0.0000101929 loss
2019-04-08 02:34:44,422 [INFO] unfolding 0, single step 64001
2019-04-08 02:34:44,423 [INFO] Sum of grad norms of most recent batch: 0.385155
2019-04-08 02:34:44,423 [INFO] ---------------------------------
2019-04-08 02:35:05,336 [INFO] ---------------------------------
2019-04-08 02:35:05,337 [INFO] Summary:
2019-04-08 02:35:05,337 [INFO] Batch 65000, worst loss 0.029659 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 02:35:05,338 [INFO] Regularization: 1016.908325 * 0.0000000100 = 0.0000101691 loss
2019-04-08 02:35:05,338 [INFO] unfolding 0, single step 65001
2019-04-08 02:35:05,339 [INFO] Sum of grad norms of most recent batch: 0.065018
2019-04-08 02:35:05,339 [INFO] ---------------------------------
2019-04-08 02:35:26,063 [INFO] ---------------------------------
2019-04-08 02:35:26,064 [INFO] Summary:
2019-04-08 02:35:26,064 [INFO] Batch 66000, worst loss 0.046268 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 02:35:26,065 [INFO] Regularization: 1013.946228 * 0.0000000100 = 0.0000101395 loss
2019-04-08 02:35:26,065 [INFO] unfolding 0, single step 66001
2019-04-08 02:35:26,066 [INFO] Sum of grad norms of most recent batch: 0.583173
2019-04-08 02:35:26,067 [INFO] ---------------------------------
2019-04-08 02:35:47,340 [INFO] ---------------------------------
2019-04-08 02:35:47,341 [INFO] Summary:
2019-04-08 02:35:47,342 [INFO] Batch 67000, worst loss 0.065638 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 02:35:47,343 [INFO] Regularization: 1013.554626 * 0.0000000100 = 0.0000101355 loss
2019-04-08 02:35:47,343 [INFO] unfolding 0, single step 67001
2019-04-08 02:35:47,344 [INFO] Sum of grad norms of most recent batch: 0.115967
2019-04-08 02:35:47,344 [INFO] ---------------------------------
2019-04-08 02:36:08,691 [INFO] ---------------------------------
2019-04-08 02:36:08,692 [INFO] Summary:
2019-04-08 02:36:08,693 [INFO] Batch 68000, worst loss 0.218947 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 02:36:08,693 [INFO] Regularization: 1010.623291 * 0.0000000100 = 0.0000101062 loss
2019-04-08 02:36:08,694 [INFO] unfolding 0, single step 68001
2019-04-08 02:36:08,694 [INFO] Sum of grad norms of most recent batch: 0.060768
2019-04-08 02:36:08,695 [INFO] ---------------------------------
2019-04-08 02:36:30,994 [INFO] ---------------------------------
2019-04-08 02:36:30,995 [INFO] Summary:
2019-04-08 02:36:30,996 [INFO] Batch 69000, worst loss 0.066693 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 02:36:30,996 [INFO] Regularization: 1010.089355 * 0.0000000100 = 0.0000101009 loss
2019-04-08 02:36:30,996 [INFO] unfolding 0, single step 69001
2019-04-08 02:36:30,997 [INFO] Sum of grad norms of most recent batch: 0.156370
2019-04-08 02:36:30,997 [INFO] ---------------------------------
2019-04-08 02:36:52,060 [INFO] ---------------------------------
2019-04-08 02:36:52,061 [INFO] Summary:
2019-04-08 02:36:52,062 [INFO] Batch 70000, worst loss 0.072079 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 02:36:52,062 [INFO] Regularization: 1008.528748 * 0.0000000100 = 0.0000100853 loss
2019-04-08 02:36:52,063 [INFO] unfolding 0, single step 70001
2019-04-08 02:36:52,063 [INFO] Sum of grad norms of most recent batch: 0.264401
2019-04-08 02:36:52,064 [INFO] ---------------------------------
2019-04-08 02:37:29,647 [INFO] ---------------------------------
2019-04-08 02:37:29,648 [INFO] Evaluation:
2019-04-08 02:37:29,649 [INFO] Batch 70000, worst loss 0.093107 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:37:29,649 [INFO] ---------------------------------
2019-04-08 02:37:50,828 [INFO] ---------------------------------
2019-04-08 02:37:50,830 [INFO] Summary:
2019-04-08 02:37:50,830 [INFO] Batch 71000, worst loss 0.032352 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 02:37:50,831 [INFO] Regularization: 1005.896667 * 0.0000000100 = 0.0000100590 loss
2019-04-08 02:37:50,831 [INFO] unfolding 0, single step 71001
2019-04-08 02:37:50,832 [INFO] Sum of grad norms of most recent batch: 0.041872
2019-04-08 02:37:50,832 [INFO] ---------------------------------
2019-04-08 02:38:11,779 [INFO] ---------------------------------
2019-04-08 02:38:11,780 [INFO] Summary:
2019-04-08 02:38:11,781 [INFO] Batch 72000, worst loss 0.066776 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 02:38:11,781 [INFO] Regularization: 1004.294373 * 0.0000000100 = 0.0000100429 loss
2019-04-08 02:38:11,782 [INFO] unfolding 0, single step 72001
2019-04-08 02:38:11,782 [INFO] Sum of grad norms of most recent batch: 0.019644
2019-04-08 02:38:11,783 [INFO] ---------------------------------
2019-04-08 02:38:33,415 [INFO] ---------------------------------
2019-04-08 02:38:33,416 [INFO] Summary:
2019-04-08 02:38:33,417 [INFO] Batch 73000, worst loss 0.030158 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 02:38:33,417 [INFO] Regularization: 1004.550049 * 0.0000000100 = 0.0000100455 loss
2019-04-08 02:38:33,417 [INFO] unfolding 0, single step 73001
2019-04-08 02:38:33,418 [INFO] Sum of grad norms of most recent batch: 0.062055
2019-04-08 02:38:33,418 [INFO] ---------------------------------
2019-04-08 02:38:54,306 [INFO] ---------------------------------
2019-04-08 02:38:54,308 [INFO] Summary:
2019-04-08 02:38:54,308 [INFO] Batch 74000, worst loss 0.026521 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 02:38:54,309 [INFO] Regularization: 1003.112427 * 0.0000000100 = 0.0000100311 loss
2019-04-08 02:38:54,309 [INFO] unfolding 0, single step 74001
2019-04-08 02:38:54,310 [INFO] Sum of grad norms of most recent batch: 0.077981
2019-04-08 02:38:54,311 [INFO] ---------------------------------
2019-04-08 02:39:15,586 [INFO] ---------------------------------
2019-04-08 02:39:15,587 [INFO] Summary:
2019-04-08 02:39:15,588 [INFO] Batch 75000, worst loss 0.096487 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 02:39:15,589 [INFO] Regularization: 1001.257568 * 0.0000000100 = 0.0000100126 loss
2019-04-08 02:39:15,590 [INFO] unfolding 0, single step 75001
2019-04-08 02:39:15,591 [INFO] Sum of grad norms of most recent batch: 0.130997
2019-04-08 02:39:15,592 [INFO] ---------------------------------
2019-04-08 02:39:37,217 [INFO] ---------------------------------
2019-04-08 02:39:37,218 [INFO] Summary:
2019-04-08 02:39:37,219 [INFO] Batch 76000, worst loss 0.096159 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 02:39:37,219 [INFO] Regularization: 999.362000 * 0.0000000100 = 0.0000099936 loss
2019-04-08 02:39:37,220 [INFO] unfolding 0, single step 76001
2019-04-08 02:39:37,220 [INFO] Sum of grad norms of most recent batch: 0.032675
2019-04-08 02:39:37,221 [INFO] ---------------------------------
2019-04-08 02:39:58,695 [INFO] ---------------------------------
2019-04-08 02:39:58,696 [INFO] Summary:
2019-04-08 02:39:58,697 [INFO] Batch 77000, worst loss 0.064149 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 02:39:58,697 [INFO] Regularization: 998.187195 * 0.0000000100 = 0.0000099819 loss
2019-04-08 02:39:58,698 [INFO] unfolding 0, single step 77001
2019-04-08 02:39:58,698 [INFO] Sum of grad norms of most recent batch: 0.160929
2019-04-08 02:39:58,699 [INFO] ---------------------------------
2019-04-08 02:40:19,900 [INFO] ---------------------------------
2019-04-08 02:40:19,901 [INFO] Summary:
2019-04-08 02:40:19,902 [INFO] Batch 78000, worst loss 0.044571 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 02:40:19,903 [INFO] Regularization: 997.010559 * 0.0000000100 = 0.0000099701 loss
2019-04-08 02:40:19,903 [INFO] unfolding 0, single step 78001
2019-04-08 02:40:19,904 [INFO] Sum of grad norms of most recent batch: 0.050916
2019-04-08 02:40:19,904 [INFO] ---------------------------------
2019-04-08 02:40:41,266 [INFO] ---------------------------------
2019-04-08 02:40:41,267 [INFO] Summary:
2019-04-08 02:40:41,267 [INFO] Batch 79000, worst loss 0.065758 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 02:40:41,268 [INFO] Regularization: 996.322571 * 0.0000000100 = 0.0000099632 loss
2019-04-08 02:40:41,268 [INFO] unfolding 0, single step 79001
2019-04-08 02:40:41,269 [INFO] Sum of grad norms of most recent batch: 0.054268
2019-04-08 02:40:41,269 [INFO] ---------------------------------
2019-04-08 02:41:02,848 [INFO] ---------------------------------
2019-04-08 02:41:02,849 [INFO] Summary:
2019-04-08 02:41:02,850 [INFO] Batch 80000, worst loss 0.099857 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 02:41:02,850 [INFO] Regularization: 994.321594 * 0.0000000100 = 0.0000099432 loss
2019-04-08 02:41:02,851 [INFO] unfolding 0, single step 80001
2019-04-08 02:41:02,851 [INFO] Sum of grad norms of most recent batch: 0.043735
2019-04-08 02:41:02,852 [INFO] ---------------------------------
2019-04-08 02:41:40,303 [INFO] ---------------------------------
2019-04-08 02:41:40,304 [INFO] Evaluation:
2019-04-08 02:41:40,304 [INFO] Batch 80000, worst loss 0.082218 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:41:40,305 [INFO] ---------------------------------
2019-04-08 02:42:01,873 [INFO] ---------------------------------
2019-04-08 02:42:01,874 [INFO] Summary:
2019-04-08 02:42:01,874 [INFO] Batch 81000, worst loss 0.068695 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 02:42:01,875 [INFO] Regularization: 992.641663 * 0.0000000100 = 0.0000099264 loss
2019-04-08 02:42:01,875 [INFO] unfolding 0, single step 81001
2019-04-08 02:42:01,876 [INFO] Sum of grad norms of most recent batch: 0.082942
2019-04-08 02:42:01,877 [INFO] ---------------------------------
2019-04-08 02:42:23,222 [INFO] ---------------------------------
2019-04-08 02:42:23,223 [INFO] Summary:
2019-04-08 02:42:23,224 [INFO] Batch 82000, worst loss 0.060326 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 02:42:23,224 [INFO] Regularization: 992.637390 * 0.0000000100 = 0.0000099264 loss
2019-04-08 02:42:23,225 [INFO] unfolding 0, single step 82001
2019-04-08 02:42:23,225 [INFO] Sum of grad norms of most recent batch: 0.061638
2019-04-08 02:42:23,226 [INFO] ---------------------------------
2019-04-08 02:42:44,102 [INFO] ---------------------------------
2019-04-08 02:42:44,103 [INFO] Summary:
2019-04-08 02:42:44,104 [INFO] Batch 83000, worst loss 0.022619 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 02:42:44,105 [INFO] Regularization: 991.441162 * 0.0000000100 = 0.0000099144 loss
2019-04-08 02:42:44,105 [INFO] unfolding 0, single step 83001
2019-04-08 02:42:44,106 [INFO] Sum of grad norms of most recent batch: 0.078440
2019-04-08 02:42:44,107 [INFO] ---------------------------------
2019-04-08 02:43:05,345 [INFO] ---------------------------------
2019-04-08 02:43:05,346 [INFO] Summary:
2019-04-08 02:43:05,347 [INFO] Batch 84000, worst loss 0.078489 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 02:43:05,347 [INFO] Regularization: 990.939453 * 0.0000000100 = 0.0000099094 loss
2019-04-08 02:43:05,347 [INFO] unfolding 0, single step 84001
2019-04-08 02:43:05,348 [INFO] Sum of grad norms of most recent batch: 2.404665
2019-04-08 02:43:05,348 [INFO] ---------------------------------
2019-04-08 02:43:26,232 [INFO] ---------------------------------
2019-04-08 02:43:26,233 [INFO] Summary:
2019-04-08 02:43:26,233 [INFO] Batch 85000, worst loss 0.033314 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 02:43:26,234 [INFO] Regularization: 990.149902 * 0.0000000100 = 0.0000099015 loss
2019-04-08 02:43:26,234 [INFO] unfolding 0, single step 85001
2019-04-08 02:43:26,235 [INFO] Sum of grad norms of most recent batch: 0.361878
2019-04-08 02:43:26,235 [INFO] ---------------------------------
2019-04-08 02:43:47,502 [INFO] ---------------------------------
2019-04-08 02:43:47,503 [INFO] Summary:
2019-04-08 02:43:47,504 [INFO] Batch 86000, worst loss 0.049692 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 02:43:47,504 [INFO] Regularization: 990.189697 * 0.0000000100 = 0.0000099019 loss
2019-04-08 02:43:47,505 [INFO] unfolding 0, single step 86001
2019-04-08 02:43:47,505 [INFO] Sum of grad norms of most recent batch: 0.017387
2019-04-08 02:43:47,506 [INFO] ---------------------------------
2019-04-08 02:44:08,188 [INFO] ---------------------------------
2019-04-08 02:44:08,190 [INFO] Summary:
2019-04-08 02:44:08,190 [INFO] Batch 87000, worst loss 0.037261 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 02:44:08,191 [INFO] Regularization: 989.538635 * 0.0000000100 = 0.0000098954 loss
2019-04-08 02:44:08,192 [INFO] unfolding 0, single step 87001
2019-04-08 02:44:08,193 [INFO] Sum of grad norms of most recent batch: 0.035394
2019-04-08 02:44:08,194 [INFO] ---------------------------------
2019-04-08 02:44:29,272 [INFO] ---------------------------------
2019-04-08 02:44:29,273 [INFO] Summary:
2019-04-08 02:44:29,274 [INFO] Batch 88000, worst loss 0.065105 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 02:44:29,274 [INFO] Regularization: 989.273865 * 0.0000000100 = 0.0000098927 loss
2019-04-08 02:44:29,275 [INFO] unfolding 0, single step 88001
2019-04-08 02:44:29,275 [INFO] Sum of grad norms of most recent batch: 0.745938
2019-04-08 02:44:29,276 [INFO] ---------------------------------
2019-04-08 02:44:51,596 [INFO] ---------------------------------
2019-04-08 02:44:51,597 [INFO] Summary:
2019-04-08 02:44:51,598 [INFO] Batch 89000, worst loss 0.064506 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 02:44:51,598 [INFO] Regularization: 988.380493 * 0.0000000100 = 0.0000098838 loss
2019-04-08 02:44:51,599 [INFO] unfolding 0, single step 89001
2019-04-08 02:44:51,600 [INFO] Sum of grad norms of most recent batch: 0.055753
2019-04-08 02:44:51,600 [INFO] ---------------------------------
2019-04-08 02:45:12,285 [INFO] ---------------------------------
2019-04-08 02:45:12,286 [INFO] Summary:
2019-04-08 02:45:12,287 [INFO] Batch 90000, worst loss 0.030121 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 02:45:12,288 [INFO] Regularization: 988.298462 * 0.0000000100 = 0.0000098830 loss
2019-04-08 02:45:12,289 [INFO] unfolding 0, single step 90001
2019-04-08 02:45:12,290 [INFO] Sum of grad norms of most recent batch: 4.779511
2019-04-08 02:45:12,291 [INFO] ---------------------------------
2019-04-08 02:45:49,732 [INFO] ---------------------------------
2019-04-08 02:45:49,733 [INFO] Evaluation:
2019-04-08 02:45:49,734 [INFO] Batch 90000, worst loss 0.081395 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:45:49,735 [INFO] ---------------------------------
2019-04-08 02:46:10,654 [INFO] ---------------------------------
2019-04-08 02:46:10,655 [INFO] Summary:
2019-04-08 02:46:10,656 [INFO] Batch 91000, worst loss 0.030877 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 02:46:10,656 [INFO] Regularization: 987.783386 * 0.0000000100 = 0.0000098778 loss
2019-04-08 02:46:10,656 [INFO] unfolding 0, single step 91001
2019-04-08 02:46:10,657 [INFO] Sum of grad norms of most recent batch: 0.040563
2019-04-08 02:46:10,657 [INFO] ---------------------------------
2019-04-08 02:46:31,181 [INFO] ---------------------------------
2019-04-08 02:46:31,182 [INFO] Summary:
2019-04-08 02:46:31,183 [INFO] Batch 92000, worst loss 0.094680 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 02:46:31,183 [INFO] Regularization: 987.033691 * 0.0000000100 = 0.0000098703 loss
2019-04-08 02:46:31,183 [INFO] unfolding 0, single step 92001
2019-04-08 02:46:31,184 [INFO] Sum of grad norms of most recent batch: 0.055313
2019-04-08 02:46:31,184 [INFO] ---------------------------------
2019-04-08 02:46:52,738 [INFO] ---------------------------------
2019-04-08 02:46:52,739 [INFO] Summary:
2019-04-08 02:46:52,740 [INFO] Batch 93000, worst loss 0.068698 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 02:46:52,741 [INFO] Regularization: 987.252808 * 0.0000000100 = 0.0000098725 loss
2019-04-08 02:46:52,741 [INFO] unfolding 0, single step 93001
2019-04-08 02:46:52,742 [INFO] Sum of grad norms of most recent batch: 0.032328
2019-04-08 02:46:52,743 [INFO] ---------------------------------
2019-04-08 02:47:14,150 [INFO] ---------------------------------
2019-04-08 02:47:14,151 [INFO] Summary:
2019-04-08 02:47:14,152 [INFO] Batch 94000, worst loss 0.057080 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 02:47:14,152 [INFO] Regularization: 986.815918 * 0.0000000100 = 0.0000098682 loss
2019-04-08 02:47:14,152 [INFO] unfolding 0, single step 94001
2019-04-08 02:47:14,153 [INFO] Sum of grad norms of most recent batch: 0.040011
2019-04-08 02:47:14,153 [INFO] ---------------------------------
2019-04-08 02:47:35,449 [INFO] ---------------------------------
2019-04-08 02:47:35,450 [INFO] Summary:
2019-04-08 02:47:35,450 [INFO] Batch 95000, worst loss 0.036959 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 02:47:35,451 [INFO] Regularization: 986.719727 * 0.0000000100 = 0.0000098672 loss
2019-04-08 02:47:35,451 [INFO] unfolding 0, single step 95001
2019-04-08 02:47:35,452 [INFO] Sum of grad norms of most recent batch: 0.037154
2019-04-08 02:47:35,452 [INFO] ---------------------------------
2019-04-08 02:47:56,677 [INFO] ---------------------------------
2019-04-08 02:47:56,678 [INFO] Summary:
2019-04-08 02:47:56,679 [INFO] Batch 96000, worst loss 0.123507 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 02:47:56,679 [INFO] Regularization: 986.029968 * 0.0000000100 = 0.0000098603 loss
2019-04-08 02:47:56,680 [INFO] unfolding 0, single step 96001
2019-04-08 02:47:56,680 [INFO] Sum of grad norms of most recent batch: 0.039229
2019-04-08 02:47:56,681 [INFO] ---------------------------------
2019-04-08 02:48:17,349 [INFO] ---------------------------------
2019-04-08 02:48:17,351 [INFO] Summary:
2019-04-08 02:48:17,351 [INFO] Batch 97000, worst loss 0.032643 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 02:48:17,352 [INFO] Regularization: 986.232849 * 0.0000000100 = 0.0000098623 loss
2019-04-08 02:48:17,352 [INFO] unfolding 0, single step 97001
2019-04-08 02:48:17,353 [INFO] Sum of grad norms of most recent batch: 6.149748
2019-04-08 02:48:17,353 [INFO] ---------------------------------
2019-04-08 02:48:38,423 [INFO] ---------------------------------
2019-04-08 02:48:38,424 [INFO] Summary:
2019-04-08 02:48:38,425 [INFO] Batch 98000, worst loss 0.054928 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 02:48:38,426 [INFO] Regularization: 985.807800 * 0.0000000100 = 0.0000098581 loss
2019-04-08 02:48:38,426 [INFO] unfolding 0, single step 98001
2019-04-08 02:48:38,427 [INFO] Sum of grad norms of most recent batch: 0.091870
2019-04-08 02:48:38,428 [INFO] ---------------------------------
2019-04-08 02:48:59,307 [INFO] ---------------------------------
2019-04-08 02:48:59,308 [INFO] Summary:
2019-04-08 02:48:59,309 [INFO] Batch 99000, worst loss 0.026126 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 02:48:59,309 [INFO] Regularization: 985.400696 * 0.0000000100 = 0.0000098540 loss
2019-04-08 02:48:59,310 [INFO] unfolding 0, single step 99001
2019-04-08 02:48:59,310 [INFO] Sum of grad norms of most recent batch: 0.040388
2019-04-08 02:48:59,311 [INFO] ---------------------------------
2019-04-08 02:49:20,413 [INFO] ---------------------------------
2019-04-08 02:49:20,414 [INFO] Summary:
2019-04-08 02:49:20,415 [INFO] Batch 100000, worst loss 0.005148 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 02:49:20,415 [INFO] Regularization: 985.130981 * 0.0000000100 = 0.0000098513 loss
2019-04-08 02:49:20,416 [INFO] unfolding 0, single step 100001
2019-04-08 02:49:20,416 [INFO] Sum of grad norms of most recent batch: 0.076475
2019-04-08 02:49:20,417 [INFO] ---------------------------------
2019-04-08 02:49:57,695 [INFO] ---------------------------------
2019-04-08 02:49:57,696 [INFO] Evaluation:
2019-04-08 02:49:57,697 [INFO] Batch 100000, worst loss 0.083387 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:49:57,697 [INFO] ---------------------------------
2019-04-08 02:50:18,992 [INFO] ---------------------------------
2019-04-08 02:50:18,993 [INFO] Summary:
2019-04-08 02:50:18,993 [INFO] Batch 101000, worst loss 0.038726 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 02:50:18,994 [INFO] Regularization: 984.932434 * 0.0000000100 = 0.0000098493 loss
2019-04-08 02:50:18,994 [INFO] unfolding 0, single step 101001
2019-04-08 02:50:18,995 [INFO] Sum of grad norms of most recent batch: 0.034280
2019-04-08 02:50:18,995 [INFO] ---------------------------------
2019-04-08 02:50:40,592 [INFO] ---------------------------------
2019-04-08 02:50:40,593 [INFO] Summary:
2019-04-08 02:50:40,594 [INFO] Batch 102000, worst loss 0.079844 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 02:50:40,595 [INFO] Regularization: 984.681580 * 0.0000000100 = 0.0000098468 loss
2019-04-08 02:50:40,596 [INFO] unfolding 0, single step 102001
2019-04-08 02:50:40,597 [INFO] Sum of grad norms of most recent batch: 0.051234
2019-04-08 02:50:40,598 [INFO] ---------------------------------
2019-04-08 02:51:02,507 [INFO] ---------------------------------
2019-04-08 02:51:02,508 [INFO] Summary:
2019-04-08 02:51:02,508 [INFO] Batch 103000, worst loss 0.086036 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 02:51:02,509 [INFO] Regularization: 984.389954 * 0.0000000100 = 0.0000098439 loss
2019-04-08 02:51:02,509 [INFO] unfolding 0, single step 103001
2019-04-08 02:51:02,510 [INFO] Sum of grad norms of most recent batch: 0.095505
2019-04-08 02:51:02,510 [INFO] ---------------------------------
2019-04-08 02:51:22,954 [INFO] ---------------------------------
2019-04-08 02:51:22,955 [INFO] Summary:
2019-04-08 02:51:22,955 [INFO] Batch 104000, worst loss 0.057307 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 02:51:22,956 [INFO] Regularization: 984.186035 * 0.0000000100 = 0.0000098419 loss
2019-04-08 02:51:22,957 [INFO] unfolding 0, single step 104001
2019-04-08 02:51:22,958 [INFO] Sum of grad norms of most recent batch: 0.021592
2019-04-08 02:51:22,959 [INFO] ---------------------------------
2019-04-08 02:51:44,304 [INFO] ---------------------------------
2019-04-08 02:51:44,305 [INFO] Summary:
2019-04-08 02:51:44,306 [INFO] Batch 105000, worst loss 0.073218 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 02:51:44,307 [INFO] Regularization: 983.952881 * 0.0000000100 = 0.0000098395 loss
2019-04-08 02:51:44,307 [INFO] unfolding 0, single step 105001
2019-04-08 02:51:44,308 [INFO] Sum of grad norms of most recent batch: 0.042342
2019-04-08 02:51:44,308 [INFO] ---------------------------------
2019-04-08 02:52:05,153 [INFO] ---------------------------------
2019-04-08 02:52:05,154 [INFO] Summary:
2019-04-08 02:52:05,155 [INFO] Batch 106000, worst loss 0.073175 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 02:52:05,155 [INFO] Regularization: 983.929810 * 0.0000000100 = 0.0000098393 loss
2019-04-08 02:52:05,156 [INFO] unfolding 0, single step 106001
2019-04-08 02:52:05,156 [INFO] Sum of grad norms of most recent batch: 3.747972
2019-04-08 02:52:05,157 [INFO] ---------------------------------
2019-04-08 02:52:26,080 [INFO] ---------------------------------
2019-04-08 02:52:26,082 [INFO] Summary:
2019-04-08 02:52:26,082 [INFO] Batch 107000, worst loss 0.071053 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 02:52:26,083 [INFO] Regularization: 983.907715 * 0.0000000100 = 0.0000098391 loss
2019-04-08 02:52:26,083 [INFO] unfolding 0, single step 107001
2019-04-08 02:52:26,084 [INFO] Sum of grad norms of most recent batch: 0.044659
2019-04-08 02:52:26,084 [INFO] ---------------------------------
2019-04-08 02:52:46,890 [INFO] ---------------------------------
2019-04-08 02:52:46,891 [INFO] Summary:
2019-04-08 02:52:46,891 [INFO] Batch 108000, worst loss 0.008261 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 02:52:46,892 [INFO] Regularization: 983.820312 * 0.0000000100 = 0.0000098382 loss
2019-04-08 02:52:46,892 [INFO] unfolding 0, single step 108001
2019-04-08 02:52:46,893 [INFO] Sum of grad norms of most recent batch: 0.035450
2019-04-08 02:52:46,893 [INFO] ---------------------------------
2019-04-08 02:53:08,330 [INFO] ---------------------------------
2019-04-08 02:53:08,331 [INFO] Summary:
2019-04-08 02:53:08,331 [INFO] Batch 109000, worst loss 0.033288 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 02:53:08,332 [INFO] Regularization: 983.722900 * 0.0000000100 = 0.0000098372 loss
2019-04-08 02:53:08,332 [INFO] unfolding 0, single step 109001
2019-04-08 02:53:08,333 [INFO] Sum of grad norms of most recent batch: 0.046797
2019-04-08 02:53:08,333 [INFO] ---------------------------------
2019-04-08 02:53:29,586 [INFO] ---------------------------------
2019-04-08 02:53:29,587 [INFO] Summary:
2019-04-08 02:53:29,588 [INFO] Batch 110000, worst loss 0.118501 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 02:53:29,589 [INFO] Regularization: 983.648987 * 0.0000000100 = 0.0000098365 loss
2019-04-08 02:53:29,589 [INFO] unfolding 0, single step 110001
2019-04-08 02:53:29,590 [INFO] Sum of grad norms of most recent batch: 0.086836
2019-04-08 02:53:29,591 [INFO] ---------------------------------
2019-04-08 02:54:06,994 [INFO] ---------------------------------
2019-04-08 02:54:06,995 [INFO] Evaluation:
2019-04-08 02:54:06,995 [INFO] Batch 110000, worst loss 0.097844 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:54:06,996 [INFO] ---------------------------------
2019-04-08 02:54:28,370 [INFO] ---------------------------------
2019-04-08 02:54:28,371 [INFO] Summary:
2019-04-08 02:54:28,372 [INFO] Batch 111000, worst loss 0.034558 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 02:54:28,373 [INFO] Regularization: 983.471313 * 0.0000000100 = 0.0000098347 loss
2019-04-08 02:54:28,373 [INFO] unfolding 0, single step 111001
2019-04-08 02:54:28,374 [INFO] Sum of grad norms of most recent batch: 0.054255
2019-04-08 02:54:28,374 [INFO] ---------------------------------
2019-04-08 02:54:48,855 [INFO] ---------------------------------
2019-04-08 02:54:48,857 [INFO] Summary:
2019-04-08 02:54:48,857 [INFO] Batch 112000, worst loss 0.032342 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 02:54:48,858 [INFO] Regularization: 983.332153 * 0.0000000100 = 0.0000098333 loss
2019-04-08 02:54:48,858 [INFO] unfolding 0, single step 112001
2019-04-08 02:54:48,859 [INFO] Sum of grad norms of most recent batch: 0.033318
2019-04-08 02:54:48,859 [INFO] ---------------------------------
2019-04-08 02:55:09,479 [INFO] ---------------------------------
2019-04-08 02:55:09,479 [INFO] Summary:
2019-04-08 02:55:09,480 [INFO] Batch 113000, worst loss 0.027970 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 02:55:09,481 [INFO] Regularization: 983.237366 * 0.0000000100 = 0.0000098324 loss
2019-04-08 02:55:09,482 [INFO] unfolding 0, single step 113001
2019-04-08 02:55:09,483 [INFO] Sum of grad norms of most recent batch: 0.056132
2019-04-08 02:55:09,483 [INFO] ---------------------------------
2019-04-08 02:55:30,832 [INFO] ---------------------------------
2019-04-08 02:55:30,833 [INFO] Summary:
2019-04-08 02:55:30,833 [INFO] Batch 114000, worst loss 0.004172 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 02:55:30,834 [INFO] Regularization: 983.117310 * 0.0000000100 = 0.0000098312 loss
2019-04-08 02:55:30,834 [INFO] unfolding 0, single step 114001
2019-04-08 02:55:30,835 [INFO] Sum of grad norms of most recent batch: 0.023875
2019-04-08 02:55:30,836 [INFO] ---------------------------------
2019-04-08 02:56:08,199 [INFO] ---------------------------------
2019-04-08 02:56:08,199 [INFO] Evaluation:
2019-04-08 02:56:08,200 [INFO] Batch 114000, worst loss 0.101668 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:56:08,201 [INFO] ---------------------------------
2019-04-08 02:56:29,287 [INFO] ---------------------------------
2019-04-08 02:56:29,288 [INFO] Summary:
2019-04-08 02:56:29,289 [INFO] Batch 115000, worst loss 0.123774 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 02:56:29,290 [INFO] Regularization: 983.067505 * 0.0000000100 = 0.0000098307 loss
2019-04-08 02:56:29,290 [INFO] unfolding 0, single step 115001
2019-04-08 02:56:29,291 [INFO] Sum of grad norms of most recent batch: 0.029955
2019-04-08 02:56:29,292 [INFO] ---------------------------------
2019-04-08 02:56:50,846 [INFO] ---------------------------------
2019-04-08 02:56:50,847 [INFO] Summary:
2019-04-08 02:56:50,848 [INFO] Batch 116000, worst loss 0.023322 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 02:56:50,848 [INFO] Regularization: 982.995361 * 0.0000000100 = 0.0000098300 loss
2019-04-08 02:56:50,848 [INFO] unfolding 0, single step 116001
2019-04-08 02:56:50,849 [INFO] Sum of grad norms of most recent batch: 0.028578
2019-04-08 02:56:50,849 [INFO] ---------------------------------
2019-04-08 02:57:12,375 [INFO] ---------------------------------
2019-04-08 02:57:12,376 [INFO] Summary:
2019-04-08 02:57:12,377 [INFO] Batch 117000, worst loss 0.069087 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 02:57:12,378 [INFO] Regularization: 982.871277 * 0.0000000100 = 0.0000098287 loss
2019-04-08 02:57:12,379 [INFO] unfolding 0, single step 117001
2019-04-08 02:57:12,380 [INFO] Sum of grad norms of most recent batch: 0.043735
2019-04-08 02:57:12,381 [INFO] ---------------------------------
2019-04-08 02:57:33,862 [INFO] ---------------------------------
2019-04-08 02:57:33,863 [INFO] Summary:
2019-04-08 02:57:33,863 [INFO] Batch 118000, worst loss 0.168151 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 02:57:33,864 [INFO] Regularization: 982.890808 * 0.0000000100 = 0.0000098289 loss
2019-04-08 02:57:33,864 [INFO] unfolding 0, single step 118001
2019-04-08 02:57:33,865 [INFO] Sum of grad norms of most recent batch: 0.030087
2019-04-08 02:57:33,865 [INFO] ---------------------------------
2019-04-08 02:57:54,645 [INFO] ---------------------------------
2019-04-08 02:57:54,646 [INFO] Summary:
2019-04-08 02:57:54,647 [INFO] Batch 119000, worst loss 0.039160 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 02:57:54,647 [INFO] Regularization: 982.765686 * 0.0000000100 = 0.0000098277 loss
2019-04-08 02:57:54,647 [INFO] unfolding 0, single step 119001
2019-04-08 02:57:54,648 [INFO] Sum of grad norms of most recent batch: 0.042671
2019-04-08 02:57:54,648 [INFO] ---------------------------------
2019-04-08 02:58:15,291 [INFO] ---------------------------------
2019-04-08 02:58:15,292 [INFO] Summary:
2019-04-08 02:58:15,293 [INFO] Batch 120000, worst loss 0.070088 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 02:58:15,293 [INFO] Regularization: 982.790771 * 0.0000000100 = 0.0000098279 loss
2019-04-08 02:58:15,293 [INFO] unfolding 0, single step 120001
2019-04-08 02:58:15,294 [INFO] Sum of grad norms of most recent batch: 0.044842
2019-04-08 02:58:15,294 [INFO] ---------------------------------
2019-04-08 02:58:52,968 [INFO] ---------------------------------
2019-04-08 02:58:52,969 [INFO] Evaluation:
2019-04-08 02:58:52,969 [INFO] Batch 120000, worst loss 0.069977 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 02:58:52,970 [INFO] ---------------------------------
2019-04-08 02:59:13,472 [INFO] ---------------------------------
2019-04-08 02:59:13,474 [INFO] Summary:
2019-04-08 02:59:13,474 [INFO] Batch 121000, worst loss 0.043331 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 02:59:13,476 [INFO] Regularization: 982.720154 * 0.0000000100 = 0.0000098272 loss
2019-04-08 02:59:13,476 [INFO] unfolding 0, single step 121001
2019-04-08 02:59:13,477 [INFO] Sum of grad norms of most recent batch: 0.082811
2019-04-08 02:59:13,478 [INFO] ---------------------------------
2019-04-08 02:59:34,661 [INFO] ---------------------------------
2019-04-08 02:59:34,662 [INFO] Summary:
2019-04-08 02:59:34,663 [INFO] Batch 122000, worst loss 0.043198 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 02:59:34,663 [INFO] Regularization: 982.647949 * 0.0000000100 = 0.0000098265 loss
2019-04-08 02:59:34,664 [INFO] unfolding 0, single step 122001
2019-04-08 02:59:34,664 [INFO] Sum of grad norms of most recent batch: 0.020954
2019-04-08 02:59:34,665 [INFO] ---------------------------------
2019-04-08 02:59:55,590 [INFO] ---------------------------------
2019-04-08 02:59:55,591 [INFO] Summary:
2019-04-08 02:59:55,592 [INFO] Batch 123000, worst loss 0.029353 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 02:59:55,593 [INFO] Regularization: 982.590454 * 0.0000000100 = 0.0000098259 loss
2019-04-08 02:59:55,593 [INFO] unfolding 0, single step 123001
2019-04-08 02:59:55,594 [INFO] Sum of grad norms of most recent batch: 0.017889
2019-04-08 02:59:55,595 [INFO] ---------------------------------
2019-04-08 03:00:17,335 [INFO] ---------------------------------
2019-04-08 03:00:17,336 [INFO] Summary:
2019-04-08 03:00:17,337 [INFO] Batch 124000, worst loss 0.025933 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 03:00:17,337 [INFO] Regularization: 982.477783 * 0.0000000100 = 0.0000098248 loss
2019-04-08 03:00:17,338 [INFO] unfolding 0, single step 124001
2019-04-08 03:00:17,338 [INFO] Sum of grad norms of most recent batch: 0.037302
2019-04-08 03:00:17,339 [INFO] ---------------------------------
2019-04-08 03:00:38,325 [INFO] ---------------------------------
2019-04-08 03:00:38,326 [INFO] Summary:
2019-04-08 03:00:38,327 [INFO] Batch 125000, worst loss 0.056805 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 03:00:38,328 [INFO] Regularization: 982.429932 * 0.0000000100 = 0.0000098243 loss
2019-04-08 03:00:38,328 [INFO] unfolding 0, single step 125001
2019-04-08 03:00:38,329 [INFO] Sum of grad norms of most recent batch: 0.012889
2019-04-08 03:00:38,330 [INFO] ---------------------------------
2019-04-08 03:00:59,913 [INFO] ---------------------------------
2019-04-08 03:00:59,913 [INFO] Summary:
2019-04-08 03:00:59,914 [INFO] Batch 126000, worst loss 0.064417 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 03:00:59,915 [INFO] Regularization: 982.435852 * 0.0000000100 = 0.0000098244 loss
2019-04-08 03:00:59,915 [INFO] unfolding 0, single step 126001
2019-04-08 03:00:59,916 [INFO] Sum of grad norms of most recent batch: 0.015373
2019-04-08 03:00:59,916 [INFO] ---------------------------------
2019-04-08 03:01:20,688 [INFO] ---------------------------------
2019-04-08 03:01:20,689 [INFO] Summary:
2019-04-08 03:01:20,690 [INFO] Batch 127000, worst loss 0.069911 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 03:01:20,691 [INFO] Regularization: 982.391174 * 0.0000000100 = 0.0000098239 loss
2019-04-08 03:01:20,691 [INFO] unfolding 0, single step 127001
2019-04-08 03:01:20,692 [INFO] Sum of grad norms of most recent batch: 0.015233
2019-04-08 03:01:20,693 [INFO] ---------------------------------
2019-04-08 03:01:41,653 [INFO] ---------------------------------
2019-04-08 03:01:41,654 [INFO] Summary:
2019-04-08 03:01:41,654 [INFO] Batch 128000, worst loss 0.038855 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 03:01:41,655 [INFO] Regularization: 982.342041 * 0.0000000100 = 0.0000098234 loss
2019-04-08 03:01:41,655 [INFO] unfolding 0, single step 128001
2019-04-08 03:01:41,656 [INFO] Sum of grad norms of most recent batch: 0.017054
2019-04-08 03:01:41,656 [INFO] ---------------------------------
2019-04-08 03:02:02,617 [INFO] ---------------------------------
2019-04-08 03:02:02,618 [INFO] Summary:
2019-04-08 03:02:02,618 [INFO] Batch 129000, worst loss 0.045458 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 03:02:02,619 [INFO] Regularization: 982.308350 * 0.0000000100 = 0.0000098231 loss
2019-04-08 03:02:02,619 [INFO] unfolding 0, single step 129001
2019-04-08 03:02:02,620 [INFO] Sum of grad norms of most recent batch: 0.008377
2019-04-08 03:02:02,621 [INFO] ---------------------------------
2019-04-08 03:02:23,673 [INFO] ---------------------------------
2019-04-08 03:02:23,675 [INFO] Summary:
2019-04-08 03:02:23,675 [INFO] Batch 130000, worst loss 0.098869 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 03:02:23,676 [INFO] Regularization: 982.352905 * 0.0000000100 = 0.0000098235 loss
2019-04-08 03:02:23,677 [INFO] unfolding 0, single step 130001
2019-04-08 03:02:23,678 [INFO] Sum of grad norms of most recent batch: 0.040572
2019-04-08 03:02:23,679 [INFO] ---------------------------------
2019-04-08 03:03:01,158 [INFO] ---------------------------------
2019-04-08 03:03:01,188 [INFO] Evaluation:
2019-04-08 03:03:01,188 [INFO] Batch 130000, worst loss 0.124275 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:03:01,189 [INFO] ---------------------------------
2019-04-08 03:03:22,255 [INFO] ---------------------------------
2019-04-08 03:03:22,256 [INFO] Summary:
2019-04-08 03:03:22,256 [INFO] Batch 131000, worst loss 0.027791 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:03:22,257 [INFO] Regularization: 982.251770 * 0.0000000100 = 0.0000098225 loss
2019-04-08 03:03:22,258 [INFO] unfolding 0, single step 131001
2019-04-08 03:03:22,258 [INFO] Sum of grad norms of most recent batch: 0.032646
2019-04-08 03:03:22,259 [INFO] ---------------------------------
2019-04-08 03:03:42,935 [INFO] ---------------------------------
2019-04-08 03:03:42,936 [INFO] Summary:
2019-04-08 03:03:42,936 [INFO] Batch 132000, worst loss 0.026594 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:03:42,937 [INFO] Regularization: 982.208130 * 0.0000000100 = 0.0000098221 loss
2019-04-08 03:03:42,937 [INFO] unfolding 0, single step 132001
2019-04-08 03:03:42,938 [INFO] Sum of grad norms of most recent batch: 0.012120
2019-04-08 03:03:42,939 [INFO] ---------------------------------
2019-04-08 03:04:04,331 [INFO] ---------------------------------
2019-04-08 03:04:04,332 [INFO] Summary:
2019-04-08 03:04:04,332 [INFO] Batch 133000, worst loss 0.031979 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:04:04,333 [INFO] Regularization: 982.198547 * 0.0000000100 = 0.0000098220 loss
2019-04-08 03:04:04,333 [INFO] unfolding 0, single step 133001
2019-04-08 03:04:04,333 [INFO] Sum of grad norms of most recent batch: 0.031141
2019-04-08 03:04:04,334 [INFO] ---------------------------------
2019-04-08 03:04:26,034 [INFO] ---------------------------------
2019-04-08 03:04:26,035 [INFO] Summary:
2019-04-08 03:04:26,036 [INFO] Batch 134000, worst loss 0.034062 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:04:26,037 [INFO] Regularization: 982.162354 * 0.0000000100 = 0.0000098216 loss
2019-04-08 03:04:26,037 [INFO] unfolding 0, single step 134001
2019-04-08 03:04:26,038 [INFO] Sum of grad norms of most recent batch: 0.018611
2019-04-08 03:04:26,039 [INFO] ---------------------------------
2019-04-08 03:04:46,666 [INFO] ---------------------------------
2019-04-08 03:04:46,667 [INFO] Summary:
2019-04-08 03:04:46,667 [INFO] Batch 135000, worst loss 0.015392 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:04:46,668 [INFO] Regularization: 982.135864 * 0.0000000100 = 0.0000098214 loss
2019-04-08 03:04:46,668 [INFO] unfolding 0, single step 135001
2019-04-08 03:04:46,669 [INFO] Sum of grad norms of most recent batch: 0.020508
2019-04-08 03:04:46,669 [INFO] ---------------------------------
2019-04-08 03:05:08,359 [INFO] ---------------------------------
2019-04-08 03:05:08,360 [INFO] Summary:
2019-04-08 03:05:08,361 [INFO] Batch 136000, worst loss 0.106622 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:05:08,362 [INFO] Regularization: 982.124817 * 0.0000000100 = 0.0000098212 loss
2019-04-08 03:05:08,362 [INFO] unfolding 0, single step 136001
2019-04-08 03:05:08,363 [INFO] Sum of grad norms of most recent batch: 0.017157
2019-04-08 03:05:08,364 [INFO] ---------------------------------
2019-04-08 03:05:30,269 [INFO] ---------------------------------
2019-04-08 03:05:30,270 [INFO] Summary:
2019-04-08 03:05:30,271 [INFO] Batch 137000, worst loss 0.050387 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:05:30,271 [INFO] Regularization: 982.123108 * 0.0000000100 = 0.0000098212 loss
2019-04-08 03:05:30,271 [INFO] unfolding 0, single step 137001
2019-04-08 03:05:30,272 [INFO] Sum of grad norms of most recent batch: 0.062137
2019-04-08 03:05:30,272 [INFO] ---------------------------------
2019-04-08 03:05:51,403 [INFO] ---------------------------------
2019-04-08 03:05:51,404 [INFO] Summary:
2019-04-08 03:05:51,404 [INFO] Batch 138000, worst loss 0.046399 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:05:51,405 [INFO] Regularization: 982.107910 * 0.0000000100 = 0.0000098211 loss
2019-04-08 03:05:51,405 [INFO] unfolding 0, single step 138001
2019-04-08 03:05:51,406 [INFO] Sum of grad norms of most recent batch: 0.040752
2019-04-08 03:05:51,407 [INFO] ---------------------------------
2019-04-08 03:06:12,724 [INFO] ---------------------------------
2019-04-08 03:06:12,726 [INFO] Summary:
2019-04-08 03:06:12,726 [INFO] Batch 139000, worst loss 0.012867 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:06:12,727 [INFO] Regularization: 982.089111 * 0.0000000100 = 0.0000098209 loss
2019-04-08 03:06:12,728 [INFO] unfolding 0, single step 139001
2019-04-08 03:06:12,729 [INFO] Sum of grad norms of most recent batch: 0.267787
2019-04-08 03:06:12,730 [INFO] ---------------------------------
2019-04-08 03:06:34,126 [INFO] ---------------------------------
2019-04-08 03:06:34,127 [INFO] Summary:
2019-04-08 03:06:34,128 [INFO] Batch 140000, worst loss 0.057804 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:06:34,128 [INFO] Regularization: 982.028137 * 0.0000000100 = 0.0000098203 loss
2019-04-08 03:06:34,129 [INFO] unfolding 0, single step 140001
2019-04-08 03:06:34,129 [INFO] Sum of grad norms of most recent batch: 0.034382
2019-04-08 03:06:34,130 [INFO] ---------------------------------
2019-04-08 03:07:11,533 [INFO] ---------------------------------
2019-04-08 03:07:11,534 [INFO] Evaluation:
2019-04-08 03:07:11,535 [INFO] Batch 140000, worst loss 0.086152 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:07:11,535 [INFO] ---------------------------------
2019-04-08 03:07:32,500 [INFO] ---------------------------------
2019-04-08 03:07:32,501 [INFO] Summary:
2019-04-08 03:07:32,502 [INFO] Batch 141000, worst loss 0.083043 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:07:32,502 [INFO] Regularization: 981.987061 * 0.0000000100 = 0.0000098199 loss
2019-04-08 03:07:32,502 [INFO] unfolding 0, single step 141001
2019-04-08 03:07:32,503 [INFO] Sum of grad norms of most recent batch: 0.016636
2019-04-08 03:07:32,503 [INFO] ---------------------------------
2019-04-08 03:07:54,167 [INFO] ---------------------------------
2019-04-08 03:07:54,168 [INFO] Summary:
2019-04-08 03:07:54,168 [INFO] Batch 142000, worst loss 0.067617 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:07:54,169 [INFO] Regularization: 981.973328 * 0.0000000100 = 0.0000098197 loss
2019-04-08 03:07:54,169 [INFO] unfolding 0, single step 142001
2019-04-08 03:07:54,170 [INFO] Sum of grad norms of most recent batch: 0.008753
2019-04-08 03:07:54,170 [INFO] ---------------------------------
2019-04-08 03:08:15,877 [INFO] ---------------------------------
2019-04-08 03:08:15,878 [INFO] Summary:
2019-04-08 03:08:15,879 [INFO] Batch 143000, worst loss 0.065964 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:08:15,880 [INFO] Regularization: 981.966370 * 0.0000000100 = 0.0000098197 loss
2019-04-08 03:08:15,880 [INFO] unfolding 0, single step 143001
2019-04-08 03:08:15,881 [INFO] Sum of grad norms of most recent batch: 0.008652
2019-04-08 03:08:15,881 [INFO] ---------------------------------
2019-04-08 03:08:36,883 [INFO] ---------------------------------
2019-04-08 03:08:36,884 [INFO] Summary:
2019-04-08 03:08:36,885 [INFO] Batch 144000, worst loss 0.091481 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:08:36,886 [INFO] Regularization: 981.952209 * 0.0000000100 = 0.0000098195 loss
2019-04-08 03:08:36,886 [INFO] unfolding 0, single step 144001
2019-04-08 03:08:36,887 [INFO] Sum of grad norms of most recent batch: 0.102260
2019-04-08 03:08:36,888 [INFO] ---------------------------------
2019-04-08 03:08:57,000 [INFO] ---------------------------------
2019-04-08 03:08:57,001 [INFO] Summary:
2019-04-08 03:08:57,001 [INFO] Batch 145000, worst loss 0.041297 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:08:57,002 [INFO] Regularization: 981.933044 * 0.0000000100 = 0.0000098193 loss
2019-04-08 03:08:57,002 [INFO] unfolding 0, single step 145001
2019-04-08 03:08:57,003 [INFO] Sum of grad norms of most recent batch: 0.039309
2019-04-08 03:08:57,003 [INFO] ---------------------------------
2019-04-08 03:09:17,492 [INFO] ---------------------------------
2019-04-08 03:09:17,493 [INFO] Summary:
2019-04-08 03:09:17,494 [INFO] Batch 146000, worst loss 0.033376 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:09:17,495 [INFO] Regularization: 981.927917 * 0.0000000100 = 0.0000098193 loss
2019-04-08 03:09:17,495 [INFO] unfolding 0, single step 146001
2019-04-08 03:09:17,496 [INFO] Sum of grad norms of most recent batch: 0.024614
2019-04-08 03:09:17,496 [INFO] ---------------------------------
2019-04-08 03:09:38,202 [INFO] ---------------------------------
2019-04-08 03:09:38,203 [INFO] Summary:
2019-04-08 03:09:38,204 [INFO] Batch 147000, worst loss 0.046450 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:09:38,204 [INFO] Regularization: 981.913269 * 0.0000000100 = 0.0000098191 loss
2019-04-08 03:09:38,204 [INFO] unfolding 0, single step 147001
2019-04-08 03:09:38,205 [INFO] Sum of grad norms of most recent batch: 0.209748
2019-04-08 03:09:38,205 [INFO] ---------------------------------
2019-04-08 03:09:59,994 [INFO] ---------------------------------
2019-04-08 03:09:59,995 [INFO] Summary:
2019-04-08 03:09:59,996 [INFO] Batch 148000, worst loss 0.080699 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:09:59,996 [INFO] Regularization: 981.909973 * 0.0000000100 = 0.0000098191 loss
2019-04-08 03:09:59,997 [INFO] unfolding 0, single step 148001
2019-04-08 03:09:59,997 [INFO] Sum of grad norms of most recent batch: 6.082698
2019-04-08 03:09:59,998 [INFO] ---------------------------------
2019-04-08 03:10:20,667 [INFO] ---------------------------------
2019-04-08 03:10:20,669 [INFO] Summary:
2019-04-08 03:10:20,669 [INFO] Batch 149000, worst loss 0.039655 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:10:20,670 [INFO] Regularization: 981.887329 * 0.0000000100 = 0.0000098189 loss
2019-04-08 03:10:20,670 [INFO] unfolding 0, single step 149001
2019-04-08 03:10:20,671 [INFO] Sum of grad norms of most recent batch: 0.048650
2019-04-08 03:10:20,671 [INFO] ---------------------------------
2019-04-08 03:10:41,265 [INFO] ---------------------------------
2019-04-08 03:10:41,266 [INFO] Summary:
2019-04-08 03:10:41,267 [INFO] Batch 150000, worst loss 0.051067 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 03:10:41,267 [INFO] Regularization: 981.886230 * 0.0000000100 = 0.0000098189 loss
2019-04-08 03:10:41,268 [INFO] unfolding 0, single step 150001
2019-04-08 03:10:41,268 [INFO] Sum of grad norms of most recent batch: 0.017486
2019-04-08 03:10:41,269 [INFO] ---------------------------------
2019-04-08 03:11:18,743 [INFO] ---------------------------------
2019-04-08 03:11:18,744 [INFO] Evaluation:
2019-04-08 03:11:18,745 [INFO] Batch 150000, worst loss 0.104917 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:11:18,745 [INFO] ---------------------------------
2019-04-08 03:11:18,745 [INFO] Finished training, saved to file transition/1554653688/1554685878_7_transition_final.pth
2019-04-08 03:11:18,931 [INFO] ---------------------------------
2019-04-08 03:11:18,933 [INFO] Training model #8: (11, 64, 8) @ 3
2019-04-08 03:11:40,280 [INFO] ---------------------------------
2019-04-08 03:11:40,281 [INFO] Summary:
2019-04-08 03:11:40,282 [INFO] Batch 1000, worst loss 79.955078 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:11:40,282 [INFO] Regularization: 6921.328613 * 0.0000000100 = 0.0000692133 loss
2019-04-08 03:11:40,283 [INFO] unfolding 0, single step 1001
2019-04-08 03:11:40,284 [INFO] Sum of grad norms of most recent batch: 22.470459
2019-04-08 03:11:40,284 [INFO] ---------------------------------
2019-04-08 03:12:01,899 [INFO] ---------------------------------
2019-04-08 03:12:01,900 [INFO] Summary:
2019-04-08 03:12:01,900 [INFO] Batch 2000, worst loss 0.146427 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:12:01,901 [INFO] Regularization: 4642.280762 * 0.0000000100 = 0.0000464228 loss
2019-04-08 03:12:01,901 [INFO] unfolding 0, single step 2001
2019-04-08 03:12:01,902 [INFO] Sum of grad norms of most recent batch: 6.785983
2019-04-08 03:12:01,902 [INFO] ---------------------------------
2019-04-08 03:12:22,453 [INFO] ---------------------------------
2019-04-08 03:12:22,453 [INFO] Summary:
2019-04-08 03:12:22,454 [INFO] Batch 3000, worst loss 0.158681 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:12:22,455 [INFO] Regularization: 3238.930908 * 0.0000000100 = 0.0000323893 loss
2019-04-08 03:12:22,455 [INFO] unfolding 0, single step 3001
2019-04-08 03:12:22,456 [INFO] Sum of grad norms of most recent batch: 8.894265
2019-04-08 03:12:22,456 [INFO] ---------------------------------
2019-04-08 03:12:44,272 [INFO] ---------------------------------
2019-04-08 03:12:44,273 [INFO] Summary:
2019-04-08 03:12:44,274 [INFO] Batch 4000, worst loss 0.040132 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:12:44,275 [INFO] Regularization: 2946.456543 * 0.0000000100 = 0.0000294646 loss
2019-04-08 03:12:44,275 [INFO] unfolding 0, single step 4001
2019-04-08 03:12:44,277 [INFO] Sum of grad norms of most recent batch: 2.140343
2019-04-08 03:12:44,278 [INFO] ---------------------------------
2019-04-08 03:13:06,576 [INFO] ---------------------------------
2019-04-08 03:13:06,577 [INFO] Summary:
2019-04-08 03:13:06,578 [INFO] Batch 5000, worst loss 0.110565 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:13:06,578 [INFO] Regularization: 2691.079834 * 0.0000000100 = 0.0000269108 loss
2019-04-08 03:13:06,579 [INFO] unfolding 0, single step 5001
2019-04-08 03:13:06,579 [INFO] Sum of grad norms of most recent batch: 2.252537
2019-04-08 03:13:06,580 [INFO] ---------------------------------
2019-04-08 03:13:28,037 [INFO] ---------------------------------
2019-04-08 03:13:28,038 [INFO] Summary:
2019-04-08 03:13:28,038 [INFO] Batch 6000, worst loss 0.163990 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:13:28,039 [INFO] Regularization: 2431.267334 * 0.0000000100 = 0.0000243127 loss
2019-04-08 03:13:28,039 [INFO] unfolding 0, single step 6001
2019-04-08 03:13:28,040 [INFO] Sum of grad norms of most recent batch: 3.459482
2019-04-08 03:13:28,041 [INFO] ---------------------------------
2019-04-08 03:13:49,845 [INFO] ---------------------------------
2019-04-08 03:13:49,846 [INFO] Summary:
2019-04-08 03:13:49,847 [INFO] Batch 7000, worst loss 0.061453 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:13:49,847 [INFO] Regularization: 2226.975098 * 0.0000000100 = 0.0000222698 loss
2019-04-08 03:13:49,848 [INFO] unfolding 0, single step 7001
2019-04-08 03:13:49,848 [INFO] Sum of grad norms of most recent batch: 2.535229
2019-04-08 03:13:49,849 [INFO] ---------------------------------
2019-04-08 03:14:11,086 [INFO] ---------------------------------
2019-04-08 03:14:11,087 [INFO] Summary:
2019-04-08 03:14:11,087 [INFO] Batch 8000, worst loss 0.110201 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:14:11,088 [INFO] Regularization: 2019.427856 * 0.0000000100 = 0.0000201943 loss
2019-04-08 03:14:11,088 [INFO] unfolding 0, single step 8001
2019-04-08 03:14:11,089 [INFO] Sum of grad norms of most recent batch: 1.393897
2019-04-08 03:14:11,089 [INFO] ---------------------------------
2019-04-08 03:14:32,497 [INFO] ---------------------------------
2019-04-08 03:14:32,498 [INFO] Summary:
2019-04-08 03:14:32,499 [INFO] Batch 9000, worst loss 0.127050 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:14:32,499 [INFO] Regularization: 1875.976440 * 0.0000000100 = 0.0000187598 loss
2019-04-08 03:14:32,500 [INFO] unfolding 0, single step 9001
2019-04-08 03:14:32,500 [INFO] Sum of grad norms of most recent batch: 1.729801
2019-04-08 03:14:32,501 [INFO] ---------------------------------
2019-04-08 03:14:54,139 [INFO] ---------------------------------
2019-04-08 03:14:54,140 [INFO] Summary:
2019-04-08 03:14:54,141 [INFO] Batch 10000, worst loss 0.097681 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:14:54,141 [INFO] Regularization: 1748.307617 * 0.0000000100 = 0.0000174831 loss
2019-04-08 03:14:54,142 [INFO] unfolding 0, single step 10001
2019-04-08 03:14:54,143 [INFO] Sum of grad norms of most recent batch: 4.359501
2019-04-08 03:14:54,143 [INFO] ---------------------------------
2019-04-08 03:15:31,460 [INFO] ---------------------------------
2019-04-08 03:15:31,461 [INFO] Evaluation:
2019-04-08 03:15:31,461 [INFO] Batch 10000, worst loss 0.162496 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:15:31,462 [INFO] ---------------------------------
2019-04-08 03:15:53,401 [INFO] ---------------------------------
2019-04-08 03:15:53,402 [INFO] Summary:
2019-04-08 03:15:53,403 [INFO] Batch 11000, worst loss 0.146239 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:15:53,403 [INFO] Regularization: 1713.296753 * 0.0000000100 = 0.0000171330 loss
2019-04-08 03:15:53,404 [INFO] unfolding 0, single step 11001
2019-04-08 03:15:53,404 [INFO] Sum of grad norms of most recent batch: 6.564374
2019-04-08 03:15:53,405 [INFO] ---------------------------------
2019-04-08 03:16:15,116 [INFO] ---------------------------------
2019-04-08 03:16:15,117 [INFO] Summary:
2019-04-08 03:16:15,117 [INFO] Batch 12000, worst loss 0.137203 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:16:15,118 [INFO] Regularization: 1654.684448 * 0.0000000100 = 0.0000165468 loss
2019-04-08 03:16:15,118 [INFO] unfolding 0, single step 12001
2019-04-08 03:16:15,118 [INFO] Sum of grad norms of most recent batch: 1.626591
2019-04-08 03:16:15,119 [INFO] ---------------------------------
2019-04-08 03:16:37,269 [INFO] ---------------------------------
2019-04-08 03:16:37,270 [INFO] Summary:
2019-04-08 03:16:37,271 [INFO] Batch 13000, worst loss 0.126684 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:16:37,271 [INFO] Regularization: 1605.402832 * 0.0000000100 = 0.0000160540 loss
2019-04-08 03:16:37,272 [INFO] unfolding 0, single step 13001
2019-04-08 03:16:37,272 [INFO] Sum of grad norms of most recent batch: 2.628752
2019-04-08 03:16:37,273 [INFO] ---------------------------------
2019-04-08 03:16:58,476 [INFO] ---------------------------------
2019-04-08 03:16:58,477 [INFO] Summary:
2019-04-08 03:16:58,478 [INFO] Batch 14000, worst loss 0.127847 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:16:58,478 [INFO] Regularization: 1624.376709 * 0.0000000100 = 0.0000162438 loss
2019-04-08 03:16:58,479 [INFO] unfolding 0, single step 14001
2019-04-08 03:16:58,480 [INFO] Sum of grad norms of most recent batch: 2.523622
2019-04-08 03:16:58,480 [INFO] ---------------------------------
2019-04-08 03:17:19,715 [INFO] ---------------------------------
2019-04-08 03:17:19,716 [INFO] Summary:
2019-04-08 03:17:19,716 [INFO] Batch 15000, worst loss 0.092448 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:17:19,717 [INFO] Regularization: 1575.363037 * 0.0000000100 = 0.0000157536 loss
2019-04-08 03:17:19,717 [INFO] unfolding 0, single step 15001
2019-04-08 03:17:19,718 [INFO] Sum of grad norms of most recent batch: 8.051368
2019-04-08 03:17:19,718 [INFO] ---------------------------------
2019-04-08 03:17:41,370 [INFO] ---------------------------------
2019-04-08 03:17:41,371 [INFO] Summary:
2019-04-08 03:17:41,371 [INFO] Batch 16000, worst loss 0.113983 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:17:41,372 [INFO] Regularization: 1507.315186 * 0.0000000100 = 0.0000150732 loss
2019-04-08 03:17:41,372 [INFO] unfolding 0, single step 16001
2019-04-08 03:17:41,373 [INFO] Sum of grad norms of most recent batch: 2.615407
2019-04-08 03:17:41,373 [INFO] ---------------------------------
2019-04-08 03:18:02,717 [INFO] ---------------------------------
2019-04-08 03:18:02,718 [INFO] Summary:
2019-04-08 03:18:02,718 [INFO] Batch 17000, worst loss 0.041998 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:18:02,719 [INFO] Regularization: 1444.608276 * 0.0000000100 = 0.0000144461 loss
2019-04-08 03:18:02,719 [INFO] unfolding 0, single step 17001
2019-04-08 03:18:02,720 [INFO] Sum of grad norms of most recent batch: 1.657145
2019-04-08 03:18:02,720 [INFO] ---------------------------------
2019-04-08 03:18:24,255 [INFO] ---------------------------------
2019-04-08 03:18:24,256 [INFO] Summary:
2019-04-08 03:18:24,257 [INFO] Batch 18000, worst loss 0.042580 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:18:24,258 [INFO] Regularization: 1387.624634 * 0.0000000100 = 0.0000138762 loss
2019-04-08 03:18:24,258 [INFO] unfolding 0, single step 18001
2019-04-08 03:18:24,259 [INFO] Sum of grad norms of most recent batch: 1.106689
2019-04-08 03:18:24,260 [INFO] ---------------------------------
2019-04-08 03:18:45,738 [INFO] ---------------------------------
2019-04-08 03:18:45,739 [INFO] Summary:
2019-04-08 03:18:45,740 [INFO] Batch 19000, worst loss 0.147948 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:18:45,740 [INFO] Regularization: 1415.920410 * 0.0000000100 = 0.0000141592 loss
2019-04-08 03:18:45,741 [INFO] unfolding 0, single step 19001
2019-04-08 03:18:45,741 [INFO] Sum of grad norms of most recent batch: 2.941591
2019-04-08 03:18:45,742 [INFO] ---------------------------------
2019-04-08 03:19:06,769 [INFO] ---------------------------------
2019-04-08 03:19:06,770 [INFO] Summary:
2019-04-08 03:19:06,770 [INFO] Batch 20000, worst loss 0.069542 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:19:06,771 [INFO] Regularization: 1349.061646 * 0.0000000100 = 0.0000134906 loss
2019-04-08 03:19:06,771 [INFO] unfolding 0, single step 20001
2019-04-08 03:19:06,772 [INFO] Sum of grad norms of most recent batch: 3.958261
2019-04-08 03:19:06,772 [INFO] ---------------------------------
2019-04-08 03:19:44,197 [INFO] ---------------------------------
2019-04-08 03:19:44,198 [INFO] Evaluation:
2019-04-08 03:19:44,198 [INFO] Batch 20000, worst loss 0.145699 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:19:44,199 [INFO] ---------------------------------
2019-04-08 03:20:06,198 [INFO] ---------------------------------
2019-04-08 03:20:06,199 [INFO] Summary:
2019-04-08 03:20:06,200 [INFO] Batch 21000, worst loss 0.080173 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:20:06,200 [INFO] Regularization: 1302.585083 * 0.0000000100 = 0.0000130259 loss
2019-04-08 03:20:06,200 [INFO] unfolding 0, single step 21001
2019-04-08 03:20:06,201 [INFO] Sum of grad norms of most recent batch: 1.372794
2019-04-08 03:20:06,201 [INFO] ---------------------------------
2019-04-08 03:20:27,543 [INFO] ---------------------------------
2019-04-08 03:20:27,544 [INFO] Summary:
2019-04-08 03:20:27,545 [INFO] Batch 22000, worst loss 0.129682 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:20:27,545 [INFO] Regularization: 1254.622803 * 0.0000000100 = 0.0000125462 loss
2019-04-08 03:20:27,546 [INFO] unfolding 0, single step 22001
2019-04-08 03:20:27,546 [INFO] Sum of grad norms of most recent batch: 1.385769
2019-04-08 03:20:27,547 [INFO] ---------------------------------
2019-04-08 03:20:49,180 [INFO] ---------------------------------
2019-04-08 03:20:49,181 [INFO] Summary:
2019-04-08 03:20:49,182 [INFO] Batch 23000, worst loss 0.079242 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:20:49,183 [INFO] Regularization: 1240.706055 * 0.0000000100 = 0.0000124071 loss
2019-04-08 03:20:49,184 [INFO] unfolding 0, single step 23001
2019-04-08 03:20:49,185 [INFO] Sum of grad norms of most recent batch: 3.175143
2019-04-08 03:20:49,186 [INFO] ---------------------------------
2019-04-08 03:21:10,896 [INFO] ---------------------------------
2019-04-08 03:21:10,897 [INFO] Summary:
2019-04-08 03:21:10,898 [INFO] Batch 24000, worst loss 0.099889 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:21:10,899 [INFO] Regularization: 1214.337524 * 0.0000000100 = 0.0000121434 loss
2019-04-08 03:21:10,899 [INFO] unfolding 0, single step 24001
2019-04-08 03:21:10,900 [INFO] Sum of grad norms of most recent batch: 1.385675
2019-04-08 03:21:10,900 [INFO] ---------------------------------
2019-04-08 03:21:32,290 [INFO] ---------------------------------
2019-04-08 03:21:32,291 [INFO] Summary:
2019-04-08 03:21:32,292 [INFO] Batch 25000, worst loss 0.055148 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:21:32,292 [INFO] Regularization: 1156.008179 * 0.0000000100 = 0.0000115601 loss
2019-04-08 03:21:32,293 [INFO] unfolding 0, single step 25001
2019-04-08 03:21:32,293 [INFO] Sum of grad norms of most recent batch: 0.550232
2019-04-08 03:21:32,294 [INFO] ---------------------------------
2019-04-08 03:21:54,263 [INFO] ---------------------------------
2019-04-08 03:21:54,264 [INFO] Summary:
2019-04-08 03:21:54,264 [INFO] Batch 26000, worst loss 0.126902 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:21:54,265 [INFO] Regularization: 1141.603027 * 0.0000000100 = 0.0000114160 loss
2019-04-08 03:21:54,265 [INFO] unfolding 0, single step 26001
2019-04-08 03:21:54,266 [INFO] Sum of grad norms of most recent batch: 0.548388
2019-04-08 03:21:54,266 [INFO] ---------------------------------
2019-04-08 03:22:15,718 [INFO] ---------------------------------
2019-04-08 03:22:15,719 [INFO] Summary:
2019-04-08 03:22:15,719 [INFO] Batch 27000, worst loss 0.148377 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:22:15,720 [INFO] Regularization: 1154.470947 * 0.0000000100 = 0.0000115447 loss
2019-04-08 03:22:15,720 [INFO] unfolding 0, single step 27001
2019-04-08 03:22:15,721 [INFO] Sum of grad norms of most recent batch: 4.180277
2019-04-08 03:22:15,722 [INFO] ---------------------------------
2019-04-08 03:22:37,526 [INFO] ---------------------------------
2019-04-08 03:22:37,528 [INFO] Summary:
2019-04-08 03:22:37,528 [INFO] Batch 28000, worst loss 0.108399 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:22:37,529 [INFO] Regularization: 1123.607788 * 0.0000000100 = 0.0000112361 loss
2019-04-08 03:22:37,530 [INFO] unfolding 0, single step 28001
2019-04-08 03:22:37,532 [INFO] Sum of grad norms of most recent batch: 1.364884
2019-04-08 03:22:37,533 [INFO] ---------------------------------
2019-04-08 03:22:58,936 [INFO] ---------------------------------
2019-04-08 03:22:58,937 [INFO] Summary:
2019-04-08 03:22:58,939 [INFO] Batch 29000, worst loss 0.137348 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:22:58,939 [INFO] Regularization: 1119.586182 * 0.0000000100 = 0.0000111959 loss
2019-04-08 03:22:58,940 [INFO] unfolding 0, single step 29001
2019-04-08 03:22:58,941 [INFO] Sum of grad norms of most recent batch: 0.857295
2019-04-08 03:22:58,942 [INFO] ---------------------------------
2019-04-08 03:23:20,247 [INFO] ---------------------------------
2019-04-08 03:23:20,248 [INFO] Summary:
2019-04-08 03:23:20,249 [INFO] Batch 30000, worst loss 0.069149 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 03:23:20,249 [INFO] Regularization: 1068.137451 * 0.0000000100 = 0.0000106814 loss
2019-04-08 03:23:20,250 [INFO] unfolding 0, single step 30001
2019-04-08 03:23:20,250 [INFO] Sum of grad norms of most recent batch: 1.426742
2019-04-08 03:23:20,251 [INFO] ---------------------------------
2019-04-08 03:23:57,671 [INFO] ---------------------------------
2019-04-08 03:23:57,672 [INFO] Evaluation:
2019-04-08 03:23:57,673 [INFO] Batch 30000, worst loss 0.181720 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:23:57,673 [INFO] ---------------------------------
2019-04-08 03:24:19,698 [INFO] ---------------------------------
2019-04-08 03:24:19,699 [INFO] Summary:
2019-04-08 03:24:19,700 [INFO] Batch 31000, worst loss 0.101514 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 03:24:19,700 [INFO] Regularization: 1027.859497 * 0.0000000100 = 0.0000102786 loss
2019-04-08 03:24:19,701 [INFO] unfolding 0, single step 31001
2019-04-08 03:24:19,701 [INFO] Sum of grad norms of most recent batch: 0.801519
2019-04-08 03:24:19,702 [INFO] ---------------------------------
2019-04-08 03:24:41,639 [INFO] ---------------------------------
2019-04-08 03:24:41,640 [INFO] Summary:
2019-04-08 03:24:41,641 [INFO] Batch 32000, worst loss 0.045721 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 03:24:41,642 [INFO] Regularization: 959.823914 * 0.0000000100 = 0.0000095982 loss
2019-04-08 03:24:41,642 [INFO] unfolding 0, single step 32001
2019-04-08 03:24:41,643 [INFO] Sum of grad norms of most recent batch: 0.378089
2019-04-08 03:24:41,644 [INFO] ---------------------------------
2019-04-08 03:25:03,951 [INFO] ---------------------------------
2019-04-08 03:25:03,952 [INFO] Summary:
2019-04-08 03:25:03,953 [INFO] Batch 33000, worst loss 0.074183 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 03:25:03,953 [INFO] Regularization: 916.479309 * 0.0000000100 = 0.0000091648 loss
2019-04-08 03:25:03,954 [INFO] unfolding 0, single step 33001
2019-04-08 03:25:03,954 [INFO] Sum of grad norms of most recent batch: 3.303721
2019-04-08 03:25:03,955 [INFO] ---------------------------------
2019-04-08 03:25:25,793 [INFO] ---------------------------------
2019-04-08 03:25:25,794 [INFO] Summary:
2019-04-08 03:25:25,795 [INFO] Batch 34000, worst loss 0.095366 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 03:25:25,795 [INFO] Regularization: 875.760376 * 0.0000000100 = 0.0000087576 loss
2019-04-08 03:25:25,796 [INFO] unfolding 0, single step 34001
2019-04-08 03:25:25,796 [INFO] Sum of grad norms of most recent batch: 1.068625
2019-04-08 03:25:25,797 [INFO] ---------------------------------
2019-04-08 03:25:46,657 [INFO] ---------------------------------
2019-04-08 03:25:46,658 [INFO] Summary:
2019-04-08 03:25:46,658 [INFO] Batch 35000, worst loss 0.078088 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 03:25:46,659 [INFO] Regularization: 881.949890 * 0.0000000100 = 0.0000088195 loss
2019-04-08 03:25:46,659 [INFO] unfolding 0, single step 35001
2019-04-08 03:25:46,660 [INFO] Sum of grad norms of most recent batch: 0.836294
2019-04-08 03:25:46,660 [INFO] ---------------------------------
2019-04-08 03:26:08,256 [INFO] ---------------------------------
2019-04-08 03:26:08,257 [INFO] Summary:
2019-04-08 03:26:08,258 [INFO] Batch 36000, worst loss 0.103725 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 03:26:08,258 [INFO] Regularization: 865.077026 * 0.0000000100 = 0.0000086508 loss
2019-04-08 03:26:08,258 [INFO] unfolding 0, single step 36001
2019-04-08 03:26:08,259 [INFO] Sum of grad norms of most recent batch: 0.922820
2019-04-08 03:26:08,259 [INFO] ---------------------------------
2019-04-08 03:26:30,170 [INFO] ---------------------------------
2019-04-08 03:26:30,171 [INFO] Summary:
2019-04-08 03:26:30,172 [INFO] Batch 37000, worst loss 0.112252 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 03:26:30,173 [INFO] Regularization: 842.725525 * 0.0000000100 = 0.0000084273 loss
2019-04-08 03:26:30,173 [INFO] unfolding 0, single step 37001
2019-04-08 03:26:30,174 [INFO] Sum of grad norms of most recent batch: 0.547822
2019-04-08 03:26:30,174 [INFO] ---------------------------------
2019-04-08 03:26:52,754 [INFO] ---------------------------------
2019-04-08 03:26:52,755 [INFO] Summary:
2019-04-08 03:26:52,756 [INFO] Batch 38000, worst loss 0.118510 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 03:26:52,756 [INFO] Regularization: 826.419617 * 0.0000000100 = 0.0000082642 loss
2019-04-08 03:26:52,756 [INFO] unfolding 0, single step 38001
2019-04-08 03:26:52,757 [INFO] Sum of grad norms of most recent batch: 0.354728
2019-04-08 03:26:52,758 [INFO] ---------------------------------
2019-04-08 03:27:14,159 [INFO] ---------------------------------
2019-04-08 03:27:14,160 [INFO] Summary:
2019-04-08 03:27:14,160 [INFO] Batch 39000, worst loss 0.079956 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 03:27:14,161 [INFO] Regularization: 832.557373 * 0.0000000100 = 0.0000083256 loss
2019-04-08 03:27:14,161 [INFO] unfolding 0, single step 39001
2019-04-08 03:27:14,162 [INFO] Sum of grad norms of most recent batch: 0.606351
2019-04-08 03:27:14,162 [INFO] ---------------------------------
2019-04-08 03:27:35,650 [INFO] ---------------------------------
2019-04-08 03:27:35,651 [INFO] Summary:
2019-04-08 03:27:35,651 [INFO] Batch 40000, worst loss 0.100932 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 03:27:35,652 [INFO] Regularization: 812.601379 * 0.0000000100 = 0.0000081260 loss
2019-04-08 03:27:35,652 [INFO] unfolding 0, single step 40001
2019-04-08 03:27:35,653 [INFO] Sum of grad norms of most recent batch: 0.843559
2019-04-08 03:27:35,653 [INFO] ---------------------------------
2019-04-08 03:28:12,861 [INFO] ---------------------------------
2019-04-08 03:28:12,862 [INFO] Evaluation:
2019-04-08 03:28:12,862 [INFO] Batch 40000, worst loss 0.149061 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:28:12,863 [INFO] ---------------------------------
2019-04-08 03:28:34,199 [INFO] ---------------------------------
2019-04-08 03:28:34,200 [INFO] Summary:
2019-04-08 03:28:34,201 [INFO] Batch 41000, worst loss 0.092090 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 03:28:34,201 [INFO] Regularization: 804.981995 * 0.0000000100 = 0.0000080498 loss
2019-04-08 03:28:34,201 [INFO] unfolding 0, single step 41001
2019-04-08 03:28:34,202 [INFO] Sum of grad norms of most recent batch: 0.718836
2019-04-08 03:28:34,202 [INFO] ---------------------------------
2019-04-08 03:28:55,589 [INFO] ---------------------------------
2019-04-08 03:28:55,590 [INFO] Summary:
2019-04-08 03:28:55,590 [INFO] Batch 42000, worst loss 0.056110 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 03:28:55,591 [INFO] Regularization: 790.040466 * 0.0000000100 = 0.0000079004 loss
2019-04-08 03:28:55,591 [INFO] unfolding 0, single step 42001
2019-04-08 03:28:55,592 [INFO] Sum of grad norms of most recent batch: 0.317393
2019-04-08 03:28:55,592 [INFO] ---------------------------------
2019-04-08 03:29:17,349 [INFO] ---------------------------------
2019-04-08 03:29:17,350 [INFO] Summary:
2019-04-08 03:29:17,350 [INFO] Batch 43000, worst loss 0.128962 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 03:29:17,351 [INFO] Regularization: 787.521240 * 0.0000000100 = 0.0000078752 loss
2019-04-08 03:29:17,351 [INFO] unfolding 0, single step 43001
2019-04-08 03:29:17,352 [INFO] Sum of grad norms of most recent batch: 0.202219
2019-04-08 03:29:17,353 [INFO] ---------------------------------
2019-04-08 03:29:38,818 [INFO] ---------------------------------
2019-04-08 03:29:38,819 [INFO] Summary:
2019-04-08 03:29:38,820 [INFO] Batch 44000, worst loss 0.079625 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 03:29:38,820 [INFO] Regularization: 777.332397 * 0.0000000100 = 0.0000077733 loss
2019-04-08 03:29:38,821 [INFO] unfolding 0, single step 44001
2019-04-08 03:29:38,821 [INFO] Sum of grad norms of most recent batch: 0.541021
2019-04-08 03:29:38,822 [INFO] ---------------------------------
2019-04-08 03:30:00,595 [INFO] ---------------------------------
2019-04-08 03:30:00,596 [INFO] Summary:
2019-04-08 03:30:00,597 [INFO] Batch 45000, worst loss 0.061405 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 03:30:00,597 [INFO] Regularization: 774.397278 * 0.0000000100 = 0.0000077440 loss
2019-04-08 03:30:00,597 [INFO] unfolding 0, single step 45001
2019-04-08 03:30:00,598 [INFO] Sum of grad norms of most recent batch: 0.316709
2019-04-08 03:30:00,598 [INFO] ---------------------------------
2019-04-08 03:30:22,601 [INFO] ---------------------------------
2019-04-08 03:30:22,603 [INFO] Summary:
2019-04-08 03:30:22,603 [INFO] Batch 46000, worst loss 0.059943 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 03:30:22,604 [INFO] Regularization: 765.384583 * 0.0000000100 = 0.0000076538 loss
2019-04-08 03:30:22,604 [INFO] unfolding 0, single step 46001
2019-04-08 03:30:22,604 [INFO] Sum of grad norms of most recent batch: 0.460856
2019-04-08 03:30:22,605 [INFO] ---------------------------------
2019-04-08 03:30:43,963 [INFO] ---------------------------------
2019-04-08 03:30:43,964 [INFO] Summary:
2019-04-08 03:30:43,965 [INFO] Batch 47000, worst loss 0.057158 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 03:30:43,966 [INFO] Regularization: 746.780823 * 0.0000000100 = 0.0000074678 loss
2019-04-08 03:30:43,967 [INFO] unfolding 0, single step 47001
2019-04-08 03:30:43,968 [INFO] Sum of grad norms of most recent batch: 0.110421
2019-04-08 03:30:43,969 [INFO] ---------------------------------
2019-04-08 03:31:05,573 [INFO] ---------------------------------
2019-04-08 03:31:05,573 [INFO] Summary:
2019-04-08 03:31:05,574 [INFO] Batch 48000, worst loss 0.141809 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 03:31:05,575 [INFO] Regularization: 750.922485 * 0.0000000100 = 0.0000075092 loss
2019-04-08 03:31:05,575 [INFO] unfolding 0, single step 48001
2019-04-08 03:31:05,576 [INFO] Sum of grad norms of most recent batch: 0.236823
2019-04-08 03:31:05,577 [INFO] ---------------------------------
2019-04-08 03:31:27,719 [INFO] ---------------------------------
2019-04-08 03:31:27,720 [INFO] Summary:
2019-04-08 03:31:27,721 [INFO] Batch 49000, worst loss 0.083620 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 03:31:27,722 [INFO] Regularization: 740.480713 * 0.0000000100 = 0.0000074048 loss
2019-04-08 03:31:27,722 [INFO] unfolding 0, single step 49001
2019-04-08 03:31:27,723 [INFO] Sum of grad norms of most recent batch: 0.758044
2019-04-08 03:31:27,724 [INFO] ---------------------------------
2019-04-08 03:31:49,562 [INFO] ---------------------------------
2019-04-08 03:31:49,563 [INFO] Summary:
2019-04-08 03:31:49,563 [INFO] Batch 50000, worst loss 0.021450 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 03:31:49,564 [INFO] Regularization: 733.942932 * 0.0000000100 = 0.0000073394 loss
2019-04-08 03:31:49,564 [INFO] unfolding 0, single step 50001
2019-04-08 03:31:49,565 [INFO] Sum of grad norms of most recent batch: 0.210038
2019-04-08 03:31:49,565 [INFO] ---------------------------------
2019-04-08 03:32:27,084 [INFO] ---------------------------------
2019-04-08 03:32:27,085 [INFO] Evaluation:
2019-04-08 03:32:27,086 [INFO] Batch 50000, worst loss 0.157983 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:32:27,087 [INFO] ---------------------------------
2019-04-08 03:32:49,054 [INFO] ---------------------------------
2019-04-08 03:32:49,055 [INFO] Summary:
2019-04-08 03:32:49,056 [INFO] Batch 51000, worst loss 0.121703 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 03:32:49,056 [INFO] Regularization: 729.731628 * 0.0000000100 = 0.0000072973 loss
2019-04-08 03:32:49,057 [INFO] unfolding 0, single step 51001
2019-04-08 03:32:49,057 [INFO] Sum of grad norms of most recent batch: 0.182518
2019-04-08 03:32:49,058 [INFO] ---------------------------------
2019-04-08 03:33:10,097 [INFO] ---------------------------------
2019-04-08 03:33:10,098 [INFO] Summary:
2019-04-08 03:33:10,098 [INFO] Batch 52000, worst loss 0.036134 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 03:33:10,099 [INFO] Regularization: 715.406494 * 0.0000000100 = 0.0000071541 loss
2019-04-08 03:33:10,099 [INFO] unfolding 0, single step 52001
2019-04-08 03:33:10,100 [INFO] Sum of grad norms of most recent batch: 0.075502
2019-04-08 03:33:10,100 [INFO] ---------------------------------
2019-04-08 03:33:31,718 [INFO] ---------------------------------
2019-04-08 03:33:31,719 [INFO] Summary:
2019-04-08 03:33:31,720 [INFO] Batch 53000, worst loss 0.060732 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 03:33:31,720 [INFO] Regularization: 709.238708 * 0.0000000100 = 0.0000070924 loss
2019-04-08 03:33:31,721 [INFO] unfolding 0, single step 53001
2019-04-08 03:33:31,721 [INFO] Sum of grad norms of most recent batch: 0.081788
2019-04-08 03:33:31,722 [INFO] ---------------------------------
2019-04-08 03:33:52,675 [INFO] ---------------------------------
2019-04-08 03:33:52,675 [INFO] Summary:
2019-04-08 03:33:52,676 [INFO] Batch 54000, worst loss 0.110556 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 03:33:52,677 [INFO] Regularization: 705.098572 * 0.0000000100 = 0.0000070510 loss
2019-04-08 03:33:52,677 [INFO] unfolding 0, single step 54001
2019-04-08 03:33:52,677 [INFO] Sum of grad norms of most recent batch: 0.257755
2019-04-08 03:33:52,678 [INFO] ---------------------------------
2019-04-08 03:34:13,431 [INFO] ---------------------------------
2019-04-08 03:34:13,432 [INFO] Summary:
2019-04-08 03:34:13,432 [INFO] Batch 55000, worst loss 0.116788 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 03:34:13,433 [INFO] Regularization: 698.657104 * 0.0000000100 = 0.0000069866 loss
2019-04-08 03:34:13,433 [INFO] unfolding 0, single step 55001
2019-04-08 03:34:13,434 [INFO] Sum of grad norms of most recent batch: 0.214849
2019-04-08 03:34:13,435 [INFO] ---------------------------------
2019-04-08 03:34:34,677 [INFO] ---------------------------------
2019-04-08 03:34:34,678 [INFO] Summary:
2019-04-08 03:34:34,679 [INFO] Batch 56000, worst loss 0.115970 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 03:34:34,679 [INFO] Regularization: 700.956726 * 0.0000000100 = 0.0000070096 loss
2019-04-08 03:34:34,680 [INFO] unfolding 0, single step 56001
2019-04-08 03:34:34,681 [INFO] Sum of grad norms of most recent batch: 1.052707
2019-04-08 03:34:34,681 [INFO] ---------------------------------
2019-04-08 03:34:56,100 [INFO] ---------------------------------
2019-04-08 03:34:56,101 [INFO] Summary:
2019-04-08 03:34:56,101 [INFO] Batch 57000, worst loss 0.070712 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 03:34:56,102 [INFO] Regularization: 696.992004 * 0.0000000100 = 0.0000069699 loss
2019-04-08 03:34:56,102 [INFO] unfolding 0, single step 57001
2019-04-08 03:34:56,103 [INFO] Sum of grad norms of most recent batch: 0.364269
2019-04-08 03:34:56,103 [INFO] ---------------------------------
2019-04-08 03:35:17,433 [INFO] ---------------------------------
2019-04-08 03:35:17,434 [INFO] Summary:
2019-04-08 03:35:17,435 [INFO] Batch 58000, worst loss 0.037103 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 03:35:17,435 [INFO] Regularization: 694.417236 * 0.0000000100 = 0.0000069442 loss
2019-04-08 03:35:17,436 [INFO] unfolding 0, single step 58001
2019-04-08 03:35:17,436 [INFO] Sum of grad norms of most recent batch: 0.315636
2019-04-08 03:35:17,437 [INFO] ---------------------------------
2019-04-08 03:35:38,788 [INFO] ---------------------------------
2019-04-08 03:35:38,789 [INFO] Summary:
2019-04-08 03:35:38,789 [INFO] Batch 59000, worst loss 0.079929 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 03:35:38,790 [INFO] Regularization: 684.872437 * 0.0000000100 = 0.0000068487 loss
2019-04-08 03:35:38,790 [INFO] unfolding 0, single step 59001
2019-04-08 03:35:38,790 [INFO] Sum of grad norms of most recent batch: 0.432478
2019-04-08 03:35:38,791 [INFO] ---------------------------------
2019-04-08 03:36:00,441 [INFO] ---------------------------------
2019-04-08 03:36:00,442 [INFO] Summary:
2019-04-08 03:36:00,443 [INFO] Batch 60000, worst loss 0.066370 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 03:36:00,443 [INFO] Regularization: 682.596802 * 0.0000000100 = 0.0000068260 loss
2019-04-08 03:36:00,444 [INFO] unfolding 0, single step 60001
2019-04-08 03:36:00,444 [INFO] Sum of grad norms of most recent batch: 0.350629
2019-04-08 03:36:00,445 [INFO] ---------------------------------
2019-04-08 03:36:37,926 [INFO] ---------------------------------
2019-04-08 03:36:37,927 [INFO] Evaluation:
2019-04-08 03:36:37,927 [INFO] Batch 60000, worst loss 0.159896 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:36:37,928 [INFO] ---------------------------------
2019-04-08 03:36:59,373 [INFO] ---------------------------------
2019-04-08 03:36:59,374 [INFO] Summary:
2019-04-08 03:36:59,375 [INFO] Batch 61000, worst loss 0.071548 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 03:36:59,376 [INFO] Regularization: 674.924438 * 0.0000000100 = 0.0000067492 loss
2019-04-08 03:36:59,376 [INFO] unfolding 0, single step 61001
2019-04-08 03:36:59,377 [INFO] Sum of grad norms of most recent batch: 0.149020
2019-04-08 03:36:59,377 [INFO] ---------------------------------
2019-04-08 03:37:20,786 [INFO] ---------------------------------
2019-04-08 03:37:20,787 [INFO] Summary:
2019-04-08 03:37:20,788 [INFO] Batch 62000, worst loss 0.226898 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 03:37:20,788 [INFO] Regularization: 670.250366 * 0.0000000100 = 0.0000067025 loss
2019-04-08 03:37:20,789 [INFO] unfolding 0, single step 62001
2019-04-08 03:37:20,789 [INFO] Sum of grad norms of most recent batch: 0.066865
2019-04-08 03:37:20,790 [INFO] ---------------------------------
2019-04-08 03:37:42,092 [INFO] ---------------------------------
2019-04-08 03:37:42,092 [INFO] Summary:
2019-04-08 03:37:42,093 [INFO] Batch 63000, worst loss 0.058356 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 03:37:42,093 [INFO] Regularization: 670.141052 * 0.0000000100 = 0.0000067014 loss
2019-04-08 03:37:42,094 [INFO] unfolding 0, single step 63001
2019-04-08 03:37:42,094 [INFO] Sum of grad norms of most recent batch: 0.028951
2019-04-08 03:37:42,095 [INFO] ---------------------------------
2019-04-08 03:38:03,748 [INFO] ---------------------------------
2019-04-08 03:38:03,749 [INFO] Summary:
2019-04-08 03:38:03,750 [INFO] Batch 64000, worst loss 0.107819 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 03:38:03,750 [INFO] Regularization: 668.254883 * 0.0000000100 = 0.0000066825 loss
2019-04-08 03:38:03,751 [INFO] unfolding 0, single step 64001
2019-04-08 03:38:03,752 [INFO] Sum of grad norms of most recent batch: 0.070813
2019-04-08 03:38:03,752 [INFO] ---------------------------------
2019-04-08 03:38:24,638 [INFO] ---------------------------------
2019-04-08 03:38:24,639 [INFO] Summary:
2019-04-08 03:38:24,639 [INFO] Batch 65000, worst loss 0.054481 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 03:38:24,640 [INFO] Regularization: 665.054138 * 0.0000000100 = 0.0000066505 loss
2019-04-08 03:38:24,640 [INFO] unfolding 0, single step 65001
2019-04-08 03:38:24,641 [INFO] Sum of grad norms of most recent batch: 0.206445
2019-04-08 03:38:24,642 [INFO] ---------------------------------
2019-04-08 03:38:45,561 [INFO] ---------------------------------
2019-04-08 03:38:45,562 [INFO] Summary:
2019-04-08 03:38:45,562 [INFO] Batch 66000, worst loss 0.099312 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 03:38:45,563 [INFO] Regularization: 662.783508 * 0.0000000100 = 0.0000066278 loss
2019-04-08 03:38:45,563 [INFO] unfolding 0, single step 66001
2019-04-08 03:38:45,564 [INFO] Sum of grad norms of most recent batch: 0.052757
2019-04-08 03:38:45,565 [INFO] ---------------------------------
2019-04-08 03:39:06,676 [INFO] ---------------------------------
2019-04-08 03:39:06,677 [INFO] Summary:
2019-04-08 03:39:06,677 [INFO] Batch 67000, worst loss 0.081000 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 03:39:06,678 [INFO] Regularization: 660.551819 * 0.0000000100 = 0.0000066055 loss
2019-04-08 03:39:06,678 [INFO] unfolding 0, single step 67001
2019-04-08 03:39:06,679 [INFO] Sum of grad norms of most recent batch: 0.041780
2019-04-08 03:39:06,679 [INFO] ---------------------------------
2019-04-08 03:39:27,815 [INFO] ---------------------------------
2019-04-08 03:39:27,816 [INFO] Summary:
2019-04-08 03:39:27,817 [INFO] Batch 68000, worst loss 0.093547 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 03:39:27,818 [INFO] Regularization: 658.622559 * 0.0000000100 = 0.0000065862 loss
2019-04-08 03:39:27,819 [INFO] unfolding 0, single step 68001
2019-04-08 03:39:27,820 [INFO] Sum of grad norms of most recent batch: 0.192923
2019-04-08 03:39:27,821 [INFO] ---------------------------------
2019-04-08 03:39:49,168 [INFO] ---------------------------------
2019-04-08 03:39:49,169 [INFO] Summary:
2019-04-08 03:39:49,170 [INFO] Batch 69000, worst loss 0.064529 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 03:39:49,171 [INFO] Regularization: 655.785095 * 0.0000000100 = 0.0000065579 loss
2019-04-08 03:39:49,171 [INFO] unfolding 0, single step 69001
2019-04-08 03:39:49,172 [INFO] Sum of grad norms of most recent batch: 0.097704
2019-04-08 03:39:49,173 [INFO] ---------------------------------
2019-04-08 03:40:10,640 [INFO] ---------------------------------
2019-04-08 03:40:10,641 [INFO] Summary:
2019-04-08 03:40:10,641 [INFO] Batch 70000, worst loss 0.049022 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 03:40:10,642 [INFO] Regularization: 652.177856 * 0.0000000100 = 0.0000065218 loss
2019-04-08 03:40:10,642 [INFO] unfolding 0, single step 70001
2019-04-08 03:40:10,643 [INFO] Sum of grad norms of most recent batch: 0.083440
2019-04-08 03:40:10,643 [INFO] ---------------------------------
2019-04-08 03:40:48,264 [INFO] ---------------------------------
2019-04-08 03:40:48,265 [INFO] Evaluation:
2019-04-08 03:40:48,265 [INFO] Batch 70000, worst loss 0.108868 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:40:48,266 [INFO] ---------------------------------
2019-04-08 03:41:09,856 [INFO] ---------------------------------
2019-04-08 03:41:09,857 [INFO] Summary:
2019-04-08 03:41:09,858 [INFO] Batch 71000, worst loss 0.030258 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 03:41:09,859 [INFO] Regularization: 649.132690 * 0.0000000100 = 0.0000064913 loss
2019-04-08 03:41:09,859 [INFO] unfolding 0, single step 71001
2019-04-08 03:41:09,860 [INFO] Sum of grad norms of most recent batch: 0.154765
2019-04-08 03:41:09,860 [INFO] ---------------------------------
2019-04-08 03:41:31,166 [INFO] ---------------------------------
2019-04-08 03:41:31,167 [INFO] Summary:
2019-04-08 03:41:31,168 [INFO] Batch 72000, worst loss 0.068711 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 03:41:31,169 [INFO] Regularization: 648.846802 * 0.0000000100 = 0.0000064885 loss
2019-04-08 03:41:31,169 [INFO] unfolding 0, single step 72001
2019-04-08 03:41:31,170 [INFO] Sum of grad norms of most recent batch: 0.034642
2019-04-08 03:41:31,170 [INFO] ---------------------------------
2019-04-08 03:41:52,432 [INFO] ---------------------------------
2019-04-08 03:41:52,433 [INFO] Summary:
2019-04-08 03:41:52,434 [INFO] Batch 73000, worst loss 0.081715 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 03:41:52,434 [INFO] Regularization: 646.530334 * 0.0000000100 = 0.0000064653 loss
2019-04-08 03:41:52,435 [INFO] unfolding 0, single step 73001
2019-04-08 03:41:52,435 [INFO] Sum of grad norms of most recent batch: 0.126448
2019-04-08 03:41:52,436 [INFO] ---------------------------------
2019-04-08 03:42:13,800 [INFO] ---------------------------------
2019-04-08 03:42:13,800 [INFO] Summary:
2019-04-08 03:42:13,801 [INFO] Batch 74000, worst loss 0.106306 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 03:42:13,801 [INFO] Regularization: 644.772034 * 0.0000000100 = 0.0000064477 loss
2019-04-08 03:42:13,802 [INFO] unfolding 0, single step 74001
2019-04-08 03:42:13,802 [INFO] Sum of grad norms of most recent batch: 0.071680
2019-04-08 03:42:13,803 [INFO] ---------------------------------
2019-04-08 03:42:34,968 [INFO] ---------------------------------
2019-04-08 03:42:34,969 [INFO] Summary:
2019-04-08 03:42:34,970 [INFO] Batch 75000, worst loss 0.031941 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 03:42:34,970 [INFO] Regularization: 643.770386 * 0.0000000100 = 0.0000064377 loss
2019-04-08 03:42:34,970 [INFO] unfolding 0, single step 75001
2019-04-08 03:42:34,971 [INFO] Sum of grad norms of most recent batch: 0.061864
2019-04-08 03:42:34,971 [INFO] ---------------------------------
2019-04-08 03:42:56,247 [INFO] ---------------------------------
2019-04-08 03:42:56,248 [INFO] Summary:
2019-04-08 03:42:56,249 [INFO] Batch 76000, worst loss 0.070692 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 03:42:56,249 [INFO] Regularization: 642.855530 * 0.0000000100 = 0.0000064286 loss
2019-04-08 03:42:56,250 [INFO] unfolding 0, single step 76001
2019-04-08 03:42:56,250 [INFO] Sum of grad norms of most recent batch: 0.131283
2019-04-08 03:42:56,251 [INFO] ---------------------------------
2019-04-08 03:43:17,638 [INFO] ---------------------------------
2019-04-08 03:43:17,639 [INFO] Summary:
2019-04-08 03:43:17,639 [INFO] Batch 77000, worst loss 0.038290 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 03:43:17,640 [INFO] Regularization: 640.833618 * 0.0000000100 = 0.0000064083 loss
2019-04-08 03:43:17,640 [INFO] unfolding 0, single step 77001
2019-04-08 03:43:17,641 [INFO] Sum of grad norms of most recent batch: 0.140703
2019-04-08 03:43:17,642 [INFO] ---------------------------------
2019-04-08 03:43:39,010 [INFO] ---------------------------------
2019-04-08 03:43:39,011 [INFO] Summary:
2019-04-08 03:43:39,013 [INFO] Batch 78000, worst loss 0.089590 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 03:43:39,013 [INFO] Regularization: 639.373474 * 0.0000000100 = 0.0000063937 loss
2019-04-08 03:43:39,014 [INFO] unfolding 0, single step 78001
2019-04-08 03:43:39,015 [INFO] Sum of grad norms of most recent batch: 0.034630
2019-04-08 03:43:39,016 [INFO] ---------------------------------
2019-04-08 03:44:00,451 [INFO] ---------------------------------
2019-04-08 03:44:00,452 [INFO] Summary:
2019-04-08 03:44:00,453 [INFO] Batch 79000, worst loss 0.067702 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 03:44:00,453 [INFO] Regularization: 637.559631 * 0.0000000100 = 0.0000063756 loss
2019-04-08 03:44:00,453 [INFO] unfolding 0, single step 79001
2019-04-08 03:44:00,454 [INFO] Sum of grad norms of most recent batch: 0.075628
2019-04-08 03:44:00,454 [INFO] ---------------------------------
2019-04-08 03:44:22,064 [INFO] ---------------------------------
2019-04-08 03:44:22,065 [INFO] Summary:
2019-04-08 03:44:22,066 [INFO] Batch 80000, worst loss 0.044665 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 03:44:22,066 [INFO] Regularization: 636.511902 * 0.0000000100 = 0.0000063651 loss
2019-04-08 03:44:22,067 [INFO] unfolding 0, single step 80001
2019-04-08 03:44:22,067 [INFO] Sum of grad norms of most recent batch: 0.062623
2019-04-08 03:44:22,068 [INFO] ---------------------------------
2019-04-08 03:44:59,513 [INFO] ---------------------------------
2019-04-08 03:44:59,514 [INFO] Evaluation:
2019-04-08 03:44:59,515 [INFO] Batch 80000, worst loss 0.084364 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:44:59,515 [INFO] ---------------------------------
2019-04-08 03:45:20,529 [INFO] ---------------------------------
2019-04-08 03:45:20,530 [INFO] Summary:
2019-04-08 03:45:20,531 [INFO] Batch 81000, worst loss 0.021692 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 03:45:20,532 [INFO] Regularization: 635.161804 * 0.0000000100 = 0.0000063516 loss
2019-04-08 03:45:20,532 [INFO] unfolding 0, single step 81001
2019-04-08 03:45:20,533 [INFO] Sum of grad norms of most recent batch: 0.091662
2019-04-08 03:45:20,534 [INFO] ---------------------------------
2019-04-08 03:45:41,902 [INFO] ---------------------------------
2019-04-08 03:45:41,903 [INFO] Summary:
2019-04-08 03:45:41,904 [INFO] Batch 82000, worst loss 0.072508 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 03:45:41,904 [INFO] Regularization: 634.506775 * 0.0000000100 = 0.0000063451 loss
2019-04-08 03:45:41,905 [INFO] unfolding 0, single step 82001
2019-04-08 03:45:41,905 [INFO] Sum of grad norms of most recent batch: 0.040464
2019-04-08 03:45:41,906 [INFO] ---------------------------------
2019-04-08 03:46:03,077 [INFO] ---------------------------------
2019-04-08 03:46:03,078 [INFO] Summary:
2019-04-08 03:46:03,079 [INFO] Batch 83000, worst loss 0.072687 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 03:46:03,080 [INFO] Regularization: 632.830688 * 0.0000000100 = 0.0000063283 loss
2019-04-08 03:46:03,080 [INFO] unfolding 0, single step 83001
2019-04-08 03:46:03,081 [INFO] Sum of grad norms of most recent batch: 0.026384
2019-04-08 03:46:03,081 [INFO] ---------------------------------
2019-04-08 03:46:24,607 [INFO] ---------------------------------
2019-04-08 03:46:24,608 [INFO] Summary:
2019-04-08 03:46:24,609 [INFO] Batch 84000, worst loss 0.074503 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 03:46:24,610 [INFO] Regularization: 631.213684 * 0.0000000100 = 0.0000063121 loss
2019-04-08 03:46:24,610 [INFO] unfolding 0, single step 84001
2019-04-08 03:46:24,612 [INFO] Sum of grad norms of most recent batch: 0.039525
2019-04-08 03:46:24,612 [INFO] ---------------------------------
2019-04-08 03:46:45,854 [INFO] ---------------------------------
2019-04-08 03:46:45,855 [INFO] Summary:
2019-04-08 03:46:45,856 [INFO] Batch 85000, worst loss 0.048532 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 03:46:45,856 [INFO] Regularization: 630.182251 * 0.0000000100 = 0.0000063018 loss
2019-04-08 03:46:45,856 [INFO] unfolding 0, single step 85001
2019-04-08 03:46:45,857 [INFO] Sum of grad norms of most recent batch: 0.040087
2019-04-08 03:46:45,857 [INFO] ---------------------------------
2019-04-08 03:47:06,944 [INFO] ---------------------------------
2019-04-08 03:47:06,945 [INFO] Summary:
2019-04-08 03:47:06,946 [INFO] Batch 86000, worst loss 0.078549 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 03:47:06,947 [INFO] Regularization: 628.668640 * 0.0000000100 = 0.0000062867 loss
2019-04-08 03:47:06,947 [INFO] unfolding 0, single step 86001
2019-04-08 03:47:06,948 [INFO] Sum of grad norms of most recent batch: 0.081676
2019-04-08 03:47:06,949 [INFO] ---------------------------------
2019-04-08 03:47:27,858 [INFO] ---------------------------------
2019-04-08 03:47:27,859 [INFO] Summary:
2019-04-08 03:47:27,859 [INFO] Batch 87000, worst loss 0.039766 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 03:47:27,860 [INFO] Regularization: 626.777832 * 0.0000000100 = 0.0000062678 loss
2019-04-08 03:47:27,860 [INFO] unfolding 0, single step 87001
2019-04-08 03:47:27,861 [INFO] Sum of grad norms of most recent batch: 0.080921
2019-04-08 03:47:27,861 [INFO] ---------------------------------
2019-04-08 03:47:48,746 [INFO] ---------------------------------
2019-04-08 03:47:48,747 [INFO] Summary:
2019-04-08 03:47:48,747 [INFO] Batch 88000, worst loss 0.057299 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 03:47:48,748 [INFO] Regularization: 625.522583 * 0.0000000100 = 0.0000062552 loss
2019-04-08 03:47:48,748 [INFO] unfolding 0, single step 88001
2019-04-08 03:47:48,749 [INFO] Sum of grad norms of most recent batch: 0.035566
2019-04-08 03:47:48,749 [INFO] ---------------------------------
2019-04-08 03:48:09,767 [INFO] ---------------------------------
2019-04-08 03:48:09,768 [INFO] Summary:
2019-04-08 03:48:09,768 [INFO] Batch 89000, worst loss 0.054414 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 03:48:09,769 [INFO] Regularization: 625.218384 * 0.0000000100 = 0.0000062522 loss
2019-04-08 03:48:09,769 [INFO] unfolding 0, single step 89001
2019-04-08 03:48:09,770 [INFO] Sum of grad norms of most recent batch: 0.091526
2019-04-08 03:48:09,771 [INFO] ---------------------------------
2019-04-08 03:48:31,254 [INFO] ---------------------------------
2019-04-08 03:48:31,255 [INFO] Summary:
2019-04-08 03:48:31,256 [INFO] Batch 90000, worst loss 0.038714 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 03:48:31,256 [INFO] Regularization: 624.460876 * 0.0000000100 = 0.0000062446 loss
2019-04-08 03:48:31,257 [INFO] unfolding 0, single step 90001
2019-04-08 03:48:31,257 [INFO] Sum of grad norms of most recent batch: 0.049905
2019-04-08 03:48:31,258 [INFO] ---------------------------------
2019-04-08 03:49:08,629 [INFO] ---------------------------------
2019-04-08 03:49:08,630 [INFO] Evaluation:
2019-04-08 03:49:08,630 [INFO] Batch 90000, worst loss 0.101549 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:49:08,631 [INFO] ---------------------------------
2019-04-08 03:49:29,968 [INFO] ---------------------------------
2019-04-08 03:49:29,969 [INFO] Summary:
2019-04-08 03:49:29,970 [INFO] Batch 91000, worst loss 0.138810 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 03:49:29,970 [INFO] Regularization: 624.016663 * 0.0000000100 = 0.0000062402 loss
2019-04-08 03:49:29,971 [INFO] unfolding 0, single step 91001
2019-04-08 03:49:29,971 [INFO] Sum of grad norms of most recent batch: 0.066552
2019-04-08 03:49:29,972 [INFO] ---------------------------------
2019-04-08 03:49:51,655 [INFO] ---------------------------------
2019-04-08 03:49:51,656 [INFO] Summary:
2019-04-08 03:49:51,657 [INFO] Batch 92000, worst loss 0.060678 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 03:49:51,658 [INFO] Regularization: 623.334167 * 0.0000000100 = 0.0000062333 loss
2019-04-08 03:49:51,659 [INFO] unfolding 0, single step 92001
2019-04-08 03:49:51,660 [INFO] Sum of grad norms of most recent batch: 0.033412
2019-04-08 03:49:51,661 [INFO] ---------------------------------
2019-04-08 03:50:13,284 [INFO] ---------------------------------
2019-04-08 03:50:13,285 [INFO] Summary:
2019-04-08 03:50:13,286 [INFO] Batch 93000, worst loss 0.028997 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 03:50:13,287 [INFO] Regularization: 622.819031 * 0.0000000100 = 0.0000062282 loss
2019-04-08 03:50:13,288 [INFO] unfolding 0, single step 93001
2019-04-08 03:50:13,289 [INFO] Sum of grad norms of most recent batch: 0.022244
2019-04-08 03:50:13,290 [INFO] ---------------------------------
2019-04-08 03:50:34,320 [INFO] ---------------------------------
2019-04-08 03:50:34,321 [INFO] Summary:
2019-04-08 03:50:34,322 [INFO] Batch 94000, worst loss 0.009272 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 03:50:34,323 [INFO] Regularization: 622.230164 * 0.0000000100 = 0.0000062223 loss
2019-04-08 03:50:34,323 [INFO] unfolding 0, single step 94001
2019-04-08 03:50:34,323 [INFO] Sum of grad norms of most recent batch: 0.036448
2019-04-08 03:50:34,324 [INFO] ---------------------------------
2019-04-08 03:50:55,494 [INFO] ---------------------------------
2019-04-08 03:50:55,495 [INFO] Summary:
2019-04-08 03:50:55,495 [INFO] Batch 95000, worst loss 0.040957 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 03:50:55,496 [INFO] Regularization: 621.810181 * 0.0000000100 = 0.0000062181 loss
2019-04-08 03:50:55,496 [INFO] unfolding 0, single step 95001
2019-04-08 03:50:55,497 [INFO] Sum of grad norms of most recent batch: 0.066042
2019-04-08 03:50:55,497 [INFO] ---------------------------------
2019-04-08 03:51:16,387 [INFO] ---------------------------------
2019-04-08 03:51:16,388 [INFO] Summary:
2019-04-08 03:51:16,389 [INFO] Batch 96000, worst loss 0.032411 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 03:51:16,390 [INFO] Regularization: 621.550293 * 0.0000000100 = 0.0000062155 loss
2019-04-08 03:51:16,390 [INFO] unfolding 0, single step 96001
2019-04-08 03:51:16,391 [INFO] Sum of grad norms of most recent batch: 0.032498
2019-04-08 03:51:16,392 [INFO] ---------------------------------
2019-04-08 03:51:37,584 [INFO] ---------------------------------
2019-04-08 03:51:37,586 [INFO] Summary:
2019-04-08 03:51:37,587 [INFO] Batch 97000, worst loss 0.059079 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 03:51:37,587 [INFO] Regularization: 620.829346 * 0.0000000100 = 0.0000062083 loss
2019-04-08 03:51:37,588 [INFO] unfolding 0, single step 97001
2019-04-08 03:51:37,589 [INFO] Sum of grad norms of most recent batch: 0.123374
2019-04-08 03:51:37,589 [INFO] ---------------------------------
2019-04-08 03:51:58,990 [INFO] ---------------------------------
2019-04-08 03:51:58,991 [INFO] Summary:
2019-04-08 03:51:58,992 [INFO] Batch 98000, worst loss 0.064775 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 03:51:58,993 [INFO] Regularization: 620.133850 * 0.0000000100 = 0.0000062013 loss
2019-04-08 03:51:58,993 [INFO] unfolding 0, single step 98001
2019-04-08 03:51:58,994 [INFO] Sum of grad norms of most recent batch: 0.023004
2019-04-08 03:51:58,994 [INFO] ---------------------------------
2019-04-08 03:52:19,807 [INFO] ---------------------------------
2019-04-08 03:52:19,808 [INFO] Summary:
2019-04-08 03:52:19,809 [INFO] Batch 99000, worst loss 0.049009 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 03:52:19,810 [INFO] Regularization: 620.331848 * 0.0000000100 = 0.0000062033 loss
2019-04-08 03:52:19,810 [INFO] unfolding 0, single step 99001
2019-04-08 03:52:19,810 [INFO] Sum of grad norms of most recent batch: 0.043562
2019-04-08 03:52:19,811 [INFO] ---------------------------------
2019-04-08 03:52:41,124 [INFO] ---------------------------------
2019-04-08 03:52:41,125 [INFO] Summary:
2019-04-08 03:52:41,126 [INFO] Batch 100000, worst loss 0.045976 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 03:52:41,126 [INFO] Regularization: 620.316406 * 0.0000000100 = 0.0000062032 loss
2019-04-08 03:52:41,127 [INFO] unfolding 0, single step 100001
2019-04-08 03:52:41,127 [INFO] Sum of grad norms of most recent batch: 0.055863
2019-04-08 03:52:41,128 [INFO] ---------------------------------
2019-04-08 03:53:18,559 [INFO] ---------------------------------
2019-04-08 03:53:18,560 [INFO] Evaluation:
2019-04-08 03:53:18,561 [INFO] Batch 100000, worst loss 0.096529 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:53:18,561 [INFO] ---------------------------------
2019-04-08 03:53:39,880 [INFO] ---------------------------------
2019-04-08 03:53:39,881 [INFO] Summary:
2019-04-08 03:53:39,881 [INFO] Batch 101000, worst loss 0.055415 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 03:53:39,882 [INFO] Regularization: 619.740356 * 0.0000000100 = 0.0000061974 loss
2019-04-08 03:53:39,883 [INFO] unfolding 0, single step 101001
2019-04-08 03:53:39,883 [INFO] Sum of grad norms of most recent batch: 0.088490
2019-04-08 03:53:39,884 [INFO] ---------------------------------
2019-04-08 03:54:01,342 [INFO] ---------------------------------
2019-04-08 03:54:01,343 [INFO] Summary:
2019-04-08 03:54:01,343 [INFO] Batch 102000, worst loss 0.080021 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 03:54:01,344 [INFO] Regularization: 619.631409 * 0.0000000100 = 0.0000061963 loss
2019-04-08 03:54:01,344 [INFO] unfolding 0, single step 102001
2019-04-08 03:54:01,345 [INFO] Sum of grad norms of most recent batch: 0.048177
2019-04-08 03:54:01,345 [INFO] ---------------------------------
2019-04-08 03:54:23,092 [INFO] ---------------------------------
2019-04-08 03:54:23,093 [INFO] Summary:
2019-04-08 03:54:23,094 [INFO] Batch 103000, worst loss 0.038082 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 03:54:23,095 [INFO] Regularization: 619.743103 * 0.0000000100 = 0.0000061974 loss
2019-04-08 03:54:23,095 [INFO] unfolding 0, single step 103001
2019-04-08 03:54:23,096 [INFO] Sum of grad norms of most recent batch: 0.019677
2019-04-08 03:54:23,097 [INFO] ---------------------------------
2019-04-08 03:54:44,218 [INFO] ---------------------------------
2019-04-08 03:54:44,219 [INFO] Summary:
2019-04-08 03:54:44,219 [INFO] Batch 104000, worst loss 0.040469 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 03:54:44,220 [INFO] Regularization: 619.540100 * 0.0000000100 = 0.0000061954 loss
2019-04-08 03:54:44,220 [INFO] unfolding 0, single step 104001
2019-04-08 03:54:44,221 [INFO] Sum of grad norms of most recent batch: 0.045931
2019-04-08 03:54:44,221 [INFO] ---------------------------------
2019-04-08 03:55:05,801 [INFO] ---------------------------------
2019-04-08 03:55:05,803 [INFO] Summary:
2019-04-08 03:55:05,803 [INFO] Batch 105000, worst loss 0.132699 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 03:55:05,805 [INFO] Regularization: 619.323120 * 0.0000000100 = 0.0000061932 loss
2019-04-08 03:55:05,805 [INFO] unfolding 0, single step 105001
2019-04-08 03:55:05,806 [INFO] Sum of grad norms of most recent batch: 0.029319
2019-04-08 03:55:05,807 [INFO] ---------------------------------
2019-04-08 03:55:27,101 [INFO] ---------------------------------
2019-04-08 03:55:27,102 [INFO] Summary:
2019-04-08 03:55:27,102 [INFO] Batch 106000, worst loss 0.033741 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 03:55:27,103 [INFO] Regularization: 619.047607 * 0.0000000100 = 0.0000061905 loss
2019-04-08 03:55:27,103 [INFO] unfolding 0, single step 106001
2019-04-08 03:55:27,104 [INFO] Sum of grad norms of most recent batch: 0.592366
2019-04-08 03:55:27,104 [INFO] ---------------------------------
2019-04-08 03:55:47,730 [INFO] ---------------------------------
2019-04-08 03:55:47,731 [INFO] Summary:
2019-04-08 03:55:47,732 [INFO] Batch 107000, worst loss 0.065695 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 03:55:47,732 [INFO] Regularization: 618.855957 * 0.0000000100 = 0.0000061886 loss
2019-04-08 03:55:47,733 [INFO] unfolding 0, single step 107001
2019-04-08 03:55:47,733 [INFO] Sum of grad norms of most recent batch: 0.064846
2019-04-08 03:55:47,734 [INFO] ---------------------------------
2019-04-08 03:56:08,709 [INFO] ---------------------------------
2019-04-08 03:56:08,710 [INFO] Summary:
2019-04-08 03:56:08,710 [INFO] Batch 108000, worst loss 0.055709 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 03:56:08,711 [INFO] Regularization: 618.714783 * 0.0000000100 = 0.0000061871 loss
2019-04-08 03:56:08,711 [INFO] unfolding 0, single step 108001
2019-04-08 03:56:08,712 [INFO] Sum of grad norms of most recent batch: 0.019469
2019-04-08 03:56:08,712 [INFO] ---------------------------------
2019-04-08 03:56:29,420 [INFO] ---------------------------------
2019-04-08 03:56:29,420 [INFO] Summary:
2019-04-08 03:56:29,421 [INFO] Batch 109000, worst loss 0.112861 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 03:56:29,421 [INFO] Regularization: 618.928467 * 0.0000000100 = 0.0000061893 loss
2019-04-08 03:56:29,422 [INFO] unfolding 0, single step 109001
2019-04-08 03:56:29,422 [INFO] Sum of grad norms of most recent batch: 0.071237
2019-04-08 03:56:29,423 [INFO] ---------------------------------
2019-04-08 03:56:50,208 [INFO] ---------------------------------
2019-04-08 03:56:50,210 [INFO] Summary:
2019-04-08 03:56:50,210 [INFO] Batch 110000, worst loss 0.040063 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 03:56:50,211 [INFO] Regularization: 618.668213 * 0.0000000100 = 0.0000061867 loss
2019-04-08 03:56:50,211 [INFO] unfolding 0, single step 110001
2019-04-08 03:56:50,211 [INFO] Sum of grad norms of most recent batch: 0.028753
2019-04-08 03:56:50,212 [INFO] ---------------------------------
2019-04-08 03:57:27,742 [INFO] ---------------------------------
2019-04-08 03:57:27,743 [INFO] Evaluation:
2019-04-08 03:57:27,744 [INFO] Batch 110000, worst loss 0.052884 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 03:57:27,745 [INFO] ---------------------------------
2019-04-08 03:57:48,507 [INFO] ---------------------------------
2019-04-08 03:57:48,508 [INFO] Summary:
2019-04-08 03:57:48,509 [INFO] Batch 111000, worst loss 0.108421 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 03:57:48,510 [INFO] Regularization: 618.790710 * 0.0000000100 = 0.0000061879 loss
2019-04-08 03:57:48,510 [INFO] unfolding 0, single step 111001
2019-04-08 03:57:48,511 [INFO] Sum of grad norms of most recent batch: 7.238210
2019-04-08 03:57:48,512 [INFO] ---------------------------------
2019-04-08 03:58:10,213 [INFO] ---------------------------------
2019-04-08 03:58:10,214 [INFO] Summary:
2019-04-08 03:58:10,215 [INFO] Batch 112000, worst loss 0.099164 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 03:58:10,215 [INFO] Regularization: 618.620239 * 0.0000000100 = 0.0000061862 loss
2019-04-08 03:58:10,216 [INFO] unfolding 0, single step 112001
2019-04-08 03:58:10,216 [INFO] Sum of grad norms of most recent batch: 0.026662
2019-04-08 03:58:10,217 [INFO] ---------------------------------
2019-04-08 03:58:31,074 [INFO] ---------------------------------
2019-04-08 03:58:31,075 [INFO] Summary:
2019-04-08 03:58:31,076 [INFO] Batch 113000, worst loss 0.086595 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 03:58:31,077 [INFO] Regularization: 618.560059 * 0.0000000100 = 0.0000061856 loss
2019-04-08 03:58:31,077 [INFO] unfolding 0, single step 113001
2019-04-08 03:58:31,078 [INFO] Sum of grad norms of most recent batch: 0.026794
2019-04-08 03:58:31,079 [INFO] ---------------------------------
2019-04-08 03:58:53,147 [INFO] ---------------------------------
2019-04-08 03:58:53,148 [INFO] Summary:
2019-04-08 03:58:53,149 [INFO] Batch 114000, worst loss 0.086568 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 03:58:53,149 [INFO] Regularization: 618.416565 * 0.0000000100 = 0.0000061842 loss
2019-04-08 03:58:53,150 [INFO] unfolding 0, single step 114001
2019-04-08 03:58:53,151 [INFO] Sum of grad norms of most recent batch: 0.013811
2019-04-08 03:58:53,151 [INFO] ---------------------------------
2019-04-08 03:59:14,413 [INFO] ---------------------------------
2019-04-08 03:59:14,414 [INFO] Summary:
2019-04-08 03:59:14,415 [INFO] Batch 115000, worst loss 0.115646 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 03:59:14,415 [INFO] Regularization: 618.258728 * 0.0000000100 = 0.0000061826 loss
2019-04-08 03:59:14,416 [INFO] unfolding 0, single step 115001
2019-04-08 03:59:14,416 [INFO] Sum of grad norms of most recent batch: 0.033985
2019-04-08 03:59:14,417 [INFO] ---------------------------------
2019-04-08 03:59:35,760 [INFO] ---------------------------------
2019-04-08 03:59:35,761 [INFO] Summary:
2019-04-08 03:59:35,761 [INFO] Batch 116000, worst loss 0.115668 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 03:59:35,762 [INFO] Regularization: 618.108459 * 0.0000000100 = 0.0000061811 loss
2019-04-08 03:59:35,762 [INFO] unfolding 0, single step 116001
2019-04-08 03:59:35,763 [INFO] Sum of grad norms of most recent batch: 0.011003
2019-04-08 03:59:35,763 [INFO] ---------------------------------
2019-04-08 03:59:56,640 [INFO] ---------------------------------
2019-04-08 03:59:56,641 [INFO] Summary:
2019-04-08 03:59:56,642 [INFO] Batch 117000, worst loss 0.020057 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 03:59:56,647 [INFO] Regularization: 617.964844 * 0.0000000100 = 0.0000061796 loss
2019-04-08 03:59:56,647 [INFO] unfolding 0, single step 117001
2019-04-08 03:59:56,648 [INFO] Sum of grad norms of most recent batch: 0.037079
2019-04-08 03:59:56,649 [INFO] ---------------------------------
2019-04-08 04:00:18,019 [INFO] ---------------------------------
2019-04-08 04:00:18,020 [INFO] Summary:
2019-04-08 04:00:18,021 [INFO] Batch 118000, worst loss 0.059879 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 04:00:18,022 [INFO] Regularization: 617.829224 * 0.0000000100 = 0.0000061783 loss
2019-04-08 04:00:18,022 [INFO] unfolding 0, single step 118001
2019-04-08 04:00:18,023 [INFO] Sum of grad norms of most recent batch: 0.009714
2019-04-08 04:00:18,024 [INFO] ---------------------------------
2019-04-08 04:00:39,593 [INFO] ---------------------------------
2019-04-08 04:00:39,594 [INFO] Summary:
2019-04-08 04:00:39,594 [INFO] Batch 119000, worst loss 0.143831 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 04:00:39,595 [INFO] Regularization: 617.733093 * 0.0000000100 = 0.0000061773 loss
2019-04-08 04:00:39,595 [INFO] unfolding 0, single step 119001
2019-04-08 04:00:39,596 [INFO] Sum of grad norms of most recent batch: 0.057577
2019-04-08 04:00:39,596 [INFO] ---------------------------------
2019-04-08 04:01:01,544 [INFO] ---------------------------------
2019-04-08 04:01:01,545 [INFO] Summary:
2019-04-08 04:01:01,546 [INFO] Batch 120000, worst loss 0.035160 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 04:01:01,546 [INFO] Regularization: 617.598633 * 0.0000000100 = 0.0000061760 loss
2019-04-08 04:01:01,547 [INFO] unfolding 0, single step 120001
2019-04-08 04:01:01,547 [INFO] Sum of grad norms of most recent batch: 0.022417
2019-04-08 04:01:01,548 [INFO] ---------------------------------
2019-04-08 04:01:39,094 [INFO] ---------------------------------
2019-04-08 04:01:39,095 [INFO] Evaluation:
2019-04-08 04:01:39,095 [INFO] Batch 120000, worst loss 0.189847 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:01:39,096 [INFO] ---------------------------------
2019-04-08 04:02:00,516 [INFO] ---------------------------------
2019-04-08 04:02:00,518 [INFO] Summary:
2019-04-08 04:02:00,519 [INFO] Batch 121000, worst loss 0.062502 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 04:02:00,520 [INFO] Regularization: 617.494385 * 0.0000000100 = 0.0000061749 loss
2019-04-08 04:02:00,521 [INFO] unfolding 0, single step 121001
2019-04-08 04:02:00,522 [INFO] Sum of grad norms of most recent batch: 0.020532
2019-04-08 04:02:00,523 [INFO] ---------------------------------
2019-04-08 04:02:22,449 [INFO] ---------------------------------
2019-04-08 04:02:22,450 [INFO] Summary:
2019-04-08 04:02:22,451 [INFO] Batch 122000, worst loss 0.070840 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 04:02:22,451 [INFO] Regularization: 617.439453 * 0.0000000100 = 0.0000061744 loss
2019-04-08 04:02:22,452 [INFO] unfolding 0, single step 122001
2019-04-08 04:02:22,453 [INFO] Sum of grad norms of most recent batch: 0.084719
2019-04-08 04:02:22,453 [INFO] ---------------------------------
2019-04-08 04:02:43,380 [INFO] ---------------------------------
2019-04-08 04:02:43,381 [INFO] Summary:
2019-04-08 04:02:43,382 [INFO] Batch 123000, worst loss 0.056390 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 04:02:43,382 [INFO] Regularization: 617.367493 * 0.0000000100 = 0.0000061737 loss
2019-04-08 04:02:43,383 [INFO] unfolding 0, single step 123001
2019-04-08 04:02:43,383 [INFO] Sum of grad norms of most recent batch: 0.018303
2019-04-08 04:02:43,384 [INFO] ---------------------------------
2019-04-08 04:03:04,413 [INFO] ---------------------------------
2019-04-08 04:03:04,413 [INFO] Summary:
2019-04-08 04:03:04,414 [INFO] Batch 124000, worst loss 0.069857 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 04:03:04,414 [INFO] Regularization: 617.270020 * 0.0000000100 = 0.0000061727 loss
2019-04-08 04:03:04,415 [INFO] unfolding 0, single step 124001
2019-04-08 04:03:04,415 [INFO] Sum of grad norms of most recent batch: 0.025943
2019-04-08 04:03:04,416 [INFO] ---------------------------------
2019-04-08 04:03:25,081 [INFO] ---------------------------------
2019-04-08 04:03:25,082 [INFO] Summary:
2019-04-08 04:03:25,083 [INFO] Batch 125000, worst loss 0.052664 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 04:03:25,083 [INFO] Regularization: 617.225891 * 0.0000000100 = 0.0000061723 loss
2019-04-08 04:03:25,084 [INFO] unfolding 0, single step 125001
2019-04-08 04:03:25,084 [INFO] Sum of grad norms of most recent batch: 0.023499
2019-04-08 04:03:25,085 [INFO] ---------------------------------
2019-04-08 04:03:46,513 [INFO] ---------------------------------
2019-04-08 04:03:46,514 [INFO] Summary:
2019-04-08 04:03:46,514 [INFO] Batch 126000, worst loss 0.031169 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 04:03:46,515 [INFO] Regularization: 617.179077 * 0.0000000100 = 0.0000061718 loss
2019-04-08 04:03:46,515 [INFO] unfolding 0, single step 126001
2019-04-08 04:03:46,516 [INFO] Sum of grad norms of most recent batch: 0.039067
2019-04-08 04:03:46,516 [INFO] ---------------------------------
2019-04-08 04:04:08,073 [INFO] ---------------------------------
2019-04-08 04:04:08,074 [INFO] Summary:
2019-04-08 04:04:08,075 [INFO] Batch 127000, worst loss 0.049311 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 04:04:08,075 [INFO] Regularization: 617.125977 * 0.0000000100 = 0.0000061713 loss
2019-04-08 04:04:08,076 [INFO] unfolding 0, single step 127001
2019-04-08 04:04:08,076 [INFO] Sum of grad norms of most recent batch: 0.052734
2019-04-08 04:04:08,077 [INFO] ---------------------------------
2019-04-08 04:04:30,163 [INFO] ---------------------------------
2019-04-08 04:04:30,164 [INFO] Summary:
2019-04-08 04:04:30,164 [INFO] Batch 128000, worst loss 0.026856 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 04:04:30,165 [INFO] Regularization: 617.062134 * 0.0000000100 = 0.0000061706 loss
2019-04-08 04:04:30,165 [INFO] unfolding 0, single step 128001
2019-04-08 04:04:30,166 [INFO] Sum of grad norms of most recent batch: 0.044021
2019-04-08 04:04:30,166 [INFO] ---------------------------------
2019-04-08 04:04:51,243 [INFO] ---------------------------------
2019-04-08 04:04:51,244 [INFO] Summary:
2019-04-08 04:04:51,245 [INFO] Batch 129000, worst loss 0.065488 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 04:04:51,245 [INFO] Regularization: 617.016724 * 0.0000000100 = 0.0000061702 loss
2019-04-08 04:04:51,246 [INFO] unfolding 0, single step 129001
2019-04-08 04:04:51,246 [INFO] Sum of grad norms of most recent batch: 0.011286
2019-04-08 04:04:51,247 [INFO] ---------------------------------
2019-04-08 04:05:12,625 [INFO] ---------------------------------
2019-04-08 04:05:12,626 [INFO] Summary:
2019-04-08 04:05:12,627 [INFO] Batch 130000, worst loss 0.072950 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 04:05:12,627 [INFO] Regularization: 616.961792 * 0.0000000100 = 0.0000061696 loss
2019-04-08 04:05:12,627 [INFO] unfolding 0, single step 130001
2019-04-08 04:05:12,628 [INFO] Sum of grad norms of most recent batch: 0.032918
2019-04-08 04:05:12,628 [INFO] ---------------------------------
2019-04-08 04:05:50,194 [INFO] ---------------------------------
2019-04-08 04:05:50,195 [INFO] Evaluation:
2019-04-08 04:05:50,195 [INFO] Batch 130000, worst loss 0.129483 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:05:50,196 [INFO] ---------------------------------
2019-04-08 04:06:10,974 [INFO] ---------------------------------
2019-04-08 04:06:10,974 [INFO] Summary:
2019-04-08 04:06:10,975 [INFO] Batch 131000, worst loss 0.040159 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:06:10,975 [INFO] Regularization: 616.950378 * 0.0000000100 = 0.0000061695 loss
2019-04-08 04:06:10,975 [INFO] unfolding 0, single step 131001
2019-04-08 04:06:10,976 [INFO] Sum of grad norms of most recent batch: 0.046679
2019-04-08 04:06:10,976 [INFO] ---------------------------------
2019-04-08 04:06:32,620 [INFO] ---------------------------------
2019-04-08 04:06:32,621 [INFO] Summary:
2019-04-08 04:06:32,621 [INFO] Batch 132000, worst loss 0.007718 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:06:32,622 [INFO] Regularization: 616.899597 * 0.0000000100 = 0.0000061690 loss
2019-04-08 04:06:32,622 [INFO] unfolding 0, single step 132001
2019-04-08 04:06:32,623 [INFO] Sum of grad norms of most recent batch: 0.019083
2019-04-08 04:06:32,623 [INFO] ---------------------------------
2019-04-08 04:06:53,581 [INFO] ---------------------------------
2019-04-08 04:06:53,582 [INFO] Summary:
2019-04-08 04:06:53,583 [INFO] Batch 133000, worst loss 0.014673 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:06:53,583 [INFO] Regularization: 616.881104 * 0.0000000100 = 0.0000061688 loss
2019-04-08 04:06:53,583 [INFO] unfolding 0, single step 133001
2019-04-08 04:06:53,584 [INFO] Sum of grad norms of most recent batch: 0.025661
2019-04-08 04:06:53,585 [INFO] ---------------------------------
2019-04-08 04:07:14,745 [INFO] ---------------------------------
2019-04-08 04:07:14,746 [INFO] Summary:
2019-04-08 04:07:14,746 [INFO] Batch 134000, worst loss 0.018997 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:07:14,747 [INFO] Regularization: 616.868591 * 0.0000000100 = 0.0000061687 loss
2019-04-08 04:07:14,747 [INFO] unfolding 0, single step 134001
2019-04-08 04:07:14,748 [INFO] Sum of grad norms of most recent batch: 0.014808
2019-04-08 04:07:14,748 [INFO] ---------------------------------
2019-04-08 04:07:35,997 [INFO] ---------------------------------
2019-04-08 04:07:35,998 [INFO] Summary:
2019-04-08 04:07:35,999 [INFO] Batch 135000, worst loss 0.032611 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:07:35,999 [INFO] Regularization: 616.920715 * 0.0000000100 = 0.0000061692 loss
2019-04-08 04:07:36,000 [INFO] unfolding 0, single step 135001
2019-04-08 04:07:36,000 [INFO] Sum of grad norms of most recent batch: 0.030301
2019-04-08 04:07:36,001 [INFO] ---------------------------------
2019-04-08 04:07:57,621 [INFO] ---------------------------------
2019-04-08 04:07:57,622 [INFO] Summary:
2019-04-08 04:07:57,623 [INFO] Batch 136000, worst loss 0.030744 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:07:57,623 [INFO] Regularization: 616.922546 * 0.0000000100 = 0.0000061692 loss
2019-04-08 04:07:57,624 [INFO] unfolding 0, single step 136001
2019-04-08 04:07:57,624 [INFO] Sum of grad norms of most recent batch: 0.017793
2019-04-08 04:07:57,625 [INFO] ---------------------------------
2019-04-08 04:08:18,692 [INFO] ---------------------------------
2019-04-08 04:08:18,693 [INFO] Summary:
2019-04-08 04:08:18,694 [INFO] Batch 137000, worst loss 0.036907 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:08:18,695 [INFO] Regularization: 616.903625 * 0.0000000100 = 0.0000061690 loss
2019-04-08 04:08:18,696 [INFO] unfolding 0, single step 137001
2019-04-08 04:08:18,696 [INFO] Sum of grad norms of most recent batch: 0.030644
2019-04-08 04:08:18,697 [INFO] ---------------------------------
2019-04-08 04:08:40,057 [INFO] ---------------------------------
2019-04-08 04:08:40,058 [INFO] Summary:
2019-04-08 04:08:40,058 [INFO] Batch 138000, worst loss 0.040657 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:08:40,059 [INFO] Regularization: 616.831726 * 0.0000000100 = 0.0000061683 loss
2019-04-08 04:08:40,059 [INFO] unfolding 0, single step 138001
2019-04-08 04:08:40,060 [INFO] Sum of grad norms of most recent batch: 0.069084
2019-04-08 04:08:40,060 [INFO] ---------------------------------
2019-04-08 04:09:01,345 [INFO] ---------------------------------
2019-04-08 04:09:01,346 [INFO] Summary:
2019-04-08 04:09:01,347 [INFO] Batch 139000, worst loss 0.040633 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:09:01,348 [INFO] Regularization: 616.801208 * 0.0000000100 = 0.0000061680 loss
2019-04-08 04:09:01,348 [INFO] unfolding 0, single step 139001
2019-04-08 04:09:01,349 [INFO] Sum of grad norms of most recent batch: 0.022243
2019-04-08 04:09:01,350 [INFO] ---------------------------------
2019-04-08 04:09:23,464 [INFO] ---------------------------------
2019-04-08 04:09:23,465 [INFO] Summary:
2019-04-08 04:09:23,466 [INFO] Batch 140000, worst loss 0.016273 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:09:23,466 [INFO] Regularization: 616.790466 * 0.0000000100 = 0.0000061679 loss
2019-04-08 04:09:23,467 [INFO] unfolding 0, single step 140001
2019-04-08 04:09:23,467 [INFO] Sum of grad norms of most recent batch: 0.009801
2019-04-08 04:09:23,468 [INFO] ---------------------------------
2019-04-08 04:10:00,828 [INFO] ---------------------------------
2019-04-08 04:10:00,829 [INFO] Evaluation:
2019-04-08 04:10:00,830 [INFO] Batch 140000, worst loss 0.105351 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:10:00,831 [INFO] ---------------------------------
2019-04-08 04:10:22,076 [INFO] ---------------------------------
2019-04-08 04:10:22,077 [INFO] Summary:
2019-04-08 04:10:22,078 [INFO] Batch 141000, worst loss 0.056675 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:10:22,078 [INFO] Regularization: 616.768188 * 0.0000000100 = 0.0000061677 loss
2019-04-08 04:10:22,079 [INFO] unfolding 0, single step 141001
2019-04-08 04:10:22,079 [INFO] Sum of grad norms of most recent batch: 0.046957
2019-04-08 04:10:22,080 [INFO] ---------------------------------
2019-04-08 04:10:43,618 [INFO] ---------------------------------
2019-04-08 04:10:43,619 [INFO] Summary:
2019-04-08 04:10:43,619 [INFO] Batch 142000, worst loss 0.056621 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:10:43,620 [INFO] Regularization: 616.748657 * 0.0000000100 = 0.0000061675 loss
2019-04-08 04:10:43,620 [INFO] unfolding 0, single step 142001
2019-04-08 04:10:43,621 [INFO] Sum of grad norms of most recent batch: 0.014970
2019-04-08 04:10:43,621 [INFO] ---------------------------------
2019-04-08 04:11:04,698 [INFO] ---------------------------------
2019-04-08 04:11:04,699 [INFO] Summary:
2019-04-08 04:11:04,699 [INFO] Batch 143000, worst loss 0.032750 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:11:04,700 [INFO] Regularization: 616.735962 * 0.0000000100 = 0.0000061674 loss
2019-04-08 04:11:04,700 [INFO] unfolding 0, single step 143001
2019-04-08 04:11:04,701 [INFO] Sum of grad norms of most recent batch: 0.025728
2019-04-08 04:11:04,702 [INFO] ---------------------------------
2019-04-08 04:11:26,174 [INFO] ---------------------------------
2019-04-08 04:11:26,175 [INFO] Summary:
2019-04-08 04:11:26,175 [INFO] Batch 144000, worst loss 0.055392 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:11:26,176 [INFO] Regularization: 616.718750 * 0.0000000100 = 0.0000061672 loss
2019-04-08 04:11:26,176 [INFO] unfolding 0, single step 144001
2019-04-08 04:11:26,177 [INFO] Sum of grad norms of most recent batch: 0.014475
2019-04-08 04:11:26,177 [INFO] ---------------------------------
2019-04-08 04:11:47,255 [INFO] ---------------------------------
2019-04-08 04:11:47,256 [INFO] Summary:
2019-04-08 04:11:47,257 [INFO] Batch 145000, worst loss 0.095199 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:11:47,257 [INFO] Regularization: 616.699890 * 0.0000000100 = 0.0000061670 loss
2019-04-08 04:11:47,258 [INFO] unfolding 0, single step 145001
2019-04-08 04:11:47,259 [INFO] Sum of grad norms of most recent batch: 0.043276
2019-04-08 04:11:47,259 [INFO] ---------------------------------
2019-04-08 04:12:08,207 [INFO] ---------------------------------
2019-04-08 04:12:08,208 [INFO] Summary:
2019-04-08 04:12:08,209 [INFO] Batch 146000, worst loss 0.071006 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:12:08,209 [INFO] Regularization: 616.683044 * 0.0000000100 = 0.0000061668 loss
2019-04-08 04:12:08,210 [INFO] unfolding 0, single step 146001
2019-04-08 04:12:08,210 [INFO] Sum of grad norms of most recent batch: 0.014994
2019-04-08 04:12:08,211 [INFO] ---------------------------------
2019-04-08 04:12:28,927 [INFO] ---------------------------------
2019-04-08 04:12:28,928 [INFO] Summary:
2019-04-08 04:12:28,928 [INFO] Batch 147000, worst loss 0.033997 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:12:28,929 [INFO] Regularization: 616.657410 * 0.0000000100 = 0.0000061666 loss
2019-04-08 04:12:28,929 [INFO] unfolding 0, single step 147001
2019-04-08 04:12:28,929 [INFO] Sum of grad norms of most recent batch: 0.022863
2019-04-08 04:12:28,930 [INFO] ---------------------------------
2019-04-08 04:12:49,867 [INFO] ---------------------------------
2019-04-08 04:12:49,868 [INFO] Summary:
2019-04-08 04:12:49,869 [INFO] Batch 148000, worst loss 0.076459 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:12:49,870 [INFO] Regularization: 616.631653 * 0.0000000100 = 0.0000061663 loss
2019-04-08 04:12:49,870 [INFO] unfolding 0, single step 148001
2019-04-08 04:12:49,871 [INFO] Sum of grad norms of most recent batch: 0.025011
2019-04-08 04:12:49,871 [INFO] ---------------------------------
2019-04-08 04:13:11,033 [INFO] ---------------------------------
2019-04-08 04:13:11,034 [INFO] Summary:
2019-04-08 04:13:11,034 [INFO] Batch 149000, worst loss 0.089889 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:13:11,035 [INFO] Regularization: 616.646179 * 0.0000000100 = 0.0000061665 loss
2019-04-08 04:13:11,035 [INFO] unfolding 0, single step 149001
2019-04-08 04:13:11,035 [INFO] Sum of grad norms of most recent batch: 0.022869
2019-04-08 04:13:11,036 [INFO] ---------------------------------
2019-04-08 04:13:31,833 [INFO] ---------------------------------
2019-04-08 04:13:31,834 [INFO] Summary:
2019-04-08 04:13:31,834 [INFO] Batch 150000, worst loss 0.046725 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 04:13:31,835 [INFO] Regularization: 616.626282 * 0.0000000100 = 0.0000061663 loss
2019-04-08 04:13:31,835 [INFO] unfolding 0, single step 150001
2019-04-08 04:13:31,836 [INFO] Sum of grad norms of most recent batch: 0.008615
2019-04-08 04:13:31,836 [INFO] ---------------------------------
2019-04-08 04:14:09,303 [INFO] ---------------------------------
2019-04-08 04:14:09,304 [INFO] Evaluation:
2019-04-08 04:14:09,304 [INFO] Batch 150000, worst loss 0.093768 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:14:09,305 [INFO] ---------------------------------
2019-04-08 04:14:09,305 [INFO] Finished training, saved to file transition/1554653688/1554689649_8_transition_final.pth
2019-04-08 04:14:09,495 [INFO] ---------------------------------
2019-04-08 04:14:09,497 [INFO] Training model #9: (11, 64, 8) @ 3
2019-04-08 04:14:31,663 [INFO] ---------------------------------
2019-04-08 04:14:31,664 [INFO] Summary:
2019-04-08 04:14:31,664 [INFO] Batch 1000, worst loss 30.360603 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:14:31,665 [INFO] Regularization: 7004.360352 * 0.0000000100 = 0.0000700436 loss
2019-04-08 04:14:31,666 [INFO] unfolding 0, single step 1001
2019-04-08 04:14:31,666 [INFO] Sum of grad norms of most recent batch: 10.721943
2019-04-08 04:14:31,667 [INFO] ---------------------------------
2019-04-08 04:14:53,623 [INFO] ---------------------------------
2019-04-08 04:14:53,624 [INFO] Summary:
2019-04-08 04:14:53,624 [INFO] Batch 2000, worst loss 0.108785 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:14:53,625 [INFO] Regularization: 4163.289551 * 0.0000000100 = 0.0000416329 loss
2019-04-08 04:14:53,625 [INFO] unfolding 0, single step 2001
2019-04-08 04:14:53,626 [INFO] Sum of grad norms of most recent batch: 3.887679
2019-04-08 04:14:53,627 [INFO] ---------------------------------
2019-04-08 04:15:15,253 [INFO] ---------------------------------
2019-04-08 04:15:15,254 [INFO] Summary:
2019-04-08 04:15:15,254 [INFO] Batch 3000, worst loss 0.123869 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:15:15,255 [INFO] Regularization: 3157.141602 * 0.0000000100 = 0.0000315714 loss
2019-04-08 04:15:15,255 [INFO] unfolding 0, single step 3001
2019-04-08 04:15:15,255 [INFO] Sum of grad norms of most recent batch: 2.222912
2019-04-08 04:15:15,256 [INFO] ---------------------------------
2019-04-08 04:15:36,877 [INFO] ---------------------------------
2019-04-08 04:15:36,878 [INFO] Summary:
2019-04-08 04:15:36,879 [INFO] Batch 4000, worst loss 0.096558 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:15:36,879 [INFO] Regularization: 2843.100830 * 0.0000000100 = 0.0000284310 loss
2019-04-08 04:15:36,880 [INFO] unfolding 0, single step 4001
2019-04-08 04:15:36,880 [INFO] Sum of grad norms of most recent batch: 3.855533
2019-04-08 04:15:36,881 [INFO] ---------------------------------
2019-04-08 04:15:58,681 [INFO] ---------------------------------
2019-04-08 04:15:58,682 [INFO] Summary:
2019-04-08 04:15:58,682 [INFO] Batch 5000, worst loss 0.073949 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:15:58,683 [INFO] Regularization: 2586.361328 * 0.0000000100 = 0.0000258636 loss
2019-04-08 04:15:58,683 [INFO] unfolding 0, single step 5001
2019-04-08 04:15:58,684 [INFO] Sum of grad norms of most recent batch: 2.335983
2019-04-08 04:15:58,685 [INFO] ---------------------------------
2019-04-08 04:16:20,146 [INFO] ---------------------------------
2019-04-08 04:16:20,147 [INFO] Summary:
2019-04-08 04:16:20,148 [INFO] Batch 6000, worst loss 0.144941 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:16:20,148 [INFO] Regularization: 2315.608643 * 0.0000000100 = 0.0000231561 loss
2019-04-08 04:16:20,149 [INFO] unfolding 0, single step 6001
2019-04-08 04:16:20,149 [INFO] Sum of grad norms of most recent batch: 1.984668
2019-04-08 04:16:20,150 [INFO] ---------------------------------
2019-04-08 04:16:41,821 [INFO] ---------------------------------
2019-04-08 04:16:41,822 [INFO] Summary:
2019-04-08 04:16:41,823 [INFO] Batch 7000, worst loss 0.168089 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:16:41,823 [INFO] Regularization: 2102.483154 * 0.0000000100 = 0.0000210248 loss
2019-04-08 04:16:41,823 [INFO] unfolding 0, single step 7001
2019-04-08 04:16:41,824 [INFO] Sum of grad norms of most recent batch: 2.656957
2019-04-08 04:16:41,825 [INFO] ---------------------------------
2019-04-08 04:17:03,627 [INFO] ---------------------------------
2019-04-08 04:17:03,628 [INFO] Summary:
2019-04-08 04:17:03,628 [INFO] Batch 8000, worst loss 0.110849 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:17:03,629 [INFO] Regularization: 1945.478149 * 0.0000000100 = 0.0000194548 loss
2019-04-08 04:17:03,629 [INFO] unfolding 0, single step 8001
2019-04-08 04:17:03,630 [INFO] Sum of grad norms of most recent batch: 0.618828
2019-04-08 04:17:03,630 [INFO] ---------------------------------
2019-04-08 04:17:25,997 [INFO] ---------------------------------
2019-04-08 04:17:25,998 [INFO] Summary:
2019-04-08 04:17:25,999 [INFO] Batch 9000, worst loss 0.040036 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:17:25,999 [INFO] Regularization: 1785.394897 * 0.0000000100 = 0.0000178539 loss
2019-04-08 04:17:26,000 [INFO] unfolding 0, single step 9001
2019-04-08 04:17:26,001 [INFO] Sum of grad norms of most recent batch: 2.603240
2019-04-08 04:17:26,001 [INFO] ---------------------------------
2019-04-08 04:17:47,618 [INFO] ---------------------------------
2019-04-08 04:17:47,619 [INFO] Summary:
2019-04-08 04:17:47,620 [INFO] Batch 10000, worst loss 0.114847 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:17:47,621 [INFO] Regularization: 1641.381470 * 0.0000000100 = 0.0000164138 loss
2019-04-08 04:17:47,621 [INFO] unfolding 0, single step 10001
2019-04-08 04:17:47,622 [INFO] Sum of grad norms of most recent batch: 2.209715
2019-04-08 04:17:47,623 [INFO] ---------------------------------
2019-04-08 04:18:25,172 [INFO] ---------------------------------
2019-04-08 04:18:25,173 [INFO] Evaluation:
2019-04-08 04:18:25,174 [INFO] Batch 10000, worst loss 0.141909 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:18:25,174 [INFO] ---------------------------------
2019-04-08 04:18:46,951 [INFO] ---------------------------------
2019-04-08 04:18:46,952 [INFO] Summary:
2019-04-08 04:18:46,953 [INFO] Batch 11000, worst loss 0.051805 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:18:46,953 [INFO] Regularization: 1507.201904 * 0.0000000100 = 0.0000150720 loss
2019-04-08 04:18:46,954 [INFO] unfolding 0, single step 11001
2019-04-08 04:18:46,954 [INFO] Sum of grad norms of most recent batch: 0.327139
2019-04-08 04:18:46,955 [INFO] ---------------------------------
2019-04-08 04:19:08,673 [INFO] ---------------------------------
2019-04-08 04:19:08,674 [INFO] Summary:
2019-04-08 04:19:08,675 [INFO] Batch 12000, worst loss 0.039957 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:19:08,675 [INFO] Regularization: 1402.191772 * 0.0000000100 = 0.0000140219 loss
2019-04-08 04:19:08,676 [INFO] unfolding 0, single step 12001
2019-04-08 04:19:08,676 [INFO] Sum of grad norms of most recent batch: 4.705141
2019-04-08 04:19:08,677 [INFO] ---------------------------------
2019-04-08 04:19:30,421 [INFO] ---------------------------------
2019-04-08 04:19:30,422 [INFO] Summary:
2019-04-08 04:19:30,422 [INFO] Batch 13000, worst loss 0.156164 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:19:30,423 [INFO] Regularization: 1345.689331 * 0.0000000100 = 0.0000134569 loss
2019-04-08 04:19:30,423 [INFO] unfolding 0, single step 13001
2019-04-08 04:19:30,424 [INFO] Sum of grad norms of most recent batch: 2.239602
2019-04-08 04:19:30,424 [INFO] ---------------------------------
2019-04-08 04:19:51,791 [INFO] ---------------------------------
2019-04-08 04:19:51,792 [INFO] Summary:
2019-04-08 04:19:51,793 [INFO] Batch 14000, worst loss 0.032911 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:19:51,795 [INFO] Regularization: 1269.701538 * 0.0000000100 = 0.0000126970 loss
2019-04-08 04:19:51,795 [INFO] unfolding 0, single step 14001
2019-04-08 04:19:51,796 [INFO] Sum of grad norms of most recent batch: 0.984059
2019-04-08 04:19:51,797 [INFO] ---------------------------------
2019-04-08 04:20:13,378 [INFO] ---------------------------------
2019-04-08 04:20:13,380 [INFO] Summary:
2019-04-08 04:20:13,380 [INFO] Batch 15000, worst loss 0.111281 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:20:13,381 [INFO] Regularization: 1203.661011 * 0.0000000100 = 0.0000120366 loss
2019-04-08 04:20:13,381 [INFO] unfolding 0, single step 15001
2019-04-08 04:20:13,382 [INFO] Sum of grad norms of most recent batch: 2.857461
2019-04-08 04:20:13,382 [INFO] ---------------------------------
2019-04-08 04:20:35,133 [INFO] ---------------------------------
2019-04-08 04:20:35,134 [INFO] Summary:
2019-04-08 04:20:35,135 [INFO] Batch 16000, worst loss 0.108477 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:20:35,136 [INFO] Regularization: 1184.750244 * 0.0000000100 = 0.0000118475 loss
2019-04-08 04:20:35,136 [INFO] unfolding 0, single step 16001
2019-04-08 04:20:35,137 [INFO] Sum of grad norms of most recent batch: 0.651671
2019-04-08 04:20:35,138 [INFO] ---------------------------------
2019-04-08 04:20:56,485 [INFO] ---------------------------------
2019-04-08 04:20:56,486 [INFO] Summary:
2019-04-08 04:20:56,487 [INFO] Batch 17000, worst loss 0.069108 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:20:56,487 [INFO] Regularization: 1165.306030 * 0.0000000100 = 0.0000116531 loss
2019-04-08 04:20:56,488 [INFO] unfolding 0, single step 17001
2019-04-08 04:20:56,488 [INFO] Sum of grad norms of most recent batch: 1.912564
2019-04-08 04:20:56,489 [INFO] ---------------------------------
2019-04-08 04:21:18,753 [INFO] ---------------------------------
2019-04-08 04:21:18,754 [INFO] Summary:
2019-04-08 04:21:18,755 [INFO] Batch 18000, worst loss 0.145007 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:21:18,755 [INFO] Regularization: 1171.627808 * 0.0000000100 = 0.0000117163 loss
2019-04-08 04:21:18,755 [INFO] unfolding 0, single step 18001
2019-04-08 04:21:18,756 [INFO] Sum of grad norms of most recent batch: 0.792750
2019-04-08 04:21:18,757 [INFO] ---------------------------------
2019-04-08 04:21:40,610 [INFO] ---------------------------------
2019-04-08 04:21:40,611 [INFO] Summary:
2019-04-08 04:21:40,611 [INFO] Batch 19000, worst loss 0.122558 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:21:40,612 [INFO] Regularization: 1109.694092 * 0.0000000100 = 0.0000110969 loss
2019-04-08 04:21:40,612 [INFO] unfolding 0, single step 19001
2019-04-08 04:21:40,613 [INFO] Sum of grad norms of most recent batch: 1.487251
2019-04-08 04:21:40,613 [INFO] ---------------------------------
2019-04-08 04:22:02,816 [INFO] ---------------------------------
2019-04-08 04:22:02,817 [INFO] Summary:
2019-04-08 04:22:02,818 [INFO] Batch 20000, worst loss 0.077923 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:22:02,819 [INFO] Regularization: 1105.002686 * 0.0000000100 = 0.0000110500 loss
2019-04-08 04:22:02,819 [INFO] unfolding 0, single step 20001
2019-04-08 04:22:02,820 [INFO] Sum of grad norms of most recent batch: 0.792738
2019-04-08 04:22:02,821 [INFO] ---------------------------------
2019-04-08 04:22:40,393 [INFO] ---------------------------------
2019-04-08 04:22:40,394 [INFO] Evaluation:
2019-04-08 04:22:40,394 [INFO] Batch 20000, worst loss 0.130874 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:22:40,395 [INFO] ---------------------------------
2019-04-08 04:23:01,449 [INFO] ---------------------------------
2019-04-08 04:23:01,450 [INFO] Summary:
2019-04-08 04:23:01,451 [INFO] Batch 21000, worst loss 0.110568 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:23:01,451 [INFO] Regularization: 1086.067993 * 0.0000000100 = 0.0000108607 loss
2019-04-08 04:23:01,452 [INFO] unfolding 0, single step 21001
2019-04-08 04:23:01,452 [INFO] Sum of grad norms of most recent batch: 0.907920
2019-04-08 04:23:01,453 [INFO] ---------------------------------
2019-04-08 04:23:23,416 [INFO] ---------------------------------
2019-04-08 04:23:23,417 [INFO] Summary:
2019-04-08 04:23:23,418 [INFO] Batch 22000, worst loss 0.134600 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:23:23,418 [INFO] Regularization: 1114.394043 * 0.0000000100 = 0.0000111439 loss
2019-04-08 04:23:23,419 [INFO] unfolding 0, single step 22001
2019-04-08 04:23:23,419 [INFO] Sum of grad norms of most recent batch: 1.353501
2019-04-08 04:23:23,420 [INFO] ---------------------------------
2019-04-08 04:23:45,080 [INFO] ---------------------------------
2019-04-08 04:23:45,081 [INFO] Summary:
2019-04-08 04:23:45,082 [INFO] Batch 23000, worst loss 0.090067 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:23:45,083 [INFO] Regularization: 1100.663940 * 0.0000000100 = 0.0000110066 loss
2019-04-08 04:23:45,084 [INFO] unfolding 0, single step 23001
2019-04-08 04:23:45,085 [INFO] Sum of grad norms of most recent batch: 0.802653
2019-04-08 04:23:45,085 [INFO] ---------------------------------
2019-04-08 04:24:06,728 [INFO] ---------------------------------
2019-04-08 04:24:06,729 [INFO] Summary:
2019-04-08 04:24:06,729 [INFO] Batch 24000, worst loss 0.131376 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:24:06,730 [INFO] Regularization: 1078.702759 * 0.0000000100 = 0.0000107870 loss
2019-04-08 04:24:06,730 [INFO] unfolding 0, single step 24001
2019-04-08 04:24:06,731 [INFO] Sum of grad norms of most recent batch: 1.200391
2019-04-08 04:24:06,731 [INFO] ---------------------------------
2019-04-08 04:24:28,928 [INFO] ---------------------------------
2019-04-08 04:24:28,930 [INFO] Summary:
2019-04-08 04:24:28,930 [INFO] Batch 25000, worst loss 0.066037 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:24:28,931 [INFO] Regularization: 1038.765259 * 0.0000000100 = 0.0000103877 loss
2019-04-08 04:24:28,931 [INFO] unfolding 0, single step 25001
2019-04-08 04:24:28,932 [INFO] Sum of grad norms of most recent batch: 1.015067
2019-04-08 04:24:28,932 [INFO] ---------------------------------
2019-04-08 04:24:51,090 [INFO] ---------------------------------
2019-04-08 04:24:51,091 [INFO] Summary:
2019-04-08 04:24:51,092 [INFO] Batch 26000, worst loss 0.044599 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:24:51,092 [INFO] Regularization: 1007.143555 * 0.0000000100 = 0.0000100714 loss
2019-04-08 04:24:51,093 [INFO] unfolding 0, single step 26001
2019-04-08 04:24:51,093 [INFO] Sum of grad norms of most recent batch: 1.291583
2019-04-08 04:24:51,094 [INFO] ---------------------------------
2019-04-08 04:25:14,149 [INFO] ---------------------------------
2019-04-08 04:25:14,150 [INFO] Summary:
2019-04-08 04:25:14,151 [INFO] Batch 27000, worst loss 0.046796 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:25:14,152 [INFO] Regularization: 1034.224121 * 0.0000000100 = 0.0000103422 loss
2019-04-08 04:25:14,152 [INFO] unfolding 0, single step 27001
2019-04-08 04:25:14,153 [INFO] Sum of grad norms of most recent batch: 0.836853
2019-04-08 04:25:14,154 [INFO] ---------------------------------
2019-04-08 04:25:35,251 [INFO] ---------------------------------
2019-04-08 04:25:35,252 [INFO] Summary:
2019-04-08 04:25:35,253 [INFO] Batch 28000, worst loss 0.084701 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:25:35,253 [INFO] Regularization: 1028.478882 * 0.0000000100 = 0.0000102848 loss
2019-04-08 04:25:35,253 [INFO] unfolding 0, single step 28001
2019-04-08 04:25:35,254 [INFO] Sum of grad norms of most recent batch: 2.723322
2019-04-08 04:25:35,254 [INFO] ---------------------------------
2019-04-08 04:25:56,823 [INFO] ---------------------------------
2019-04-08 04:25:56,824 [INFO] Summary:
2019-04-08 04:25:56,825 [INFO] Batch 29000, worst loss 0.040151 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:25:56,825 [INFO] Regularization: 1009.399048 * 0.0000000100 = 0.0000100940 loss
2019-04-08 04:25:56,826 [INFO] unfolding 0, single step 29001
2019-04-08 04:25:56,826 [INFO] Sum of grad norms of most recent batch: 2.531152
2019-04-08 04:25:56,827 [INFO] ---------------------------------
2019-04-08 04:26:18,657 [INFO] ---------------------------------
2019-04-08 04:26:18,658 [INFO] Summary:
2019-04-08 04:26:18,659 [INFO] Batch 30000, worst loss 0.061787 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 3
2019-04-08 04:26:18,660 [INFO] Regularization: 997.443359 * 0.0000000100 = 0.0000099744 loss
2019-04-08 04:26:18,660 [INFO] unfolding 0, single step 30001
2019-04-08 04:26:18,660 [INFO] Sum of grad norms of most recent batch: 8.829584
2019-04-08 04:26:18,661 [INFO] ---------------------------------
2019-04-08 04:26:56,128 [INFO] ---------------------------------
2019-04-08 04:26:56,129 [INFO] Evaluation:
2019-04-08 04:26:56,130 [INFO] Batch 30000, worst loss 0.106507 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:26:56,130 [INFO] ---------------------------------
2019-04-08 04:27:17,872 [INFO] ---------------------------------
2019-04-08 04:27:17,873 [INFO] Summary:
2019-04-08 04:27:17,874 [INFO] Batch 31000, worst loss 0.098023 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 04:27:17,874 [INFO] Regularization: 1007.513855 * 0.0000000100 = 0.0000100751 loss
2019-04-08 04:27:17,875 [INFO] unfolding 0, single step 31001
2019-04-08 04:27:17,875 [INFO] Sum of grad norms of most recent batch: 4.637455
2019-04-08 04:27:17,876 [INFO] ---------------------------------
2019-04-08 04:27:39,893 [INFO] ---------------------------------
2019-04-08 04:27:39,894 [INFO] Summary:
2019-04-08 04:27:39,894 [INFO] Batch 32000, worst loss 0.116354 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 04:27:39,895 [INFO] Regularization: 951.396606 * 0.0000000100 = 0.0000095140 loss
2019-04-08 04:27:39,895 [INFO] unfolding 0, single step 32001
2019-04-08 04:27:39,896 [INFO] Sum of grad norms of most recent batch: 0.498117
2019-04-08 04:27:39,896 [INFO] ---------------------------------
2019-04-08 04:28:02,086 [INFO] ---------------------------------
2019-04-08 04:28:02,087 [INFO] Summary:
2019-04-08 04:28:02,087 [INFO] Batch 33000, worst loss 0.037229 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 04:28:02,088 [INFO] Regularization: 908.283630 * 0.0000000100 = 0.0000090828 loss
2019-04-08 04:28:02,088 [INFO] unfolding 0, single step 33001
2019-04-08 04:28:02,089 [INFO] Sum of grad norms of most recent batch: 0.384373
2019-04-08 04:28:02,089 [INFO] ---------------------------------
2019-04-08 04:28:23,807 [INFO] ---------------------------------
2019-04-08 04:28:23,808 [INFO] Summary:
2019-04-08 04:28:23,809 [INFO] Batch 34000, worst loss 0.067009 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 04:28:23,809 [INFO] Regularization: 874.513550 * 0.0000000100 = 0.0000087451 loss
2019-04-08 04:28:23,810 [INFO] unfolding 0, single step 34001
2019-04-08 04:28:23,810 [INFO] Sum of grad norms of most recent batch: 0.330663
2019-04-08 04:28:23,811 [INFO] ---------------------------------
2019-04-08 04:28:45,298 [INFO] ---------------------------------
2019-04-08 04:28:45,299 [INFO] Summary:
2019-04-08 04:28:45,300 [INFO] Batch 35000, worst loss 0.127006 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 04:28:45,301 [INFO] Regularization: 845.837585 * 0.0000000100 = 0.0000084584 loss
2019-04-08 04:28:45,301 [INFO] unfolding 0, single step 35001
2019-04-08 04:28:45,302 [INFO] Sum of grad norms of most recent batch: 0.646634
2019-04-08 04:28:45,303 [INFO] ---------------------------------
2019-04-08 04:29:06,538 [INFO] ---------------------------------
2019-04-08 04:29:06,539 [INFO] Summary:
2019-04-08 04:29:06,540 [INFO] Batch 36000, worst loss 0.026900 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 04:29:06,540 [INFO] Regularization: 821.061890 * 0.0000000100 = 0.0000082106 loss
2019-04-08 04:29:06,541 [INFO] unfolding 0, single step 36001
2019-04-08 04:29:06,541 [INFO] Sum of grad norms of most recent batch: 0.817140
2019-04-08 04:29:06,542 [INFO] ---------------------------------
2019-04-08 04:29:28,179 [INFO] ---------------------------------
2019-04-08 04:29:28,180 [INFO] Summary:
2019-04-08 04:29:28,181 [INFO] Batch 37000, worst loss 0.090040 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 04:29:28,181 [INFO] Regularization: 804.982788 * 0.0000000100 = 0.0000080498 loss
2019-04-08 04:29:28,181 [INFO] unfolding 0, single step 37001
2019-04-08 04:29:28,182 [INFO] Sum of grad norms of most recent batch: 0.365451
2019-04-08 04:29:28,182 [INFO] ---------------------------------
2019-04-08 04:29:50,244 [INFO] ---------------------------------
2019-04-08 04:29:50,245 [INFO] Summary:
2019-04-08 04:29:50,245 [INFO] Batch 38000, worst loss 0.145063 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 04:29:50,246 [INFO] Regularization: 787.055176 * 0.0000000100 = 0.0000078706 loss
2019-04-08 04:29:50,246 [INFO] unfolding 0, single step 38001
2019-04-08 04:29:50,247 [INFO] Sum of grad norms of most recent batch: 0.783332
2019-04-08 04:29:50,247 [INFO] ---------------------------------
2019-04-08 04:30:12,250 [INFO] ---------------------------------
2019-04-08 04:30:12,251 [INFO] Summary:
2019-04-08 04:30:12,251 [INFO] Batch 39000, worst loss 0.053322 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 04:30:12,252 [INFO] Regularization: 776.335815 * 0.0000000100 = 0.0000077634 loss
2019-04-08 04:30:12,252 [INFO] unfolding 0, single step 39001
2019-04-08 04:30:12,253 [INFO] Sum of grad norms of most recent batch: 0.975924
2019-04-08 04:30:12,253 [INFO] ---------------------------------
2019-04-08 04:30:34,172 [INFO] ---------------------------------
2019-04-08 04:30:34,172 [INFO] Summary:
2019-04-08 04:30:34,173 [INFO] Batch 40000, worst loss 0.095583 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 3
2019-04-08 04:30:34,173 [INFO] Regularization: 782.285583 * 0.0000000100 = 0.0000078229 loss
2019-04-08 04:30:34,174 [INFO] unfolding 0, single step 40001
2019-04-08 04:30:34,175 [INFO] Sum of grad norms of most recent batch: 0.958107
2019-04-08 04:30:34,175 [INFO] ---------------------------------
2019-04-08 04:31:11,667 [INFO] ---------------------------------
2019-04-08 04:31:11,668 [INFO] Evaluation:
2019-04-08 04:31:11,669 [INFO] Batch 40000, worst loss 0.101907 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:31:11,669 [INFO] ---------------------------------
2019-04-08 04:31:33,702 [INFO] ---------------------------------
2019-04-08 04:31:33,703 [INFO] Summary:
2019-04-08 04:31:33,703 [INFO] Batch 41000, worst loss 0.058426 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 04:31:33,704 [INFO] Regularization: 766.239868 * 0.0000000100 = 0.0000076624 loss
2019-04-08 04:31:33,704 [INFO] unfolding 0, single step 41001
2019-04-08 04:31:33,705 [INFO] Sum of grad norms of most recent batch: 0.436638
2019-04-08 04:31:33,705 [INFO] ---------------------------------
2019-04-08 04:31:55,232 [INFO] ---------------------------------
2019-04-08 04:31:55,233 [INFO] Summary:
2019-04-08 04:31:55,234 [INFO] Batch 42000, worst loss 0.137834 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 04:31:55,234 [INFO] Regularization: 751.120605 * 0.0000000100 = 0.0000075112 loss
2019-04-08 04:31:55,234 [INFO] unfolding 0, single step 42001
2019-04-08 04:31:55,235 [INFO] Sum of grad norms of most recent batch: 0.387711
2019-04-08 04:31:55,236 [INFO] ---------------------------------
2019-04-08 04:32:17,432 [INFO] ---------------------------------
2019-04-08 04:32:17,433 [INFO] Summary:
2019-04-08 04:32:17,434 [INFO] Batch 43000, worst loss 0.073069 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 04:32:17,435 [INFO] Regularization: 741.211365 * 0.0000000100 = 0.0000074121 loss
2019-04-08 04:32:17,436 [INFO] unfolding 0, single step 43001
2019-04-08 04:32:17,437 [INFO] Sum of grad norms of most recent batch: 5.726809
2019-04-08 04:32:17,437 [INFO] ---------------------------------
2019-04-08 04:32:39,310 [INFO] ---------------------------------
2019-04-08 04:32:39,311 [INFO] Summary:
2019-04-08 04:32:39,311 [INFO] Batch 44000, worst loss 0.073568 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 04:32:39,312 [INFO] Regularization: 727.318420 * 0.0000000100 = 0.0000072732 loss
2019-04-08 04:32:39,312 [INFO] unfolding 0, single step 44001
2019-04-08 04:32:39,313 [INFO] Sum of grad norms of most recent batch: 0.345218
2019-04-08 04:32:39,313 [INFO] ---------------------------------
2019-04-08 04:33:00,311 [INFO] ---------------------------------
2019-04-08 04:33:00,312 [INFO] Summary:
2019-04-08 04:33:00,313 [INFO] Batch 45000, worst loss 0.087095 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 04:33:00,313 [INFO] Regularization: 714.498840 * 0.0000000100 = 0.0000071450 loss
2019-04-08 04:33:00,314 [INFO] unfolding 0, single step 45001
2019-04-08 04:33:00,314 [INFO] Sum of grad norms of most recent batch: 0.475080
2019-04-08 04:33:00,315 [INFO] ---------------------------------
2019-04-08 04:33:22,746 [INFO] ---------------------------------
2019-04-08 04:33:22,747 [INFO] Summary:
2019-04-08 04:33:22,748 [INFO] Batch 46000, worst loss 0.073718 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 04:33:22,748 [INFO] Regularization: 701.876892 * 0.0000000100 = 0.0000070188 loss
2019-04-08 04:33:22,749 [INFO] unfolding 0, single step 46001
2019-04-08 04:33:22,750 [INFO] Sum of grad norms of most recent batch: 0.547867
2019-04-08 04:33:22,750 [INFO] ---------------------------------
2019-04-08 04:33:45,007 [INFO] ---------------------------------
2019-04-08 04:33:45,008 [INFO] Summary:
2019-04-08 04:33:45,009 [INFO] Batch 47000, worst loss 0.140086 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 04:33:45,009 [INFO] Regularization: 704.193970 * 0.0000000100 = 0.0000070419 loss
2019-04-08 04:33:45,010 [INFO] unfolding 0, single step 47001
2019-04-08 04:33:45,010 [INFO] Sum of grad norms of most recent batch: 0.376718
2019-04-08 04:33:45,011 [INFO] ---------------------------------
2019-04-08 04:34:06,763 [INFO] ---------------------------------
2019-04-08 04:34:06,763 [INFO] Summary:
2019-04-08 04:34:06,764 [INFO] Batch 48000, worst loss 0.096646 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 04:34:06,765 [INFO] Regularization: 702.332520 * 0.0000000100 = 0.0000070233 loss
2019-04-08 04:34:06,765 [INFO] unfolding 0, single step 48001
2019-04-08 04:34:06,766 [INFO] Sum of grad norms of most recent batch: 0.302039
2019-04-08 04:34:06,766 [INFO] ---------------------------------
2019-04-08 04:34:29,213 [INFO] ---------------------------------
2019-04-08 04:34:29,214 [INFO] Summary:
2019-04-08 04:34:29,214 [INFO] Batch 49000, worst loss 0.124028 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 04:34:29,215 [INFO] Regularization: 692.036255 * 0.0000000100 = 0.0000069204 loss
2019-04-08 04:34:29,215 [INFO] unfolding 0, single step 49001
2019-04-08 04:34:29,216 [INFO] Sum of grad norms of most recent batch: 0.332869
2019-04-08 04:34:29,216 [INFO] ---------------------------------
2019-04-08 04:34:51,208 [INFO] ---------------------------------
2019-04-08 04:34:51,209 [INFO] Summary:
2019-04-08 04:34:51,209 [INFO] Batch 50000, worst loss 0.144441 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 3
2019-04-08 04:34:51,210 [INFO] Regularization: 683.640991 * 0.0000000100 = 0.0000068364 loss
2019-04-08 04:34:51,210 [INFO] unfolding 0, single step 50001
2019-04-08 04:34:51,210 [INFO] Sum of grad norms of most recent batch: 0.473498
2019-04-08 04:34:51,211 [INFO] ---------------------------------
2019-04-08 04:35:28,630 [INFO] ---------------------------------
2019-04-08 04:35:28,631 [INFO] Evaluation:
2019-04-08 04:35:28,631 [INFO] Batch 50000, worst loss 0.150307 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:35:28,632 [INFO] ---------------------------------
2019-04-08 04:35:50,548 [INFO] ---------------------------------
2019-04-08 04:35:50,549 [INFO] Summary:
2019-04-08 04:35:50,550 [INFO] Batch 51000, worst loss 0.140349 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 04:35:50,551 [INFO] Regularization: 677.771240 * 0.0000000100 = 0.0000067777 loss
2019-04-08 04:35:50,551 [INFO] unfolding 0, single step 51001
2019-04-08 04:35:50,552 [INFO] Sum of grad norms of most recent batch: 0.193744
2019-04-08 04:35:50,552 [INFO] ---------------------------------
2019-04-08 04:36:11,847 [INFO] ---------------------------------
2019-04-08 04:36:11,849 [INFO] Summary:
2019-04-08 04:36:11,849 [INFO] Batch 52000, worst loss 0.080213 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 04:36:11,850 [INFO] Regularization: 668.665222 * 0.0000000100 = 0.0000066867 loss
2019-04-08 04:36:11,850 [INFO] unfolding 0, single step 52001
2019-04-08 04:36:11,850 [INFO] Sum of grad norms of most recent batch: 0.455793
2019-04-08 04:36:11,851 [INFO] ---------------------------------
2019-04-08 04:36:33,371 [INFO] ---------------------------------
2019-04-08 04:36:33,372 [INFO] Summary:
2019-04-08 04:36:33,372 [INFO] Batch 53000, worst loss 0.115785 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 04:36:33,373 [INFO] Regularization: 668.381104 * 0.0000000100 = 0.0000066838 loss
2019-04-08 04:36:33,373 [INFO] unfolding 0, single step 53001
2019-04-08 04:36:33,374 [INFO] Sum of grad norms of most recent batch: 0.225047
2019-04-08 04:36:33,375 [INFO] ---------------------------------
2019-04-08 04:36:54,482 [INFO] ---------------------------------
2019-04-08 04:36:54,483 [INFO] Summary:
2019-04-08 04:36:54,484 [INFO] Batch 54000, worst loss 0.093414 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 04:36:54,484 [INFO] Regularization: 660.698364 * 0.0000000100 = 0.0000066070 loss
2019-04-08 04:36:54,485 [INFO] unfolding 0, single step 54001
2019-04-08 04:36:54,485 [INFO] Sum of grad norms of most recent batch: 0.080380
2019-04-08 04:36:54,486 [INFO] ---------------------------------
2019-04-08 04:37:15,699 [INFO] ---------------------------------
2019-04-08 04:37:15,700 [INFO] Summary:
2019-04-08 04:37:15,701 [INFO] Batch 55000, worst loss 0.059264 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 04:37:15,701 [INFO] Regularization: 655.965759 * 0.0000000100 = 0.0000065597 loss
2019-04-08 04:37:15,702 [INFO] unfolding 0, single step 55001
2019-04-08 04:37:15,702 [INFO] Sum of grad norms of most recent batch: 0.080037
2019-04-08 04:37:15,703 [INFO] ---------------------------------
2019-04-08 04:37:36,516 [INFO] ---------------------------------
2019-04-08 04:37:36,517 [INFO] Summary:
2019-04-08 04:37:36,517 [INFO] Batch 56000, worst loss 0.157952 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 04:37:36,518 [INFO] Regularization: 650.472717 * 0.0000000100 = 0.0000065047 loss
2019-04-08 04:37:36,519 [INFO] unfolding 0, single step 56001
2019-04-08 04:37:36,520 [INFO] Sum of grad norms of most recent batch: 0.114288
2019-04-08 04:37:36,520 [INFO] ---------------------------------
2019-04-08 04:37:57,010 [INFO] ---------------------------------
2019-04-08 04:37:57,011 [INFO] Summary:
2019-04-08 04:37:57,011 [INFO] Batch 57000, worst loss 0.158066 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 04:37:57,012 [INFO] Regularization: 645.969421 * 0.0000000100 = 0.0000064597 loss
2019-04-08 04:37:57,012 [INFO] unfolding 0, single step 57001
2019-04-08 04:37:57,013 [INFO] Sum of grad norms of most recent batch: 0.091709
2019-04-08 04:37:57,013 [INFO] ---------------------------------
2019-04-08 04:38:17,662 [INFO] ---------------------------------
2019-04-08 04:38:17,663 [INFO] Summary:
2019-04-08 04:38:17,663 [INFO] Batch 58000, worst loss 0.109248 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 04:38:17,664 [INFO] Regularization: 648.565552 * 0.0000000100 = 0.0000064857 loss
2019-04-08 04:38:17,664 [INFO] unfolding 0, single step 58001
2019-04-08 04:38:17,665 [INFO] Sum of grad norms of most recent batch: 0.102373
2019-04-08 04:38:17,666 [INFO] ---------------------------------
2019-04-08 04:38:38,684 [INFO] ---------------------------------
2019-04-08 04:38:38,685 [INFO] Summary:
2019-04-08 04:38:38,685 [INFO] Batch 59000, worst loss 0.054133 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 04:38:38,686 [INFO] Regularization: 647.063599 * 0.0000000100 = 0.0000064706 loss
2019-04-08 04:38:38,686 [INFO] unfolding 0, single step 59001
2019-04-08 04:38:38,687 [INFO] Sum of grad norms of most recent batch: 0.062133
2019-04-08 04:38:38,687 [INFO] ---------------------------------
2019-04-08 04:39:00,357 [INFO] ---------------------------------
2019-04-08 04:39:00,358 [INFO] Summary:
2019-04-08 04:39:00,359 [INFO] Batch 60000, worst loss 0.050634 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000125 @est.-depth 3
2019-04-08 04:39:00,359 [INFO] Regularization: 641.386963 * 0.0000000100 = 0.0000064139 loss
2019-04-08 04:39:00,360 [INFO] unfolding 0, single step 60001
2019-04-08 04:39:00,360 [INFO] Sum of grad norms of most recent batch: 0.092880
2019-04-08 04:39:00,361 [INFO] ---------------------------------
2019-04-08 04:39:37,946 [INFO] ---------------------------------
2019-04-08 04:39:37,947 [INFO] Evaluation:
2019-04-08 04:39:37,948 [INFO] Batch 60000, worst loss 0.148783 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:39:37,949 [INFO] ---------------------------------
2019-04-08 04:39:59,618 [INFO] ---------------------------------
2019-04-08 04:39:59,619 [INFO] Summary:
2019-04-08 04:39:59,620 [INFO] Batch 61000, worst loss 0.108377 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 04:39:59,620 [INFO] Regularization: 640.602600 * 0.0000000100 = 0.0000064060 loss
2019-04-08 04:39:59,621 [INFO] unfolding 0, single step 61001
2019-04-08 04:39:59,621 [INFO] Sum of grad norms of most recent batch: 0.269591
2019-04-08 04:39:59,622 [INFO] ---------------------------------
2019-04-08 04:40:20,215 [INFO] ---------------------------------
2019-04-08 04:40:20,216 [INFO] Summary:
2019-04-08 04:40:20,217 [INFO] Batch 62000, worst loss 0.119388 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 04:40:20,217 [INFO] Regularization: 635.589661 * 0.0000000100 = 0.0000063559 loss
2019-04-08 04:40:20,218 [INFO] unfolding 0, single step 62001
2019-04-08 04:40:20,218 [INFO] Sum of grad norms of most recent batch: 0.055207
2019-04-08 04:40:20,219 [INFO] ---------------------------------
2019-04-08 04:40:40,980 [INFO] ---------------------------------
2019-04-08 04:40:40,981 [INFO] Summary:
2019-04-08 04:40:40,981 [INFO] Batch 63000, worst loss 0.086597 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 04:40:40,982 [INFO] Regularization: 633.245728 * 0.0000000100 = 0.0000063325 loss
2019-04-08 04:40:40,982 [INFO] unfolding 0, single step 63001
2019-04-08 04:40:40,983 [INFO] Sum of grad norms of most recent batch: 0.134923
2019-04-08 04:40:40,983 [INFO] ---------------------------------
2019-04-08 04:41:02,383 [INFO] ---------------------------------
2019-04-08 04:41:02,384 [INFO] Summary:
2019-04-08 04:41:02,385 [INFO] Batch 64000, worst loss 0.099789 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 04:41:02,385 [INFO] Regularization: 632.226868 * 0.0000000100 = 0.0000063223 loss
2019-04-08 04:41:02,386 [INFO] unfolding 0, single step 64001
2019-04-08 04:41:02,386 [INFO] Sum of grad norms of most recent batch: 0.041941
2019-04-08 04:41:02,387 [INFO] ---------------------------------
2019-04-08 04:41:23,770 [INFO] ---------------------------------
2019-04-08 04:41:23,771 [INFO] Summary:
2019-04-08 04:41:23,772 [INFO] Batch 65000, worst loss 0.048594 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 04:41:23,772 [INFO] Regularization: 628.896423 * 0.0000000100 = 0.0000062890 loss
2019-04-08 04:41:23,772 [INFO] unfolding 0, single step 65001
2019-04-08 04:41:23,773 [INFO] Sum of grad norms of most recent batch: 1.684127
2019-04-08 04:41:23,773 [INFO] ---------------------------------
2019-04-08 04:41:45,813 [INFO] ---------------------------------
2019-04-08 04:41:45,814 [INFO] Summary:
2019-04-08 04:41:45,814 [INFO] Batch 66000, worst loss 0.073990 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 04:41:45,815 [INFO] Regularization: 628.655273 * 0.0000000100 = 0.0000062866 loss
2019-04-08 04:41:45,816 [INFO] unfolding 0, single step 66001
2019-04-08 04:41:45,816 [INFO] Sum of grad norms of most recent batch: 0.053526
2019-04-08 04:41:45,817 [INFO] ---------------------------------
2019-04-08 04:42:07,139 [INFO] ---------------------------------
2019-04-08 04:42:07,140 [INFO] Summary:
2019-04-08 04:42:07,140 [INFO] Batch 67000, worst loss 0.075672 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 04:42:07,141 [INFO] Regularization: 626.609192 * 0.0000000100 = 0.0000062661 loss
2019-04-08 04:42:07,141 [INFO] unfolding 0, single step 67001
2019-04-08 04:42:07,142 [INFO] Sum of grad norms of most recent batch: 0.183536
2019-04-08 04:42:07,142 [INFO] ---------------------------------
2019-04-08 04:42:28,112 [INFO] ---------------------------------
2019-04-08 04:42:28,113 [INFO] Summary:
2019-04-08 04:42:28,114 [INFO] Batch 68000, worst loss 0.066841 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 04:42:28,114 [INFO] Regularization: 623.303040 * 0.0000000100 = 0.0000062330 loss
2019-04-08 04:42:28,115 [INFO] unfolding 0, single step 68001
2019-04-08 04:42:28,116 [INFO] Sum of grad norms of most recent batch: 0.082537
2019-04-08 04:42:28,116 [INFO] ---------------------------------
2019-04-08 04:42:49,698 [INFO] ---------------------------------
2019-04-08 04:42:49,699 [INFO] Summary:
2019-04-08 04:42:49,699 [INFO] Batch 69000, worst loss 0.018657 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 04:42:49,700 [INFO] Regularization: 620.844421 * 0.0000000100 = 0.0000062084 loss
2019-04-08 04:42:49,700 [INFO] unfolding 0, single step 69001
2019-04-08 04:42:49,701 [INFO] Sum of grad norms of most recent batch: 0.068568
2019-04-08 04:42:49,701 [INFO] ---------------------------------
2019-04-08 04:43:11,862 [INFO] ---------------------------------
2019-04-08 04:43:11,863 [INFO] Summary:
2019-04-08 04:43:11,864 [INFO] Batch 70000, worst loss 0.077397 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000063 @est.-depth 3
2019-04-08 04:43:11,865 [INFO] Regularization: 619.468018 * 0.0000000100 = 0.0000061947 loss
2019-04-08 04:43:11,865 [INFO] unfolding 0, single step 70001
2019-04-08 04:43:11,866 [INFO] Sum of grad norms of most recent batch: 0.265679
2019-04-08 04:43:11,866 [INFO] ---------------------------------
2019-04-08 04:43:49,236 [INFO] ---------------------------------
2019-04-08 04:43:49,237 [INFO] Evaluation:
2019-04-08 04:43:49,238 [INFO] Batch 70000, worst loss 0.115853 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:43:49,238 [INFO] ---------------------------------
2019-04-08 04:44:10,192 [INFO] ---------------------------------
2019-04-08 04:44:10,193 [INFO] Summary:
2019-04-08 04:44:10,194 [INFO] Batch 71000, worst loss 0.079583 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 04:44:10,194 [INFO] Regularization: 618.221069 * 0.0000000100 = 0.0000061822 loss
2019-04-08 04:44:10,195 [INFO] unfolding 0, single step 71001
2019-04-08 04:44:10,196 [INFO] Sum of grad norms of most recent batch: 0.115518
2019-04-08 04:44:10,196 [INFO] ---------------------------------
2019-04-08 04:44:31,445 [INFO] ---------------------------------
2019-04-08 04:44:31,446 [INFO] Summary:
2019-04-08 04:44:31,447 [INFO] Batch 72000, worst loss 0.061635 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 04:44:31,447 [INFO] Regularization: 617.290649 * 0.0000000100 = 0.0000061729 loss
2019-04-08 04:44:31,448 [INFO] unfolding 0, single step 72001
2019-04-08 04:44:31,449 [INFO] Sum of grad norms of most recent batch: 0.177657
2019-04-08 04:44:31,450 [INFO] ---------------------------------
2019-04-08 04:44:52,557 [INFO] ---------------------------------
2019-04-08 04:44:52,558 [INFO] Summary:
2019-04-08 04:44:52,559 [INFO] Batch 73000, worst loss 0.023775 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 04:44:52,560 [INFO] Regularization: 616.041565 * 0.0000000100 = 0.0000061604 loss
2019-04-08 04:44:52,561 [INFO] unfolding 0, single step 73001
2019-04-08 04:44:52,562 [INFO] Sum of grad norms of most recent batch: 0.149505
2019-04-08 04:44:52,563 [INFO] ---------------------------------
2019-04-08 04:45:13,419 [INFO] ---------------------------------
2019-04-08 04:45:13,420 [INFO] Summary:
2019-04-08 04:45:13,420 [INFO] Batch 74000, worst loss 0.061050 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 04:45:13,421 [INFO] Regularization: 615.244019 * 0.0000000100 = 0.0000061524 loss
2019-04-08 04:45:13,422 [INFO] unfolding 0, single step 74001
2019-04-08 04:45:13,423 [INFO] Sum of grad norms of most recent batch: 0.094862
2019-04-08 04:45:13,423 [INFO] ---------------------------------
2019-04-08 04:45:34,283 [INFO] ---------------------------------
2019-04-08 04:45:34,284 [INFO] Summary:
2019-04-08 04:45:34,284 [INFO] Batch 75000, worst loss 0.027860 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 04:45:34,285 [INFO] Regularization: 614.299622 * 0.0000000100 = 0.0000061430 loss
2019-04-08 04:45:34,285 [INFO] unfolding 0, single step 75001
2019-04-08 04:45:34,286 [INFO] Sum of grad norms of most recent batch: 0.434696
2019-04-08 04:45:34,286 [INFO] ---------------------------------
2019-04-08 04:45:55,711 [INFO] ---------------------------------
2019-04-08 04:45:55,712 [INFO] Summary:
2019-04-08 04:45:55,712 [INFO] Batch 76000, worst loss 0.039255 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 04:45:55,713 [INFO] Regularization: 613.888550 * 0.0000000100 = 0.0000061389 loss
2019-04-08 04:45:55,713 [INFO] unfolding 0, single step 76001
2019-04-08 04:45:55,714 [INFO] Sum of grad norms of most recent batch: 0.096908
2019-04-08 04:45:55,714 [INFO] ---------------------------------
2019-04-08 04:46:16,765 [INFO] ---------------------------------
2019-04-08 04:46:16,766 [INFO] Summary:
2019-04-08 04:46:16,766 [INFO] Batch 77000, worst loss 0.036668 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 04:46:16,767 [INFO] Regularization: 613.312500 * 0.0000000100 = 0.0000061331 loss
2019-04-08 04:46:16,767 [INFO] unfolding 0, single step 77001
2019-04-08 04:46:16,767 [INFO] Sum of grad norms of most recent batch: 0.033536
2019-04-08 04:46:16,768 [INFO] ---------------------------------
2019-04-08 04:46:38,275 [INFO] ---------------------------------
2019-04-08 04:46:38,276 [INFO] Summary:
2019-04-08 04:46:38,277 [INFO] Batch 78000, worst loss 0.038993 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 04:46:38,277 [INFO] Regularization: 612.620605 * 0.0000000100 = 0.0000061262 loss
2019-04-08 04:46:38,278 [INFO] unfolding 0, single step 78001
2019-04-08 04:46:38,278 [INFO] Sum of grad norms of most recent batch: 0.147645
2019-04-08 04:46:38,279 [INFO] ---------------------------------
2019-04-08 04:46:58,829 [INFO] ---------------------------------
2019-04-08 04:46:58,830 [INFO] Summary:
2019-04-08 04:46:58,830 [INFO] Batch 79000, worst loss 0.036848 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 04:46:58,831 [INFO] Regularization: 612.292786 * 0.0000000100 = 0.0000061229 loss
2019-04-08 04:46:58,831 [INFO] unfolding 0, single step 79001
2019-04-08 04:46:58,831 [INFO] Sum of grad norms of most recent batch: 0.087807
2019-04-08 04:46:58,832 [INFO] ---------------------------------
2019-04-08 04:47:19,964 [INFO] ---------------------------------
2019-04-08 04:47:19,966 [INFO] Summary:
2019-04-08 04:47:19,966 [INFO] Batch 80000, worst loss 0.043915 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000031 @est.-depth 3
2019-04-08 04:47:19,967 [INFO] Regularization: 611.213562 * 0.0000000100 = 0.0000061121 loss
2019-04-08 04:47:19,967 [INFO] unfolding 0, single step 80001
2019-04-08 04:47:19,967 [INFO] Sum of grad norms of most recent batch: 1.148023
2019-04-08 04:47:19,968 [INFO] ---------------------------------
2019-04-08 04:47:57,587 [INFO] ---------------------------------
2019-04-08 04:47:57,588 [INFO] Evaluation:
2019-04-08 04:47:57,589 [INFO] Batch 80000, worst loss 0.096184 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:47:57,589 [INFO] ---------------------------------
2019-04-08 04:48:18,871 [INFO] ---------------------------------
2019-04-08 04:48:18,872 [INFO] Summary:
2019-04-08 04:48:18,874 [INFO] Batch 81000, worst loss 0.135672 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 04:48:18,874 [INFO] Regularization: 610.624390 * 0.0000000100 = 0.0000061062 loss
2019-04-08 04:48:18,875 [INFO] unfolding 0, single step 81001
2019-04-08 04:48:18,876 [INFO] Sum of grad norms of most recent batch: 0.033206
2019-04-08 04:48:18,877 [INFO] ---------------------------------
2019-04-08 04:48:40,133 [INFO] ---------------------------------
2019-04-08 04:48:40,134 [INFO] Summary:
2019-04-08 04:48:40,134 [INFO] Batch 82000, worst loss 0.009645 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 04:48:40,135 [INFO] Regularization: 610.066040 * 0.0000000100 = 0.0000061007 loss
2019-04-08 04:48:40,135 [INFO] unfolding 0, single step 82001
2019-04-08 04:48:40,136 [INFO] Sum of grad norms of most recent batch: 0.132779
2019-04-08 04:48:40,136 [INFO] ---------------------------------
2019-04-08 04:49:01,406 [INFO] ---------------------------------
2019-04-08 04:49:01,407 [INFO] Summary:
2019-04-08 04:49:01,407 [INFO] Batch 83000, worst loss 0.024860 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 04:49:01,408 [INFO] Regularization: 609.477905 * 0.0000000100 = 0.0000060948 loss
2019-04-08 04:49:01,408 [INFO] unfolding 0, single step 83001
2019-04-08 04:49:01,409 [INFO] Sum of grad norms of most recent batch: 0.022449
2019-04-08 04:49:01,409 [INFO] ---------------------------------
2019-04-08 04:49:22,970 [INFO] ---------------------------------
2019-04-08 04:49:22,971 [INFO] Summary:
2019-04-08 04:49:22,971 [INFO] Batch 84000, worst loss 0.052070 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 04:49:22,972 [INFO] Regularization: 608.958313 * 0.0000000100 = 0.0000060896 loss
2019-04-08 04:49:22,972 [INFO] unfolding 0, single step 84001
2019-04-08 04:49:22,973 [INFO] Sum of grad norms of most recent batch: 0.037993
2019-04-08 04:49:22,973 [INFO] ---------------------------------
2019-04-08 04:49:44,489 [INFO] ---------------------------------
2019-04-08 04:49:44,490 [INFO] Summary:
2019-04-08 04:49:44,490 [INFO] Batch 85000, worst loss 0.018591 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 04:49:44,491 [INFO] Regularization: 608.508972 * 0.0000000100 = 0.0000060851 loss
2019-04-08 04:49:44,491 [INFO] unfolding 0, single step 85001
2019-04-08 04:49:44,492 [INFO] Sum of grad norms of most recent batch: 0.040951
2019-04-08 04:49:44,492 [INFO] ---------------------------------
2019-04-08 04:50:06,025 [INFO] ---------------------------------
2019-04-08 04:50:06,026 [INFO] Summary:
2019-04-08 04:50:06,027 [INFO] Batch 86000, worst loss 0.013429 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 04:50:06,027 [INFO] Regularization: 608.014282 * 0.0000000100 = 0.0000060801 loss
2019-04-08 04:50:06,027 [INFO] unfolding 0, single step 86001
2019-04-08 04:50:06,028 [INFO] Sum of grad norms of most recent batch: 0.117801
2019-04-08 04:50:06,029 [INFO] ---------------------------------
2019-04-08 04:50:27,476 [INFO] ---------------------------------
2019-04-08 04:50:27,477 [INFO] Summary:
2019-04-08 04:50:27,477 [INFO] Batch 87000, worst loss 0.054975 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 04:50:27,478 [INFO] Regularization: 607.904602 * 0.0000000100 = 0.0000060790 loss
2019-04-08 04:50:27,478 [INFO] unfolding 0, single step 87001
2019-04-08 04:50:27,479 [INFO] Sum of grad norms of most recent batch: 0.105874
2019-04-08 04:50:27,479 [INFO] ---------------------------------
2019-04-08 04:50:48,476 [INFO] ---------------------------------
2019-04-08 04:50:48,477 [INFO] Summary:
2019-04-08 04:50:48,478 [INFO] Batch 88000, worst loss 0.078996 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 04:50:48,478 [INFO] Regularization: 608.065918 * 0.0000000100 = 0.0000060807 loss
2019-04-08 04:50:48,478 [INFO] unfolding 0, single step 88001
2019-04-08 04:50:48,479 [INFO] Sum of grad norms of most recent batch: 0.029841
2019-04-08 04:50:48,479 [INFO] ---------------------------------
2019-04-08 04:51:08,960 [INFO] ---------------------------------
2019-04-08 04:51:08,961 [INFO] Summary:
2019-04-08 04:51:08,961 [INFO] Batch 89000, worst loss 0.032948 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 04:51:08,962 [INFO] Regularization: 607.522827 * 0.0000000100 = 0.0000060752 loss
2019-04-08 04:51:08,962 [INFO] unfolding 0, single step 89001
2019-04-08 04:51:08,963 [INFO] Sum of grad norms of most recent batch: 0.017266
2019-04-08 04:51:08,963 [INFO] ---------------------------------
2019-04-08 04:51:30,141 [INFO] ---------------------------------
2019-04-08 04:51:30,142 [INFO] Summary:
2019-04-08 04:51:30,142 [INFO] Batch 90000, worst loss 0.035483 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000016 @est.-depth 3
2019-04-08 04:51:30,143 [INFO] Regularization: 607.780457 * 0.0000000100 = 0.0000060778 loss
2019-04-08 04:51:30,143 [INFO] unfolding 0, single step 90001
2019-04-08 04:51:30,144 [INFO] Sum of grad norms of most recent batch: 0.052994
2019-04-08 04:51:30,144 [INFO] ---------------------------------
2019-04-08 04:52:07,672 [INFO] ---------------------------------
2019-04-08 04:52:07,673 [INFO] Evaluation:
2019-04-08 04:52:07,674 [INFO] Batch 90000, worst loss 0.080786 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:52:07,674 [INFO] ---------------------------------
2019-04-08 04:52:28,862 [INFO] ---------------------------------
2019-04-08 04:52:28,863 [INFO] Summary:
2019-04-08 04:52:28,864 [INFO] Batch 91000, worst loss 0.021849 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 04:52:28,864 [INFO] Regularization: 607.019592 * 0.0000000100 = 0.0000060702 loss
2019-04-08 04:52:28,864 [INFO] unfolding 0, single step 91001
2019-04-08 04:52:28,865 [INFO] Sum of grad norms of most recent batch: 0.072965
2019-04-08 04:52:28,866 [INFO] ---------------------------------
2019-04-08 04:52:50,117 [INFO] ---------------------------------
2019-04-08 04:52:50,118 [INFO] Summary:
2019-04-08 04:52:50,119 [INFO] Batch 92000, worst loss 0.012549 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 04:52:50,120 [INFO] Regularization: 606.735046 * 0.0000000100 = 0.0000060674 loss
2019-04-08 04:52:50,120 [INFO] unfolding 0, single step 92001
2019-04-08 04:52:50,121 [INFO] Sum of grad norms of most recent batch: 0.021623
2019-04-08 04:52:50,121 [INFO] ---------------------------------
2019-04-08 04:53:12,261 [INFO] ---------------------------------
2019-04-08 04:53:12,262 [INFO] Summary:
2019-04-08 04:53:12,263 [INFO] Batch 93000, worst loss 0.041647 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 04:53:12,264 [INFO] Regularization: 606.458374 * 0.0000000100 = 0.0000060646 loss
2019-04-08 04:53:12,265 [INFO] unfolding 0, single step 93001
2019-04-08 04:53:12,266 [INFO] Sum of grad norms of most recent batch: 0.035176
2019-04-08 04:53:12,267 [INFO] ---------------------------------
2019-04-08 04:53:32,939 [INFO] ---------------------------------
2019-04-08 04:53:32,940 [INFO] Summary:
2019-04-08 04:53:32,940 [INFO] Batch 94000, worst loss 0.008889 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 04:53:32,941 [INFO] Regularization: 606.068237 * 0.0000000100 = 0.0000060607 loss
2019-04-08 04:53:32,941 [INFO] unfolding 0, single step 94001
2019-04-08 04:53:32,942 [INFO] Sum of grad norms of most recent batch: 0.062031
2019-04-08 04:53:32,942 [INFO] ---------------------------------
2019-04-08 04:53:53,554 [INFO] ---------------------------------
2019-04-08 04:53:53,555 [INFO] Summary:
2019-04-08 04:53:53,555 [INFO] Batch 95000, worst loss 0.016772 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 04:53:53,556 [INFO] Regularization: 606.026855 * 0.0000000100 = 0.0000060603 loss
2019-04-08 04:53:53,556 [INFO] unfolding 0, single step 95001
2019-04-08 04:53:53,557 [INFO] Sum of grad norms of most recent batch: 0.031665
2019-04-08 04:53:53,557 [INFO] ---------------------------------
2019-04-08 04:54:14,748 [INFO] ---------------------------------
2019-04-08 04:54:14,749 [INFO] Summary:
2019-04-08 04:54:14,749 [INFO] Batch 96000, worst loss 0.017420 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 04:54:14,750 [INFO] Regularization: 605.682678 * 0.0000000100 = 0.0000060568 loss
2019-04-08 04:54:14,750 [INFO] unfolding 0, single step 96001
2019-04-08 04:54:14,751 [INFO] Sum of grad norms of most recent batch: 0.026994
2019-04-08 04:54:14,752 [INFO] ---------------------------------
2019-04-08 04:54:36,521 [INFO] ---------------------------------
2019-04-08 04:54:36,522 [INFO] Summary:
2019-04-08 04:54:36,523 [INFO] Batch 97000, worst loss 0.041567 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 04:54:36,524 [INFO] Regularization: 605.749817 * 0.0000000100 = 0.0000060575 loss
2019-04-08 04:54:36,524 [INFO] unfolding 0, single step 97001
2019-04-08 04:54:36,526 [INFO] Sum of grad norms of most recent batch: 0.038285
2019-04-08 04:54:36,526 [INFO] ---------------------------------
2019-04-08 04:54:57,720 [INFO] ---------------------------------
2019-04-08 04:54:57,721 [INFO] Summary:
2019-04-08 04:54:57,722 [INFO] Batch 98000, worst loss 0.013787 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 04:54:57,723 [INFO] Regularization: 605.567505 * 0.0000000100 = 0.0000060557 loss
2019-04-08 04:54:57,723 [INFO] unfolding 0, single step 98001
2019-04-08 04:54:57,724 [INFO] Sum of grad norms of most recent batch: 1.317540
2019-04-08 04:54:57,725 [INFO] ---------------------------------
2019-04-08 04:55:19,566 [INFO] ---------------------------------
2019-04-08 04:55:19,567 [INFO] Summary:
2019-04-08 04:55:19,568 [INFO] Batch 99000, worst loss 0.027161 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 04:55:19,568 [INFO] Regularization: 605.402710 * 0.0000000100 = 0.0000060540 loss
2019-04-08 04:55:19,569 [INFO] unfolding 0, single step 99001
2019-04-08 04:55:19,569 [INFO] Sum of grad norms of most recent batch: 0.030163
2019-04-08 04:55:19,570 [INFO] ---------------------------------
2019-04-08 04:55:40,526 [INFO] ---------------------------------
2019-04-08 04:55:40,527 [INFO] Summary:
2019-04-08 04:55:40,528 [INFO] Batch 100000, worst loss 0.038457 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000008 @est.-depth 3
2019-04-08 04:55:40,529 [INFO] Regularization: 605.351074 * 0.0000000100 = 0.0000060535 loss
2019-04-08 04:55:40,529 [INFO] unfolding 0, single step 100001
2019-04-08 04:55:40,530 [INFO] Sum of grad norms of most recent batch: 0.049144
2019-04-08 04:55:40,531 [INFO] ---------------------------------
2019-04-08 04:56:17,932 [INFO] ---------------------------------
2019-04-08 04:56:17,933 [INFO] Evaluation:
2019-04-08 04:56:17,933 [INFO] Batch 100000, worst loss 0.042340 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 04:56:17,934 [INFO] ---------------------------------
2019-04-08 04:56:39,708 [INFO] ---------------------------------
2019-04-08 04:56:39,709 [INFO] Summary:
2019-04-08 04:56:39,709 [INFO] Batch 101000, worst loss 0.014353 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 04:56:39,710 [INFO] Regularization: 605.129395 * 0.0000000100 = 0.0000060513 loss
2019-04-08 04:56:39,710 [INFO] unfolding 0, single step 101001
2019-04-08 04:56:39,711 [INFO] Sum of grad norms of most recent batch: 0.043170
2019-04-08 04:56:39,711 [INFO] ---------------------------------
2019-04-08 04:57:01,542 [INFO] ---------------------------------
2019-04-08 04:57:01,543 [INFO] Summary:
2019-04-08 04:57:01,543 [INFO] Batch 102000, worst loss 0.013272 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 04:57:01,544 [INFO] Regularization: 605.088440 * 0.0000000100 = 0.0000060509 loss
2019-04-08 04:57:01,544 [INFO] unfolding 0, single step 102001
2019-04-08 04:57:01,545 [INFO] Sum of grad norms of most recent batch: 0.032114
2019-04-08 04:57:01,546 [INFO] ---------------------------------
2019-04-08 04:57:23,558 [INFO] ---------------------------------
2019-04-08 04:57:23,559 [INFO] Summary:
2019-04-08 04:57:23,559 [INFO] Batch 103000, worst loss 0.016920 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 04:57:23,560 [INFO] Regularization: 604.937195 * 0.0000000100 = 0.0000060494 loss
2019-04-08 04:57:23,560 [INFO] unfolding 0, single step 103001
2019-04-08 04:57:23,561 [INFO] Sum of grad norms of most recent batch: 0.021665
2019-04-08 04:57:23,562 [INFO] ---------------------------------
2019-04-08 04:57:44,702 [INFO] ---------------------------------
2019-04-08 04:57:44,703 [INFO] Summary:
2019-04-08 04:57:44,703 [INFO] Batch 104000, worst loss 0.061085 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 04:57:44,704 [INFO] Regularization: 604.858826 * 0.0000000100 = 0.0000060486 loss
2019-04-08 04:57:44,704 [INFO] unfolding 0, single step 104001
2019-04-08 04:57:44,705 [INFO] Sum of grad norms of most recent batch: 0.055800
2019-04-08 04:57:44,705 [INFO] ---------------------------------
2019-04-08 04:58:06,445 [INFO] ---------------------------------
2019-04-08 04:58:06,446 [INFO] Summary:
2019-04-08 04:58:06,447 [INFO] Batch 105000, worst loss 0.023478 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 04:58:06,448 [INFO] Regularization: 604.801208 * 0.0000000100 = 0.0000060480 loss
2019-04-08 04:58:06,448 [INFO] unfolding 0, single step 105001
2019-04-08 04:58:06,449 [INFO] Sum of grad norms of most recent batch: 0.039944
2019-04-08 04:58:06,450 [INFO] ---------------------------------
2019-04-08 04:58:27,831 [INFO] ---------------------------------
2019-04-08 04:58:27,832 [INFO] Summary:
2019-04-08 04:58:27,833 [INFO] Batch 106000, worst loss 0.055515 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 04:58:27,833 [INFO] Regularization: 604.700500 * 0.0000000100 = 0.0000060470 loss
2019-04-08 04:58:27,833 [INFO] unfolding 0, single step 106001
2019-04-08 04:58:27,834 [INFO] Sum of grad norms of most recent batch: 0.095163
2019-04-08 04:58:27,834 [INFO] ---------------------------------
2019-04-08 04:58:48,533 [INFO] ---------------------------------
2019-04-08 04:58:48,534 [INFO] Summary:
2019-04-08 04:58:48,535 [INFO] Batch 107000, worst loss 0.041486 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 04:58:48,535 [INFO] Regularization: 604.555542 * 0.0000000100 = 0.0000060456 loss
2019-04-08 04:58:48,535 [INFO] unfolding 0, single step 107001
2019-04-08 04:58:48,536 [INFO] Sum of grad norms of most recent batch: 0.108402
2019-04-08 04:58:48,537 [INFO] ---------------------------------
2019-04-08 04:59:10,085 [INFO] ---------------------------------
2019-04-08 04:59:10,087 [INFO] Summary:
2019-04-08 04:59:10,087 [INFO] Batch 108000, worst loss 0.023705 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 04:59:10,088 [INFO] Regularization: 604.564697 * 0.0000000100 = 0.0000060456 loss
2019-04-08 04:59:10,088 [INFO] unfolding 0, single step 108001
2019-04-08 04:59:10,088 [INFO] Sum of grad norms of most recent batch: 0.058916
2019-04-08 04:59:10,089 [INFO] ---------------------------------
2019-04-08 04:59:31,547 [INFO] ---------------------------------
2019-04-08 04:59:31,548 [INFO] Summary:
2019-04-08 04:59:31,549 [INFO] Batch 109000, worst loss 0.051446 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 04:59:31,549 [INFO] Regularization: 604.405212 * 0.0000000100 = 0.0000060441 loss
2019-04-08 04:59:31,549 [INFO] unfolding 0, single step 109001
2019-04-08 04:59:31,550 [INFO] Sum of grad norms of most recent batch: 0.011777
2019-04-08 04:59:31,551 [INFO] ---------------------------------
2019-04-08 04:59:52,809 [INFO] ---------------------------------
2019-04-08 04:59:52,810 [INFO] Summary:
2019-04-08 04:59:52,811 [INFO] Batch 110000, worst loss 0.017754 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000004 @est.-depth 3
2019-04-08 04:59:52,811 [INFO] Regularization: 604.347961 * 0.0000000100 = 0.0000060435 loss
2019-04-08 04:59:52,811 [INFO] unfolding 0, single step 110001
2019-04-08 04:59:52,812 [INFO] Sum of grad norms of most recent batch: 0.079840
2019-04-08 04:59:52,812 [INFO] ---------------------------------
2019-04-08 05:00:30,219 [INFO] ---------------------------------
2019-04-08 05:00:30,220 [INFO] Evaluation:
2019-04-08 05:00:30,221 [INFO] Batch 110000, worst loss 0.063487 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 05:00:30,222 [INFO] ---------------------------------
2019-04-08 05:00:52,182 [INFO] ---------------------------------
2019-04-08 05:00:52,184 [INFO] Summary:
2019-04-08 05:00:52,185 [INFO] Batch 111000, worst loss 0.045978 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 05:00:52,186 [INFO] Regularization: 604.244446 * 0.0000000100 = 0.0000060424 loss
2019-04-08 05:00:52,186 [INFO] unfolding 0, single step 111001
2019-04-08 05:00:52,187 [INFO] Sum of grad norms of most recent batch: 0.046012
2019-04-08 05:00:52,188 [INFO] ---------------------------------
2019-04-08 05:01:14,075 [INFO] ---------------------------------
2019-04-08 05:01:14,076 [INFO] Summary:
2019-04-08 05:01:14,076 [INFO] Batch 112000, worst loss 0.027501 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 05:01:14,077 [INFO] Regularization: 604.113892 * 0.0000000100 = 0.0000060411 loss
2019-04-08 05:01:14,077 [INFO] unfolding 0, single step 112001
2019-04-08 05:01:14,078 [INFO] Sum of grad norms of most recent batch: 0.025504
2019-04-08 05:01:14,078 [INFO] ---------------------------------
2019-04-08 05:01:35,539 [INFO] ---------------------------------
2019-04-08 05:01:35,540 [INFO] Summary:
2019-04-08 05:01:35,541 [INFO] Batch 113000, worst loss 0.020261 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 05:01:35,541 [INFO] Regularization: 604.100159 * 0.0000000100 = 0.0000060410 loss
2019-04-08 05:01:35,541 [INFO] unfolding 0, single step 113001
2019-04-08 05:01:35,542 [INFO] Sum of grad norms of most recent batch: 0.012360
2019-04-08 05:01:35,543 [INFO] ---------------------------------
2019-04-08 05:01:56,773 [INFO] ---------------------------------
2019-04-08 05:01:56,774 [INFO] Summary:
2019-04-08 05:01:56,775 [INFO] Batch 114000, worst loss 0.031612 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 05:01:56,775 [INFO] Regularization: 603.994995 * 0.0000000100 = 0.0000060399 loss
2019-04-08 05:01:56,776 [INFO] unfolding 0, single step 114001
2019-04-08 05:01:56,776 [INFO] Sum of grad norms of most recent batch: 0.298531
2019-04-08 05:01:56,777 [INFO] ---------------------------------
2019-04-08 05:02:17,850 [INFO] ---------------------------------
2019-04-08 05:02:17,852 [INFO] Summary:
2019-04-08 05:02:17,852 [INFO] Batch 115000, worst loss 0.015562 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 05:02:17,853 [INFO] Regularization: 603.878479 * 0.0000000100 = 0.0000060388 loss
2019-04-08 05:02:17,853 [INFO] unfolding 0, single step 115001
2019-04-08 05:02:17,854 [INFO] Sum of grad norms of most recent batch: 0.009067
2019-04-08 05:02:17,854 [INFO] ---------------------------------
2019-04-08 05:02:39,405 [INFO] ---------------------------------
2019-04-08 05:02:39,406 [INFO] Summary:
2019-04-08 05:02:39,407 [INFO] Batch 116000, worst loss 0.019416 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 05:02:39,407 [INFO] Regularization: 603.837769 * 0.0000000100 = 0.0000060384 loss
2019-04-08 05:02:39,408 [INFO] unfolding 0, single step 116001
2019-04-08 05:02:39,408 [INFO] Sum of grad norms of most recent batch: 0.038394
2019-04-08 05:02:39,409 [INFO] ---------------------------------
2019-04-08 05:03:00,641 [INFO] ---------------------------------
2019-04-08 05:03:00,642 [INFO] Summary:
2019-04-08 05:03:00,642 [INFO] Batch 117000, worst loss 0.038080 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 05:03:00,643 [INFO] Regularization: 603.816833 * 0.0000000100 = 0.0000060382 loss
2019-04-08 05:03:00,643 [INFO] unfolding 0, single step 117001
2019-04-08 05:03:00,644 [INFO] Sum of grad norms of most recent batch: 0.112102
2019-04-08 05:03:00,644 [INFO] ---------------------------------
2019-04-08 05:03:22,276 [INFO] ---------------------------------
2019-04-08 05:03:22,277 [INFO] Summary:
2019-04-08 05:03:22,278 [INFO] Batch 118000, worst loss 0.069061 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 05:03:22,278 [INFO] Regularization: 603.736633 * 0.0000000100 = 0.0000060374 loss
2019-04-08 05:03:22,279 [INFO] unfolding 0, single step 118001
2019-04-08 05:03:22,279 [INFO] Sum of grad norms of most recent batch: 12.639771
2019-04-08 05:03:22,280 [INFO] ---------------------------------
2019-04-08 05:03:43,016 [INFO] ---------------------------------
2019-04-08 05:03:43,017 [INFO] Summary:
2019-04-08 05:03:43,018 [INFO] Batch 119000, worst loss 0.066591 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 05:03:43,018 [INFO] Regularization: 603.765015 * 0.0000000100 = 0.0000060377 loss
2019-04-08 05:03:43,019 [INFO] unfolding 0, single step 119001
2019-04-08 05:03:43,019 [INFO] Sum of grad norms of most recent batch: 0.030646
2019-04-08 05:03:43,020 [INFO] ---------------------------------
2019-04-08 05:04:04,727 [INFO] ---------------------------------
2019-04-08 05:04:04,728 [INFO] Summary:
2019-04-08 05:04:04,729 [INFO] Batch 120000, worst loss 0.027891 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 3
2019-04-08 05:04:04,729 [INFO] Regularization: 603.613220 * 0.0000000100 = 0.0000060361 loss
2019-04-08 05:04:04,730 [INFO] unfolding 0, single step 120001
2019-04-08 05:04:04,731 [INFO] Sum of grad norms of most recent batch: 0.040976
2019-04-08 05:04:04,731 [INFO] ---------------------------------
2019-04-08 05:04:42,148 [INFO] ---------------------------------
2019-04-08 05:04:42,149 [INFO] Evaluation:
2019-04-08 05:04:42,150 [INFO] Batch 120000, worst loss 0.057408 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 05:04:42,150 [INFO] ---------------------------------
2019-04-08 05:05:03,500 [INFO] ---------------------------------
2019-04-08 05:05:03,501 [INFO] Summary:
2019-04-08 05:05:03,502 [INFO] Batch 121000, worst loss 0.015255 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 05:05:03,502 [INFO] Regularization: 603.545044 * 0.0000000100 = 0.0000060355 loss
2019-04-08 05:05:03,503 [INFO] unfolding 0, single step 121001
2019-04-08 05:05:03,504 [INFO] Sum of grad norms of most recent batch: 0.034139
2019-04-08 05:05:03,505 [INFO] ---------------------------------
2019-04-08 05:05:25,308 [INFO] ---------------------------------
2019-04-08 05:05:25,309 [INFO] Summary:
2019-04-08 05:05:25,309 [INFO] Batch 122000, worst loss 0.018223 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 05:05:25,310 [INFO] Regularization: 603.450562 * 0.0000000100 = 0.0000060345 loss
2019-04-08 05:05:25,310 [INFO] unfolding 0, single step 122001
2019-04-08 05:05:25,311 [INFO] Sum of grad norms of most recent batch: 2.637594
2019-04-08 05:05:25,311 [INFO] ---------------------------------
2019-04-08 05:05:46,192 [INFO] ---------------------------------
2019-04-08 05:05:46,193 [INFO] Summary:
2019-04-08 05:05:46,193 [INFO] Batch 123000, worst loss 0.019444 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 05:05:46,194 [INFO] Regularization: 603.391418 * 0.0000000100 = 0.0000060339 loss
2019-04-08 05:05:46,194 [INFO] unfolding 0, single step 123001
2019-04-08 05:05:46,195 [INFO] Sum of grad norms of most recent batch: 0.017405
2019-04-08 05:05:46,195 [INFO] ---------------------------------
2019-04-08 05:06:07,554 [INFO] ---------------------------------
2019-04-08 05:06:07,555 [INFO] Summary:
2019-04-08 05:06:07,556 [INFO] Batch 124000, worst loss 0.013012 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 05:06:07,557 [INFO] Regularization: 603.400513 * 0.0000000100 = 0.0000060340 loss
2019-04-08 05:06:07,558 [INFO] unfolding 0, single step 124001
2019-04-08 05:06:07,559 [INFO] Sum of grad norms of most recent batch: 0.078840
2019-04-08 05:06:07,560 [INFO] ---------------------------------
2019-04-08 05:06:28,987 [INFO] ---------------------------------
2019-04-08 05:06:28,988 [INFO] Summary:
2019-04-08 05:06:28,988 [INFO] Batch 125000, worst loss 0.018500 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 05:06:28,989 [INFO] Regularization: 603.400146 * 0.0000000100 = 0.0000060340 loss
2019-04-08 05:06:28,989 [INFO] unfolding 0, single step 125001
2019-04-08 05:06:28,990 [INFO] Sum of grad norms of most recent batch: 0.010791
2019-04-08 05:06:28,990 [INFO] ---------------------------------
2019-04-08 05:06:50,221 [INFO] ---------------------------------
2019-04-08 05:06:50,222 [INFO] Summary:
2019-04-08 05:06:50,222 [INFO] Batch 126000, worst loss 0.032779 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 05:06:50,223 [INFO] Regularization: 603.343262 * 0.0000000100 = 0.0000060334 loss
2019-04-08 05:06:50,223 [INFO] unfolding 0, single step 126001
2019-04-08 05:06:50,224 [INFO] Sum of grad norms of most recent batch: 0.096666
2019-04-08 05:06:50,225 [INFO] ---------------------------------
2019-04-08 05:07:11,230 [INFO] ---------------------------------
2019-04-08 05:07:11,232 [INFO] Summary:
2019-04-08 05:07:11,232 [INFO] Batch 127000, worst loss 0.032645 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 05:07:11,233 [INFO] Regularization: 603.318237 * 0.0000000100 = 0.0000060332 loss
2019-04-08 05:07:11,233 [INFO] unfolding 0, single step 127001
2019-04-08 05:07:11,233 [INFO] Sum of grad norms of most recent batch: 0.021749
2019-04-08 05:07:11,234 [INFO] ---------------------------------
2019-04-08 05:07:32,709 [INFO] ---------------------------------
2019-04-08 05:07:32,710 [INFO] Summary:
2019-04-08 05:07:32,711 [INFO] Batch 128000, worst loss 0.016870 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 05:07:32,712 [INFO] Regularization: 603.278259 * 0.0000000100 = 0.0000060328 loss
2019-04-08 05:07:32,712 [INFO] unfolding 0, single step 128001
2019-04-08 05:07:32,712 [INFO] Sum of grad norms of most recent batch: 0.051276
2019-04-08 05:07:32,713 [INFO] ---------------------------------
2019-04-08 05:07:53,791 [INFO] ---------------------------------
2019-04-08 05:07:53,792 [INFO] Summary:
2019-04-08 05:07:53,793 [INFO] Batch 129000, worst loss 0.044000 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 05:07:53,793 [INFO] Regularization: 603.269592 * 0.0000000100 = 0.0000060327 loss
2019-04-08 05:07:53,794 [INFO] unfolding 0, single step 129001
2019-04-08 05:07:53,795 [INFO] Sum of grad norms of most recent batch: 0.057733
2019-04-08 05:07:53,795 [INFO] ---------------------------------
2019-04-08 05:08:15,265 [INFO] ---------------------------------
2019-04-08 05:08:15,266 [INFO] Summary:
2019-04-08 05:08:15,266 [INFO] Batch 130000, worst loss 0.027356 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 3
2019-04-08 05:08:15,267 [INFO] Regularization: 603.263428 * 0.0000000100 = 0.0000060326 loss
2019-04-08 05:08:15,267 [INFO] unfolding 0, single step 130001
2019-04-08 05:08:15,268 [INFO] Sum of grad norms of most recent batch: 0.032839
2019-04-08 05:08:15,269 [INFO] ---------------------------------
2019-04-08 05:08:52,883 [INFO] ---------------------------------
2019-04-08 05:08:52,884 [INFO] Evaluation:
2019-04-08 05:08:52,884 [INFO] Batch 130000, worst loss 0.055828 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 05:08:52,885 [INFO] ---------------------------------
2019-04-08 05:09:13,975 [INFO] ---------------------------------
2019-04-08 05:09:13,976 [INFO] Summary:
2019-04-08 05:09:13,977 [INFO] Batch 131000, worst loss 0.053457 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:09:13,978 [INFO] Regularization: 603.270386 * 0.0000000100 = 0.0000060327 loss
2019-04-08 05:09:13,978 [INFO] unfolding 0, single step 131001
2019-04-08 05:09:13,979 [INFO] Sum of grad norms of most recent batch: 0.024043
2019-04-08 05:09:13,979 [INFO] ---------------------------------
2019-04-08 05:09:35,412 [INFO] ---------------------------------
2019-04-08 05:09:35,413 [INFO] Summary:
2019-04-08 05:09:35,414 [INFO] Batch 132000, worst loss 0.052659 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:09:35,415 [INFO] Regularization: 603.242981 * 0.0000000100 = 0.0000060324 loss
2019-04-08 05:09:35,415 [INFO] unfolding 0, single step 132001
2019-04-08 05:09:35,416 [INFO] Sum of grad norms of most recent batch: 0.011629
2019-04-08 05:09:35,417 [INFO] ---------------------------------
2019-04-08 05:09:56,876 [INFO] ---------------------------------
2019-04-08 05:09:56,877 [INFO] Summary:
2019-04-08 05:09:56,877 [INFO] Batch 133000, worst loss 0.015834 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:09:56,878 [INFO] Regularization: 603.205017 * 0.0000000100 = 0.0000060321 loss
2019-04-08 05:09:56,878 [INFO] unfolding 0, single step 133001
2019-04-08 05:09:56,879 [INFO] Sum of grad norms of most recent batch: 0.034764
2019-04-08 05:09:56,879 [INFO] ---------------------------------
2019-04-08 05:10:18,303 [INFO] ---------------------------------
2019-04-08 05:10:18,304 [INFO] Summary:
2019-04-08 05:10:18,305 [INFO] Batch 134000, worst loss 0.018274 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:10:18,305 [INFO] Regularization: 603.175537 * 0.0000000100 = 0.0000060318 loss
2019-04-08 05:10:18,305 [INFO] unfolding 0, single step 134001
2019-04-08 05:10:18,306 [INFO] Sum of grad norms of most recent batch: 2.299631
2019-04-08 05:10:18,307 [INFO] ---------------------------------
2019-04-08 05:10:39,245 [INFO] ---------------------------------
2019-04-08 05:10:39,246 [INFO] Summary:
2019-04-08 05:10:39,246 [INFO] Batch 135000, worst loss 0.028949 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:10:39,247 [INFO] Regularization: 603.141052 * 0.0000000100 = 0.0000060314 loss
2019-04-08 05:10:39,247 [INFO] unfolding 0, single step 135001
2019-04-08 05:10:39,248 [INFO] Sum of grad norms of most recent batch: 0.040687
2019-04-08 05:10:39,248 [INFO] ---------------------------------
2019-04-08 05:11:00,482 [INFO] ---------------------------------
2019-04-08 05:11:00,483 [INFO] Summary:
2019-04-08 05:11:00,483 [INFO] Batch 136000, worst loss 0.039109 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:11:00,484 [INFO] Regularization: 603.126160 * 0.0000000100 = 0.0000060313 loss
2019-04-08 05:11:00,484 [INFO] unfolding 0, single step 136001
2019-04-08 05:11:00,485 [INFO] Sum of grad norms of most recent batch: 0.022644
2019-04-08 05:11:00,485 [INFO] ---------------------------------
2019-04-08 05:11:21,978 [INFO] ---------------------------------
2019-04-08 05:11:21,979 [INFO] Summary:
2019-04-08 05:11:21,979 [INFO] Batch 137000, worst loss 0.023051 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:11:21,980 [INFO] Regularization: 603.099121 * 0.0000000100 = 0.0000060310 loss
2019-04-08 05:11:21,980 [INFO] unfolding 0, single step 137001
2019-04-08 05:11:21,981 [INFO] Sum of grad norms of most recent batch: 0.017510
2019-04-08 05:11:21,981 [INFO] ---------------------------------
2019-04-08 05:11:43,372 [INFO] ---------------------------------
2019-04-08 05:11:43,373 [INFO] Summary:
2019-04-08 05:11:43,374 [INFO] Batch 138000, worst loss 0.005675 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:11:43,374 [INFO] Regularization: 603.079163 * 0.0000000100 = 0.0000060308 loss
2019-04-08 05:11:43,375 [INFO] unfolding 0, single step 138001
2019-04-08 05:11:43,375 [INFO] Sum of grad norms of most recent batch: 0.018455
2019-04-08 05:11:43,376 [INFO] ---------------------------------
2019-04-08 05:12:04,543 [INFO] ---------------------------------
2019-04-08 05:12:04,544 [INFO] Summary:
2019-04-08 05:12:04,544 [INFO] Batch 139000, worst loss 0.033024 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:12:04,545 [INFO] Regularization: 603.039429 * 0.0000000100 = 0.0000060304 loss
2019-04-08 05:12:04,545 [INFO] unfolding 0, single step 139001
2019-04-08 05:12:04,546 [INFO] Sum of grad norms of most recent batch: 0.041315
2019-04-08 05:12:04,546 [INFO] ---------------------------------
2019-04-08 05:12:26,209 [INFO] ---------------------------------
2019-04-08 05:12:26,211 [INFO] Summary:
2019-04-08 05:12:26,211 [INFO] Batch 140000, worst loss 0.008596 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:12:26,212 [INFO] Regularization: 603.029358 * 0.0000000100 = 0.0000060303 loss
2019-04-08 05:12:26,212 [INFO] unfolding 0, single step 140001
2019-04-08 05:12:26,213 [INFO] Sum of grad norms of most recent batch: 3.716825
2019-04-08 05:12:26,213 [INFO] ---------------------------------
2019-04-08 05:13:03,603 [INFO] ---------------------------------
2019-04-08 05:13:03,604 [INFO] Evaluation:
2019-04-08 05:13:03,605 [INFO] Batch 140000, worst loss 0.045184 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 05:13:03,606 [INFO] ---------------------------------
2019-04-08 05:13:24,637 [INFO] ---------------------------------
2019-04-08 05:13:24,638 [INFO] Summary:
2019-04-08 05:13:24,638 [INFO] Batch 141000, worst loss 0.035803 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:13:24,639 [INFO] Regularization: 603.021484 * 0.0000000100 = 0.0000060302 loss
2019-04-08 05:13:24,639 [INFO] unfolding 0, single step 141001
2019-04-08 05:13:24,640 [INFO] Sum of grad norms of most recent batch: 6.132377
2019-04-08 05:13:24,640 [INFO] ---------------------------------
2019-04-08 05:13:46,305 [INFO] ---------------------------------
2019-04-08 05:13:46,306 [INFO] Summary:
2019-04-08 05:13:46,306 [INFO] Batch 142000, worst loss 0.017976 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:13:46,307 [INFO] Regularization: 603.005005 * 0.0000000100 = 0.0000060300 loss
2019-04-08 05:13:46,308 [INFO] unfolding 0, single step 142001
2019-04-08 05:13:46,308 [INFO] Sum of grad norms of most recent batch: 0.008420
2019-04-08 05:13:46,309 [INFO] ---------------------------------
2019-04-08 05:14:06,889 [INFO] ---------------------------------
2019-04-08 05:14:06,890 [INFO] Summary:
2019-04-08 05:14:06,891 [INFO] Batch 143000, worst loss 0.021821 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:14:06,891 [INFO] Regularization: 602.993286 * 0.0000000100 = 0.0000060299 loss
2019-04-08 05:14:06,891 [INFO] unfolding 0, single step 143001
2019-04-08 05:14:06,892 [INFO] Sum of grad norms of most recent batch: 0.016442
2019-04-08 05:14:06,892 [INFO] ---------------------------------
2019-04-08 05:14:28,050 [INFO] ---------------------------------
2019-04-08 05:14:28,050 [INFO] Summary:
2019-04-08 05:14:28,051 [INFO] Batch 144000, worst loss 0.036450 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:14:28,053 [INFO] Regularization: 602.996643 * 0.0000000100 = 0.0000060300 loss
2019-04-08 05:14:28,053 [INFO] unfolding 0, single step 144001
2019-04-08 05:14:28,055 [INFO] Sum of grad norms of most recent batch: 0.013053
2019-04-08 05:14:28,055 [INFO] ---------------------------------
2019-04-08 05:14:49,221 [INFO] ---------------------------------
2019-04-08 05:14:49,223 [INFO] Summary:
2019-04-08 05:14:49,223 [INFO] Batch 145000, worst loss 0.036215 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:14:49,224 [INFO] Regularization: 602.982117 * 0.0000000100 = 0.0000060298 loss
2019-04-08 05:14:49,224 [INFO] unfolding 0, single step 145001
2019-04-08 05:14:49,225 [INFO] Sum of grad norms of most recent batch: 0.037377
2019-04-08 05:14:49,225 [INFO] ---------------------------------
2019-04-08 05:15:10,497 [INFO] ---------------------------------
2019-04-08 05:15:10,498 [INFO] Summary:
2019-04-08 05:15:10,498 [INFO] Batch 146000, worst loss 0.048088 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:15:10,499 [INFO] Regularization: 602.975769 * 0.0000000100 = 0.0000060298 loss
2019-04-08 05:15:10,499 [INFO] unfolding 0, single step 146001
2019-04-08 05:15:10,500 [INFO] Sum of grad norms of most recent batch: 0.066949
2019-04-08 05:15:10,500 [INFO] ---------------------------------
2019-04-08 05:15:31,724 [INFO] ---------------------------------
2019-04-08 05:15:31,725 [INFO] Summary:
2019-04-08 05:15:31,726 [INFO] Batch 147000, worst loss 0.061255 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:15:31,727 [INFO] Regularization: 602.964722 * 0.0000000100 = 0.0000060296 loss
2019-04-08 05:15:31,727 [INFO] unfolding 0, single step 147001
2019-04-08 05:15:31,728 [INFO] Sum of grad norms of most recent batch: 0.031502
2019-04-08 05:15:31,728 [INFO] ---------------------------------
2019-04-08 05:15:52,937 [INFO] ---------------------------------
2019-04-08 05:15:52,938 [INFO] Summary:
2019-04-08 05:15:52,939 [INFO] Batch 148000, worst loss 0.047555 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:15:52,939 [INFO] Regularization: 602.952637 * 0.0000000100 = 0.0000060295 loss
2019-04-08 05:15:52,939 [INFO] unfolding 0, single step 148001
2019-04-08 05:15:52,940 [INFO] Sum of grad norms of most recent batch: 0.046724
2019-04-08 05:15:52,940 [INFO] ---------------------------------
2019-04-08 05:16:14,385 [INFO] ---------------------------------
2019-04-08 05:16:14,386 [INFO] Summary:
2019-04-08 05:16:14,386 [INFO] Batch 149000, worst loss 0.075677 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:16:14,387 [INFO] Regularization: 602.946777 * 0.0000000100 = 0.0000060295 loss
2019-04-08 05:16:14,387 [INFO] unfolding 0, single step 149001
2019-04-08 05:16:14,388 [INFO] Sum of grad norms of most recent batch: 0.018087
2019-04-08 05:16:14,388 [INFO] ---------------------------------
2019-04-08 05:16:35,724 [INFO] ---------------------------------
2019-04-08 05:16:35,725 [INFO] Summary:
2019-04-08 05:16:35,725 [INFO] Batch 150000, worst loss 0.075614 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 3
2019-04-08 05:16:35,726 [INFO] Regularization: 602.949829 * 0.0000000100 = 0.0000060295 loss
2019-04-08 05:16:35,726 [INFO] unfolding 0, single step 150001
2019-04-08 05:16:35,727 [INFO] Sum of grad norms of most recent batch: 0.030455
2019-04-08 05:16:35,727 [INFO] ---------------------------------
2019-04-08 05:17:13,197 [INFO] ---------------------------------
2019-04-08 05:17:13,198 [INFO] Evaluation:
2019-04-08 05:17:13,199 [INFO] Batch 150000, worst loss 0.087355 of 10000 batches (without reg.) @est.-depth 3
2019-04-08 05:17:13,199 [INFO] ---------------------------------
2019-04-08 05:17:13,199 [INFO] Finished training, saved to file transition/1554653688/1554693433_9_transition_final.pth
