2019-03-22 18:20:01,182 [INFO] batch_size: 32
2019-03-22 18:20:01,183 [INFO] learning_rate_initialization: 0.001000, learning_rate_loss_factor: 0.010000, learning_rate_decay_after: 30000, learning_rate_decay_at: 10000, learning_rate_decay_factor: 0.250000
2019-03-22 18:20:01,184 [INFO] regularization factor: 0.0001000000
2019-03-22 18:20:01,457 [INFO] ---------------------------------
2019-03-22 18:20:01,459 [INFO] Training model #0: (8, 64, 2) @ 2
2019-03-22 18:20:16,605 [INFO] ---------------------------------
2019-03-22 18:20:16,606 [INFO] Summary:
2019-03-22 18:20:16,606 [INFO] Batch 1000, worst loss 1038.126587 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:20:16,607 [INFO] Regularization: 12295.109375 * 0.0001000000 = 1.2295109034
2019-03-22 18:20:16,608 [INFO] Sum of grad norms: 1263.995850
2019-03-22 18:20:16,608 [INFO] ---------------------------------
2019-03-22 18:20:31,786 [INFO] ---------------------------------
2019-03-22 18:20:31,787 [INFO] Summary:
2019-03-22 18:20:31,787 [INFO] Batch 2000, worst loss 17.734251 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:20:31,788 [INFO] Regularization: 8068.279785 * 0.0001000000 = 0.8068279624
2019-03-22 18:20:31,788 [INFO] Sum of grad norms: 413.194794
2019-03-22 18:20:31,789 [INFO] ---------------------------------
2019-03-22 18:20:46,762 [INFO] ---------------------------------
2019-03-22 18:20:46,763 [INFO] Summary:
2019-03-22 18:20:46,763 [INFO] Batch 3000, worst loss 16.580572 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:20:46,764 [INFO] Regularization: 7831.409180 * 0.0001000000 = 0.7831408978
2019-03-22 18:20:46,765 [INFO] Sum of grad norms: 131.172348
2019-03-22 18:20:46,765 [INFO] ---------------------------------
2019-03-22 18:21:01,684 [INFO] ---------------------------------
2019-03-22 18:21:01,685 [INFO] Summary:
2019-03-22 18:21:01,686 [INFO] Batch 4000, worst loss 5.841061 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:21:01,687 [INFO] Regularization: 7576.026367 * 0.0001000000 = 0.7576026320
2019-03-22 18:21:01,688 [INFO] reducing reg_loss_factor
2019-03-22 18:21:01,689 [INFO] Sum of grad norms: 206.885696
2019-03-22 18:21:01,690 [INFO] ---------------------------------
2019-03-22 18:21:16,493 [INFO] ---------------------------------
2019-03-22 18:21:16,494 [INFO] Summary:
2019-03-22 18:21:16,495 [INFO] Batch 5000, worst loss 3.205649 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:21:16,495 [INFO] Regularization: 7530.907715 * 0.0000100000 = 0.0753090754
2019-03-22 18:21:16,496 [INFO] Sum of grad norms: 88.338226
2019-03-22 18:21:16,497 [INFO] ---------------------------------
2019-03-22 18:21:31,501 [INFO] ---------------------------------
2019-03-22 18:21:31,502 [INFO] Summary:
2019-03-22 18:21:31,502 [INFO] Batch 6000, worst loss 2.329482 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:21:31,504 [INFO] Regularization: 7452.419922 * 0.0000100000 = 0.0745241940
2019-03-22 18:21:31,505 [INFO] Sum of grad norms: 16.432558
2019-03-22 18:21:31,506 [INFO] ---------------------------------
2019-03-22 18:21:46,107 [INFO] ---------------------------------
2019-03-22 18:21:46,108 [INFO] Summary:
2019-03-22 18:21:46,109 [INFO] Batch 7000, worst loss 1.401968 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:21:46,109 [INFO] Regularization: 7351.555664 * 0.0000100000 = 0.0735155568
2019-03-22 18:21:46,110 [INFO] Sum of grad norms: 16.109528
2019-03-22 18:21:46,111 [INFO] ---------------------------------
2019-03-22 18:22:00,925 [INFO] ---------------------------------
2019-03-22 18:22:00,926 [INFO] Summary:
2019-03-22 18:22:00,927 [INFO] Batch 8000, worst loss 1.181437 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:22:00,927 [INFO] Regularization: 7151.779297 * 0.0000100000 = 0.0715177879
2019-03-22 18:22:00,928 [INFO] Sum of grad norms: 21.288990
2019-03-22 18:22:00,928 [INFO] ---------------------------------
2019-03-22 18:22:16,085 [INFO] ---------------------------------
2019-03-22 18:22:16,086 [INFO] Summary:
2019-03-22 18:22:16,087 [INFO] Batch 9000, worst loss 0.746734 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:22:16,088 [INFO] Regularization: 6810.059082 * 0.0000100000 = 0.0681005865
2019-03-22 18:22:16,088 [INFO] Sum of grad norms: 40.016666
2019-03-22 18:22:16,089 [INFO] ---------------------------------
2019-03-22 18:22:30,176 [INFO] ---------------------------------
2019-03-22 18:22:30,177 [INFO] Summary:
2019-03-22 18:22:30,177 [INFO] Batch 10000, worst loss 0.591483 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:22:30,178 [INFO] Regularization: 6427.138672 * 0.0000100000 = 0.0642713830
2019-03-22 18:22:30,178 [INFO] reducing reg_loss_factor
2019-03-22 18:22:30,179 [INFO] Sum of grad norms: 9.280536
2019-03-22 18:22:30,179 [INFO] ---------------------------------
2019-03-22 18:22:32,936 [INFO] ---------------------------------
2019-03-22 18:22:32,937 [INFO] Evaluation:
2019-03-22 18:22:32,938 [INFO] Batch 10000, worst loss 0.314063 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:22:32,938 [INFO] ---------------------------------
2019-03-22 18:22:47,689 [INFO] ---------------------------------
2019-03-22 18:22:47,690 [INFO] Summary:
2019-03-22 18:22:47,690 [INFO] Batch 11000, worst loss 0.585123 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:22:47,691 [INFO] Regularization: 6383.917969 * 0.0000010000 = 0.0063839178
2019-03-22 18:22:47,691 [INFO] Sum of grad norms: 8.072324
2019-03-22 18:22:47,692 [INFO] ---------------------------------
2019-03-22 18:23:01,990 [INFO] ---------------------------------
2019-03-22 18:23:01,991 [INFO] Summary:
2019-03-22 18:23:01,992 [INFO] Batch 12000, worst loss 0.375729 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:23:01,992 [INFO] Regularization: 6292.238770 * 0.0000010000 = 0.0062922388
2019-03-22 18:23:01,993 [INFO] Sum of grad norms: 10.796475
2019-03-22 18:23:01,994 [INFO] ---------------------------------
2019-03-22 18:23:16,660 [INFO] ---------------------------------
2019-03-22 18:23:16,661 [INFO] Summary:
2019-03-22 18:23:16,662 [INFO] Batch 13000, worst loss 0.334851 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:23:16,662 [INFO] Regularization: 6172.091797 * 0.0000010000 = 0.0061720917
2019-03-22 18:23:16,663 [INFO] Sum of grad norms: 2.910403
2019-03-22 18:23:16,663 [INFO] ---------------------------------
2019-03-22 18:23:32,049 [INFO] ---------------------------------
2019-03-22 18:23:32,050 [INFO] Summary:
2019-03-22 18:23:32,050 [INFO] Batch 14000, worst loss 0.377743 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:23:32,051 [INFO] Regularization: 6041.823730 * 0.0000010000 = 0.0060418239
2019-03-22 18:23:32,051 [INFO] Sum of grad norms: 19.844784
2019-03-22 18:23:32,052 [INFO] ---------------------------------
2019-03-22 18:23:46,941 [INFO] ---------------------------------
2019-03-22 18:23:46,942 [INFO] Summary:
2019-03-22 18:23:46,942 [INFO] Batch 15000, worst loss 0.320099 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:23:46,943 [INFO] Regularization: 5826.931152 * 0.0000010000 = 0.0058269310
2019-03-22 18:23:46,943 [INFO] Sum of grad norms: 4.715878
2019-03-22 18:23:46,944 [INFO] ---------------------------------
2019-03-22 18:24:01,928 [INFO] ---------------------------------
2019-03-22 18:24:01,929 [INFO] Summary:
2019-03-22 18:24:01,930 [INFO] Batch 16000, worst loss 0.403819 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:24:01,931 [INFO] Regularization: 5598.042480 * 0.0000010000 = 0.0055980426
2019-03-22 18:24:01,931 [INFO] Sum of grad norms: 10.040698
2019-03-22 18:24:01,932 [INFO] ---------------------------------
2019-03-22 18:24:16,582 [INFO] ---------------------------------
2019-03-22 18:24:16,583 [INFO] Summary:
2019-03-22 18:24:16,584 [INFO] Batch 17000, worst loss 0.340787 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:24:16,585 [INFO] Regularization: 5399.286621 * 0.0000010000 = 0.0053992867
2019-03-22 18:24:16,585 [INFO] Sum of grad norms: 8.289027
2019-03-22 18:24:16,586 [INFO] ---------------------------------
2019-03-22 18:24:31,294 [INFO] ---------------------------------
2019-03-22 18:24:31,295 [INFO] Summary:
2019-03-22 18:24:31,295 [INFO] Batch 18000, worst loss 0.329646 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:24:31,296 [INFO] Regularization: 5162.187012 * 0.0000010000 = 0.0051621869
2019-03-22 18:24:31,296 [INFO] Sum of grad norms: 7.775333
2019-03-22 18:24:31,297 [INFO] ---------------------------------
2019-03-22 18:24:45,959 [INFO] ---------------------------------
2019-03-22 18:24:45,960 [INFO] Summary:
2019-03-22 18:24:45,961 [INFO] Batch 19000, worst loss 0.521072 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:24:45,961 [INFO] Regularization: 4955.639160 * 0.0000010000 = 0.0049556391
2019-03-22 18:24:45,962 [INFO] Sum of grad norms: 2.395504
2019-03-22 18:24:45,962 [INFO] ---------------------------------
2019-03-22 18:25:00,144 [INFO] ---------------------------------
2019-03-22 18:25:00,145 [INFO] Summary:
2019-03-22 18:25:00,146 [INFO] Batch 20000, worst loss 0.354118 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:25:00,146 [INFO] Regularization: 4751.711426 * 0.0000010000 = 0.0047517116
2019-03-22 18:25:00,147 [INFO] Sum of grad norms: 6.810284
2019-03-22 18:25:00,147 [INFO] ---------------------------------
2019-03-22 18:25:02,891 [INFO] ---------------------------------
2019-03-22 18:25:02,891 [INFO] Evaluation:
2019-03-22 18:25:02,892 [INFO] Batch 20000, worst loss 0.333381 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:25:02,893 [INFO] ---------------------------------
2019-03-22 18:25:17,754 [INFO] ---------------------------------
2019-03-22 18:25:17,755 [INFO] Summary:
2019-03-22 18:25:17,755 [INFO] Batch 21000, worst loss 0.360440 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:25:17,756 [INFO] Regularization: 4581.737305 * 0.0000010000 = 0.0045817373
2019-03-22 18:25:17,756 [INFO] Sum of grad norms: 0.283078
2019-03-22 18:25:17,757 [INFO] ---------------------------------
2019-03-22 18:25:32,478 [INFO] ---------------------------------
2019-03-22 18:25:32,479 [INFO] Summary:
2019-03-22 18:25:32,480 [INFO] Batch 22000, worst loss 0.451202 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:25:32,481 [INFO] Regularization: 4420.953613 * 0.0000010000 = 0.0044209538
2019-03-22 18:25:32,481 [INFO] Sum of grad norms: 32.734898
2019-03-22 18:25:32,482 [INFO] ---------------------------------
2019-03-22 18:25:47,313 [INFO] ---------------------------------
2019-03-22 18:25:47,314 [INFO] Summary:
2019-03-22 18:25:47,314 [INFO] Batch 23000, worst loss 0.452078 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:25:47,315 [INFO] Regularization: 4253.993164 * 0.0000010000 = 0.0042539933
2019-03-22 18:25:47,315 [INFO] Sum of grad norms: 5.984191
2019-03-22 18:25:47,316 [INFO] ---------------------------------
2019-03-22 18:26:02,240 [INFO] ---------------------------------
2019-03-22 18:26:02,241 [INFO] Summary:
2019-03-22 18:26:02,242 [INFO] Batch 24000, worst loss 0.307251 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:26:02,242 [INFO] Regularization: 4133.880859 * 0.0000010000 = 0.0041338811
2019-03-22 18:26:02,243 [INFO] Sum of grad norms: 18.581610
2019-03-22 18:26:02,243 [INFO] ---------------------------------
2019-03-22 18:26:17,371 [INFO] ---------------------------------
2019-03-22 18:26:17,372 [INFO] Summary:
2019-03-22 18:26:17,373 [INFO] Batch 25000, worst loss 0.329329 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:26:17,373 [INFO] Regularization: 4002.973389 * 0.0000010000 = 0.0040029734
2019-03-22 18:26:17,374 [INFO] Sum of grad norms: 8.900842
2019-03-22 18:26:17,375 [INFO] ---------------------------------
2019-03-22 18:26:31,949 [INFO] ---------------------------------
2019-03-22 18:26:31,950 [INFO] Summary:
2019-03-22 18:26:31,951 [INFO] Batch 26000, worst loss 0.532515 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:26:31,951 [INFO] Regularization: 3913.738037 * 0.0000010000 = 0.0039137378
2019-03-22 18:26:31,952 [INFO] Sum of grad norms: 1.092290
2019-03-22 18:26:31,952 [INFO] ---------------------------------
2019-03-22 18:26:46,988 [INFO] ---------------------------------
2019-03-22 18:26:46,989 [INFO] Summary:
2019-03-22 18:26:46,990 [INFO] Batch 27000, worst loss 0.285965 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:26:46,990 [INFO] Regularization: 3801.684814 * 0.0000010000 = 0.0038016848
2019-03-22 18:26:46,991 [INFO] Sum of grad norms: 0.208787
2019-03-22 18:26:46,992 [INFO] ---------------------------------
2019-03-22 18:27:01,430 [INFO] ---------------------------------
2019-03-22 18:27:01,431 [INFO] Summary:
2019-03-22 18:27:01,432 [INFO] Batch 28000, worst loss 0.474838 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:27:01,433 [INFO] Regularization: 3692.421875 * 0.0000010000 = 0.0036924218
2019-03-22 18:27:01,433 [INFO] Sum of grad norms: 0.524291
2019-03-22 18:27:01,434 [INFO] ---------------------------------
2019-03-22 18:27:15,231 [INFO] ---------------------------------
2019-03-22 18:27:15,233 [INFO] Summary:
2019-03-22 18:27:15,234 [INFO] Batch 29000, worst loss 0.348037 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:27:15,234 [INFO] Regularization: 3576.321289 * 0.0000010000 = 0.0035763213
2019-03-22 18:27:15,235 [INFO] Sum of grad norms: 15.771889
2019-03-22 18:27:15,236 [INFO] ---------------------------------
2019-03-22 18:27:30,057 [INFO] ---------------------------------
2019-03-22 18:27:30,059 [INFO] Summary:
2019-03-22 18:27:30,059 [INFO] Batch 30000, worst loss 0.395707 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:27:30,060 [INFO] Regularization: 3475.215820 * 0.0000010000 = 0.0034752158
2019-03-22 18:27:30,061 [INFO] Sum of grad norms: 3.935916
2019-03-22 18:27:30,062 [INFO] ---------------------------------
2019-03-22 18:27:32,801 [INFO] ---------------------------------
2019-03-22 18:27:32,802 [INFO] Evaluation:
2019-03-22 18:27:32,803 [INFO] Batch 30000, worst loss 0.416588 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:27:32,804 [INFO] ---------------------------------
2019-03-22 18:27:47,029 [INFO] ---------------------------------
2019-03-22 18:27:47,030 [INFO] Summary:
2019-03-22 18:27:47,031 [INFO] Batch 31000, worst loss 0.348078 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:27:47,031 [INFO] Regularization: 3418.756348 * 0.0000010000 = 0.0034187564
2019-03-22 18:27:47,032 [INFO] Sum of grad norms: 7.432905
2019-03-22 18:27:47,033 [INFO] ---------------------------------
2019-03-22 18:28:01,654 [INFO] ---------------------------------
2019-03-22 18:28:01,654 [INFO] Summary:
2019-03-22 18:28:01,655 [INFO] Batch 32000, worst loss 0.338686 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:28:01,656 [INFO] Regularization: 3349.655762 * 0.0000010000 = 0.0033496558
2019-03-22 18:28:01,656 [INFO] Sum of grad norms: 9.964674
2019-03-22 18:28:01,657 [INFO] ---------------------------------
2019-03-22 18:28:16,235 [INFO] ---------------------------------
2019-03-22 18:28:16,236 [INFO] Summary:
2019-03-22 18:28:16,237 [INFO] Batch 33000, worst loss 0.366290 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:28:16,238 [INFO] Regularization: 3285.856201 * 0.0000010000 = 0.0032858562
2019-03-22 18:28:16,238 [INFO] Sum of grad norms: 3.462532
2019-03-22 18:28:16,239 [INFO] ---------------------------------
2019-03-22 18:28:30,974 [INFO] ---------------------------------
2019-03-22 18:28:30,975 [INFO] Summary:
2019-03-22 18:28:30,977 [INFO] Batch 34000, worst loss 0.389802 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:28:30,977 [INFO] Regularization: 3240.643311 * 0.0000010000 = 0.0032406433
2019-03-22 18:28:30,978 [INFO] Sum of grad norms: 1.696515
2019-03-22 18:28:30,979 [INFO] ---------------------------------
2019-03-22 18:28:44,685 [INFO] ---------------------------------
2019-03-22 18:28:44,686 [INFO] Summary:
2019-03-22 18:28:44,687 [INFO] Batch 35000, worst loss 0.377579 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:28:44,689 [INFO] Regularization: 3173.190918 * 0.0000010000 = 0.0031731909
2019-03-22 18:28:44,690 [INFO] Sum of grad norms: 5.478423
2019-03-22 18:28:44,691 [INFO] ---------------------------------
2019-03-22 18:28:59,311 [INFO] ---------------------------------
2019-03-22 18:28:59,312 [INFO] Summary:
2019-03-22 18:28:59,313 [INFO] Batch 36000, worst loss 0.284221 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:28:59,314 [INFO] Regularization: 3125.969971 * 0.0000010000 = 0.0031259700
2019-03-22 18:28:59,314 [INFO] Sum of grad norms: 0.359549
2019-03-22 18:28:59,315 [INFO] ---------------------------------
2019-03-22 18:29:13,906 [INFO] ---------------------------------
2019-03-22 18:29:13,907 [INFO] Summary:
2019-03-22 18:29:13,908 [INFO] Batch 37000, worst loss 0.368031 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:29:13,908 [INFO] Regularization: 3105.180664 * 0.0000010000 = 0.0031051806
2019-03-22 18:29:13,909 [INFO] Sum of grad norms: 6.874300
2019-03-22 18:29:13,910 [INFO] ---------------------------------
2019-03-22 18:29:28,645 [INFO] ---------------------------------
2019-03-22 18:29:28,646 [INFO] Summary:
2019-03-22 18:29:28,647 [INFO] Batch 38000, worst loss 0.298504 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:29:28,647 [INFO] Regularization: 3054.914307 * 0.0000010000 = 0.0030549143
2019-03-22 18:29:28,648 [INFO] Sum of grad norms: 6.187949
2019-03-22 18:29:28,648 [INFO] ---------------------------------
2019-03-22 18:29:43,130 [INFO] ---------------------------------
2019-03-22 18:29:43,130 [INFO] Summary:
2019-03-22 18:29:43,131 [INFO] Batch 39000, worst loss 0.333617 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:29:43,132 [INFO] Regularization: 2998.800293 * 0.0000010000 = 0.0029988002
2019-03-22 18:29:43,132 [INFO] Sum of grad norms: 2.860957
2019-03-22 18:29:43,133 [INFO] ---------------------------------
2019-03-22 18:29:59,075 [INFO] ---------------------------------
2019-03-22 18:29:59,076 [INFO] Summary:
2019-03-22 18:29:59,077 [INFO] Batch 40000, worst loss 0.382661 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:29:59,078 [INFO] Regularization: 2883.035156 * 0.0000010000 = 0.0028830352
2019-03-22 18:29:59,078 [INFO] Sum of grad norms: 5.155505
2019-03-22 18:29:59,079 [INFO] ---------------------------------
2019-03-22 18:30:01,866 [INFO] ---------------------------------
2019-03-22 18:30:01,867 [INFO] Evaluation:
2019-03-22 18:30:01,868 [INFO] Batch 40000, worst loss 0.308708 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:30:01,868 [INFO] ---------------------------------
2019-03-22 18:30:17,118 [INFO] ---------------------------------
2019-03-22 18:30:17,119 [INFO] Summary:
2019-03-22 18:30:17,120 [INFO] Batch 41000, worst loss 0.546515 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:30:17,120 [INFO] Regularization: 2821.979492 * 0.0000010000 = 0.0028219796
2019-03-22 18:30:17,121 [INFO] Sum of grad norms: 2.741134
2019-03-22 18:30:17,121 [INFO] ---------------------------------
2019-03-22 18:30:32,132 [INFO] ---------------------------------
2019-03-22 18:30:32,133 [INFO] Summary:
2019-03-22 18:30:32,134 [INFO] Batch 42000, worst loss 0.253513 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:30:32,135 [INFO] Regularization: 2772.061523 * 0.0000010000 = 0.0027720616
2019-03-22 18:30:32,136 [INFO] Sum of grad norms: 9.111934
2019-03-22 18:30:32,136 [INFO] ---------------------------------
2019-03-22 18:30:46,761 [INFO] ---------------------------------
2019-03-22 18:30:46,761 [INFO] Summary:
2019-03-22 18:30:46,762 [INFO] Batch 43000, worst loss 0.273108 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:30:46,762 [INFO] Regularization: 2716.189209 * 0.0000010000 = 0.0027161893
2019-03-22 18:30:46,763 [INFO] Sum of grad norms: 0.021810
2019-03-22 18:30:46,763 [INFO] ---------------------------------
2019-03-22 18:31:01,281 [INFO] ---------------------------------
2019-03-22 18:31:01,282 [INFO] Summary:
2019-03-22 18:31:01,282 [INFO] Batch 44000, worst loss 0.316110 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:31:01,283 [INFO] Regularization: 2667.895996 * 0.0000010000 = 0.0026678960
2019-03-22 18:31:01,283 [INFO] Sum of grad norms: 3.168741
2019-03-22 18:31:01,284 [INFO] ---------------------------------
2019-03-22 18:31:16,039 [INFO] ---------------------------------
2019-03-22 18:31:16,040 [INFO] Summary:
2019-03-22 18:31:16,041 [INFO] Batch 45000, worst loss 0.261434 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:31:16,041 [INFO] Regularization: 2622.770752 * 0.0000010000 = 0.0026227708
2019-03-22 18:31:16,042 [INFO] Sum of grad norms: 0.685821
2019-03-22 18:31:16,043 [INFO] ---------------------------------
2019-03-22 18:31:31,000 [INFO] ---------------------------------
2019-03-22 18:31:31,001 [INFO] Summary:
2019-03-22 18:31:31,002 [INFO] Batch 46000, worst loss 0.333413 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:31:31,003 [INFO] Regularization: 2578.710449 * 0.0000010000 = 0.0025787104
2019-03-22 18:31:31,003 [INFO] Sum of grad norms: 0.071575
2019-03-22 18:31:31,004 [INFO] ---------------------------------
2019-03-22 18:31:45,337 [INFO] ---------------------------------
2019-03-22 18:31:45,338 [INFO] Summary:
2019-03-22 18:31:45,339 [INFO] Batch 47000, worst loss 0.302065 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:31:45,339 [INFO] Regularization: 2540.868652 * 0.0000010000 = 0.0025408687
2019-03-22 18:31:45,340 [INFO] Sum of grad norms: 10.705588
2019-03-22 18:31:45,340 [INFO] ---------------------------------
2019-03-22 18:31:59,704 [INFO] ---------------------------------
2019-03-22 18:31:59,705 [INFO] Summary:
2019-03-22 18:31:59,706 [INFO] Batch 48000, worst loss 0.348666 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:31:59,706 [INFO] Regularization: 2497.653076 * 0.0000010000 = 0.0024976530
2019-03-22 18:31:59,707 [INFO] Sum of grad norms: 0.018588
2019-03-22 18:31:59,707 [INFO] ---------------------------------
2019-03-22 18:32:14,562 [INFO] ---------------------------------
2019-03-22 18:32:14,564 [INFO] Summary:
2019-03-22 18:32:14,564 [INFO] Batch 49000, worst loss 0.284674 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:32:14,565 [INFO] Regularization: 2469.296143 * 0.0000010000 = 0.0024692961
2019-03-22 18:32:14,565 [INFO] Sum of grad norms: 0.028327
2019-03-22 18:32:14,566 [INFO] ---------------------------------
2019-03-22 18:32:28,910 [INFO] ---------------------------------
2019-03-22 18:32:28,911 [INFO] Summary:
2019-03-22 18:32:28,912 [INFO] Batch 50000, worst loss 0.290519 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:32:28,913 [INFO] Regularization: 2437.730225 * 0.0000010000 = 0.0024377303
2019-03-22 18:32:28,913 [INFO] Sum of grad norms: 1.896641
2019-03-22 18:32:28,914 [INFO] ---------------------------------
2019-03-22 18:32:31,669 [INFO] ---------------------------------
2019-03-22 18:32:31,670 [INFO] Evaluation:
2019-03-22 18:32:31,671 [INFO] Batch 50000, worst loss 0.267738 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:32:31,671 [INFO] ---------------------------------
2019-03-22 18:32:46,190 [INFO] ---------------------------------
2019-03-22 18:32:46,191 [INFO] Summary:
2019-03-22 18:32:46,191 [INFO] Batch 51000, worst loss 0.285158 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:32:46,192 [INFO] Regularization: 2455.731934 * 0.0000010000 = 0.0024557319
2019-03-22 18:32:46,193 [INFO] Sum of grad norms: 0.069834
2019-03-22 18:32:46,193 [INFO] ---------------------------------
2019-03-22 18:33:01,238 [INFO] ---------------------------------
2019-03-22 18:33:01,239 [INFO] Summary:
2019-03-22 18:33:01,239 [INFO] Batch 52000, worst loss 0.302552 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:33:01,240 [INFO] Regularization: 2438.915039 * 0.0000010000 = 0.0024389150
2019-03-22 18:33:01,240 [INFO] Sum of grad norms: 1.503981
2019-03-22 18:33:01,241 [INFO] ---------------------------------
2019-03-22 18:33:14,986 [INFO] ---------------------------------
2019-03-22 18:33:14,987 [INFO] Summary:
2019-03-22 18:33:14,988 [INFO] Batch 53000, worst loss 0.200141 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:33:14,988 [INFO] Regularization: 2426.854492 * 0.0000010000 = 0.0024268546
2019-03-22 18:33:14,989 [INFO] Sum of grad norms: 28.088135
2019-03-22 18:33:14,989 [INFO] ---------------------------------
2019-03-22 18:33:30,227 [INFO] ---------------------------------
2019-03-22 18:33:30,228 [INFO] Summary:
2019-03-22 18:33:30,229 [INFO] Batch 54000, worst loss 0.196407 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:33:30,229 [INFO] Regularization: 2412.980957 * 0.0000010000 = 0.0024129809
2019-03-22 18:33:30,230 [INFO] Sum of grad norms: 8.231189
2019-03-22 18:33:30,230 [INFO] ---------------------------------
2019-03-22 18:33:32,993 [INFO] ---------------------------------
2019-03-22 18:33:32,994 [INFO] Evaluation:
2019-03-22 18:33:32,995 [INFO] Batch 54000, worst loss 0.198723 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:33:32,995 [INFO] New best loss 0.198723, saved to file classifier/1553275201/1553276012_0_classifier_54000.pth
2019-03-22 18:33:33,004 [INFO] Target
2019-03-22 18:33:33,005 [INFO] [[0.01 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.01 0.99]
 [0.01 0.99]
 [0.99 0.99]
 [0.99 0.01]
 [0.99 0.01]
 [0.99 0.01]
 [0.01 0.99]
 [0.99 0.01]
 [0.01 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.01]
 [0.01 0.99]
 [0.01 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.01 0.99]
 [0.99 0.01]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.01]
 [0.99 0.01]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.99]]
2019-03-22 18:33:33,008 [INFO] Classifier output
2019-03-22 18:33:33,008 [INFO] [[0.009866 0.990555]
 [0.989628 0.990055]
 [0.989856 0.99044 ]
 [0.989941 0.990465]
 [0.992227 0.065505]
 [0.989779 0.990287]
 [0.869039 0.085308]
 [0.009829 0.990098]
 [0.919475 0.990844]
 [0.989326 0.010781]
 [0.990271 0.010169]
 [0.989913 0.010312]
 [0.009534 0.991225]
 [0.990109 0.009937]
 [0.009091 0.990157]
 [0.98946  0.990031]
 [0.989738 0.989901]
 [0.990125 0.009818]
 [0.009927 0.990235]
 [0.010058 0.990083]
 [0.989871 0.990058]
 [0.990535 0.990201]
 [0.010307 0.990314]
 [0.9905   0.00959 ]
 [0.989747 0.990345]
 [0.989768 0.990217]
 [0.989635 0.990351]
 [0.98947  0.009943]
 [0.988317 0.009167]
 [0.990178 0.990062]
 [0.989803 0.989969]
 [0.113641 0.990869]]
2019-03-22 18:33:33,011 [INFO] ---------------------------------
2019-03-22 18:33:47,167 [INFO] ---------------------------------
2019-03-22 18:33:47,168 [INFO] Summary:
2019-03-22 18:33:47,168 [INFO] Batch 55000, worst loss 0.248547 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:33:47,169 [INFO] Regularization: 2400.455322 * 0.0000010000 = 0.0024004553
2019-03-22 18:33:47,169 [INFO] Sum of grad norms: 6.803032
2019-03-22 18:33:47,170 [INFO] ---------------------------------
2019-03-22 18:34:00,953 [INFO] ---------------------------------
2019-03-22 18:34:00,954 [INFO] Summary:
2019-03-22 18:34:00,954 [INFO] Batch 56000, worst loss 0.336861 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:34:00,955 [INFO] Regularization: 2387.908447 * 0.0000010000 = 0.0023879085
2019-03-22 18:34:00,955 [INFO] Sum of grad norms: 2.315071
2019-03-22 18:34:00,956 [INFO] ---------------------------------
2019-03-22 18:34:16,130 [INFO] ---------------------------------
2019-03-22 18:34:16,131 [INFO] Summary:
2019-03-22 18:34:16,132 [INFO] Batch 57000, worst loss 0.224180 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:34:16,132 [INFO] Regularization: 2376.462402 * 0.0000010000 = 0.0023764623
2019-03-22 18:34:16,133 [INFO] Sum of grad norms: 12.683644
2019-03-22 18:34:16,133 [INFO] ---------------------------------
2019-03-22 18:34:30,844 [INFO] ---------------------------------
2019-03-22 18:34:30,845 [INFO] Summary:
2019-03-22 18:34:30,845 [INFO] Batch 58000, worst loss 0.239903 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:34:30,846 [INFO] Regularization: 2364.113281 * 0.0000010000 = 0.0023641132
2019-03-22 18:34:30,846 [INFO] Sum of grad norms: 0.018413
2019-03-22 18:34:30,847 [INFO] ---------------------------------
2019-03-22 18:34:45,371 [INFO] ---------------------------------
2019-03-22 18:34:45,372 [INFO] Summary:
2019-03-22 18:34:45,373 [INFO] Batch 59000, worst loss 0.240229 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:34:45,373 [INFO] Regularization: 2354.842285 * 0.0000010000 = 0.0023548424
2019-03-22 18:34:45,374 [INFO] Sum of grad norms: 0.014004
2019-03-22 18:34:45,374 [INFO] ---------------------------------
2019-03-22 18:34:59,916 [INFO] ---------------------------------
2019-03-22 18:34:59,917 [INFO] Summary:
2019-03-22 18:34:59,917 [INFO] Batch 60000, worst loss 0.238484 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:34:59,918 [INFO] Regularization: 2342.696289 * 0.0000010000 = 0.0023426963
2019-03-22 18:34:59,918 [INFO] Sum of grad norms: 0.172767
2019-03-22 18:34:59,919 [INFO] ---------------------------------
2019-03-22 18:35:02,653 [INFO] ---------------------------------
2019-03-22 18:35:02,654 [INFO] Evaluation:
2019-03-22 18:35:02,654 [INFO] Batch 60000, worst loss 0.216689 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:35:02,655 [INFO] ---------------------------------
2019-03-22 18:35:17,261 [INFO] ---------------------------------
2019-03-22 18:35:17,261 [INFO] Summary:
2019-03-22 18:35:17,262 [INFO] Batch 61000, worst loss 0.238291 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 18:35:17,262 [INFO] Regularization: 2330.750977 * 0.0000010000 = 0.0023307509
2019-03-22 18:35:17,263 [INFO] Sum of grad norms: 0.338690
2019-03-22 18:35:17,263 [INFO] ---------------------------------
2019-03-22 18:35:31,035 [INFO] ---------------------------------
2019-03-22 18:35:31,036 [INFO] Summary:
2019-03-22 18:35:31,037 [INFO] Batch 62000, worst loss 0.189802 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 18:35:31,038 [INFO] Regularization: 2326.663086 * 0.0000010000 = 0.0023266631
2019-03-22 18:35:31,038 [INFO] Sum of grad norms: 16.039864
2019-03-22 18:35:31,039 [INFO] ---------------------------------
2019-03-22 18:35:33,741 [INFO] ---------------------------------
2019-03-22 18:35:33,742 [INFO] Evaluation:
2019-03-22 18:35:33,742 [INFO] Batch 62000, worst loss 0.204378 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:35:33,743 [INFO] ---------------------------------
2019-03-22 18:35:48,315 [INFO] ---------------------------------
2019-03-22 18:35:48,316 [INFO] Summary:
2019-03-22 18:35:48,316 [INFO] Batch 63000, worst loss 0.186428 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 18:35:48,317 [INFO] Regularization: 2323.703125 * 0.0000010000 = 0.0023237031
2019-03-22 18:35:48,317 [INFO] Sum of grad norms: 8.215766
2019-03-22 18:35:48,318 [INFO] ---------------------------------
2019-03-22 18:35:51,058 [INFO] ---------------------------------
2019-03-22 18:35:51,059 [INFO] Evaluation:
2019-03-22 18:35:51,060 [INFO] Batch 63000, worst loss 0.230923 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:35:51,061 [INFO] ---------------------------------
2019-03-22 18:36:04,944 [INFO] ---------------------------------
2019-03-22 18:36:04,945 [INFO] Summary:
2019-03-22 18:36:04,946 [INFO] Batch 64000, worst loss 0.250681 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 18:36:04,947 [INFO] Regularization: 2320.415283 * 0.0000010000 = 0.0023204153
2019-03-22 18:36:04,948 [INFO] Sum of grad norms: 0.018601
2019-03-22 18:36:04,948 [INFO] ---------------------------------
2019-03-22 18:36:19,779 [INFO] ---------------------------------
2019-03-22 18:36:19,780 [INFO] Summary:
2019-03-22 18:36:19,781 [INFO] Batch 65000, worst loss 0.215217 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 18:36:19,782 [INFO] Regularization: 2316.895996 * 0.0000010000 = 0.0023168961
2019-03-22 18:36:19,782 [INFO] Sum of grad norms: 4.441135
2019-03-22 18:36:19,783 [INFO] ---------------------------------
2019-03-22 18:36:34,444 [INFO] ---------------------------------
2019-03-22 18:36:34,445 [INFO] Summary:
2019-03-22 18:36:34,446 [INFO] Batch 66000, worst loss 0.279159 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 18:36:34,446 [INFO] Regularization: 2313.502441 * 0.0000010000 = 0.0023135024
2019-03-22 18:36:34,447 [INFO] Sum of grad norms: 0.004805
2019-03-22 18:36:34,448 [INFO] ---------------------------------
2019-03-22 18:36:49,288 [INFO] ---------------------------------
2019-03-22 18:36:49,289 [INFO] Summary:
2019-03-22 18:36:49,289 [INFO] Batch 67000, worst loss 0.225175 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 18:36:49,290 [INFO] Regularization: 2310.384766 * 0.0000010000 = 0.0023103848
2019-03-22 18:36:49,290 [INFO] Sum of grad norms: 4.510269
2019-03-22 18:36:49,291 [INFO] ---------------------------------
2019-03-22 18:37:04,339 [INFO] ---------------------------------
2019-03-22 18:37:04,340 [INFO] Summary:
2019-03-22 18:37:04,340 [INFO] Batch 68000, worst loss 0.227235 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 18:37:04,341 [INFO] Regularization: 2307.908691 * 0.0000010000 = 0.0023079086
2019-03-22 18:37:04,342 [INFO] Sum of grad norms: 0.038855
2019-03-22 18:37:04,342 [INFO] ---------------------------------
2019-03-22 18:37:19,339 [INFO] ---------------------------------
2019-03-22 18:37:19,340 [INFO] Summary:
2019-03-22 18:37:19,341 [INFO] Batch 69000, worst loss 0.230207 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 18:37:19,341 [INFO] Regularization: 2304.203125 * 0.0000010000 = 0.0023042031
2019-03-22 18:37:19,342 [INFO] Sum of grad norms: 0.197478
2019-03-22 18:37:19,343 [INFO] ---------------------------------
2019-03-22 18:37:33,370 [INFO] ---------------------------------
2019-03-22 18:37:33,371 [INFO] Summary:
2019-03-22 18:37:33,372 [INFO] Batch 70000, worst loss 0.200560 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 18:37:33,373 [INFO] Regularization: 2300.991455 * 0.0000010000 = 0.0023009914
2019-03-22 18:37:33,373 [INFO] Sum of grad norms: 2.584000
2019-03-22 18:37:33,374 [INFO] ---------------------------------
2019-03-22 18:37:36,189 [INFO] ---------------------------------
2019-03-22 18:37:36,190 [INFO] Evaluation:
2019-03-22 18:37:36,191 [INFO] Batch 70000, worst loss 0.150553 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:37:36,192 [INFO] New best loss 0.150553, saved to file classifier/1553275201/1553276256_0_classifier_70000.pth
2019-03-22 18:37:36,200 [INFO] Target
2019-03-22 18:37:36,201 [INFO] [[0.99 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.01]
 [0.99 0.01]
 [0.99 0.99]
 [0.01 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.01]
 [0.99 0.01]
 [0.99 0.99]
 [0.99 0.01]
 [0.99 0.99]
 [0.99 0.01]
 [0.99 0.01]
 [0.01 0.99]
 [0.99 0.99]
 [0.99 0.99]
 [0.01 0.99]
 [0.99 0.01]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.01]
 [0.99 0.99]
 [0.99 0.99]
 [0.99 0.01]
 [0.99 0.99]
 [0.99 0.99]]
2019-03-22 18:37:36,204 [INFO] Classifier output
2019-03-22 18:37:36,204 [INFO] [[0.989882 0.990239]
 [0.989731 0.989873]
 [0.989847 0.989861]
 [0.989801 0.990204]
 [0.990122 0.009856]
 [0.990275 0.008177]
 [0.98983  0.989865]
 [0.958943 0.990913]
 [0.98996  0.990212]
 [0.989868 0.990138]
 [0.990128 0.98988 ]
 [0.989823 0.990093]
 [0.990144 0.009822]
 [0.989227 0.010336]
 [0.99014  0.990463]
 [0.98991  0.95174 ]
 [0.989861 0.989894]
 [0.989797 0.010189]
 [0.99031  0.008683]
 [0.009982 0.990039]
 [0.989894 0.989991]
 [0.989866 0.989865]
 [0.008652 0.989935]
 [0.990041 0.008826]
 [0.989879 0.98995 ]
 [0.98977  0.9901  ]
 [0.990103 0.009931]
 [0.989859 0.99014 ]
 [0.990388 0.990223]
 [0.989142 0.010578]
 [0.9899   0.989922]
 [0.989997 0.989955]]
2019-03-22 18:37:36,207 [INFO] ---------------------------------
2019-03-22 18:37:50,164 [INFO] ---------------------------------
2019-03-22 18:37:50,165 [INFO] Summary:
2019-03-22 18:37:50,165 [INFO] Batch 71000, worst loss 0.205873 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 18:37:50,166 [INFO] Regularization: 2298.358643 * 0.0000010000 = 0.0022983586
2019-03-22 18:37:50,166 [INFO] Sum of grad norms: 0.744816
2019-03-22 18:37:50,167 [INFO] ---------------------------------
2019-03-22 18:38:04,018 [INFO] ---------------------------------
2019-03-22 18:38:04,019 [INFO] Summary:
2019-03-22 18:38:04,019 [INFO] Batch 72000, worst loss 0.193631 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 18:38:04,020 [INFO] Regularization: 2297.357178 * 0.0000010000 = 0.0022973572
2019-03-22 18:38:04,021 [INFO] Sum of grad norms: 32.691311
2019-03-22 18:38:04,021 [INFO] ---------------------------------
2019-03-22 18:38:18,618 [INFO] ---------------------------------
2019-03-22 18:38:18,619 [INFO] Summary:
2019-03-22 18:38:18,620 [INFO] Batch 73000, worst loss 0.227835 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 18:38:18,620 [INFO] Regularization: 2296.627686 * 0.0000010000 = 0.0022966277
2019-03-22 18:38:18,621 [INFO] Sum of grad norms: 0.029617
2019-03-22 18:38:18,622 [INFO] ---------------------------------
2019-03-22 18:38:33,334 [INFO] ---------------------------------
2019-03-22 18:38:33,335 [INFO] Summary:
2019-03-22 18:38:33,336 [INFO] Batch 74000, worst loss 0.236796 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 18:38:33,336 [INFO] Regularization: 2295.935791 * 0.0000010000 = 0.0022959358
2019-03-22 18:38:33,337 [INFO] Sum of grad norms: 0.019631
2019-03-22 18:38:33,337 [INFO] ---------------------------------
2019-03-22 18:38:47,799 [INFO] ---------------------------------
2019-03-22 18:38:47,800 [INFO] Summary:
2019-03-22 18:38:47,801 [INFO] Batch 75000, worst loss 0.287045 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 18:38:47,801 [INFO] Regularization: 2295.191162 * 0.0000010000 = 0.0022951912
2019-03-22 18:38:47,802 [INFO] Sum of grad norms: 0.067137
2019-03-22 18:38:47,803 [INFO] ---------------------------------
2019-03-22 18:39:02,140 [INFO] ---------------------------------
2019-03-22 18:39:02,141 [INFO] Summary:
2019-03-22 18:39:02,142 [INFO] Batch 76000, worst loss 0.228475 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 18:39:02,142 [INFO] Regularization: 2294.448486 * 0.0000010000 = 0.0022944484
2019-03-22 18:39:02,143 [INFO] Sum of grad norms: 0.004050
2019-03-22 18:39:02,144 [INFO] ---------------------------------
2019-03-22 18:39:17,069 [INFO] ---------------------------------
2019-03-22 18:39:17,070 [INFO] Summary:
2019-03-22 18:39:17,070 [INFO] Batch 77000, worst loss 0.199261 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 18:39:17,071 [INFO] Regularization: 2293.673828 * 0.0000010000 = 0.0022936738
2019-03-22 18:39:17,071 [INFO] Sum of grad norms: 12.665323
2019-03-22 18:39:17,072 [INFO] ---------------------------------
2019-03-22 18:39:31,719 [INFO] ---------------------------------
2019-03-22 18:39:31,720 [INFO] Summary:
2019-03-22 18:39:31,721 [INFO] Batch 78000, worst loss 0.240262 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 18:39:31,721 [INFO] Regularization: 2292.967773 * 0.0000010000 = 0.0022929679
2019-03-22 18:39:31,722 [INFO] Sum of grad norms: 0.006462
2019-03-22 18:39:31,723 [INFO] ---------------------------------
2019-03-22 18:39:46,767 [INFO] ---------------------------------
2019-03-22 18:39:46,768 [INFO] Summary:
2019-03-22 18:39:46,769 [INFO] Batch 79000, worst loss 0.300730 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 18:39:46,769 [INFO] Regularization: 2292.332764 * 0.0000010000 = 0.0022923327
2019-03-22 18:39:46,770 [INFO] Sum of grad norms: 0.035901
2019-03-22 18:39:46,770 [INFO] ---------------------------------
2019-03-22 18:40:01,407 [INFO] ---------------------------------
2019-03-22 18:40:01,408 [INFO] Summary:
2019-03-22 18:40:01,409 [INFO] Batch 80000, worst loss 0.297603 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 18:40:01,409 [INFO] Regularization: 2291.799561 * 0.0000010000 = 0.0022917995
2019-03-22 18:40:01,410 [INFO] Sum of grad norms: 25.882849
2019-03-22 18:40:01,410 [INFO] ---------------------------------
2019-03-22 18:40:04,132 [INFO] ---------------------------------
2019-03-22 18:40:04,133 [INFO] Evaluation:
2019-03-22 18:40:04,135 [INFO] Batch 80000, worst loss 0.184956 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:40:04,136 [INFO] ---------------------------------
2019-03-22 18:40:19,278 [INFO] ---------------------------------
2019-03-22 18:40:19,279 [INFO] Summary:
2019-03-22 18:40:19,280 [INFO] Batch 81000, worst loss 0.273676 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 18:40:19,280 [INFO] Regularization: 2291.098633 * 0.0000010000 = 0.0022910987
2019-03-22 18:40:19,281 [INFO] Sum of grad norms: 29.120930
2019-03-22 18:40:19,281 [INFO] ---------------------------------
2019-03-22 18:40:33,493 [INFO] ---------------------------------
2019-03-22 18:40:33,494 [INFO] Summary:
2019-03-22 18:40:33,494 [INFO] Batch 82000, worst loss 0.182350 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 18:40:33,495 [INFO] Regularization: 2290.760254 * 0.0000010000 = 0.0022907602
2019-03-22 18:40:33,495 [INFO] Sum of grad norms: 13.203345
2019-03-22 18:40:33,496 [INFO] ---------------------------------
2019-03-22 18:40:48,263 [INFO] ---------------------------------
2019-03-22 18:40:48,264 [INFO] Summary:
2019-03-22 18:40:48,264 [INFO] Batch 83000, worst loss 0.187848 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 18:40:48,265 [INFO] Regularization: 2290.579346 * 0.0000010000 = 0.0022905793
2019-03-22 18:40:48,266 [INFO] Sum of grad norms: 0.002696
2019-03-22 18:40:48,267 [INFO] ---------------------------------
2019-03-22 18:41:02,462 [INFO] ---------------------------------
2019-03-22 18:41:02,463 [INFO] Summary:
2019-03-22 18:41:02,463 [INFO] Batch 84000, worst loss 0.285558 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 18:41:02,464 [INFO] Regularization: 2290.379150 * 0.0000010000 = 0.0022903793
2019-03-22 18:41:02,465 [INFO] Sum of grad norms: 28.322042
2019-03-22 18:41:02,465 [INFO] ---------------------------------
2019-03-22 18:41:16,405 [INFO] ---------------------------------
2019-03-22 18:41:16,406 [INFO] Summary:
2019-03-22 18:41:16,407 [INFO] Batch 85000, worst loss 0.250721 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 18:41:16,407 [INFO] Regularization: 2290.169189 * 0.0000010000 = 0.0022901692
2019-03-22 18:41:16,408 [INFO] Sum of grad norms: 2.703679
2019-03-22 18:41:16,408 [INFO] ---------------------------------
2019-03-22 18:41:30,253 [INFO] ---------------------------------
2019-03-22 18:41:30,254 [INFO] Summary:
2019-03-22 18:41:30,254 [INFO] Batch 86000, worst loss 0.302659 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 18:41:30,255 [INFO] Regularization: 2289.961182 * 0.0000010000 = 0.0022899611
2019-03-22 18:41:30,256 [INFO] Sum of grad norms: 0.122570
2019-03-22 18:41:30,256 [INFO] ---------------------------------
2019-03-22 18:41:45,747 [INFO] ---------------------------------
2019-03-22 18:41:45,748 [INFO] Summary:
2019-03-22 18:41:45,749 [INFO] Batch 87000, worst loss 0.191023 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 18:41:45,749 [INFO] Regularization: 2289.764404 * 0.0000010000 = 0.0022897644
2019-03-22 18:41:45,750 [INFO] Sum of grad norms: 17.923420
2019-03-22 18:41:45,750 [INFO] ---------------------------------
2019-03-22 18:41:59,293 [INFO] ---------------------------------
2019-03-22 18:41:59,294 [INFO] Summary:
2019-03-22 18:41:59,294 [INFO] Batch 88000, worst loss 0.197598 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 18:41:59,295 [INFO] Regularization: 2289.565430 * 0.0000010000 = 0.0022895655
2019-03-22 18:41:59,295 [INFO] Sum of grad norms: 11.588311
2019-03-22 18:41:59,296 [INFO] ---------------------------------
2019-03-22 18:42:13,947 [INFO] ---------------------------------
2019-03-22 18:42:13,948 [INFO] Summary:
2019-03-22 18:42:13,948 [INFO] Batch 89000, worst loss 0.222229 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 18:42:13,949 [INFO] Regularization: 2289.371826 * 0.0000010000 = 0.0022893718
2019-03-22 18:42:13,950 [INFO] Sum of grad norms: 0.002638
2019-03-22 18:42:13,951 [INFO] ---------------------------------
2019-03-22 18:42:28,069 [INFO] ---------------------------------
2019-03-22 18:42:28,070 [INFO] Summary:
2019-03-22 18:42:28,071 [INFO] Batch 90000, worst loss 0.240161 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 18:42:28,071 [INFO] Regularization: 2289.196777 * 0.0000010000 = 0.0022891967
2019-03-22 18:42:28,072 [INFO] Sum of grad norms: 0.003947
2019-03-22 18:42:28,072 [INFO] ---------------------------------
2019-03-22 18:42:30,822 [INFO] ---------------------------------
2019-03-22 18:42:30,823 [INFO] Evaluation:
2019-03-22 18:42:30,823 [INFO] Batch 90000, worst loss 0.245543 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:42:30,825 [INFO] ---------------------------------
2019-03-22 18:42:45,786 [INFO] ---------------------------------
2019-03-22 18:42:45,787 [INFO] Summary:
2019-03-22 18:42:45,788 [INFO] Batch 91000, worst loss 0.236593 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 18:42:45,789 [INFO] Regularization: 2289.091553 * 0.0000010000 = 0.0022890915
2019-03-22 18:42:45,790 [INFO] Sum of grad norms: 0.030484
2019-03-22 18:42:45,791 [INFO] ---------------------------------
2019-03-22 18:42:59,959 [INFO] ---------------------------------
2019-03-22 18:42:59,960 [INFO] Summary:
2019-03-22 18:42:59,960 [INFO] Batch 92000, worst loss 0.283593 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 18:42:59,961 [INFO] Regularization: 2288.993652 * 0.0000010000 = 0.0022889937
2019-03-22 18:42:59,961 [INFO] Sum of grad norms: 0.004191
2019-03-22 18:42:59,962 [INFO] ---------------------------------
2019-03-22 18:43:14,395 [INFO] ---------------------------------
2019-03-22 18:43:14,396 [INFO] Summary:
2019-03-22 18:43:14,396 [INFO] Batch 93000, worst loss 0.221563 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 18:43:14,397 [INFO] Regularization: 2288.949219 * 0.0000010000 = 0.0022889492
2019-03-22 18:43:14,397 [INFO] Sum of grad norms: 0.020063
2019-03-22 18:43:14,398 [INFO] ---------------------------------
2019-03-22 18:43:28,824 [INFO] ---------------------------------
2019-03-22 18:43:28,825 [INFO] Summary:
2019-03-22 18:43:28,826 [INFO] Batch 94000, worst loss 0.221280 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 18:43:28,826 [INFO] Regularization: 2288.895264 * 0.0000010000 = 0.0022888952
2019-03-22 18:43:28,827 [INFO] Sum of grad norms: 2.353519
2019-03-22 18:43:28,827 [INFO] ---------------------------------
2019-03-22 18:43:43,369 [INFO] ---------------------------------
2019-03-22 18:43:43,370 [INFO] Summary:
2019-03-22 18:43:43,371 [INFO] Batch 95000, worst loss 0.283436 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 18:43:43,372 [INFO] Regularization: 2288.837402 * 0.0000010000 = 0.0022888375
2019-03-22 18:43:43,372 [INFO] Sum of grad norms: 1.216331
2019-03-22 18:43:43,373 [INFO] ---------------------------------
2019-03-22 18:43:57,698 [INFO] ---------------------------------
2019-03-22 18:43:57,699 [INFO] Summary:
2019-03-22 18:43:57,699 [INFO] Batch 96000, worst loss 0.211470 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 18:43:57,700 [INFO] Regularization: 2288.789795 * 0.0000010000 = 0.0022887897
2019-03-22 18:43:57,701 [INFO] Sum of grad norms: 0.007002
2019-03-22 18:43:57,701 [INFO] ---------------------------------
2019-03-22 18:44:12,772 [INFO] ---------------------------------
2019-03-22 18:44:12,774 [INFO] Summary:
2019-03-22 18:44:12,774 [INFO] Batch 97000, worst loss 0.204054 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 18:44:12,775 [INFO] Regularization: 2288.746826 * 0.0000010000 = 0.0022887469
2019-03-22 18:44:12,775 [INFO] Sum of grad norms: 0.123828
2019-03-22 18:44:12,776 [INFO] ---------------------------------
2019-03-22 18:44:27,483 [INFO] ---------------------------------
2019-03-22 18:44:27,485 [INFO] Summary:
2019-03-22 18:44:27,485 [INFO] Batch 98000, worst loss 0.249849 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 18:44:27,486 [INFO] Regularization: 2288.703369 * 0.0000010000 = 0.0022887033
2019-03-22 18:44:27,487 [INFO] Sum of grad norms: 0.002060
2019-03-22 18:44:27,488 [INFO] ---------------------------------
2019-03-22 18:44:41,525 [INFO] ---------------------------------
2019-03-22 18:44:41,526 [INFO] Summary:
2019-03-22 18:44:41,526 [INFO] Batch 99000, worst loss 0.197424 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 18:44:41,527 [INFO] Regularization: 2288.656006 * 0.0000010000 = 0.0022886561
2019-03-22 18:44:41,527 [INFO] Sum of grad norms: 6.794753
2019-03-22 18:44:41,528 [INFO] ---------------------------------
2019-03-22 18:44:56,175 [INFO] ---------------------------------
2019-03-22 18:44:56,176 [INFO] Summary:
2019-03-22 18:44:56,177 [INFO] Batch 100000, worst loss 0.203035 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 18:44:56,177 [INFO] Regularization: 2288.611084 * 0.0000010000 = 0.0022886111
2019-03-22 18:44:56,178 [INFO] Sum of grad norms: 18.375162
2019-03-22 18:44:56,178 [INFO] ---------------------------------
2019-03-22 18:44:58,990 [INFO] ---------------------------------
2019-03-22 18:44:58,990 [INFO] Evaluation:
2019-03-22 18:44:58,991 [INFO] Batch 100000, worst loss 0.234159 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:44:58,991 [INFO] ---------------------------------
2019-03-22 18:44:58,992 [INFO] Finished training, saved to file classifier/1553275201/1553276698_0_classifier_final.pth
2019-03-22 18:44:59,157 [INFO] ---------------------------------
2019-03-22 18:44:59,161 [INFO] Training model #1: (8, 64, 2) @ 2
2019-03-22 18:45:14,229 [INFO] ---------------------------------
2019-03-22 18:45:14,230 [INFO] Summary:
2019-03-22 18:45:14,231 [INFO] Batch 1000, worst loss 1073.177490 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:45:14,231 [INFO] Regularization: 30633.312500 * 0.0000010000 = 0.0306333117
2019-03-22 18:45:14,232 [INFO] Sum of grad norms: 1286.091431
2019-03-22 18:45:14,233 [INFO] ---------------------------------
2019-03-22 18:45:28,226 [INFO] ---------------------------------
2019-03-22 18:45:28,227 [INFO] Summary:
2019-03-22 18:45:28,227 [INFO] Batch 2000, worst loss 34.836845 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:45:28,228 [INFO] Regularization: 28790.464844 * 0.0000010000 = 0.0287904646
2019-03-22 18:45:28,228 [INFO] Sum of grad norms: 519.934753
2019-03-22 18:45:28,229 [INFO] ---------------------------------
2019-03-22 18:45:42,917 [INFO] ---------------------------------
2019-03-22 18:45:42,918 [INFO] Summary:
2019-03-22 18:45:42,919 [INFO] Batch 3000, worst loss 16.735767 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:45:42,919 [INFO] Regularization: 25675.431641 * 0.0000010000 = 0.0256754309
2019-03-22 18:45:42,920 [INFO] Sum of grad norms: 209.641830
2019-03-22 18:45:42,920 [INFO] ---------------------------------
2019-03-22 18:45:57,734 [INFO] ---------------------------------
2019-03-22 18:45:57,734 [INFO] Summary:
2019-03-22 18:45:57,735 [INFO] Batch 4000, worst loss 9.241387 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:45:57,736 [INFO] Regularization: 21104.955078 * 0.0000010000 = 0.0211049542
2019-03-22 18:45:57,736 [INFO] Sum of grad norms: 212.094650
2019-03-22 18:45:57,737 [INFO] ---------------------------------
2019-03-22 18:46:12,066 [INFO] ---------------------------------
2019-03-22 18:46:12,067 [INFO] Summary:
2019-03-22 18:46:12,067 [INFO] Batch 5000, worst loss 3.624716 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:46:12,068 [INFO] Regularization: 15141.261719 * 0.0000010000 = 0.0151412617
2019-03-22 18:46:12,069 [INFO] Sum of grad norms: 55.061989
2019-03-22 18:46:12,069 [INFO] ---------------------------------
2019-03-22 18:46:25,958 [INFO] ---------------------------------
2019-03-22 18:46:25,959 [INFO] Summary:
2019-03-22 18:46:25,960 [INFO] Batch 6000, worst loss 1.678560 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:46:25,961 [INFO] Regularization: 10616.794922 * 0.0000010000 = 0.0106167952
2019-03-22 18:46:25,961 [INFO] Sum of grad norms: 63.166428
2019-03-22 18:46:25,962 [INFO] ---------------------------------
2019-03-22 18:46:40,093 [INFO] ---------------------------------
2019-03-22 18:46:40,094 [INFO] Summary:
2019-03-22 18:46:40,095 [INFO] Batch 7000, worst loss 1.640102 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:46:40,095 [INFO] Regularization: 10004.203125 * 0.0000010000 = 0.0100042028
2019-03-22 18:46:40,096 [INFO] Sum of grad norms: 57.681278
2019-03-22 18:46:40,096 [INFO] ---------------------------------
2019-03-22 18:46:54,923 [INFO] ---------------------------------
2019-03-22 18:46:54,924 [INFO] Summary:
2019-03-22 18:46:54,924 [INFO] Batch 8000, worst loss 0.810605 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:46:54,925 [INFO] Regularization: 9787.288086 * 0.0000010000 = 0.0097872885
2019-03-22 18:46:54,926 [INFO] Sum of grad norms: 15.070434
2019-03-22 18:46:54,926 [INFO] ---------------------------------
2019-03-22 18:47:10,619 [INFO] ---------------------------------
2019-03-22 18:47:10,620 [INFO] Summary:
2019-03-22 18:47:10,620 [INFO] Batch 9000, worst loss 0.627396 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:47:10,621 [INFO] Regularization: 9493.447266 * 0.0000010000 = 0.0094934469
2019-03-22 18:47:10,621 [INFO] Sum of grad norms: 45.678368
2019-03-22 18:47:10,622 [INFO] ---------------------------------
2019-03-22 18:47:25,560 [INFO] ---------------------------------
2019-03-22 18:47:25,561 [INFO] Summary:
2019-03-22 18:47:25,561 [INFO] Batch 10000, worst loss 0.471376 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:47:25,562 [INFO] Regularization: 9141.716797 * 0.0000010000 = 0.0091417171
2019-03-22 18:47:25,562 [INFO] Sum of grad norms: 33.105980
2019-03-22 18:47:25,563 [INFO] ---------------------------------
2019-03-22 18:47:28,312 [INFO] ---------------------------------
2019-03-22 18:47:28,312 [INFO] Evaluation:
2019-03-22 18:47:28,313 [INFO] Batch 10000, worst loss 0.331548 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:47:28,314 [INFO] ---------------------------------
2019-03-22 18:47:42,826 [INFO] ---------------------------------
2019-03-22 18:47:42,827 [INFO] Summary:
2019-03-22 18:47:42,827 [INFO] Batch 11000, worst loss 0.354202 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:47:42,828 [INFO] Regularization: 8797.184570 * 0.0000010000 = 0.0087971846
2019-03-22 18:47:42,829 [INFO] Sum of grad norms: 17.362705
2019-03-22 18:47:42,829 [INFO] ---------------------------------
2019-03-22 18:47:57,674 [INFO] ---------------------------------
2019-03-22 18:47:57,675 [INFO] Summary:
2019-03-22 18:47:57,676 [INFO] Batch 12000, worst loss 0.402953 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:47:57,677 [INFO] Regularization: 8381.352539 * 0.0000010000 = 0.0083813528
2019-03-22 18:47:57,677 [INFO] Sum of grad norms: 2.550848
2019-03-22 18:47:57,678 [INFO] ---------------------------------
2019-03-22 18:48:12,911 [INFO] ---------------------------------
2019-03-22 18:48:12,911 [INFO] Summary:
2019-03-22 18:48:12,912 [INFO] Batch 13000, worst loss 0.390769 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:48:12,912 [INFO] Regularization: 7890.534668 * 0.0000010000 = 0.0078905346
2019-03-22 18:48:12,913 [INFO] Sum of grad norms: 8.443959
2019-03-22 18:48:12,913 [INFO] ---------------------------------
2019-03-22 18:48:26,743 [INFO] ---------------------------------
2019-03-22 18:48:26,744 [INFO] Summary:
2019-03-22 18:48:26,745 [INFO] Batch 14000, worst loss 0.356232 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:48:26,746 [INFO] Regularization: 7380.793945 * 0.0000010000 = 0.0073807938
2019-03-22 18:48:26,747 [INFO] Sum of grad norms: 6.462203
2019-03-22 18:48:26,748 [INFO] ---------------------------------
2019-03-22 18:48:41,505 [INFO] ---------------------------------
2019-03-22 18:48:41,506 [INFO] Summary:
2019-03-22 18:48:41,507 [INFO] Batch 15000, worst loss 0.356538 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:48:41,508 [INFO] Regularization: 6945.000000 * 0.0000010000 = 0.0069450000
2019-03-22 18:48:41,509 [INFO] Sum of grad norms: 0.998895
2019-03-22 18:48:41,510 [INFO] ---------------------------------
2019-03-22 18:48:56,493 [INFO] ---------------------------------
2019-03-22 18:48:56,494 [INFO] Summary:
2019-03-22 18:48:56,495 [INFO] Batch 16000, worst loss 0.390173 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:48:56,496 [INFO] Regularization: 6541.214355 * 0.0000010000 = 0.0065412144
2019-03-22 18:48:56,497 [INFO] Sum of grad norms: 5.689634
2019-03-22 18:48:56,498 [INFO] ---------------------------------
2019-03-22 18:49:11,230 [INFO] ---------------------------------
2019-03-22 18:49:11,231 [INFO] Summary:
2019-03-22 18:49:11,231 [INFO] Batch 17000, worst loss 0.370638 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:49:11,232 [INFO] Regularization: 6017.748535 * 0.0000010000 = 0.0060177487
2019-03-22 18:49:11,232 [INFO] Sum of grad norms: 3.235287
2019-03-22 18:49:11,233 [INFO] ---------------------------------
2019-03-22 18:49:26,509 [INFO] ---------------------------------
2019-03-22 18:49:26,510 [INFO] Summary:
2019-03-22 18:49:26,511 [INFO] Batch 18000, worst loss 0.305166 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:49:26,512 [INFO] Regularization: 5688.833008 * 0.0000010000 = 0.0056888331
2019-03-22 18:49:26,512 [INFO] Sum of grad norms: 4.523949
2019-03-22 18:49:26,513 [INFO] ---------------------------------
2019-03-22 18:49:41,399 [INFO] ---------------------------------
2019-03-22 18:49:41,400 [INFO] Summary:
2019-03-22 18:49:41,400 [INFO] Batch 19000, worst loss 0.374135 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:49:41,401 [INFO] Regularization: 5264.472656 * 0.0000010000 = 0.0052644727
2019-03-22 18:49:41,401 [INFO] Sum of grad norms: 10.572598
2019-03-22 18:49:41,402 [INFO] ---------------------------------
2019-03-22 18:49:56,355 [INFO] ---------------------------------
2019-03-22 18:49:56,356 [INFO] Summary:
2019-03-22 18:49:56,356 [INFO] Batch 20000, worst loss 0.300924 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:49:56,357 [INFO] Regularization: 5019.150391 * 0.0000010000 = 0.0050191502
2019-03-22 18:49:56,357 [INFO] Sum of grad norms: 6.685122
2019-03-22 18:49:56,358 [INFO] ---------------------------------
2019-03-22 18:49:59,045 [INFO] ---------------------------------
2019-03-22 18:49:59,046 [INFO] Evaluation:
2019-03-22 18:49:59,048 [INFO] Batch 20000, worst loss 0.382875 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:49:59,049 [INFO] ---------------------------------
2019-03-22 18:50:13,130 [INFO] ---------------------------------
2019-03-22 18:50:13,132 [INFO] Summary:
2019-03-22 18:50:13,133 [INFO] Batch 21000, worst loss 0.429238 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:50:13,134 [INFO] Regularization: 4897.723633 * 0.0000010000 = 0.0048977234
2019-03-22 18:50:13,135 [INFO] Sum of grad norms: 21.646036
2019-03-22 18:50:13,136 [INFO] ---------------------------------
2019-03-22 18:50:27,400 [INFO] ---------------------------------
2019-03-22 18:50:27,401 [INFO] Summary:
2019-03-22 18:50:27,402 [INFO] Batch 22000, worst loss 0.479704 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:50:27,403 [INFO] Regularization: 4775.871094 * 0.0000010000 = 0.0047758711
2019-03-22 18:50:27,403 [INFO] Sum of grad norms: 4.152398
2019-03-22 18:50:27,404 [INFO] ---------------------------------
2019-03-22 18:50:41,870 [INFO] ---------------------------------
2019-03-22 18:50:41,871 [INFO] Summary:
2019-03-22 18:50:41,872 [INFO] Batch 23000, worst loss 0.339864 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:50:41,872 [INFO] Regularization: 4595.142578 * 0.0000010000 = 0.0045951428
2019-03-22 18:50:41,873 [INFO] Sum of grad norms: 2.860795
2019-03-22 18:50:41,873 [INFO] ---------------------------------
2019-03-22 18:50:56,362 [INFO] ---------------------------------
2019-03-22 18:50:56,363 [INFO] Summary:
2019-03-22 18:50:56,363 [INFO] Batch 24000, worst loss 0.480193 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:50:56,364 [INFO] Regularization: 4521.668945 * 0.0000010000 = 0.0045216689
2019-03-22 18:50:56,365 [INFO] Sum of grad norms: 4.666091
2019-03-22 18:50:56,366 [INFO] ---------------------------------
2019-03-22 18:51:11,571 [INFO] ---------------------------------
2019-03-22 18:51:11,572 [INFO] Summary:
2019-03-22 18:51:11,573 [INFO] Batch 25000, worst loss 0.406490 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:51:11,574 [INFO] Regularization: 4388.121582 * 0.0000010000 = 0.0043881214
2019-03-22 18:51:11,575 [INFO] Sum of grad norms: 12.265806
2019-03-22 18:51:11,576 [INFO] ---------------------------------
2019-03-22 18:51:26,731 [INFO] ---------------------------------
2019-03-22 18:51:26,732 [INFO] Summary:
2019-03-22 18:51:26,733 [INFO] Batch 26000, worst loss 0.341853 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:51:26,733 [INFO] Regularization: 4197.308105 * 0.0000010000 = 0.0041973079
2019-03-22 18:51:26,734 [INFO] Sum of grad norms: 1.830086
2019-03-22 18:51:26,734 [INFO] ---------------------------------
2019-03-22 18:51:41,373 [INFO] ---------------------------------
2019-03-22 18:51:41,374 [INFO] Summary:
2019-03-22 18:51:41,374 [INFO] Batch 27000, worst loss 0.377301 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:51:41,375 [INFO] Regularization: 4025.998047 * 0.0000010000 = 0.0040259981
2019-03-22 18:51:41,375 [INFO] Sum of grad norms: 6.153889
2019-03-22 18:51:41,376 [INFO] ---------------------------------
2019-03-22 18:51:56,447 [INFO] ---------------------------------
2019-03-22 18:51:56,448 [INFO] Summary:
2019-03-22 18:51:56,449 [INFO] Batch 28000, worst loss 0.364109 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:51:56,449 [INFO] Regularization: 3874.573730 * 0.0000010000 = 0.0038745736
2019-03-22 18:51:56,450 [INFO] Sum of grad norms: 2.751002
2019-03-22 18:51:56,450 [INFO] ---------------------------------
2019-03-22 18:52:12,011 [INFO] ---------------------------------
2019-03-22 18:52:12,011 [INFO] Summary:
2019-03-22 18:52:12,012 [INFO] Batch 29000, worst loss 0.471165 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:52:12,013 [INFO] Regularization: 3725.158691 * 0.0000010000 = 0.0037251587
2019-03-22 18:52:12,013 [INFO] Sum of grad norms: 0.471756
2019-03-22 18:52:12,014 [INFO] ---------------------------------
2019-03-22 18:52:27,029 [INFO] ---------------------------------
2019-03-22 18:52:27,030 [INFO] Summary:
2019-03-22 18:52:27,031 [INFO] Batch 30000, worst loss 0.354222 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:52:27,031 [INFO] Regularization: 3574.131836 * 0.0000010000 = 0.0035741318
2019-03-22 18:52:27,032 [INFO] Sum of grad norms: 2.496123
2019-03-22 18:52:27,033 [INFO] ---------------------------------
2019-03-22 18:52:29,780 [INFO] ---------------------------------
2019-03-22 18:52:29,781 [INFO] Evaluation:
2019-03-22 18:52:29,782 [INFO] Batch 30000, worst loss 0.265144 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:52:29,783 [INFO] ---------------------------------
2019-03-22 18:52:44,140 [INFO] ---------------------------------
2019-03-22 18:52:44,141 [INFO] Summary:
2019-03-22 18:52:44,142 [INFO] Batch 31000, worst loss 0.534577 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:52:44,142 [INFO] Regularization: 3444.372314 * 0.0000010000 = 0.0034443722
2019-03-22 18:52:44,143 [INFO] Sum of grad norms: 3.955164
2019-03-22 18:52:44,143 [INFO] ---------------------------------
2019-03-22 18:52:59,906 [INFO] ---------------------------------
2019-03-22 18:52:59,907 [INFO] Summary:
2019-03-22 18:52:59,908 [INFO] Batch 32000, worst loss 0.334054 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:52:59,908 [INFO] Regularization: 3294.766113 * 0.0000010000 = 0.0032947662
2019-03-22 18:52:59,909 [INFO] Sum of grad norms: 4.086117
2019-03-22 18:52:59,909 [INFO] ---------------------------------
2019-03-22 18:53:14,557 [INFO] ---------------------------------
2019-03-22 18:53:14,557 [INFO] Summary:
2019-03-22 18:53:14,558 [INFO] Batch 33000, worst loss 0.423272 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:53:14,559 [INFO] Regularization: 3146.685303 * 0.0000010000 = 0.0031466852
2019-03-22 18:53:14,559 [INFO] Sum of grad norms: 0.178571
2019-03-22 18:53:14,560 [INFO] ---------------------------------
2019-03-22 18:53:28,817 [INFO] ---------------------------------
2019-03-22 18:53:28,818 [INFO] Summary:
2019-03-22 18:53:28,818 [INFO] Batch 34000, worst loss 0.402786 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:53:28,819 [INFO] Regularization: 3025.939209 * 0.0000010000 = 0.0030259392
2019-03-22 18:53:28,819 [INFO] Sum of grad norms: 8.080705
2019-03-22 18:53:28,820 [INFO] ---------------------------------
2019-03-22 18:53:43,252 [INFO] ---------------------------------
2019-03-22 18:53:43,253 [INFO] Summary:
2019-03-22 18:53:43,253 [INFO] Batch 35000, worst loss 0.453319 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:53:43,254 [INFO] Regularization: 2937.208984 * 0.0000010000 = 0.0029372089
2019-03-22 18:53:43,254 [INFO] Sum of grad norms: 5.710912
2019-03-22 18:53:43,255 [INFO] ---------------------------------
2019-03-22 18:53:57,788 [INFO] ---------------------------------
2019-03-22 18:53:57,789 [INFO] Summary:
2019-03-22 18:53:57,790 [INFO] Batch 36000, worst loss 0.356637 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:53:57,791 [INFO] Regularization: 2878.569092 * 0.0000010000 = 0.0028785691
2019-03-22 18:53:57,792 [INFO] Sum of grad norms: 3.918329
2019-03-22 18:53:57,793 [INFO] ---------------------------------
2019-03-22 18:54:12,274 [INFO] ---------------------------------
2019-03-22 18:54:12,275 [INFO] Summary:
2019-03-22 18:54:12,275 [INFO] Batch 37000, worst loss 0.411305 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:54:12,276 [INFO] Regularization: 2825.021240 * 0.0000010000 = 0.0028250213
2019-03-22 18:54:12,276 [INFO] Sum of grad norms: 9.126808
2019-03-22 18:54:12,277 [INFO] ---------------------------------
2019-03-22 18:54:27,084 [INFO] ---------------------------------
2019-03-22 18:54:27,085 [INFO] Summary:
2019-03-22 18:54:27,085 [INFO] Batch 38000, worst loss 0.348939 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:54:27,086 [INFO] Regularization: 2789.804932 * 0.0000010000 = 0.0027898049
2019-03-22 18:54:27,086 [INFO] Sum of grad norms: 0.080994
2019-03-22 18:54:27,087 [INFO] ---------------------------------
2019-03-22 18:54:41,866 [INFO] ---------------------------------
2019-03-22 18:54:41,867 [INFO] Summary:
2019-03-22 18:54:41,867 [INFO] Batch 39000, worst loss 0.394544 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:54:41,868 [INFO] Regularization: 2662.409668 * 0.0000010000 = 0.0026624096
2019-03-22 18:54:41,868 [INFO] Sum of grad norms: 0.343252
2019-03-22 18:54:41,869 [INFO] ---------------------------------
2019-03-22 18:54:56,245 [INFO] ---------------------------------
2019-03-22 18:54:56,246 [INFO] Summary:
2019-03-22 18:54:56,246 [INFO] Batch 40000, worst loss 0.363470 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 18:54:56,247 [INFO] Regularization: 2580.982178 * 0.0000010000 = 0.0025809822
2019-03-22 18:54:56,248 [INFO] Sum of grad norms: 0.046627
2019-03-22 18:54:56,248 [INFO] ---------------------------------
2019-03-22 18:54:58,949 [INFO] ---------------------------------
2019-03-22 18:54:58,950 [INFO] Evaluation:
2019-03-22 18:54:58,952 [INFO] Batch 40000, worst loss 0.246746 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:54:58,953 [INFO] ---------------------------------
2019-03-22 18:55:13,997 [INFO] ---------------------------------
2019-03-22 18:55:13,998 [INFO] Summary:
2019-03-22 18:55:13,998 [INFO] Batch 41000, worst loss 0.347393 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:55:13,999 [INFO] Regularization: 2527.846191 * 0.0000010000 = 0.0025278463
2019-03-22 18:55:14,000 [INFO] Sum of grad norms: 0.060662
2019-03-22 18:55:14,000 [INFO] ---------------------------------
2019-03-22 18:55:28,585 [INFO] ---------------------------------
2019-03-22 18:55:28,586 [INFO] Summary:
2019-03-22 18:55:28,587 [INFO] Batch 42000, worst loss 0.281498 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:55:28,588 [INFO] Regularization: 2405.200928 * 0.0000010000 = 0.0024052009
2019-03-22 18:55:28,589 [INFO] Sum of grad norms: 22.898748
2019-03-22 18:55:28,589 [INFO] ---------------------------------
2019-03-22 18:55:43,364 [INFO] ---------------------------------
2019-03-22 18:55:43,365 [INFO] Summary:
2019-03-22 18:55:43,366 [INFO] Batch 43000, worst loss 0.249491 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:55:43,366 [INFO] Regularization: 2338.177490 * 0.0000010000 = 0.0023381775
2019-03-22 18:55:43,367 [INFO] Sum of grad norms: 0.108488
2019-03-22 18:55:43,367 [INFO] ---------------------------------
2019-03-22 18:55:57,035 [INFO] ---------------------------------
2019-03-22 18:55:57,036 [INFO] Summary:
2019-03-22 18:55:57,037 [INFO] Batch 44000, worst loss 0.256505 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:55:57,038 [INFO] Regularization: 2274.389893 * 0.0000010000 = 0.0022743898
2019-03-22 18:55:57,039 [INFO] Sum of grad norms: 8.164410
2019-03-22 18:55:57,040 [INFO] ---------------------------------
2019-03-22 18:56:11,352 [INFO] ---------------------------------
2019-03-22 18:56:11,353 [INFO] Summary:
2019-03-22 18:56:11,354 [INFO] Batch 45000, worst loss 0.209111 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:56:11,355 [INFO] Regularization: 2211.808594 * 0.0000010000 = 0.0022118087
2019-03-22 18:56:11,356 [INFO] Sum of grad norms: 0.060918
2019-03-22 18:56:11,357 [INFO] ---------------------------------
2019-03-22 18:56:25,910 [INFO] ---------------------------------
2019-03-22 18:56:25,911 [INFO] Summary:
2019-03-22 18:56:25,912 [INFO] Batch 46000, worst loss 0.276981 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:56:25,912 [INFO] Regularization: 2147.845947 * 0.0000010000 = 0.0021478459
2019-03-22 18:56:25,913 [INFO] Sum of grad norms: 1.358820
2019-03-22 18:56:25,914 [INFO] ---------------------------------
2019-03-22 18:56:40,084 [INFO] ---------------------------------
2019-03-22 18:56:40,085 [INFO] Summary:
2019-03-22 18:56:40,085 [INFO] Batch 47000, worst loss 0.253306 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:56:40,086 [INFO] Regularization: 2095.218750 * 0.0000010000 = 0.0020952187
2019-03-22 18:56:40,086 [INFO] Sum of grad norms: 1.028714
2019-03-22 18:56:40,087 [INFO] ---------------------------------
2019-03-22 18:56:54,579 [INFO] ---------------------------------
2019-03-22 18:56:54,580 [INFO] Summary:
2019-03-22 18:56:54,581 [INFO] Batch 48000, worst loss 0.333540 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:56:54,581 [INFO] Regularization: 2052.162354 * 0.0000010000 = 0.0020521623
2019-03-22 18:56:54,582 [INFO] Sum of grad norms: 0.039560
2019-03-22 18:56:54,582 [INFO] ---------------------------------
2019-03-22 18:57:08,709 [INFO] ---------------------------------
2019-03-22 18:57:08,710 [INFO] Summary:
2019-03-22 18:57:08,711 [INFO] Batch 49000, worst loss 0.246824 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:57:08,711 [INFO] Regularization: 2021.483154 * 0.0000010000 = 0.0020214831
2019-03-22 18:57:08,712 [INFO] Sum of grad norms: 23.166281
2019-03-22 18:57:08,713 [INFO] ---------------------------------
2019-03-22 18:57:23,604 [INFO] ---------------------------------
2019-03-22 18:57:23,605 [INFO] Summary:
2019-03-22 18:57:23,606 [INFO] Batch 50000, worst loss 0.303573 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 18:57:23,606 [INFO] Regularization: 1993.619873 * 0.0000010000 = 0.0019936198
2019-03-22 18:57:23,607 [INFO] Sum of grad norms: 2.615814
2019-03-22 18:57:23,608 [INFO] ---------------------------------
2019-03-22 18:57:26,362 [INFO] ---------------------------------
2019-03-22 18:57:26,362 [INFO] Evaluation:
2019-03-22 18:57:26,363 [INFO] Batch 50000, worst loss 0.184250 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:57:26,364 [INFO] ---------------------------------
2019-03-22 18:57:40,760 [INFO] ---------------------------------
2019-03-22 18:57:40,761 [INFO] Summary:
2019-03-22 18:57:40,761 [INFO] Batch 51000, worst loss 0.253075 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:57:40,762 [INFO] Regularization: 1969.683838 * 0.0000010000 = 0.0019696839
2019-03-22 18:57:40,762 [INFO] Sum of grad norms: 1.949482
2019-03-22 18:57:40,763 [INFO] ---------------------------------
2019-03-22 18:57:55,125 [INFO] ---------------------------------
2019-03-22 18:57:55,126 [INFO] Summary:
2019-03-22 18:57:55,126 [INFO] Batch 52000, worst loss 0.256902 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:57:55,127 [INFO] Regularization: 1954.681885 * 0.0000010000 = 0.0019546819
2019-03-22 18:57:55,128 [INFO] Sum of grad norms: 0.012405
2019-03-22 18:57:55,128 [INFO] ---------------------------------
2019-03-22 18:58:09,502 [INFO] ---------------------------------
2019-03-22 18:58:09,503 [INFO] Summary:
2019-03-22 18:58:09,504 [INFO] Batch 53000, worst loss 0.199785 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:58:09,504 [INFO] Regularization: 1944.380981 * 0.0000010000 = 0.0019443809
2019-03-22 18:58:09,505 [INFO] Sum of grad norms: 0.127776
2019-03-22 18:58:09,505 [INFO] ---------------------------------
2019-03-22 18:58:23,599 [INFO] ---------------------------------
2019-03-22 18:58:23,600 [INFO] Summary:
2019-03-22 18:58:23,600 [INFO] Batch 54000, worst loss 0.205610 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:58:23,601 [INFO] Regularization: 1933.845093 * 0.0000010000 = 0.0019338451
2019-03-22 18:58:23,601 [INFO] Sum of grad norms: 0.047434
2019-03-22 18:58:23,602 [INFO] ---------------------------------
2019-03-22 18:58:38,086 [INFO] ---------------------------------
2019-03-22 18:58:38,087 [INFO] Summary:
2019-03-22 18:58:38,087 [INFO] Batch 55000, worst loss 0.242953 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:58:38,088 [INFO] Regularization: 1925.744507 * 0.0000010000 = 0.0019257445
2019-03-22 18:58:38,088 [INFO] Sum of grad norms: 0.365308
2019-03-22 18:58:38,089 [INFO] ---------------------------------
2019-03-22 18:58:52,816 [INFO] ---------------------------------
2019-03-22 18:58:52,817 [INFO] Summary:
2019-03-22 18:58:52,818 [INFO] Batch 56000, worst loss 0.181610 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:58:52,818 [INFO] Regularization: 1917.039795 * 0.0000010000 = 0.0019170397
2019-03-22 18:58:52,819 [INFO] Sum of grad norms: 0.008506
2019-03-22 18:58:52,819 [INFO] ---------------------------------
2019-03-22 18:59:07,578 [INFO] ---------------------------------
2019-03-22 18:59:07,579 [INFO] Summary:
2019-03-22 18:59:07,580 [INFO] Batch 57000, worst loss 0.304542 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:59:07,580 [INFO] Regularization: 1907.747803 * 0.0000010000 = 0.0019077478
2019-03-22 18:59:07,581 [INFO] Sum of grad norms: 0.010754
2019-03-22 18:59:07,581 [INFO] ---------------------------------
2019-03-22 18:59:21,682 [INFO] ---------------------------------
2019-03-22 18:59:21,683 [INFO] Summary:
2019-03-22 18:59:21,683 [INFO] Batch 58000, worst loss 0.224999 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:59:21,684 [INFO] Regularization: 1899.785034 * 0.0000010000 = 0.0018997850
2019-03-22 18:59:21,684 [INFO] Sum of grad norms: 0.009658
2019-03-22 18:59:21,685 [INFO] ---------------------------------
2019-03-22 18:59:36,298 [INFO] ---------------------------------
2019-03-22 18:59:36,299 [INFO] Summary:
2019-03-22 18:59:36,299 [INFO] Batch 59000, worst loss 0.292365 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:59:36,300 [INFO] Regularization: 1895.720703 * 0.0000010000 = 0.0018957207
2019-03-22 18:59:36,300 [INFO] Sum of grad norms: 0.073268
2019-03-22 18:59:36,301 [INFO] ---------------------------------
2019-03-22 18:59:50,483 [INFO] ---------------------------------
2019-03-22 18:59:50,484 [INFO] Summary:
2019-03-22 18:59:50,485 [INFO] Batch 60000, worst loss 0.248474 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 18:59:50,485 [INFO] Regularization: 1887.401001 * 0.0000010000 = 0.0018874010
2019-03-22 18:59:50,486 [INFO] Sum of grad norms: 0.025810
2019-03-22 18:59:50,486 [INFO] ---------------------------------
2019-03-22 18:59:53,278 [INFO] ---------------------------------
2019-03-22 18:59:53,279 [INFO] Evaluation:
2019-03-22 18:59:53,280 [INFO] Batch 60000, worst loss 0.256647 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 18:59:53,280 [INFO] ---------------------------------
2019-03-22 19:00:07,960 [INFO] ---------------------------------
2019-03-22 19:00:07,961 [INFO] Summary:
2019-03-22 19:00:07,962 [INFO] Batch 61000, worst loss 0.256028 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:00:07,962 [INFO] Regularization: 1881.332642 * 0.0000010000 = 0.0018813326
2019-03-22 19:00:07,963 [INFO] Sum of grad norms: 1.422794
2019-03-22 19:00:07,964 [INFO] ---------------------------------
2019-03-22 19:00:22,093 [INFO] ---------------------------------
2019-03-22 19:00:22,094 [INFO] Summary:
2019-03-22 19:00:22,095 [INFO] Batch 62000, worst loss 0.213273 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:00:22,095 [INFO] Regularization: 1876.865601 * 0.0000010000 = 0.0018768656
2019-03-22 19:00:22,096 [INFO] Sum of grad norms: 0.012267
2019-03-22 19:00:22,097 [INFO] ---------------------------------
2019-03-22 19:00:36,794 [INFO] ---------------------------------
2019-03-22 19:00:36,795 [INFO] Summary:
2019-03-22 19:00:36,796 [INFO] Batch 63000, worst loss 0.263348 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:00:36,796 [INFO] Regularization: 1874.870239 * 0.0000010000 = 0.0018748703
2019-03-22 19:00:36,797 [INFO] Sum of grad norms: 0.462037
2019-03-22 19:00:36,797 [INFO] ---------------------------------
2019-03-22 19:00:51,420 [INFO] ---------------------------------
2019-03-22 19:00:51,421 [INFO] Summary:
2019-03-22 19:00:51,422 [INFO] Batch 64000, worst loss 0.198330 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:00:51,422 [INFO] Regularization: 1872.843262 * 0.0000010000 = 0.0018728432
2019-03-22 19:00:51,423 [INFO] Sum of grad norms: 4.811449
2019-03-22 19:00:51,423 [INFO] ---------------------------------
2019-03-22 19:01:06,127 [INFO] ---------------------------------
2019-03-22 19:01:06,129 [INFO] Summary:
2019-03-22 19:01:06,129 [INFO] Batch 65000, worst loss 0.256066 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:01:06,130 [INFO] Regularization: 1871.417847 * 0.0000010000 = 0.0018714179
2019-03-22 19:01:06,130 [INFO] Sum of grad norms: 3.714587
2019-03-22 19:01:06,131 [INFO] ---------------------------------
2019-03-22 19:01:21,183 [INFO] ---------------------------------
2019-03-22 19:01:21,184 [INFO] Summary:
2019-03-22 19:01:21,185 [INFO] Batch 66000, worst loss 0.295252 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:01:21,185 [INFO] Regularization: 1869.235107 * 0.0000010000 = 0.0018692351
2019-03-22 19:01:21,186 [INFO] Sum of grad norms: 0.022566
2019-03-22 19:01:21,186 [INFO] ---------------------------------
2019-03-22 19:01:35,246 [INFO] ---------------------------------
2019-03-22 19:01:35,247 [INFO] Summary:
2019-03-22 19:01:35,248 [INFO] Batch 67000, worst loss 0.264745 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:01:35,248 [INFO] Regularization: 1867.251587 * 0.0000010000 = 0.0018672516
2019-03-22 19:01:35,249 [INFO] Sum of grad norms: 0.022100
2019-03-22 19:01:35,249 [INFO] ---------------------------------
2019-03-22 19:01:49,855 [INFO] ---------------------------------
2019-03-22 19:01:49,855 [INFO] Summary:
2019-03-22 19:01:49,856 [INFO] Batch 68000, worst loss 0.244673 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:01:49,857 [INFO] Regularization: 1865.249146 * 0.0000010000 = 0.0018652491
2019-03-22 19:01:49,857 [INFO] Sum of grad norms: 2.846459
2019-03-22 19:01:49,858 [INFO] ---------------------------------
2019-03-22 19:02:04,892 [INFO] ---------------------------------
2019-03-22 19:02:04,893 [INFO] Summary:
2019-03-22 19:02:04,893 [INFO] Batch 69000, worst loss 0.300054 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:02:04,894 [INFO] Regularization: 1863.627686 * 0.0000010000 = 0.0018636277
2019-03-22 19:02:04,894 [INFO] Sum of grad norms: 0.006530
2019-03-22 19:02:04,895 [INFO] ---------------------------------
2019-03-22 19:02:19,909 [INFO] ---------------------------------
2019-03-22 19:02:19,910 [INFO] Summary:
2019-03-22 19:02:19,911 [INFO] Batch 70000, worst loss 0.221237 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:02:19,911 [INFO] Regularization: 1861.580200 * 0.0000010000 = 0.0018615802
2019-03-22 19:02:19,912 [INFO] Sum of grad norms: 22.276815
2019-03-22 19:02:19,913 [INFO] ---------------------------------
2019-03-22 19:02:22,634 [INFO] ---------------------------------
2019-03-22 19:02:22,635 [INFO] Evaluation:
2019-03-22 19:02:22,636 [INFO] Batch 70000, worst loss 0.269309 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:02:22,637 [INFO] ---------------------------------
2019-03-22 19:02:36,986 [INFO] ---------------------------------
2019-03-22 19:02:36,987 [INFO] Summary:
2019-03-22 19:02:36,988 [INFO] Batch 71000, worst loss 0.238359 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:02:36,988 [INFO] Regularization: 1859.880371 * 0.0000010000 = 0.0018598804
2019-03-22 19:02:36,989 [INFO] Sum of grad norms: 0.130914
2019-03-22 19:02:36,990 [INFO] ---------------------------------
2019-03-22 19:02:50,898 [INFO] ---------------------------------
2019-03-22 19:02:50,899 [INFO] Summary:
2019-03-22 19:02:50,899 [INFO] Batch 72000, worst loss 0.217012 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:02:50,900 [INFO] Regularization: 1858.930420 * 0.0000010000 = 0.0018589305
2019-03-22 19:02:50,900 [INFO] Sum of grad norms: 0.272398
2019-03-22 19:02:50,901 [INFO] ---------------------------------
2019-03-22 19:03:04,849 [INFO] ---------------------------------
2019-03-22 19:03:04,850 [INFO] Summary:
2019-03-22 19:03:04,850 [INFO] Batch 73000, worst loss 0.190002 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:03:04,851 [INFO] Regularization: 1858.418091 * 0.0000010000 = 0.0018584181
2019-03-22 19:03:04,852 [INFO] Sum of grad norms: 0.002467
2019-03-22 19:03:04,852 [INFO] ---------------------------------
2019-03-22 19:03:19,208 [INFO] ---------------------------------
2019-03-22 19:03:19,209 [INFO] Summary:
2019-03-22 19:03:19,209 [INFO] Batch 74000, worst loss 0.250251 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:03:19,210 [INFO] Regularization: 1857.912354 * 0.0000010000 = 0.0018579124
2019-03-22 19:03:19,210 [INFO] Sum of grad norms: 0.015329
2019-03-22 19:03:19,211 [INFO] ---------------------------------
2019-03-22 19:03:33,570 [INFO] ---------------------------------
2019-03-22 19:03:33,571 [INFO] Summary:
2019-03-22 19:03:33,572 [INFO] Batch 75000, worst loss 0.192138 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:03:33,572 [INFO] Regularization: 1857.443848 * 0.0000010000 = 0.0018574438
2019-03-22 19:03:33,573 [INFO] Sum of grad norms: 0.006530
2019-03-22 19:03:33,573 [INFO] ---------------------------------
2019-03-22 19:03:48,660 [INFO] ---------------------------------
2019-03-22 19:03:48,661 [INFO] Summary:
2019-03-22 19:03:48,662 [INFO] Batch 76000, worst loss 0.187443 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:03:48,662 [INFO] Regularization: 1856.845215 * 0.0000010000 = 0.0018568452
2019-03-22 19:03:48,663 [INFO] Sum of grad norms: 0.003069
2019-03-22 19:03:48,663 [INFO] ---------------------------------
2019-03-22 19:04:02,372 [INFO] ---------------------------------
2019-03-22 19:04:02,372 [INFO] Summary:
2019-03-22 19:04:02,373 [INFO] Batch 77000, worst loss 0.237622 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:04:02,374 [INFO] Regularization: 1856.336792 * 0.0000010000 = 0.0018563368
2019-03-22 19:04:02,374 [INFO] Sum of grad norms: 11.867595
2019-03-22 19:04:02,375 [INFO] ---------------------------------
2019-03-22 19:04:16,442 [INFO] ---------------------------------
2019-03-22 19:04:16,443 [INFO] Summary:
2019-03-22 19:04:16,444 [INFO] Batch 78000, worst loss 0.296193 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:04:16,445 [INFO] Regularization: 1855.945801 * 0.0000010000 = 0.0018559458
2019-03-22 19:04:16,445 [INFO] Sum of grad norms: 6.174000
2019-03-22 19:04:16,446 [INFO] ---------------------------------
2019-03-22 19:04:32,161 [INFO] ---------------------------------
2019-03-22 19:04:32,162 [INFO] Summary:
2019-03-22 19:04:32,162 [INFO] Batch 79000, worst loss 0.262990 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:04:32,163 [INFO] Regularization: 1855.515137 * 0.0000010000 = 0.0018555152
2019-03-22 19:04:32,163 [INFO] Sum of grad norms: 0.020634
2019-03-22 19:04:32,164 [INFO] ---------------------------------
2019-03-22 19:04:46,920 [INFO] ---------------------------------
2019-03-22 19:04:46,921 [INFO] Summary:
2019-03-22 19:04:46,922 [INFO] Batch 80000, worst loss 0.252124 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:04:46,923 [INFO] Regularization: 1854.999634 * 0.0000010000 = 0.0018549996
2019-03-22 19:04:46,924 [INFO] Sum of grad norms: 0.045211
2019-03-22 19:04:46,925 [INFO] ---------------------------------
2019-03-22 19:04:49,715 [INFO] ---------------------------------
2019-03-22 19:04:49,716 [INFO] Evaluation:
2019-03-22 19:04:49,716 [INFO] Batch 80000, worst loss 0.235148 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:04:49,717 [INFO] ---------------------------------
2019-03-22 19:05:04,288 [INFO] ---------------------------------
2019-03-22 19:05:04,289 [INFO] Summary:
2019-03-22 19:05:04,289 [INFO] Batch 81000, worst loss 0.209581 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:05:04,290 [INFO] Regularization: 1854.651367 * 0.0000010000 = 0.0018546514
2019-03-22 19:05:04,291 [INFO] Sum of grad norms: 0.025446
2019-03-22 19:05:04,291 [INFO] ---------------------------------
2019-03-22 19:05:18,772 [INFO] ---------------------------------
2019-03-22 19:05:18,773 [INFO] Summary:
2019-03-22 19:05:18,773 [INFO] Batch 82000, worst loss 0.256486 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:05:18,774 [INFO] Regularization: 1854.463257 * 0.0000010000 = 0.0018544632
2019-03-22 19:05:18,774 [INFO] Sum of grad norms: 0.034155
2019-03-22 19:05:18,775 [INFO] ---------------------------------
2019-03-22 19:05:32,929 [INFO] ---------------------------------
2019-03-22 19:05:32,930 [INFO] Summary:
2019-03-22 19:05:32,931 [INFO] Batch 83000, worst loss 0.237915 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:05:32,932 [INFO] Regularization: 1854.313599 * 0.0000010000 = 0.0018543135
2019-03-22 19:05:32,933 [INFO] Sum of grad norms: 28.126915
2019-03-22 19:05:32,934 [INFO] ---------------------------------
2019-03-22 19:05:48,232 [INFO] ---------------------------------
2019-03-22 19:05:48,233 [INFO] Summary:
2019-03-22 19:05:48,234 [INFO] Batch 84000, worst loss 0.297721 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:05:48,235 [INFO] Regularization: 1854.177124 * 0.0000010000 = 0.0018541771
2019-03-22 19:05:48,235 [INFO] Sum of grad norms: 0.010609
2019-03-22 19:05:48,236 [INFO] ---------------------------------
2019-03-22 19:06:02,320 [INFO] ---------------------------------
2019-03-22 19:06:02,321 [INFO] Summary:
2019-03-22 19:06:02,321 [INFO] Batch 85000, worst loss 0.188182 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:06:02,322 [INFO] Regularization: 1854.018188 * 0.0000010000 = 0.0018540182
2019-03-22 19:06:02,322 [INFO] Sum of grad norms: 29.255173
2019-03-22 19:06:02,323 [INFO] ---------------------------------
2019-03-22 19:06:17,350 [INFO] ---------------------------------
2019-03-22 19:06:17,351 [INFO] Summary:
2019-03-22 19:06:17,352 [INFO] Batch 86000, worst loss 0.188950 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:06:17,352 [INFO] Regularization: 1853.909424 * 0.0000010000 = 0.0018539095
2019-03-22 19:06:17,353 [INFO] Sum of grad norms: 2.481251
2019-03-22 19:06:17,354 [INFO] ---------------------------------
2019-03-22 19:06:31,836 [INFO] ---------------------------------
2019-03-22 19:06:31,837 [INFO] Summary:
2019-03-22 19:06:31,838 [INFO] Batch 87000, worst loss 0.277533 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:06:31,838 [INFO] Regularization: 1853.771606 * 0.0000010000 = 0.0018537716
2019-03-22 19:06:31,839 [INFO] Sum of grad norms: 0.017409
2019-03-22 19:06:31,840 [INFO] ---------------------------------
2019-03-22 19:06:47,327 [INFO] ---------------------------------
2019-03-22 19:06:47,328 [INFO] Summary:
2019-03-22 19:06:47,329 [INFO] Batch 88000, worst loss 0.306963 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:06:47,329 [INFO] Regularization: 1853.663940 * 0.0000010000 = 0.0018536639
2019-03-22 19:06:47,330 [INFO] Sum of grad norms: 32.456169
2019-03-22 19:06:47,330 [INFO] ---------------------------------
2019-03-22 19:07:01,317 [INFO] ---------------------------------
2019-03-22 19:07:01,318 [INFO] Summary:
2019-03-22 19:07:01,319 [INFO] Batch 89000, worst loss 0.304335 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:07:01,320 [INFO] Regularization: 1853.516357 * 0.0000010000 = 0.0018535163
2019-03-22 19:07:01,321 [INFO] Sum of grad norms: 0.134897
2019-03-22 19:07:01,322 [INFO] ---------------------------------
2019-03-22 19:07:16,871 [INFO] ---------------------------------
2019-03-22 19:07:16,871 [INFO] Summary:
2019-03-22 19:07:16,872 [INFO] Batch 90000, worst loss 0.197045 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:07:16,872 [INFO] Regularization: 1853.392212 * 0.0000010000 = 0.0018533922
2019-03-22 19:07:16,873 [INFO] Sum of grad norms: 18.109299
2019-03-22 19:07:16,873 [INFO] ---------------------------------
2019-03-22 19:07:19,612 [INFO] ---------------------------------
2019-03-22 19:07:19,612 [INFO] Evaluation:
2019-03-22 19:07:19,614 [INFO] Batch 90000, worst loss 0.192182 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:07:19,615 [INFO] ---------------------------------
2019-03-22 19:07:34,851 [INFO] ---------------------------------
2019-03-22 19:07:34,852 [INFO] Summary:
2019-03-22 19:07:34,853 [INFO] Batch 91000, worst loss 0.235064 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:07:34,853 [INFO] Regularization: 1853.270508 * 0.0000010000 = 0.0018532705
2019-03-22 19:07:34,854 [INFO] Sum of grad norms: 0.025186
2019-03-22 19:07:34,855 [INFO] ---------------------------------
2019-03-22 19:07:50,074 [INFO] ---------------------------------
2019-03-22 19:07:50,075 [INFO] Summary:
2019-03-22 19:07:50,075 [INFO] Batch 92000, worst loss 0.240186 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:07:50,077 [INFO] Regularization: 1853.212280 * 0.0000010000 = 0.0018532122
2019-03-22 19:07:50,078 [INFO] Sum of grad norms: 10.798218
2019-03-22 19:07:50,079 [INFO] ---------------------------------
2019-03-22 19:08:05,205 [INFO] ---------------------------------
2019-03-22 19:08:05,206 [INFO] Summary:
2019-03-22 19:08:05,206 [INFO] Batch 93000, worst loss 0.235534 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:08:05,207 [INFO] Regularization: 1853.176270 * 0.0000010000 = 0.0018531763
2019-03-22 19:08:05,207 [INFO] Sum of grad norms: 0.006638
2019-03-22 19:08:05,208 [INFO] ---------------------------------
2019-03-22 19:08:20,114 [INFO] ---------------------------------
2019-03-22 19:08:20,115 [INFO] Summary:
2019-03-22 19:08:20,116 [INFO] Batch 94000, worst loss 0.269804 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:08:20,116 [INFO] Regularization: 1853.148682 * 0.0000010000 = 0.0018531487
2019-03-22 19:08:20,117 [INFO] Sum of grad norms: 44.488487
2019-03-22 19:08:20,117 [INFO] ---------------------------------
2019-03-22 19:08:34,056 [INFO] ---------------------------------
2019-03-22 19:08:34,057 [INFO] Summary:
2019-03-22 19:08:34,058 [INFO] Batch 95000, worst loss 0.280063 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:08:34,058 [INFO] Regularization: 1853.115479 * 0.0000010000 = 0.0018531155
2019-03-22 19:08:34,059 [INFO] Sum of grad norms: 0.012848
2019-03-22 19:08:34,059 [INFO] ---------------------------------
2019-03-22 19:08:48,928 [INFO] ---------------------------------
2019-03-22 19:08:48,929 [INFO] Summary:
2019-03-22 19:08:48,930 [INFO] Batch 96000, worst loss 0.223829 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:08:48,930 [INFO] Regularization: 1853.083740 * 0.0000010000 = 0.0018530837
2019-03-22 19:08:48,931 [INFO] Sum of grad norms: 0.017193
2019-03-22 19:08:48,932 [INFO] ---------------------------------
2019-03-22 19:09:03,164 [INFO] ---------------------------------
2019-03-22 19:09:03,165 [INFO] Summary:
2019-03-22 19:09:03,166 [INFO] Batch 97000, worst loss 0.241013 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:09:03,167 [INFO] Regularization: 1853.047241 * 0.0000010000 = 0.0018530473
2019-03-22 19:09:03,167 [INFO] Sum of grad norms: 0.015331
2019-03-22 19:09:03,168 [INFO] ---------------------------------
2019-03-22 19:09:16,931 [INFO] ---------------------------------
2019-03-22 19:09:16,932 [INFO] Summary:
2019-03-22 19:09:16,932 [INFO] Batch 98000, worst loss 0.297463 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:09:16,933 [INFO] Regularization: 1853.008789 * 0.0000010000 = 0.0018530088
2019-03-22 19:09:16,933 [INFO] Sum of grad norms: 0.028131
2019-03-22 19:09:16,934 [INFO] ---------------------------------
2019-03-22 19:09:30,739 [INFO] ---------------------------------
2019-03-22 19:09:30,740 [INFO] Summary:
2019-03-22 19:09:30,740 [INFO] Batch 99000, worst loss 0.185619 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:09:30,741 [INFO] Regularization: 1852.983643 * 0.0000010000 = 0.0018529836
2019-03-22 19:09:30,741 [INFO] Sum of grad norms: 0.022665
2019-03-22 19:09:30,742 [INFO] ---------------------------------
2019-03-22 19:09:44,937 [INFO] ---------------------------------
2019-03-22 19:09:44,938 [INFO] Summary:
2019-03-22 19:09:44,939 [INFO] Batch 100000, worst loss 0.194043 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:09:44,940 [INFO] Regularization: 1852.959106 * 0.0000010000 = 0.0018529590
2019-03-22 19:09:44,940 [INFO] Sum of grad norms: 1.591577
2019-03-22 19:09:44,941 [INFO] ---------------------------------
2019-03-22 19:09:47,694 [INFO] ---------------------------------
2019-03-22 19:09:47,694 [INFO] Evaluation:
2019-03-22 19:09:47,695 [INFO] Batch 100000, worst loss 0.227843 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:09:47,696 [INFO] ---------------------------------
2019-03-22 19:09:47,696 [INFO] Finished training, saved to file classifier/1553275201/1553278187_1_classifier_final.pth
2019-03-22 19:09:47,870 [INFO] ---------------------------------
2019-03-22 19:09:47,872 [INFO] Training model #2: (8, 64, 2) @ 2
2019-03-22 19:10:02,606 [INFO] ---------------------------------
2019-03-22 19:10:02,607 [INFO] Summary:
2019-03-22 19:10:02,608 [INFO] Batch 1000, worst loss 3186.045654 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:10:02,608 [INFO] Regularization: 30885.968750 * 0.0000010000 = 0.0308859684
2019-03-22 19:10:02,609 [INFO] Sum of grad norms: 1320.291138
2019-03-22 19:10:02,609 [INFO] ---------------------------------
2019-03-22 19:10:17,070 [INFO] ---------------------------------
2019-03-22 19:10:17,071 [INFO] Summary:
2019-03-22 19:10:17,072 [INFO] Batch 2000, worst loss 28.285530 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:10:17,073 [INFO] Regularization: 29166.437500 * 0.0000010000 = 0.0291664377
2019-03-22 19:10:17,073 [INFO] Sum of grad norms: 244.653000
2019-03-22 19:10:17,074 [INFO] ---------------------------------
2019-03-22 19:10:32,671 [INFO] ---------------------------------
2019-03-22 19:10:32,672 [INFO] Summary:
2019-03-22 19:10:32,673 [INFO] Batch 3000, worst loss 12.766670 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:10:32,673 [INFO] Regularization: 25894.828125 * 0.0000010000 = 0.0258948281
2019-03-22 19:10:32,674 [INFO] Sum of grad norms: 701.870178
2019-03-22 19:10:32,674 [INFO] ---------------------------------
2019-03-22 19:10:46,750 [INFO] ---------------------------------
2019-03-22 19:10:46,751 [INFO] Summary:
2019-03-22 19:10:46,752 [INFO] Batch 4000, worst loss 6.958928 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:10:46,753 [INFO] Regularization: 21061.029297 * 0.0000010000 = 0.0210610293
2019-03-22 19:10:46,754 [INFO] Sum of grad norms: 214.492966
2019-03-22 19:10:46,754 [INFO] ---------------------------------
2019-03-22 19:11:02,504 [INFO] ---------------------------------
2019-03-22 19:11:02,505 [INFO] Summary:
2019-03-22 19:11:02,506 [INFO] Batch 5000, worst loss 5.859957 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:11:02,507 [INFO] Regularization: 15454.503906 * 0.0000010000 = 0.0154545037
2019-03-22 19:11:02,507 [INFO] Sum of grad norms: 86.610153
2019-03-22 19:11:02,508 [INFO] ---------------------------------
2019-03-22 19:11:17,618 [INFO] ---------------------------------
2019-03-22 19:11:17,619 [INFO] Summary:
2019-03-22 19:11:17,619 [INFO] Batch 6000, worst loss 2.885664 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:11:17,620 [INFO] Regularization: 10555.985352 * 0.0000010000 = 0.0105559854
2019-03-22 19:11:17,620 [INFO] Sum of grad norms: 56.420113
2019-03-22 19:11:17,621 [INFO] ---------------------------------
2019-03-22 19:11:31,952 [INFO] ---------------------------------
2019-03-22 19:11:31,953 [INFO] Summary:
2019-03-22 19:11:31,954 [INFO] Batch 7000, worst loss 2.411102 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:11:31,954 [INFO] Regularization: 8874.352539 * 0.0000010000 = 0.0088743521
2019-03-22 19:11:31,955 [INFO] Sum of grad norms: 129.137756
2019-03-22 19:11:31,955 [INFO] ---------------------------------
2019-03-22 19:11:47,680 [INFO] ---------------------------------
2019-03-22 19:11:47,681 [INFO] Summary:
2019-03-22 19:11:47,682 [INFO] Batch 8000, worst loss 0.973842 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:11:47,683 [INFO] Regularization: 8673.596680 * 0.0000010000 = 0.0086735962
2019-03-22 19:11:47,684 [INFO] Sum of grad norms: 15.170913
2019-03-22 19:11:47,685 [INFO] ---------------------------------
2019-03-22 19:12:02,376 [INFO] ---------------------------------
2019-03-22 19:12:02,376 [INFO] Summary:
2019-03-22 19:12:02,377 [INFO] Batch 9000, worst loss 0.706002 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:12:02,378 [INFO] Regularization: 8403.306641 * 0.0000010000 = 0.0084033068
2019-03-22 19:12:02,378 [INFO] Sum of grad norms: 11.039454
2019-03-22 19:12:02,379 [INFO] ---------------------------------
2019-03-22 19:12:17,045 [INFO] ---------------------------------
2019-03-22 19:12:17,046 [INFO] Summary:
2019-03-22 19:12:17,046 [INFO] Batch 10000, worst loss 0.610181 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:12:17,047 [INFO] Regularization: 8084.454590 * 0.0000010000 = 0.0080844546
2019-03-22 19:12:17,047 [INFO] Sum of grad norms: 13.887471
2019-03-22 19:12:17,048 [INFO] ---------------------------------
2019-03-22 19:12:19,750 [INFO] ---------------------------------
2019-03-22 19:12:19,751 [INFO] Evaluation:
2019-03-22 19:12:19,751 [INFO] Batch 10000, worst loss 0.309633 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:12:19,752 [INFO] ---------------------------------
2019-03-22 19:12:33,889 [INFO] ---------------------------------
2019-03-22 19:12:33,890 [INFO] Summary:
2019-03-22 19:12:33,890 [INFO] Batch 11000, worst loss 0.531233 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:12:33,891 [INFO] Regularization: 7752.068359 * 0.0000010000 = 0.0077520683
2019-03-22 19:12:33,891 [INFO] Sum of grad norms: 14.205302
2019-03-22 19:12:33,892 [INFO] ---------------------------------
2019-03-22 19:12:47,822 [INFO] ---------------------------------
2019-03-22 19:12:47,823 [INFO] Summary:
2019-03-22 19:12:47,824 [INFO] Batch 12000, worst loss 0.370609 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:12:47,824 [INFO] Regularization: 7390.417480 * 0.0000010000 = 0.0073904176
2019-03-22 19:12:47,825 [INFO] Sum of grad norms: 2.051236
2019-03-22 19:12:47,825 [INFO] ---------------------------------
2019-03-22 19:13:02,185 [INFO] ---------------------------------
2019-03-22 19:13:02,186 [INFO] Summary:
2019-03-22 19:13:02,186 [INFO] Batch 13000, worst loss 0.332717 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:13:02,187 [INFO] Regularization: 7004.645508 * 0.0000010000 = 0.0070046457
2019-03-22 19:13:02,187 [INFO] Sum of grad norms: 13.773737
2019-03-22 19:13:02,188 [INFO] ---------------------------------
2019-03-22 19:13:16,444 [INFO] ---------------------------------
2019-03-22 19:13:16,445 [INFO] Summary:
2019-03-22 19:13:16,445 [INFO] Batch 14000, worst loss 0.361031 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:13:16,446 [INFO] Regularization: 6445.527344 * 0.0000010000 = 0.0064455275
2019-03-22 19:13:16,446 [INFO] Sum of grad norms: 17.121347
2019-03-22 19:13:16,447 [INFO] ---------------------------------
2019-03-22 19:13:31,048 [INFO] ---------------------------------
2019-03-22 19:13:31,049 [INFO] Summary:
2019-03-22 19:13:31,050 [INFO] Batch 15000, worst loss 0.309583 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:13:31,050 [INFO] Regularization: 6005.082520 * 0.0000010000 = 0.0060050823
2019-03-22 19:13:31,051 [INFO] Sum of grad norms: 3.557840
2019-03-22 19:13:31,051 [INFO] ---------------------------------
2019-03-22 19:13:45,417 [INFO] ---------------------------------
2019-03-22 19:13:45,418 [INFO] Summary:
2019-03-22 19:13:45,419 [INFO] Batch 16000, worst loss 0.337481 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:13:45,419 [INFO] Regularization: 5597.416016 * 0.0000010000 = 0.0055974158
2019-03-22 19:13:45,420 [INFO] Sum of grad norms: 13.649455
2019-03-22 19:13:45,420 [INFO] ---------------------------------
2019-03-22 19:14:00,091 [INFO] ---------------------------------
2019-03-22 19:14:00,092 [INFO] Summary:
2019-03-22 19:14:00,092 [INFO] Batch 17000, worst loss 0.360830 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:14:00,093 [INFO] Regularization: 5160.708984 * 0.0000010000 = 0.0051607089
2019-03-22 19:14:00,094 [INFO] Sum of grad norms: 10.447445
2019-03-22 19:14:00,094 [INFO] ---------------------------------
2019-03-22 19:14:14,787 [INFO] ---------------------------------
2019-03-22 19:14:14,788 [INFO] Summary:
2019-03-22 19:14:14,789 [INFO] Batch 18000, worst loss 0.368312 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:14:14,789 [INFO] Regularization: 4829.738281 * 0.0000010000 = 0.0048297383
2019-03-22 19:14:14,790 [INFO] Sum of grad norms: 0.410286
2019-03-22 19:14:14,791 [INFO] ---------------------------------
2019-03-22 19:14:29,794 [INFO] ---------------------------------
2019-03-22 19:14:29,795 [INFO] Summary:
2019-03-22 19:14:29,796 [INFO] Batch 19000, worst loss 0.372534 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:14:29,796 [INFO] Regularization: 4391.181641 * 0.0000010000 = 0.0043911817
2019-03-22 19:14:29,797 [INFO] Sum of grad norms: 8.295196
2019-03-22 19:14:29,797 [INFO] ---------------------------------
2019-03-22 19:14:44,298 [INFO] ---------------------------------
2019-03-22 19:14:44,299 [INFO] Summary:
2019-03-22 19:14:44,299 [INFO] Batch 20000, worst loss 0.358790 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:14:44,300 [INFO] Regularization: 4117.430664 * 0.0000010000 = 0.0041174307
2019-03-22 19:14:44,300 [INFO] Sum of grad norms: 2.580933
2019-03-22 19:14:44,301 [INFO] ---------------------------------
2019-03-22 19:14:47,090 [INFO] ---------------------------------
2019-03-22 19:14:47,090 [INFO] Evaluation:
2019-03-22 19:14:47,091 [INFO] Batch 20000, worst loss 0.268166 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:14:47,092 [INFO] ---------------------------------
2019-03-22 19:15:02,026 [INFO] ---------------------------------
2019-03-22 19:15:02,026 [INFO] Summary:
2019-03-22 19:15:02,027 [INFO] Batch 21000, worst loss 0.324329 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:15:02,028 [INFO] Regularization: 3874.307617 * 0.0000010000 = 0.0038743075
2019-03-22 19:15:02,029 [INFO] Sum of grad norms: 13.152486
2019-03-22 19:15:02,030 [INFO] ---------------------------------
2019-03-22 19:15:17,226 [INFO] ---------------------------------
2019-03-22 19:15:17,227 [INFO] Summary:
2019-03-22 19:15:17,228 [INFO] Batch 22000, worst loss 0.338183 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:15:17,228 [INFO] Regularization: 3624.907227 * 0.0000010000 = 0.0036249072
2019-03-22 19:15:17,229 [INFO] Sum of grad norms: 1.460185
2019-03-22 19:15:17,229 [INFO] ---------------------------------
2019-03-22 19:15:31,644 [INFO] ---------------------------------
2019-03-22 19:15:31,645 [INFO] Summary:
2019-03-22 19:15:31,646 [INFO] Batch 23000, worst loss 0.402534 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:15:31,646 [INFO] Regularization: 3427.515137 * 0.0000010000 = 0.0034275150
2019-03-22 19:15:31,647 [INFO] Sum of grad norms: 5.532642
2019-03-22 19:15:31,647 [INFO] ---------------------------------
2019-03-22 19:15:47,019 [INFO] ---------------------------------
2019-03-22 19:15:47,020 [INFO] Summary:
2019-03-22 19:15:47,021 [INFO] Batch 24000, worst loss 0.364991 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:15:47,022 [INFO] Regularization: 3263.279541 * 0.0000010000 = 0.0032632796
2019-03-22 19:15:47,022 [INFO] Sum of grad norms: 2.506638
2019-03-22 19:15:47,023 [INFO] ---------------------------------
2019-03-22 19:16:01,845 [INFO] ---------------------------------
2019-03-22 19:16:01,846 [INFO] Summary:
2019-03-22 19:16:01,846 [INFO] Batch 25000, worst loss 0.287837 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:16:01,847 [INFO] Regularization: 3120.816650 * 0.0000010000 = 0.0031208165
2019-03-22 19:16:01,847 [INFO] Sum of grad norms: 9.084986
2019-03-22 19:16:01,848 [INFO] ---------------------------------
2019-03-22 19:16:16,662 [INFO] ---------------------------------
2019-03-22 19:16:16,663 [INFO] Summary:
2019-03-22 19:16:16,663 [INFO] Batch 26000, worst loss 0.390011 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:16:16,664 [INFO] Regularization: 2988.990234 * 0.0000010000 = 0.0029889902
2019-03-22 19:16:16,664 [INFO] Sum of grad norms: 15.851924
2019-03-22 19:16:16,665 [INFO] ---------------------------------
2019-03-22 19:16:31,526 [INFO] ---------------------------------
2019-03-22 19:16:31,526 [INFO] Summary:
2019-03-22 19:16:31,527 [INFO] Batch 27000, worst loss 0.322203 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:16:31,527 [INFO] Regularization: 2852.289307 * 0.0000010000 = 0.0028522892
2019-03-22 19:16:31,528 [INFO] Sum of grad norms: 0.735773
2019-03-22 19:16:31,529 [INFO] ---------------------------------
2019-03-22 19:16:45,942 [INFO] ---------------------------------
2019-03-22 19:16:45,943 [INFO] Summary:
2019-03-22 19:16:45,944 [INFO] Batch 28000, worst loss 0.337864 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:16:45,944 [INFO] Regularization: 2741.923828 * 0.0000010000 = 0.0027419238
2019-03-22 19:16:45,945 [INFO] Sum of grad norms: 6.346850
2019-03-22 19:16:45,945 [INFO] ---------------------------------
2019-03-22 19:17:00,465 [INFO] ---------------------------------
2019-03-22 19:17:00,466 [INFO] Summary:
2019-03-22 19:17:00,467 [INFO] Batch 29000, worst loss 0.322550 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:17:00,468 [INFO] Regularization: 2711.738037 * 0.0000010000 = 0.0027117380
2019-03-22 19:17:00,468 [INFO] Sum of grad norms: 0.195087
2019-03-22 19:17:00,469 [INFO] ---------------------------------
2019-03-22 19:17:15,301 [INFO] ---------------------------------
2019-03-22 19:17:15,302 [INFO] Summary:
2019-03-22 19:17:15,302 [INFO] Batch 30000, worst loss 0.363005 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:17:15,303 [INFO] Regularization: 2574.750244 * 0.0000010000 = 0.0025747502
2019-03-22 19:17:15,303 [INFO] Sum of grad norms: 0.062250
2019-03-22 19:17:15,304 [INFO] ---------------------------------
2019-03-22 19:17:18,027 [INFO] ---------------------------------
2019-03-22 19:17:18,028 [INFO] Evaluation:
2019-03-22 19:17:18,029 [INFO] Batch 30000, worst loss 0.257162 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:17:18,029 [INFO] ---------------------------------
2019-03-22 19:17:32,784 [INFO] ---------------------------------
2019-03-22 19:17:32,785 [INFO] Summary:
2019-03-22 19:17:32,786 [INFO] Batch 31000, worst loss 0.602514 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:17:32,786 [INFO] Regularization: 2584.482422 * 0.0000010000 = 0.0025844823
2019-03-22 19:17:32,787 [INFO] Sum of grad norms: 9.888930
2019-03-22 19:17:32,788 [INFO] ---------------------------------
2019-03-22 19:17:48,537 [INFO] ---------------------------------
2019-03-22 19:17:48,538 [INFO] Summary:
2019-03-22 19:17:48,538 [INFO] Batch 32000, worst loss 0.422164 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:17:48,539 [INFO] Regularization: 2553.134766 * 0.0000010000 = 0.0025531347
2019-03-22 19:17:48,539 [INFO] Sum of grad norms: 17.074062
2019-03-22 19:17:48,540 [INFO] ---------------------------------
2019-03-22 19:18:02,584 [INFO] ---------------------------------
2019-03-22 19:18:02,585 [INFO] Summary:
2019-03-22 19:18:02,586 [INFO] Batch 33000, worst loss 0.362523 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:18:02,586 [INFO] Regularization: 2473.164062 * 0.0000010000 = 0.0024731641
2019-03-22 19:18:02,587 [INFO] Sum of grad norms: 8.177199
2019-03-22 19:18:02,587 [INFO] ---------------------------------
2019-03-22 19:18:17,781 [INFO] ---------------------------------
2019-03-22 19:18:17,782 [INFO] Summary:
2019-03-22 19:18:17,783 [INFO] Batch 34000, worst loss 0.354442 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:18:17,783 [INFO] Regularization: 2453.333740 * 0.0000010000 = 0.0024533337
2019-03-22 19:18:17,784 [INFO] Sum of grad norms: 10.130574
2019-03-22 19:18:17,785 [INFO] ---------------------------------
2019-03-22 19:18:32,146 [INFO] ---------------------------------
2019-03-22 19:18:32,147 [INFO] Summary:
2019-03-22 19:18:32,148 [INFO] Batch 35000, worst loss 0.335272 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:18:32,148 [INFO] Regularization: 2379.244141 * 0.0000010000 = 0.0023792442
2019-03-22 19:18:32,149 [INFO] Sum of grad norms: 5.165554
2019-03-22 19:18:32,150 [INFO] ---------------------------------
2019-03-22 19:18:46,392 [INFO] ---------------------------------
2019-03-22 19:18:46,392 [INFO] Summary:
2019-03-22 19:18:46,393 [INFO] Batch 36000, worst loss 0.312415 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:18:46,394 [INFO] Regularization: 2311.774902 * 0.0000010000 = 0.0023117750
2019-03-22 19:18:46,394 [INFO] Sum of grad norms: 0.076249
2019-03-22 19:18:46,395 [INFO] ---------------------------------
2019-03-22 19:19:01,203 [INFO] ---------------------------------
2019-03-22 19:19:01,204 [INFO] Summary:
2019-03-22 19:19:01,205 [INFO] Batch 37000, worst loss 0.377244 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:19:01,205 [INFO] Regularization: 2249.974609 * 0.0000010000 = 0.0022499745
2019-03-22 19:19:01,206 [INFO] Sum of grad norms: 5.524998
2019-03-22 19:19:01,206 [INFO] ---------------------------------
2019-03-22 19:19:15,553 [INFO] ---------------------------------
2019-03-22 19:19:15,554 [INFO] Summary:
2019-03-22 19:19:15,554 [INFO] Batch 38000, worst loss 0.475670 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:19:15,555 [INFO] Regularization: 2174.978027 * 0.0000010000 = 0.0021749779
2019-03-22 19:19:15,555 [INFO] Sum of grad norms: 10.127043
2019-03-22 19:19:15,556 [INFO] ---------------------------------
2019-03-22 19:19:30,835 [INFO] ---------------------------------
2019-03-22 19:19:30,836 [INFO] Summary:
2019-03-22 19:19:30,837 [INFO] Batch 39000, worst loss 0.464268 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:19:30,837 [INFO] Regularization: 2096.898926 * 0.0000010000 = 0.0020968989
2019-03-22 19:19:30,838 [INFO] Sum of grad norms: 3.626250
2019-03-22 19:19:30,838 [INFO] ---------------------------------
2019-03-22 19:19:45,549 [INFO] ---------------------------------
2019-03-22 19:19:45,550 [INFO] Summary:
2019-03-22 19:19:45,551 [INFO] Batch 40000, worst loss 0.354458 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:19:45,552 [INFO] Regularization: 2004.068115 * 0.0000010000 = 0.0020040681
2019-03-22 19:19:45,552 [INFO] Sum of grad norms: 6.482613
2019-03-22 19:19:45,553 [INFO] ---------------------------------
2019-03-22 19:19:48,297 [INFO] ---------------------------------
2019-03-22 19:19:48,298 [INFO] Evaluation:
2019-03-22 19:19:48,299 [INFO] Batch 40000, worst loss 0.285152 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:19:48,299 [INFO] ---------------------------------
2019-03-22 19:20:02,596 [INFO] ---------------------------------
2019-03-22 19:20:02,597 [INFO] Summary:
2019-03-22 19:20:02,597 [INFO] Batch 41000, worst loss 0.498964 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:20:02,598 [INFO] Regularization: 1932.809570 * 0.0000010000 = 0.0019328096
2019-03-22 19:20:02,598 [INFO] Sum of grad norms: 0.139811
2019-03-22 19:20:02,599 [INFO] ---------------------------------
2019-03-22 19:20:17,413 [INFO] ---------------------------------
2019-03-22 19:20:17,414 [INFO] Summary:
2019-03-22 19:20:17,415 [INFO] Batch 42000, worst loss 0.245505 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:20:17,416 [INFO] Regularization: 1866.339966 * 0.0000010000 = 0.0018663399
2019-03-22 19:20:17,416 [INFO] Sum of grad norms: 14.393897
2019-03-22 19:20:17,417 [INFO] ---------------------------------
2019-03-22 19:20:32,286 [INFO] ---------------------------------
2019-03-22 19:20:32,287 [INFO] Summary:
2019-03-22 19:20:32,288 [INFO] Batch 43000, worst loss 0.245165 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:20:32,288 [INFO] Regularization: 1813.967163 * 0.0000010000 = 0.0018139671
2019-03-22 19:20:32,289 [INFO] Sum of grad norms: 0.273556
2019-03-22 19:20:32,289 [INFO] ---------------------------------
2019-03-22 19:20:46,822 [INFO] ---------------------------------
2019-03-22 19:20:46,823 [INFO] Summary:
2019-03-22 19:20:46,824 [INFO] Batch 44000, worst loss 0.243765 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:20:46,825 [INFO] Regularization: 1765.297974 * 0.0000010000 = 0.0017652980
2019-03-22 19:20:46,825 [INFO] Sum of grad norms: 1.183998
2019-03-22 19:20:46,826 [INFO] ---------------------------------
2019-03-22 19:21:00,816 [INFO] ---------------------------------
2019-03-22 19:21:00,817 [INFO] Summary:
2019-03-22 19:21:00,818 [INFO] Batch 45000, worst loss 0.229203 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:21:00,819 [INFO] Regularization: 1711.441162 * 0.0000010000 = 0.0017114412
2019-03-22 19:21:00,820 [INFO] Sum of grad norms: 10.510890
2019-03-22 19:21:00,820 [INFO] ---------------------------------
2019-03-22 19:21:14,684 [INFO] ---------------------------------
2019-03-22 19:21:14,685 [INFO] Summary:
2019-03-22 19:21:14,685 [INFO] Batch 46000, worst loss 0.313187 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:21:14,686 [INFO] Regularization: 1663.386475 * 0.0000010000 = 0.0016633865
2019-03-22 19:21:14,686 [INFO] Sum of grad norms: 13.068956
2019-03-22 19:21:14,687 [INFO] ---------------------------------
2019-03-22 19:21:28,314 [INFO] ---------------------------------
2019-03-22 19:21:28,315 [INFO] Summary:
2019-03-22 19:21:28,315 [INFO] Batch 47000, worst loss 0.250808 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:21:28,316 [INFO] Regularization: 1616.285034 * 0.0000010000 = 0.0016162851
2019-03-22 19:21:28,316 [INFO] Sum of grad norms: 0.071847
2019-03-22 19:21:28,317 [INFO] ---------------------------------
2019-03-22 19:21:42,800 [INFO] ---------------------------------
2019-03-22 19:21:42,801 [INFO] Summary:
2019-03-22 19:21:42,801 [INFO] Batch 48000, worst loss 0.254870 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:21:42,802 [INFO] Regularization: 1579.051880 * 0.0000010000 = 0.0015790518
2019-03-22 19:21:42,803 [INFO] Sum of grad norms: 5.521884
2019-03-22 19:21:42,803 [INFO] ---------------------------------
2019-03-22 19:21:57,529 [INFO] ---------------------------------
2019-03-22 19:21:57,530 [INFO] Summary:
2019-03-22 19:21:57,531 [INFO] Batch 49000, worst loss 0.276033 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:21:57,532 [INFO] Regularization: 1547.409058 * 0.0000010000 = 0.0015474091
2019-03-22 19:21:57,532 [INFO] Sum of grad norms: 0.389723
2019-03-22 19:21:57,533 [INFO] ---------------------------------
2019-03-22 19:22:11,793 [INFO] ---------------------------------
2019-03-22 19:22:11,794 [INFO] Summary:
2019-03-22 19:22:11,795 [INFO] Batch 50000, worst loss 0.344521 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:22:11,795 [INFO] Regularization: 1522.884033 * 0.0000010000 = 0.0015228840
2019-03-22 19:22:11,796 [INFO] Sum of grad norms: 0.059910
2019-03-22 19:22:11,796 [INFO] ---------------------------------
2019-03-22 19:22:14,496 [INFO] ---------------------------------
2019-03-22 19:22:14,497 [INFO] Evaluation:
2019-03-22 19:22:14,498 [INFO] Batch 50000, worst loss 0.190947 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:22:14,499 [INFO] ---------------------------------
2019-03-22 19:22:28,959 [INFO] ---------------------------------
2019-03-22 19:22:28,960 [INFO] Summary:
2019-03-22 19:22:28,961 [INFO] Batch 51000, worst loss 0.242677 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:22:28,961 [INFO] Regularization: 1500.675415 * 0.0000010000 = 0.0015006755
2019-03-22 19:22:28,962 [INFO] Sum of grad norms: 0.349515
2019-03-22 19:22:28,962 [INFO] ---------------------------------
2019-03-22 19:22:42,597 [INFO] ---------------------------------
2019-03-22 19:22:42,598 [INFO] Summary:
2019-03-22 19:22:42,599 [INFO] Batch 52000, worst loss 0.248179 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:22:42,599 [INFO] Regularization: 1486.334473 * 0.0000010000 = 0.0014863345
2019-03-22 19:22:42,600 [INFO] Sum of grad norms: 15.528564
2019-03-22 19:22:42,600 [INFO] ---------------------------------
2019-03-22 19:22:56,504 [INFO] ---------------------------------
2019-03-22 19:22:56,505 [INFO] Summary:
2019-03-22 19:22:56,505 [INFO] Batch 53000, worst loss 0.176907 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:22:56,506 [INFO] Regularization: 1476.176514 * 0.0000010000 = 0.0014761765
2019-03-22 19:22:56,506 [INFO] Sum of grad norms: 0.018161
2019-03-22 19:22:56,507 [INFO] ---------------------------------
2019-03-22 19:23:10,736 [INFO] ---------------------------------
2019-03-22 19:23:10,737 [INFO] Summary:
2019-03-22 19:23:10,737 [INFO] Batch 54000, worst loss 0.185117 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:23:10,738 [INFO] Regularization: 1468.595337 * 0.0000010000 = 0.0014685954
2019-03-22 19:23:10,738 [INFO] Sum of grad norms: 22.842461
2019-03-22 19:23:10,739 [INFO] ---------------------------------
2019-03-22 19:23:25,067 [INFO] ---------------------------------
2019-03-22 19:23:25,068 [INFO] Summary:
2019-03-22 19:23:25,068 [INFO] Batch 55000, worst loss 0.226324 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:23:25,069 [INFO] Regularization: 1461.751343 * 0.0000010000 = 0.0014617513
2019-03-22 19:23:25,069 [INFO] Sum of grad norms: 0.010731
2019-03-22 19:23:25,070 [INFO] ---------------------------------
2019-03-22 19:23:40,329 [INFO] ---------------------------------
2019-03-22 19:23:40,330 [INFO] Summary:
2019-03-22 19:23:40,331 [INFO] Batch 56000, worst loss 0.221320 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:23:40,331 [INFO] Regularization: 1454.235474 * 0.0000010000 = 0.0014542355
2019-03-22 19:23:40,332 [INFO] Sum of grad norms: 0.006467
2019-03-22 19:23:40,332 [INFO] ---------------------------------
2019-03-22 19:23:54,758 [INFO] ---------------------------------
2019-03-22 19:23:54,759 [INFO] Summary:
2019-03-22 19:23:54,760 [INFO] Batch 57000, worst loss 0.331338 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:23:54,760 [INFO] Regularization: 1446.434692 * 0.0000010000 = 0.0014464346
2019-03-22 19:23:54,761 [INFO] Sum of grad norms: 3.875007
2019-03-22 19:23:54,761 [INFO] ---------------------------------
2019-03-22 19:24:08,428 [INFO] ---------------------------------
2019-03-22 19:24:08,429 [INFO] Summary:
2019-03-22 19:24:08,429 [INFO] Batch 58000, worst loss 0.190170 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:24:08,430 [INFO] Regularization: 1440.350830 * 0.0000010000 = 0.0014403508
2019-03-22 19:24:08,430 [INFO] Sum of grad norms: 43.366959
2019-03-22 19:24:08,431 [INFO] ---------------------------------
2019-03-22 19:24:23,103 [INFO] ---------------------------------
2019-03-22 19:24:23,105 [INFO] Summary:
2019-03-22 19:24:23,105 [INFO] Batch 59000, worst loss 0.257221 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:24:23,106 [INFO] Regularization: 1435.468262 * 0.0000010000 = 0.0014354682
2019-03-22 19:24:23,106 [INFO] Sum of grad norms: 0.012167
2019-03-22 19:24:23,107 [INFO] ---------------------------------
2019-03-22 19:24:37,688 [INFO] ---------------------------------
2019-03-22 19:24:37,689 [INFO] Summary:
2019-03-22 19:24:37,689 [INFO] Batch 60000, worst loss 0.224062 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:24:37,690 [INFO] Regularization: 1428.856689 * 0.0000010000 = 0.0014288566
2019-03-22 19:24:37,690 [INFO] Sum of grad norms: 0.008567
2019-03-22 19:24:37,691 [INFO] ---------------------------------
2019-03-22 19:24:40,501 [INFO] ---------------------------------
2019-03-22 19:24:40,502 [INFO] Evaluation:
2019-03-22 19:24:40,506 [INFO] Batch 60000, worst loss 0.221534 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:24:40,507 [INFO] ---------------------------------
2019-03-22 19:24:55,716 [INFO] ---------------------------------
2019-03-22 19:24:55,717 [INFO] Summary:
2019-03-22 19:24:55,718 [INFO] Batch 61000, worst loss 0.282322 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:24:55,719 [INFO] Regularization: 1420.606934 * 0.0000010000 = 0.0014206070
2019-03-22 19:24:55,719 [INFO] Sum of grad norms: 0.005978
2019-03-22 19:24:55,720 [INFO] ---------------------------------
2019-03-22 19:25:10,532 [INFO] ---------------------------------
2019-03-22 19:25:10,533 [INFO] Summary:
2019-03-22 19:25:10,533 [INFO] Batch 62000, worst loss 0.201973 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:25:10,534 [INFO] Regularization: 1417.581299 * 0.0000010000 = 0.0014175813
2019-03-22 19:25:10,534 [INFO] Sum of grad norms: 0.008343
2019-03-22 19:25:10,535 [INFO] ---------------------------------
2019-03-22 19:25:25,869 [INFO] ---------------------------------
2019-03-22 19:25:25,870 [INFO] Summary:
2019-03-22 19:25:25,870 [INFO] Batch 63000, worst loss 0.197820 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:25:25,871 [INFO] Regularization: 1415.885376 * 0.0000010000 = 0.0014158854
2019-03-22 19:25:25,871 [INFO] Sum of grad norms: 0.253997
2019-03-22 19:25:25,872 [INFO] ---------------------------------
2019-03-22 19:25:39,898 [INFO] ---------------------------------
2019-03-22 19:25:39,899 [INFO] Summary:
2019-03-22 19:25:39,899 [INFO] Batch 64000, worst loss 0.195989 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:25:39,900 [INFO] Regularization: 1414.079712 * 0.0000010000 = 0.0014140797
2019-03-22 19:25:39,900 [INFO] Sum of grad norms: 0.048388
2019-03-22 19:25:39,901 [INFO] ---------------------------------
2019-03-22 19:25:53,613 [INFO] ---------------------------------
2019-03-22 19:25:53,614 [INFO] Summary:
2019-03-22 19:25:53,614 [INFO] Batch 65000, worst loss 0.211995 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:25:53,615 [INFO] Regularization: 1412.657227 * 0.0000010000 = 0.0014126572
2019-03-22 19:25:53,615 [INFO] Sum of grad norms: 0.115082
2019-03-22 19:25:53,616 [INFO] ---------------------------------
2019-03-22 19:26:07,743 [INFO] ---------------------------------
2019-03-22 19:26:07,744 [INFO] Summary:
2019-03-22 19:26:07,745 [INFO] Batch 66000, worst loss 0.246016 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:26:07,745 [INFO] Regularization: 1410.789551 * 0.0000010000 = 0.0014107896
2019-03-22 19:26:07,746 [INFO] Sum of grad norms: 0.849579
2019-03-22 19:26:07,746 [INFO] ---------------------------------
2019-03-22 19:26:21,578 [INFO] ---------------------------------
2019-03-22 19:26:21,579 [INFO] Summary:
2019-03-22 19:26:21,580 [INFO] Batch 67000, worst loss 0.252815 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:26:21,580 [INFO] Regularization: 1408.910522 * 0.0000010000 = 0.0014089105
2019-03-22 19:26:21,581 [INFO] Sum of grad norms: 0.017945
2019-03-22 19:26:21,582 [INFO] ---------------------------------
2019-03-22 19:26:35,928 [INFO] ---------------------------------
2019-03-22 19:26:35,929 [INFO] Summary:
2019-03-22 19:26:35,929 [INFO] Batch 68000, worst loss 0.211483 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:26:35,930 [INFO] Regularization: 1406.873535 * 0.0000010000 = 0.0014068736
2019-03-22 19:26:35,931 [INFO] Sum of grad norms: 0.018250
2019-03-22 19:26:35,931 [INFO] ---------------------------------
2019-03-22 19:26:50,895 [INFO] ---------------------------------
2019-03-22 19:26:50,896 [INFO] Summary:
2019-03-22 19:26:50,897 [INFO] Batch 69000, worst loss 0.284790 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:26:50,897 [INFO] Regularization: 1405.276733 * 0.0000010000 = 0.0014052767
2019-03-22 19:26:50,898 [INFO] Sum of grad norms: 13.695189
2019-03-22 19:26:50,898 [INFO] ---------------------------------
2019-03-22 19:27:05,463 [INFO] ---------------------------------
2019-03-22 19:27:05,464 [INFO] Summary:
2019-03-22 19:27:05,464 [INFO] Batch 70000, worst loss 0.221699 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:27:05,465 [INFO] Regularization: 1403.167603 * 0.0000010000 = 0.0014031676
2019-03-22 19:27:05,465 [INFO] Sum of grad norms: 0.012522
2019-03-22 19:27:05,466 [INFO] ---------------------------------
2019-03-22 19:27:08,221 [INFO] ---------------------------------
2019-03-22 19:27:08,222 [INFO] Evaluation:
2019-03-22 19:27:08,223 [INFO] Batch 70000, worst loss 0.228460 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:27:08,224 [INFO] ---------------------------------
2019-03-22 19:27:22,677 [INFO] ---------------------------------
2019-03-22 19:27:22,677 [INFO] Summary:
2019-03-22 19:27:22,678 [INFO] Batch 71000, worst loss 0.234218 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:27:22,678 [INFO] Regularization: 1401.571655 * 0.0000010000 = 0.0014015717
2019-03-22 19:27:22,679 [INFO] Sum of grad norms: 0.010755
2019-03-22 19:27:22,680 [INFO] ---------------------------------
2019-03-22 19:27:37,936 [INFO] ---------------------------------
2019-03-22 19:27:37,937 [INFO] Summary:
2019-03-22 19:27:37,938 [INFO] Batch 72000, worst loss 0.170673 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:27:37,938 [INFO] Regularization: 1400.870361 * 0.0000010000 = 0.0014008704
2019-03-22 19:27:37,939 [INFO] Sum of grad norms: 7.030047
2019-03-22 19:27:37,940 [INFO] ---------------------------------
2019-03-22 19:27:52,008 [INFO] ---------------------------------
2019-03-22 19:27:52,009 [INFO] Summary:
2019-03-22 19:27:52,009 [INFO] Batch 73000, worst loss 0.176185 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:27:52,010 [INFO] Regularization: 1400.277710 * 0.0000010000 = 0.0014002777
2019-03-22 19:27:52,010 [INFO] Sum of grad norms: 25.753017
2019-03-22 19:27:52,011 [INFO] ---------------------------------
2019-03-22 19:28:05,920 [INFO] ---------------------------------
2019-03-22 19:28:05,921 [INFO] Summary:
2019-03-22 19:28:05,922 [INFO] Batch 74000, worst loss 0.242991 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:28:05,922 [INFO] Regularization: 1399.906860 * 0.0000010000 = 0.0013999068
2019-03-22 19:28:05,923 [INFO] Sum of grad norms: 0.016623
2019-03-22 19:28:05,923 [INFO] ---------------------------------
2019-03-22 19:28:20,401 [INFO] ---------------------------------
2019-03-22 19:28:20,402 [INFO] Summary:
2019-03-22 19:28:20,402 [INFO] Batch 75000, worst loss 0.185032 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:28:20,403 [INFO] Regularization: 1399.540283 * 0.0000010000 = 0.0013995402
2019-03-22 19:28:20,403 [INFO] Sum of grad norms: 3.307972
2019-03-22 19:28:20,404 [INFO] ---------------------------------
2019-03-22 19:28:35,205 [INFO] ---------------------------------
2019-03-22 19:28:35,206 [INFO] Summary:
2019-03-22 19:28:35,206 [INFO] Batch 76000, worst loss 0.204389 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:28:35,207 [INFO] Regularization: 1398.919312 * 0.0000010000 = 0.0013989193
2019-03-22 19:28:35,207 [INFO] Sum of grad norms: 0.076492
2019-03-22 19:28:35,208 [INFO] ---------------------------------
2019-03-22 19:28:49,404 [INFO] ---------------------------------
2019-03-22 19:28:49,405 [INFO] Summary:
2019-03-22 19:28:49,405 [INFO] Batch 77000, worst loss 0.215016 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:28:49,406 [INFO] Regularization: 1398.462036 * 0.0000010000 = 0.0013984620
2019-03-22 19:28:49,407 [INFO] Sum of grad norms: 13.956409
2019-03-22 19:28:49,407 [INFO] ---------------------------------
2019-03-22 19:29:03,432 [INFO] ---------------------------------
2019-03-22 19:29:03,432 [INFO] Summary:
2019-03-22 19:29:03,433 [INFO] Batch 78000, worst loss 0.289739 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:29:03,434 [INFO] Regularization: 1398.133057 * 0.0000010000 = 0.0013981330
2019-03-22 19:29:03,434 [INFO] Sum of grad norms: 10.681126
2019-03-22 19:29:03,435 [INFO] ---------------------------------
2019-03-22 19:29:17,038 [INFO] ---------------------------------
2019-03-22 19:29:17,039 [INFO] Summary:
2019-03-22 19:29:17,039 [INFO] Batch 79000, worst loss 0.235617 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:29:17,040 [INFO] Regularization: 1397.549194 * 0.0000010000 = 0.0013975492
2019-03-22 19:29:17,040 [INFO] Sum of grad norms: 9.945908
2019-03-22 19:29:17,041 [INFO] ---------------------------------
2019-03-22 19:29:31,206 [INFO] ---------------------------------
2019-03-22 19:29:31,207 [INFO] Summary:
2019-03-22 19:29:31,207 [INFO] Batch 80000, worst loss 0.239885 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:29:31,208 [INFO] Regularization: 1397.080200 * 0.0000010000 = 0.0013970803
2019-03-22 19:29:31,209 [INFO] Sum of grad norms: 0.104011
2019-03-22 19:29:31,209 [INFO] ---------------------------------
2019-03-22 19:29:34,002 [INFO] ---------------------------------
2019-03-22 19:29:34,003 [INFO] Evaluation:
2019-03-22 19:29:34,004 [INFO] Batch 80000, worst loss 0.216282 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:29:34,005 [INFO] ---------------------------------
2019-03-22 19:29:48,458 [INFO] ---------------------------------
2019-03-22 19:29:48,459 [INFO] Summary:
2019-03-22 19:29:48,459 [INFO] Batch 81000, worst loss 0.294592 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:29:48,460 [INFO] Regularization: 1396.623657 * 0.0000010000 = 0.0013966237
2019-03-22 19:29:48,461 [INFO] Sum of grad norms: 0.753983
2019-03-22 19:29:48,461 [INFO] ---------------------------------
2019-03-22 19:30:02,675 [INFO] ---------------------------------
2019-03-22 19:30:02,676 [INFO] Summary:
2019-03-22 19:30:02,677 [INFO] Batch 82000, worst loss 0.293337 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:30:02,677 [INFO] Regularization: 1396.478394 * 0.0000010000 = 0.0013964784
2019-03-22 19:30:02,678 [INFO] Sum of grad norms: 0.004097
2019-03-22 19:30:02,678 [INFO] ---------------------------------
2019-03-22 19:30:17,001 [INFO] ---------------------------------
2019-03-22 19:30:17,002 [INFO] Summary:
2019-03-22 19:30:17,003 [INFO] Batch 83000, worst loss 0.209367 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:30:17,003 [INFO] Regularization: 1396.323975 * 0.0000010000 = 0.0013963240
2019-03-22 19:30:17,004 [INFO] Sum of grad norms: 0.973981
2019-03-22 19:30:17,004 [INFO] ---------------------------------
2019-03-22 19:30:31,317 [INFO] ---------------------------------
2019-03-22 19:30:31,318 [INFO] Summary:
2019-03-22 19:30:31,319 [INFO] Batch 84000, worst loss 0.276560 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:30:31,319 [INFO] Regularization: 1396.201294 * 0.0000010000 = 0.0013962013
2019-03-22 19:30:31,320 [INFO] Sum of grad norms: 4.798535
2019-03-22 19:30:31,320 [INFO] ---------------------------------
2019-03-22 19:30:45,752 [INFO] ---------------------------------
2019-03-22 19:30:45,753 [INFO] Summary:
2019-03-22 19:30:45,754 [INFO] Batch 85000, worst loss 0.180679 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:30:45,754 [INFO] Regularization: 1396.045776 * 0.0000010000 = 0.0013960458
2019-03-22 19:30:45,755 [INFO] Sum of grad norms: 0.072661
2019-03-22 19:30:45,755 [INFO] ---------------------------------
2019-03-22 19:30:59,363 [INFO] ---------------------------------
2019-03-22 19:30:59,363 [INFO] Summary:
2019-03-22 19:30:59,364 [INFO] Batch 86000, worst loss 0.174306 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:30:59,364 [INFO] Regularization: 1395.912231 * 0.0000010000 = 0.0013959123
2019-03-22 19:30:59,365 [INFO] Sum of grad norms: 0.024463
2019-03-22 19:30:59,366 [INFO] ---------------------------------
2019-03-22 19:31:14,210 [INFO] ---------------------------------
2019-03-22 19:31:14,212 [INFO] Summary:
2019-03-22 19:31:14,212 [INFO] Batch 87000, worst loss 0.273125 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:31:14,213 [INFO] Regularization: 1395.765381 * 0.0000010000 = 0.0013957653
2019-03-22 19:31:14,214 [INFO] Sum of grad norms: 0.134313
2019-03-22 19:31:14,214 [INFO] ---------------------------------
2019-03-22 19:31:28,864 [INFO] ---------------------------------
2019-03-22 19:31:28,865 [INFO] Summary:
2019-03-22 19:31:28,866 [INFO] Batch 88000, worst loss 0.262274 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:31:28,866 [INFO] Regularization: 1395.642090 * 0.0000010000 = 0.0013956421
2019-03-22 19:31:28,867 [INFO] Sum of grad norms: 0.173865
2019-03-22 19:31:28,867 [INFO] ---------------------------------
2019-03-22 19:31:43,260 [INFO] ---------------------------------
2019-03-22 19:31:43,261 [INFO] Summary:
2019-03-22 19:31:43,262 [INFO] Batch 89000, worst loss 0.250656 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:31:43,263 [INFO] Regularization: 1395.506592 * 0.0000010000 = 0.0013955066
2019-03-22 19:31:43,264 [INFO] Sum of grad norms: 0.047851
2019-03-22 19:31:43,265 [INFO] ---------------------------------
2019-03-22 19:31:57,870 [INFO] ---------------------------------
2019-03-22 19:31:57,871 [INFO] Summary:
2019-03-22 19:31:57,872 [INFO] Batch 90000, worst loss 0.184443 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:31:57,872 [INFO] Regularization: 1395.362305 * 0.0000010000 = 0.0013953623
2019-03-22 19:31:57,873 [INFO] Sum of grad norms: 0.200181
2019-03-22 19:31:57,873 [INFO] ---------------------------------
2019-03-22 19:32:00,652 [INFO] ---------------------------------
2019-03-22 19:32:00,653 [INFO] Evaluation:
2019-03-22 19:32:00,654 [INFO] Batch 90000, worst loss 0.181942 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:32:00,654 [INFO] ---------------------------------
2019-03-22 19:32:14,256 [INFO] ---------------------------------
2019-03-22 19:32:14,257 [INFO] Summary:
2019-03-22 19:32:14,257 [INFO] Batch 91000, worst loss 0.186196 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:32:14,258 [INFO] Regularization: 1395.222534 * 0.0000010000 = 0.0013952225
2019-03-22 19:32:14,258 [INFO] Sum of grad norms: 0.607518
2019-03-22 19:32:14,259 [INFO] ---------------------------------
2019-03-22 19:32:28,779 [INFO] ---------------------------------
2019-03-22 19:32:28,780 [INFO] Summary:
2019-03-22 19:32:28,780 [INFO] Batch 92000, worst loss 0.214441 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:32:28,781 [INFO] Regularization: 1395.182983 * 0.0000010000 = 0.0013951830
2019-03-22 19:32:28,782 [INFO] Sum of grad norms: 12.286115
2019-03-22 19:32:28,782 [INFO] ---------------------------------
2019-03-22 19:32:43,034 [INFO] ---------------------------------
2019-03-22 19:32:43,035 [INFO] Summary:
2019-03-22 19:32:43,036 [INFO] Batch 93000, worst loss 0.249206 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:32:43,037 [INFO] Regularization: 1395.143799 * 0.0000010000 = 0.0013951438
2019-03-22 19:32:43,037 [INFO] Sum of grad norms: 2.117534
2019-03-22 19:32:43,038 [INFO] ---------------------------------
2019-03-22 19:32:57,442 [INFO] ---------------------------------
2019-03-22 19:32:57,443 [INFO] Summary:
2019-03-22 19:32:57,443 [INFO] Batch 94000, worst loss 0.220307 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:32:57,444 [INFO] Regularization: 1395.120972 * 0.0000010000 = 0.0013951210
2019-03-22 19:32:57,444 [INFO] Sum of grad norms: 0.021036
2019-03-22 19:32:57,445 [INFO] ---------------------------------
2019-03-22 19:33:11,759 [INFO] ---------------------------------
2019-03-22 19:33:11,760 [INFO] Summary:
2019-03-22 19:33:11,761 [INFO] Batch 95000, worst loss 0.282082 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:33:11,761 [INFO] Regularization: 1395.089600 * 0.0000010000 = 0.0013950896
2019-03-22 19:33:11,762 [INFO] Sum of grad norms: 0.074389
2019-03-22 19:33:11,763 [INFO] ---------------------------------
2019-03-22 19:33:25,817 [INFO] ---------------------------------
2019-03-22 19:33:25,818 [INFO] Summary:
2019-03-22 19:33:25,819 [INFO] Batch 96000, worst loss 0.181831 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:33:25,819 [INFO] Regularization: 1395.062500 * 0.0000010000 = 0.0013950625
2019-03-22 19:33:25,820 [INFO] Sum of grad norms: 7.658099
2019-03-22 19:33:25,820 [INFO] ---------------------------------
2019-03-22 19:33:40,179 [INFO] ---------------------------------
2019-03-22 19:33:40,180 [INFO] Summary:
2019-03-22 19:33:40,181 [INFO] Batch 97000, worst loss 0.218942 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:33:40,181 [INFO] Regularization: 1395.030640 * 0.0000010000 = 0.0013950306
2019-03-22 19:33:40,182 [INFO] Sum of grad norms: 17.044624
2019-03-22 19:33:40,182 [INFO] ---------------------------------
2019-03-22 19:33:54,794 [INFO] ---------------------------------
2019-03-22 19:33:54,795 [INFO] Summary:
2019-03-22 19:33:54,796 [INFO] Batch 98000, worst loss 0.328527 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:33:54,796 [INFO] Regularization: 1395.001709 * 0.0000010000 = 0.0013950017
2019-03-22 19:33:54,797 [INFO] Sum of grad norms: 0.354942
2019-03-22 19:33:54,797 [INFO] ---------------------------------
2019-03-22 19:34:09,549 [INFO] ---------------------------------
2019-03-22 19:34:09,550 [INFO] Summary:
2019-03-22 19:34:09,551 [INFO] Batch 99000, worst loss 0.203587 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:34:09,551 [INFO] Regularization: 1394.971436 * 0.0000010000 = 0.0013949714
2019-03-22 19:34:09,552 [INFO] Sum of grad norms: 5.359034
2019-03-22 19:34:09,552 [INFO] ---------------------------------
2019-03-22 19:34:23,656 [INFO] ---------------------------------
2019-03-22 19:34:23,657 [INFO] Summary:
2019-03-22 19:34:23,657 [INFO] Batch 100000, worst loss 0.193621 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:34:23,658 [INFO] Regularization: 1394.950562 * 0.0000010000 = 0.0013949506
2019-03-22 19:34:23,658 [INFO] Sum of grad norms: 1.525321
2019-03-22 19:34:23,659 [INFO] ---------------------------------
2019-03-22 19:34:26,352 [INFO] ---------------------------------
2019-03-22 19:34:26,352 [INFO] Evaluation:
2019-03-22 19:34:26,353 [INFO] Batch 100000, worst loss 0.218436 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:34:26,353 [INFO] ---------------------------------
2019-03-22 19:34:26,354 [INFO] Finished training, saved to file classifier/1553275201/1553279666_2_classifier_final.pth
2019-03-22 19:34:26,524 [INFO] ---------------------------------
2019-03-22 19:34:26,527 [INFO] Training model #3: (8, 64, 2) @ 2
2019-03-22 19:34:41,254 [INFO] ---------------------------------
2019-03-22 19:34:41,255 [INFO] Summary:
2019-03-22 19:34:41,255 [INFO] Batch 1000, worst loss 934.562378 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:34:41,256 [INFO] Regularization: 30768.080078 * 0.0000010000 = 0.0307680797
2019-03-22 19:34:41,256 [INFO] Sum of grad norms: 1650.514526
2019-03-22 19:34:41,257 [INFO] ---------------------------------
2019-03-22 19:34:56,085 [INFO] ---------------------------------
2019-03-22 19:34:56,085 [INFO] Summary:
2019-03-22 19:34:56,086 [INFO] Batch 2000, worst loss 32.801804 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:34:56,087 [INFO] Regularization: 28571.835938 * 0.0000010000 = 0.0285718367
2019-03-22 19:34:56,087 [INFO] Sum of grad norms: 688.924622
2019-03-22 19:34:56,088 [INFO] ---------------------------------
2019-03-22 19:35:10,936 [INFO] ---------------------------------
2019-03-22 19:35:10,937 [INFO] Summary:
2019-03-22 19:35:10,937 [INFO] Batch 3000, worst loss 9.600928 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:35:10,938 [INFO] Regularization: 24453.835938 * 0.0000010000 = 0.0244538356
2019-03-22 19:35:10,939 [INFO] Sum of grad norms: 370.521729
2019-03-22 19:35:10,939 [INFO] ---------------------------------
2019-03-22 19:35:25,462 [INFO] ---------------------------------
2019-03-22 19:35:25,463 [INFO] Summary:
2019-03-22 19:35:25,464 [INFO] Batch 4000, worst loss 4.616456 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:35:25,465 [INFO] Regularization: 18983.974609 * 0.0000010000 = 0.0189839751
2019-03-22 19:35:25,465 [INFO] Sum of grad norms: 98.147629
2019-03-22 19:35:25,466 [INFO] ---------------------------------
2019-03-22 19:35:40,604 [INFO] ---------------------------------
2019-03-22 19:35:40,605 [INFO] Summary:
2019-03-22 19:35:40,605 [INFO] Batch 5000, worst loss 3.129786 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:35:40,606 [INFO] Regularization: 12796.688477 * 0.0000010000 = 0.0127966888
2019-03-22 19:35:40,607 [INFO] Sum of grad norms: 23.172455
2019-03-22 19:35:40,607 [INFO] ---------------------------------
2019-03-22 19:35:54,760 [INFO] ---------------------------------
2019-03-22 19:35:54,761 [INFO] Summary:
2019-03-22 19:35:54,761 [INFO] Batch 6000, worst loss 2.068554 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:35:54,762 [INFO] Regularization: 8750.467773 * 0.0000010000 = 0.0087504676
2019-03-22 19:35:54,762 [INFO] Sum of grad norms: 44.179752
2019-03-22 19:35:54,763 [INFO] ---------------------------------
2019-03-22 19:36:09,548 [INFO] ---------------------------------
2019-03-22 19:36:09,549 [INFO] Summary:
2019-03-22 19:36:09,549 [INFO] Batch 7000, worst loss 1.005569 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:36:09,550 [INFO] Regularization: 8400.453125 * 0.0000010000 = 0.0084004533
2019-03-22 19:36:09,550 [INFO] Sum of grad norms: 9.234036
2019-03-22 19:36:09,551 [INFO] ---------------------------------
2019-03-22 19:36:23,414 [INFO] ---------------------------------
2019-03-22 19:36:23,415 [INFO] Summary:
2019-03-22 19:36:23,415 [INFO] Batch 8000, worst loss 0.771004 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:36:23,416 [INFO] Regularization: 8196.677734 * 0.0000010000 = 0.0081966780
2019-03-22 19:36:23,416 [INFO] Sum of grad norms: 5.286259
2019-03-22 19:36:23,417 [INFO] ---------------------------------
2019-03-22 19:36:38,724 [INFO] ---------------------------------
2019-03-22 19:36:38,725 [INFO] Summary:
2019-03-22 19:36:38,726 [INFO] Batch 9000, worst loss 0.536857 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:36:38,726 [INFO] Regularization: 7908.081055 * 0.0000010000 = 0.0079080807
2019-03-22 19:36:38,727 [INFO] Sum of grad norms: 16.717590
2019-03-22 19:36:38,728 [INFO] ---------------------------------
2019-03-22 19:36:53,367 [INFO] ---------------------------------
2019-03-22 19:36:53,368 [INFO] Summary:
2019-03-22 19:36:53,369 [INFO] Batch 10000, worst loss 0.706058 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:36:53,369 [INFO] Regularization: 7630.052734 * 0.0000010000 = 0.0076300525
2019-03-22 19:36:53,370 [INFO] Sum of grad norms: 15.955612
2019-03-22 19:36:53,371 [INFO] ---------------------------------
2019-03-22 19:36:56,105 [INFO] ---------------------------------
2019-03-22 19:36:56,106 [INFO] Evaluation:
2019-03-22 19:36:56,107 [INFO] Batch 10000, worst loss 0.344350 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:36:56,108 [INFO] ---------------------------------
2019-03-22 19:37:11,225 [INFO] ---------------------------------
2019-03-22 19:37:11,226 [INFO] Summary:
2019-03-22 19:37:11,227 [INFO] Batch 11000, worst loss 0.416391 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:37:11,227 [INFO] Regularization: 7374.783691 * 0.0000010000 = 0.0073747835
2019-03-22 19:37:11,228 [INFO] Sum of grad norms: 6.294539
2019-03-22 19:37:11,228 [INFO] ---------------------------------
2019-03-22 19:37:25,548 [INFO] ---------------------------------
2019-03-22 19:37:25,549 [INFO] Summary:
2019-03-22 19:37:25,550 [INFO] Batch 12000, worst loss 0.404528 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:37:25,550 [INFO] Regularization: 7044.395996 * 0.0000010000 = 0.0070443959
2019-03-22 19:37:25,551 [INFO] Sum of grad norms: 9.872393
2019-03-22 19:37:25,551 [INFO] ---------------------------------
2019-03-22 19:37:40,762 [INFO] ---------------------------------
2019-03-22 19:37:40,763 [INFO] Summary:
2019-03-22 19:37:40,764 [INFO] Batch 13000, worst loss 0.400939 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:37:40,764 [INFO] Regularization: 6727.572266 * 0.0000010000 = 0.0067275721
2019-03-22 19:37:40,765 [INFO] Sum of grad norms: 2.847936
2019-03-22 19:37:40,766 [INFO] ---------------------------------
2019-03-22 19:37:54,900 [INFO] ---------------------------------
2019-03-22 19:37:54,901 [INFO] Summary:
2019-03-22 19:37:54,901 [INFO] Batch 14000, worst loss 0.370663 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:37:54,902 [INFO] Regularization: 6356.225586 * 0.0000010000 = 0.0063562253
2019-03-22 19:37:54,903 [INFO] Sum of grad norms: 23.382311
2019-03-22 19:37:54,903 [INFO] ---------------------------------
2019-03-22 19:38:08,998 [INFO] ---------------------------------
2019-03-22 19:38:08,999 [INFO] Summary:
2019-03-22 19:38:09,000 [INFO] Batch 15000, worst loss 0.327656 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:38:09,000 [INFO] Regularization: 6005.242188 * 0.0000010000 = 0.0060052420
2019-03-22 19:38:09,001 [INFO] Sum of grad norms: 10.049011
2019-03-22 19:38:09,001 [INFO] ---------------------------------
2019-03-22 19:38:23,870 [INFO] ---------------------------------
2019-03-22 19:38:23,871 [INFO] Summary:
2019-03-22 19:38:23,871 [INFO] Batch 16000, worst loss 0.362721 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:38:23,872 [INFO] Regularization: 5645.821289 * 0.0000010000 = 0.0056458213
2019-03-22 19:38:23,872 [INFO] Sum of grad norms: 13.856552
2019-03-22 19:38:23,873 [INFO] ---------------------------------
2019-03-22 19:38:38,641 [INFO] ---------------------------------
2019-03-22 19:38:38,641 [INFO] Summary:
2019-03-22 19:38:38,642 [INFO] Batch 17000, worst loss 0.343951 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:38:38,643 [INFO] Regularization: 5182.846191 * 0.0000010000 = 0.0051828460
2019-03-22 19:38:38,643 [INFO] Sum of grad norms: 18.713766
2019-03-22 19:38:38,644 [INFO] ---------------------------------
2019-03-22 19:38:52,653 [INFO] ---------------------------------
2019-03-22 19:38:52,654 [INFO] Summary:
2019-03-22 19:38:52,655 [INFO] Batch 18000, worst loss 0.329030 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:38:52,655 [INFO] Regularization: 4877.243164 * 0.0000010000 = 0.0048772432
2019-03-22 19:38:52,656 [INFO] Sum of grad norms: 9.671162
2019-03-22 19:38:52,657 [INFO] ---------------------------------
2019-03-22 19:39:08,998 [INFO] ---------------------------------
2019-03-22 19:39:08,999 [INFO] Summary:
2019-03-22 19:39:08,999 [INFO] Batch 19000, worst loss 0.343407 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:39:09,000 [INFO] Regularization: 4612.331543 * 0.0000010000 = 0.0046123317
2019-03-22 19:39:09,000 [INFO] Sum of grad norms: 1.945501
2019-03-22 19:39:09,001 [INFO] ---------------------------------
2019-03-22 19:39:24,444 [INFO] ---------------------------------
2019-03-22 19:39:24,445 [INFO] Summary:
2019-03-22 19:39:24,446 [INFO] Batch 20000, worst loss 0.285923 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:39:24,447 [INFO] Regularization: 4223.394043 * 0.0000010000 = 0.0042233942
2019-03-22 19:39:24,448 [INFO] Sum of grad norms: 7.588922
2019-03-22 19:39:24,449 [INFO] ---------------------------------
2019-03-22 19:39:27,219 [INFO] ---------------------------------
2019-03-22 19:39:27,220 [INFO] Evaluation:
2019-03-22 19:39:27,221 [INFO] Batch 20000, worst loss 0.316613 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:39:27,221 [INFO] ---------------------------------
2019-03-22 19:39:42,281 [INFO] ---------------------------------
2019-03-22 19:39:42,282 [INFO] Summary:
2019-03-22 19:39:42,282 [INFO] Batch 21000, worst loss 0.331637 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:39:42,283 [INFO] Regularization: 4114.372559 * 0.0000010000 = 0.0041143727
2019-03-22 19:39:42,283 [INFO] Sum of grad norms: 14.947208
2019-03-22 19:39:42,284 [INFO] ---------------------------------
2019-03-22 19:39:56,635 [INFO] ---------------------------------
2019-03-22 19:39:56,635 [INFO] Summary:
2019-03-22 19:39:56,636 [INFO] Batch 22000, worst loss 0.378713 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:39:56,636 [INFO] Regularization: 3931.706299 * 0.0000010000 = 0.0039317063
2019-03-22 19:39:56,637 [INFO] Sum of grad norms: 4.607113
2019-03-22 19:39:56,637 [INFO] ---------------------------------
2019-03-22 19:40:11,266 [INFO] ---------------------------------
2019-03-22 19:40:11,267 [INFO] Summary:
2019-03-22 19:40:11,267 [INFO] Batch 23000, worst loss 0.389620 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:40:11,268 [INFO] Regularization: 3792.404541 * 0.0000010000 = 0.0037924044
2019-03-22 19:40:11,268 [INFO] Sum of grad norms: 3.775675
2019-03-22 19:40:11,269 [INFO] ---------------------------------
2019-03-22 19:40:25,474 [INFO] ---------------------------------
2019-03-22 19:40:25,475 [INFO] Summary:
2019-03-22 19:40:25,476 [INFO] Batch 24000, worst loss 0.396576 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:40:25,477 [INFO] Regularization: 3705.680664 * 0.0000010000 = 0.0037056806
2019-03-22 19:40:25,477 [INFO] Sum of grad norms: 0.904577
2019-03-22 19:40:25,478 [INFO] ---------------------------------
2019-03-22 19:40:40,507 [INFO] ---------------------------------
2019-03-22 19:40:40,508 [INFO] Summary:
2019-03-22 19:40:40,509 [INFO] Batch 25000, worst loss 0.395138 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:40:40,509 [INFO] Regularization: 3559.917236 * 0.0000010000 = 0.0035599172
2019-03-22 19:40:40,510 [INFO] Sum of grad norms: 1.143208
2019-03-22 19:40:40,511 [INFO] ---------------------------------
2019-03-22 19:40:55,744 [INFO] ---------------------------------
2019-03-22 19:40:55,745 [INFO] Summary:
2019-03-22 19:40:55,745 [INFO] Batch 26000, worst loss 0.416912 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:40:55,746 [INFO] Regularization: 3478.421387 * 0.0000010000 = 0.0034784214
2019-03-22 19:40:55,746 [INFO] Sum of grad norms: 3.763280
2019-03-22 19:40:55,747 [INFO] ---------------------------------
2019-03-22 19:41:10,080 [INFO] ---------------------------------
2019-03-22 19:41:10,081 [INFO] Summary:
2019-03-22 19:41:10,082 [INFO] Batch 27000, worst loss 0.304694 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:41:10,082 [INFO] Regularization: 3324.912109 * 0.0000010000 = 0.0033249122
2019-03-22 19:41:10,083 [INFO] Sum of grad norms: 0.130512
2019-03-22 19:41:10,083 [INFO] ---------------------------------
2019-03-22 19:41:25,954 [INFO] ---------------------------------
2019-03-22 19:41:25,954 [INFO] Summary:
2019-03-22 19:41:25,955 [INFO] Batch 28000, worst loss 0.354162 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:41:25,956 [INFO] Regularization: 3159.999023 * 0.0000010000 = 0.0031599989
2019-03-22 19:41:25,957 [INFO] Sum of grad norms: 9.634789
2019-03-22 19:41:25,958 [INFO] ---------------------------------
2019-03-22 19:41:40,408 [INFO] ---------------------------------
2019-03-22 19:41:40,409 [INFO] Summary:
2019-03-22 19:41:40,409 [INFO] Batch 29000, worst loss 0.375475 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:41:40,410 [INFO] Regularization: 3019.164795 * 0.0000010000 = 0.0030191648
2019-03-22 19:41:40,411 [INFO] Sum of grad norms: 0.296933
2019-03-22 19:41:40,411 [INFO] ---------------------------------
2019-03-22 19:41:55,028 [INFO] ---------------------------------
2019-03-22 19:41:55,029 [INFO] Summary:
2019-03-22 19:41:55,029 [INFO] Batch 30000, worst loss 0.339694 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:41:55,030 [INFO] Regularization: 2920.545166 * 0.0000010000 = 0.0029205452
2019-03-22 19:41:55,031 [INFO] Sum of grad norms: 2.761590
2019-03-22 19:41:55,032 [INFO] ---------------------------------
2019-03-22 19:41:57,771 [INFO] ---------------------------------
2019-03-22 19:41:57,772 [INFO] Evaluation:
2019-03-22 19:41:57,773 [INFO] Batch 30000, worst loss 0.304879 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:41:57,774 [INFO] ---------------------------------
2019-03-22 19:42:12,248 [INFO] ---------------------------------
2019-03-22 19:42:12,249 [INFO] Summary:
2019-03-22 19:42:12,249 [INFO] Batch 31000, worst loss 0.397861 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:42:12,250 [INFO] Regularization: 2873.456787 * 0.0000010000 = 0.0028734568
2019-03-22 19:42:12,250 [INFO] Sum of grad norms: 19.671539
2019-03-22 19:42:12,251 [INFO] ---------------------------------
2019-03-22 19:42:26,467 [INFO] ---------------------------------
2019-03-22 19:42:26,468 [INFO] Summary:
2019-03-22 19:42:26,468 [INFO] Batch 32000, worst loss 0.419579 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:42:26,469 [INFO] Regularization: 2767.211182 * 0.0000010000 = 0.0027672111
2019-03-22 19:42:26,471 [INFO] Sum of grad norms: 11.212233
2019-03-22 19:42:26,471 [INFO] ---------------------------------
2019-03-22 19:42:40,775 [INFO] ---------------------------------
2019-03-22 19:42:40,776 [INFO] Summary:
2019-03-22 19:42:40,776 [INFO] Batch 33000, worst loss 0.482109 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:42:40,777 [INFO] Regularization: 2680.615479 * 0.0000010000 = 0.0026806155
2019-03-22 19:42:40,777 [INFO] Sum of grad norms: 3.777486
2019-03-22 19:42:40,778 [INFO] ---------------------------------
2019-03-22 19:42:55,642 [INFO] ---------------------------------
2019-03-22 19:42:55,643 [INFO] Summary:
2019-03-22 19:42:55,645 [INFO] Batch 34000, worst loss 0.383522 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:42:55,646 [INFO] Regularization: 2636.670166 * 0.0000010000 = 0.0026366701
2019-03-22 19:42:55,647 [INFO] Sum of grad norms: 12.149711
2019-03-22 19:42:55,648 [INFO] ---------------------------------
2019-03-22 19:43:10,044 [INFO] ---------------------------------
2019-03-22 19:43:10,045 [INFO] Summary:
2019-03-22 19:43:10,046 [INFO] Batch 35000, worst loss 0.444518 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:43:10,047 [INFO] Regularization: 2615.453857 * 0.0000010000 = 0.0026154539
2019-03-22 19:43:10,047 [INFO] Sum of grad norms: 11.634932
2019-03-22 19:43:10,048 [INFO] ---------------------------------
2019-03-22 19:43:25,027 [INFO] ---------------------------------
2019-03-22 19:43:25,028 [INFO] Summary:
2019-03-22 19:43:25,028 [INFO] Batch 36000, worst loss 0.306389 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:43:25,029 [INFO] Regularization: 2516.636963 * 0.0000010000 = 0.0025166369
2019-03-22 19:43:25,029 [INFO] Sum of grad norms: 2.473038
2019-03-22 19:43:25,030 [INFO] ---------------------------------
2019-03-22 19:43:39,900 [INFO] ---------------------------------
2019-03-22 19:43:39,901 [INFO] Summary:
2019-03-22 19:43:39,902 [INFO] Batch 37000, worst loss 0.366125 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:43:39,903 [INFO] Regularization: 2429.083740 * 0.0000010000 = 0.0024290837
2019-03-22 19:43:39,903 [INFO] Sum of grad norms: 9.298591
2019-03-22 19:43:39,904 [INFO] ---------------------------------
2019-03-22 19:43:55,271 [INFO] ---------------------------------
2019-03-22 19:43:55,272 [INFO] Summary:
2019-03-22 19:43:55,272 [INFO] Batch 38000, worst loss 0.383329 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:43:55,273 [INFO] Regularization: 2405.143311 * 0.0000010000 = 0.0024051433
2019-03-22 19:43:55,273 [INFO] Sum of grad norms: 9.999657
2019-03-22 19:43:55,274 [INFO] ---------------------------------
2019-03-22 19:44:09,871 [INFO] ---------------------------------
2019-03-22 19:44:09,872 [INFO] Summary:
2019-03-22 19:44:09,872 [INFO] Batch 39000, worst loss 0.413118 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:44:09,873 [INFO] Regularization: 2339.670654 * 0.0000010000 = 0.0023396707
2019-03-22 19:44:09,874 [INFO] Sum of grad norms: 7.166572
2019-03-22 19:44:09,874 [INFO] ---------------------------------
2019-03-22 19:44:23,737 [INFO] ---------------------------------
2019-03-22 19:44:23,738 [INFO] Summary:
2019-03-22 19:44:23,738 [INFO] Batch 40000, worst loss 0.394202 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:44:23,739 [INFO] Regularization: 2259.010010 * 0.0000010000 = 0.0022590100
2019-03-22 19:44:23,739 [INFO] Sum of grad norms: 7.241632
2019-03-22 19:44:23,740 [INFO] ---------------------------------
2019-03-22 19:44:26,543 [INFO] ---------------------------------
2019-03-22 19:44:26,543 [INFO] Evaluation:
2019-03-22 19:44:26,544 [INFO] Batch 40000, worst loss 0.339242 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:44:26,544 [INFO] ---------------------------------
2019-03-22 19:44:41,475 [INFO] ---------------------------------
2019-03-22 19:44:41,476 [INFO] Summary:
2019-03-22 19:44:41,476 [INFO] Batch 41000, worst loss 0.342660 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:44:41,477 [INFO] Regularization: 2196.607178 * 0.0000010000 = 0.0021966072
2019-03-22 19:44:41,477 [INFO] Sum of grad norms: 8.181123
2019-03-22 19:44:41,478 [INFO] ---------------------------------
2019-03-22 19:44:56,239 [INFO] ---------------------------------
2019-03-22 19:44:56,240 [INFO] Summary:
2019-03-22 19:44:56,240 [INFO] Batch 42000, worst loss 0.291355 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:44:56,241 [INFO] Regularization: 2139.184814 * 0.0000010000 = 0.0021391849
2019-03-22 19:44:56,241 [INFO] Sum of grad norms: 3.971841
2019-03-22 19:44:56,242 [INFO] ---------------------------------
2019-03-22 19:45:10,204 [INFO] ---------------------------------
2019-03-22 19:45:10,205 [INFO] Summary:
2019-03-22 19:45:10,206 [INFO] Batch 43000, worst loss 0.301213 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:45:10,206 [INFO] Regularization: 2087.943848 * 0.0000010000 = 0.0020879440
2019-03-22 19:45:10,207 [INFO] Sum of grad norms: 6.273749
2019-03-22 19:45:10,207 [INFO] ---------------------------------
2019-03-22 19:45:24,457 [INFO] ---------------------------------
2019-03-22 19:45:24,458 [INFO] Summary:
2019-03-22 19:45:24,459 [INFO] Batch 44000, worst loss 0.262937 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:45:24,460 [INFO] Regularization: 2031.193848 * 0.0000010000 = 0.0020311938
2019-03-22 19:45:24,460 [INFO] Sum of grad norms: 2.336008
2019-03-22 19:45:24,461 [INFO] ---------------------------------
2019-03-22 19:45:38,987 [INFO] ---------------------------------
2019-03-22 19:45:38,988 [INFO] Summary:
2019-03-22 19:45:38,989 [INFO] Batch 45000, worst loss 0.232050 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:45:38,989 [INFO] Regularization: 1981.965820 * 0.0000010000 = 0.0019819657
2019-03-22 19:45:38,990 [INFO] Sum of grad norms: 15.066497
2019-03-22 19:45:38,990 [INFO] ---------------------------------
2019-03-22 19:45:53,111 [INFO] ---------------------------------
2019-03-22 19:45:53,112 [INFO] Summary:
2019-03-22 19:45:53,112 [INFO] Batch 46000, worst loss 0.269467 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:45:53,113 [INFO] Regularization: 1935.615479 * 0.0000010000 = 0.0019356154
2019-03-22 19:45:53,114 [INFO] Sum of grad norms: 0.352000
2019-03-22 19:45:53,114 [INFO] ---------------------------------
2019-03-22 19:46:07,815 [INFO] ---------------------------------
2019-03-22 19:46:07,816 [INFO] Summary:
2019-03-22 19:46:07,817 [INFO] Batch 47000, worst loss 0.247968 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:46:07,818 [INFO] Regularization: 1899.902466 * 0.0000010000 = 0.0018999025
2019-03-22 19:46:07,819 [INFO] Sum of grad norms: 0.106644
2019-03-22 19:46:07,819 [INFO] ---------------------------------
2019-03-22 19:46:23,231 [INFO] ---------------------------------
2019-03-22 19:46:23,232 [INFO] Summary:
2019-03-22 19:46:23,233 [INFO] Batch 48000, worst loss 0.307661 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:46:23,233 [INFO] Regularization: 1872.723755 * 0.0000010000 = 0.0018727238
2019-03-22 19:46:23,234 [INFO] Sum of grad norms: 0.061772
2019-03-22 19:46:23,235 [INFO] ---------------------------------
2019-03-22 19:46:37,810 [INFO] ---------------------------------
2019-03-22 19:46:37,812 [INFO] Summary:
2019-03-22 19:46:37,812 [INFO] Batch 49000, worst loss 0.310031 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:46:37,813 [INFO] Regularization: 1854.205933 * 0.0000010000 = 0.0018542060
2019-03-22 19:46:37,813 [INFO] Sum of grad norms: 4.305116
2019-03-22 19:46:37,814 [INFO] ---------------------------------
2019-03-22 19:46:52,030 [INFO] ---------------------------------
2019-03-22 19:46:52,031 [INFO] Summary:
2019-03-22 19:46:52,031 [INFO] Batch 50000, worst loss 0.282062 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 19:46:52,032 [INFO] Regularization: 1828.343506 * 0.0000010000 = 0.0018283435
2019-03-22 19:46:52,032 [INFO] Sum of grad norms: 3.276539
2019-03-22 19:46:52,033 [INFO] ---------------------------------
2019-03-22 19:46:54,791 [INFO] ---------------------------------
2019-03-22 19:46:54,792 [INFO] Evaluation:
2019-03-22 19:46:54,793 [INFO] Batch 50000, worst loss 0.225776 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:46:54,793 [INFO] ---------------------------------
2019-03-22 19:47:08,926 [INFO] ---------------------------------
2019-03-22 19:47:08,926 [INFO] Summary:
2019-03-22 19:47:08,927 [INFO] Batch 51000, worst loss 0.256277 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:47:08,927 [INFO] Regularization: 1806.201904 * 0.0000010000 = 0.0018062019
2019-03-22 19:47:08,928 [INFO] Sum of grad norms: 0.011490
2019-03-22 19:47:08,929 [INFO] ---------------------------------
2019-03-22 19:47:23,426 [INFO] ---------------------------------
2019-03-22 19:47:23,426 [INFO] Summary:
2019-03-22 19:47:23,427 [INFO] Batch 52000, worst loss 0.243777 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:47:23,428 [INFO] Regularization: 1794.534058 * 0.0000010000 = 0.0017945340
2019-03-22 19:47:23,429 [INFO] Sum of grad norms: 9.373861
2019-03-22 19:47:23,430 [INFO] ---------------------------------
2019-03-22 19:47:37,905 [INFO] ---------------------------------
2019-03-22 19:47:37,906 [INFO] Summary:
2019-03-22 19:47:37,907 [INFO] Batch 53000, worst loss 0.240945 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:47:37,907 [INFO] Regularization: 1787.140137 * 0.0000010000 = 0.0017871402
2019-03-22 19:47:37,908 [INFO] Sum of grad norms: 15.230189
2019-03-22 19:47:37,909 [INFO] ---------------------------------
2019-03-22 19:47:52,372 [INFO] ---------------------------------
2019-03-22 19:47:52,373 [INFO] Summary:
2019-03-22 19:47:52,374 [INFO] Batch 54000, worst loss 0.217390 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:47:52,375 [INFO] Regularization: 1779.346313 * 0.0000010000 = 0.0017793463
2019-03-22 19:47:52,375 [INFO] Sum of grad norms: 0.381802
2019-03-22 19:47:52,376 [INFO] ---------------------------------
2019-03-22 19:48:06,461 [INFO] ---------------------------------
2019-03-22 19:48:06,462 [INFO] Summary:
2019-03-22 19:48:06,462 [INFO] Batch 55000, worst loss 0.228059 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:48:06,463 [INFO] Regularization: 1772.310425 * 0.0000010000 = 0.0017723105
2019-03-22 19:48:06,463 [INFO] Sum of grad norms: 0.012297
2019-03-22 19:48:06,464 [INFO] ---------------------------------
2019-03-22 19:48:21,487 [INFO] ---------------------------------
2019-03-22 19:48:21,488 [INFO] Summary:
2019-03-22 19:48:21,489 [INFO] Batch 56000, worst loss 0.192982 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:48:21,489 [INFO] Regularization: 1764.380859 * 0.0000010000 = 0.0017643808
2019-03-22 19:48:21,490 [INFO] Sum of grad norms: 1.409695
2019-03-22 19:48:21,490 [INFO] ---------------------------------
2019-03-22 19:48:35,831 [INFO] ---------------------------------
2019-03-22 19:48:35,832 [INFO] Summary:
2019-03-22 19:48:35,833 [INFO] Batch 57000, worst loss 0.386783 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:48:35,834 [INFO] Regularization: 1757.413330 * 0.0000010000 = 0.0017574134
2019-03-22 19:48:35,834 [INFO] Sum of grad norms: 0.121919
2019-03-22 19:48:35,835 [INFO] ---------------------------------
2019-03-22 19:48:49,985 [INFO] ---------------------------------
2019-03-22 19:48:49,986 [INFO] Summary:
2019-03-22 19:48:49,987 [INFO] Batch 58000, worst loss 0.266465 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:48:49,987 [INFO] Regularization: 1748.865723 * 0.0000010000 = 0.0017488657
2019-03-22 19:48:49,988 [INFO] Sum of grad norms: 0.402652
2019-03-22 19:48:49,988 [INFO] ---------------------------------
2019-03-22 19:49:04,550 [INFO] ---------------------------------
2019-03-22 19:49:04,551 [INFO] Summary:
2019-03-22 19:49:04,551 [INFO] Batch 59000, worst loss 0.216271 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:49:04,552 [INFO] Regularization: 1741.093262 * 0.0000010000 = 0.0017410932
2019-03-22 19:49:04,552 [INFO] Sum of grad norms: 1.524833
2019-03-22 19:49:04,553 [INFO] ---------------------------------
2019-03-22 19:49:19,724 [INFO] ---------------------------------
2019-03-22 19:49:19,725 [INFO] Summary:
2019-03-22 19:49:19,725 [INFO] Batch 60000, worst loss 0.206196 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 19:49:19,726 [INFO] Regularization: 1735.014160 * 0.0000010000 = 0.0017350141
2019-03-22 19:49:19,726 [INFO] Sum of grad norms: 0.031913
2019-03-22 19:49:19,727 [INFO] ---------------------------------
2019-03-22 19:49:22,488 [INFO] ---------------------------------
2019-03-22 19:49:22,489 [INFO] Evaluation:
2019-03-22 19:49:22,491 [INFO] Batch 60000, worst loss 0.243931 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:49:22,492 [INFO] ---------------------------------
2019-03-22 19:49:36,402 [INFO] ---------------------------------
2019-03-22 19:49:36,403 [INFO] Summary:
2019-03-22 19:49:36,404 [INFO] Batch 61000, worst loss 0.255964 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:49:36,404 [INFO] Regularization: 1728.779053 * 0.0000010000 = 0.0017287791
2019-03-22 19:49:36,404 [INFO] Sum of grad norms: 0.009374
2019-03-22 19:49:36,405 [INFO] ---------------------------------
2019-03-22 19:49:49,954 [INFO] ---------------------------------
2019-03-22 19:49:49,955 [INFO] Summary:
2019-03-22 19:49:49,955 [INFO] Batch 62000, worst loss 0.194070 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:49:49,956 [INFO] Regularization: 1725.047241 * 0.0000010000 = 0.0017250472
2019-03-22 19:49:49,956 [INFO] Sum of grad norms: 0.107445
2019-03-22 19:49:49,957 [INFO] ---------------------------------
2019-03-22 19:50:03,954 [INFO] ---------------------------------
2019-03-22 19:50:03,955 [INFO] Summary:
2019-03-22 19:50:03,955 [INFO] Batch 63000, worst loss 0.192103 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:50:03,956 [INFO] Regularization: 1722.888062 * 0.0000010000 = 0.0017228881
2019-03-22 19:50:03,956 [INFO] Sum of grad norms: 0.009266
2019-03-22 19:50:03,957 [INFO] ---------------------------------
2019-03-22 19:50:17,947 [INFO] ---------------------------------
2019-03-22 19:50:17,948 [INFO] Summary:
2019-03-22 19:50:17,948 [INFO] Batch 64000, worst loss 0.201478 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:50:17,949 [INFO] Regularization: 1720.695312 * 0.0000010000 = 0.0017206953
2019-03-22 19:50:17,949 [INFO] Sum of grad norms: 4.630850
2019-03-22 19:50:17,950 [INFO] ---------------------------------
2019-03-22 19:50:32,517 [INFO] ---------------------------------
2019-03-22 19:50:32,518 [INFO] Summary:
2019-03-22 19:50:32,519 [INFO] Batch 65000, worst loss 0.246124 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:50:32,519 [INFO] Regularization: 1719.771118 * 0.0000010000 = 0.0017197711
2019-03-22 19:50:32,520 [INFO] Sum of grad norms: 0.004419
2019-03-22 19:50:32,521 [INFO] ---------------------------------
2019-03-22 19:50:47,288 [INFO] ---------------------------------
2019-03-22 19:50:47,289 [INFO] Summary:
2019-03-22 19:50:47,290 [INFO] Batch 66000, worst loss 0.272253 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:50:47,291 [INFO] Regularization: 1717.159546 * 0.0000010000 = 0.0017171595
2019-03-22 19:50:47,291 [INFO] Sum of grad norms: 0.144079
2019-03-22 19:50:47,292 [INFO] ---------------------------------
2019-03-22 19:51:02,466 [INFO] ---------------------------------
2019-03-22 19:51:02,467 [INFO] Summary:
2019-03-22 19:51:02,467 [INFO] Batch 67000, worst loss 0.240113 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:51:02,468 [INFO] Regularization: 1715.225586 * 0.0000010000 = 0.0017152256
2019-03-22 19:51:02,468 [INFO] Sum of grad norms: 44.468147
2019-03-22 19:51:02,469 [INFO] ---------------------------------
2019-03-22 19:51:16,692 [INFO] ---------------------------------
2019-03-22 19:51:16,693 [INFO] Summary:
2019-03-22 19:51:16,693 [INFO] Batch 68000, worst loss 0.223177 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:51:16,694 [INFO] Regularization: 1712.944336 * 0.0000010000 = 0.0017129444
2019-03-22 19:51:16,694 [INFO] Sum of grad norms: 0.004296
2019-03-22 19:51:16,695 [INFO] ---------------------------------
2019-03-22 19:51:30,732 [INFO] ---------------------------------
2019-03-22 19:51:30,733 [INFO] Summary:
2019-03-22 19:51:30,734 [INFO] Batch 69000, worst loss 0.268367 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:51:30,735 [INFO] Regularization: 1711.036865 * 0.0000010000 = 0.0017110369
2019-03-22 19:51:30,735 [INFO] Sum of grad norms: 33.349106
2019-03-22 19:51:30,736 [INFO] ---------------------------------
2019-03-22 19:51:44,879 [INFO] ---------------------------------
2019-03-22 19:51:44,880 [INFO] Summary:
2019-03-22 19:51:44,881 [INFO] Batch 70000, worst loss 0.244140 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 19:51:44,881 [INFO] Regularization: 1708.868652 * 0.0000010000 = 0.0017088687
2019-03-22 19:51:44,882 [INFO] Sum of grad norms: 0.214546
2019-03-22 19:51:44,882 [INFO] ---------------------------------
2019-03-22 19:51:47,665 [INFO] ---------------------------------
2019-03-22 19:51:47,666 [INFO] Evaluation:
2019-03-22 19:51:47,666 [INFO] Batch 70000, worst loss 0.257398 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:51:47,668 [INFO] ---------------------------------
2019-03-22 19:52:02,303 [INFO] ---------------------------------
2019-03-22 19:52:02,304 [INFO] Summary:
2019-03-22 19:52:02,305 [INFO] Batch 71000, worst loss 0.259074 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:52:02,305 [INFO] Regularization: 1706.731934 * 0.0000010000 = 0.0017067320
2019-03-22 19:52:02,306 [INFO] Sum of grad norms: 0.470174
2019-03-22 19:52:02,306 [INFO] ---------------------------------
2019-03-22 19:52:17,198 [INFO] ---------------------------------
2019-03-22 19:52:17,199 [INFO] Summary:
2019-03-22 19:52:17,199 [INFO] Batch 72000, worst loss 0.194778 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:52:17,200 [INFO] Regularization: 1705.989868 * 0.0000010000 = 0.0017059898
2019-03-22 19:52:17,200 [INFO] Sum of grad norms: 0.059853
2019-03-22 19:52:17,201 [INFO] ---------------------------------
2019-03-22 19:52:32,300 [INFO] ---------------------------------
2019-03-22 19:52:32,301 [INFO] Summary:
2019-03-22 19:52:32,301 [INFO] Batch 73000, worst loss 0.168641 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:52:32,302 [INFO] Regularization: 1705.427368 * 0.0000010000 = 0.0017054274
2019-03-22 19:52:32,302 [INFO] Sum of grad norms: 0.008265
2019-03-22 19:52:32,303 [INFO] ---------------------------------
2019-03-22 19:52:46,038 [INFO] ---------------------------------
2019-03-22 19:52:46,039 [INFO] Summary:
2019-03-22 19:52:46,039 [INFO] Batch 74000, worst loss 0.211666 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:52:46,040 [INFO] Regularization: 1704.992065 * 0.0000010000 = 0.0017049920
2019-03-22 19:52:46,040 [INFO] Sum of grad norms: 0.124250
2019-03-22 19:52:46,041 [INFO] ---------------------------------
2019-03-22 19:52:59,875 [INFO] ---------------------------------
2019-03-22 19:52:59,876 [INFO] Summary:
2019-03-22 19:52:59,877 [INFO] Batch 75000, worst loss 0.204895 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:52:59,877 [INFO] Regularization: 1704.506348 * 0.0000010000 = 0.0017045063
2019-03-22 19:52:59,878 [INFO] Sum of grad norms: 0.018243
2019-03-22 19:52:59,879 [INFO] ---------------------------------
2019-03-22 19:53:14,206 [INFO] ---------------------------------
2019-03-22 19:53:14,207 [INFO] Summary:
2019-03-22 19:53:14,208 [INFO] Batch 76000, worst loss 0.184407 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:53:14,208 [INFO] Regularization: 1704.048828 * 0.0000010000 = 0.0017040488
2019-03-22 19:53:14,209 [INFO] Sum of grad norms: 0.005694
2019-03-22 19:53:14,209 [INFO] ---------------------------------
2019-03-22 19:53:27,924 [INFO] ---------------------------------
2019-03-22 19:53:27,925 [INFO] Summary:
2019-03-22 19:53:27,926 [INFO] Batch 77000, worst loss 0.230123 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:53:27,926 [INFO] Regularization: 1703.563110 * 0.0000010000 = 0.0017035631
2019-03-22 19:53:27,927 [INFO] Sum of grad norms: 5.083977
2019-03-22 19:53:27,928 [INFO] ---------------------------------
2019-03-22 19:53:41,870 [INFO] ---------------------------------
2019-03-22 19:53:41,871 [INFO] Summary:
2019-03-22 19:53:41,872 [INFO] Batch 78000, worst loss 0.284324 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:53:41,872 [INFO] Regularization: 1703.214600 * 0.0000010000 = 0.0017032146
2019-03-22 19:53:41,873 [INFO] Sum of grad norms: 35.009766
2019-03-22 19:53:41,874 [INFO] ---------------------------------
2019-03-22 19:53:55,872 [INFO] ---------------------------------
2019-03-22 19:53:55,873 [INFO] Summary:
2019-03-22 19:53:55,874 [INFO] Batch 79000, worst loss 0.226351 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:53:55,875 [INFO] Regularization: 1702.640991 * 0.0000010000 = 0.0017026410
2019-03-22 19:53:55,875 [INFO] Sum of grad norms: 0.105034
2019-03-22 19:53:55,876 [INFO] ---------------------------------
2019-03-22 19:54:10,538 [INFO] ---------------------------------
2019-03-22 19:54:10,539 [INFO] Summary:
2019-03-22 19:54:10,540 [INFO] Batch 80000, worst loss 0.185330 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 19:54:10,540 [INFO] Regularization: 1702.123779 * 0.0000010000 = 0.0017021238
2019-03-22 19:54:10,541 [INFO] Sum of grad norms: 2.535390
2019-03-22 19:54:10,541 [INFO] ---------------------------------
2019-03-22 19:54:13,301 [INFO] ---------------------------------
2019-03-22 19:54:13,302 [INFO] Evaluation:
2019-03-22 19:54:13,304 [INFO] Batch 80000, worst loss 0.222827 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:54:13,305 [INFO] ---------------------------------
2019-03-22 19:54:27,744 [INFO] ---------------------------------
2019-03-22 19:54:27,745 [INFO] Summary:
2019-03-22 19:54:27,745 [INFO] Batch 81000, worst loss 0.203201 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:54:27,746 [INFO] Regularization: 1701.710938 * 0.0000010000 = 0.0017017110
2019-03-22 19:54:27,746 [INFO] Sum of grad norms: 0.692813
2019-03-22 19:54:27,747 [INFO] ---------------------------------
2019-03-22 19:54:42,248 [INFO] ---------------------------------
2019-03-22 19:54:42,249 [INFO] Summary:
2019-03-22 19:54:42,250 [INFO] Batch 82000, worst loss 0.240151 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:54:42,251 [INFO] Regularization: 1701.496704 * 0.0000010000 = 0.0017014967
2019-03-22 19:54:42,251 [INFO] Sum of grad norms: 36.397015
2019-03-22 19:54:42,252 [INFO] ---------------------------------
2019-03-22 19:54:57,295 [INFO] ---------------------------------
2019-03-22 19:54:57,296 [INFO] Summary:
2019-03-22 19:54:57,297 [INFO] Batch 83000, worst loss 0.251198 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:54:57,297 [INFO] Regularization: 1701.368164 * 0.0000010000 = 0.0017013681
2019-03-22 19:54:57,298 [INFO] Sum of grad norms: 0.096686
2019-03-22 19:54:57,299 [INFO] ---------------------------------
2019-03-22 19:55:11,980 [INFO] ---------------------------------
2019-03-22 19:55:11,982 [INFO] Summary:
2019-03-22 19:55:11,982 [INFO] Batch 84000, worst loss 0.323242 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:55:11,983 [INFO] Regularization: 1701.248047 * 0.0000010000 = 0.0017012480
2019-03-22 19:55:11,983 [INFO] Sum of grad norms: 0.025085
2019-03-22 19:55:11,984 [INFO] ---------------------------------
2019-03-22 19:55:26,097 [INFO] ---------------------------------
2019-03-22 19:55:26,098 [INFO] Summary:
2019-03-22 19:55:26,098 [INFO] Batch 85000, worst loss 0.192212 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:55:26,099 [INFO] Regularization: 1701.110596 * 0.0000010000 = 0.0017011106
2019-03-22 19:55:26,099 [INFO] Sum of grad norms: 28.891098
2019-03-22 19:55:26,100 [INFO] ---------------------------------
2019-03-22 19:55:40,925 [INFO] ---------------------------------
2019-03-22 19:55:40,926 [INFO] Summary:
2019-03-22 19:55:40,926 [INFO] Batch 86000, worst loss 0.189902 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:55:40,927 [INFO] Regularization: 1700.950928 * 0.0000010000 = 0.0017009509
2019-03-22 19:55:40,927 [INFO] Sum of grad norms: 0.033175
2019-03-22 19:55:40,928 [INFO] ---------------------------------
2019-03-22 19:55:55,368 [INFO] ---------------------------------
2019-03-22 19:55:55,369 [INFO] Summary:
2019-03-22 19:55:55,369 [INFO] Batch 87000, worst loss 0.270446 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:55:55,370 [INFO] Regularization: 1700.823730 * 0.0000010000 = 0.0017008238
2019-03-22 19:55:55,371 [INFO] Sum of grad norms: 19.826300
2019-03-22 19:55:55,371 [INFO] ---------------------------------
2019-03-22 19:56:09,010 [INFO] ---------------------------------
2019-03-22 19:56:09,011 [INFO] Summary:
2019-03-22 19:56:09,012 [INFO] Batch 88000, worst loss 0.291070 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:56:09,013 [INFO] Regularization: 1700.748291 * 0.0000010000 = 0.0017007483
2019-03-22 19:56:09,013 [INFO] Sum of grad norms: 3.630963
2019-03-22 19:56:09,014 [INFO] ---------------------------------
2019-03-22 19:56:23,543 [INFO] ---------------------------------
2019-03-22 19:56:23,544 [INFO] Summary:
2019-03-22 19:56:23,544 [INFO] Batch 89000, worst loss 0.289731 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:56:23,545 [INFO] Regularization: 1700.633301 * 0.0000010000 = 0.0017006333
2019-03-22 19:56:23,546 [INFO] Sum of grad norms: 0.635088
2019-03-22 19:56:23,546 [INFO] ---------------------------------
2019-03-22 19:56:38,284 [INFO] ---------------------------------
2019-03-22 19:56:38,285 [INFO] Summary:
2019-03-22 19:56:38,285 [INFO] Batch 90000, worst loss 0.193987 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 19:56:38,286 [INFO] Regularization: 1700.528076 * 0.0000010000 = 0.0017005281
2019-03-22 19:56:38,286 [INFO] Sum of grad norms: 0.146929
2019-03-22 19:56:38,287 [INFO] ---------------------------------
2019-03-22 19:56:41,009 [INFO] ---------------------------------
2019-03-22 19:56:41,009 [INFO] Evaluation:
2019-03-22 19:56:41,010 [INFO] Batch 90000, worst loss 0.193265 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:56:41,011 [INFO] ---------------------------------
2019-03-22 19:56:55,443 [INFO] ---------------------------------
2019-03-22 19:56:55,444 [INFO] Summary:
2019-03-22 19:56:55,444 [INFO] Batch 91000, worst loss 0.212734 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:56:55,445 [INFO] Regularization: 1700.396118 * 0.0000010000 = 0.0017003961
2019-03-22 19:56:55,445 [INFO] Sum of grad norms: 1.240630
2019-03-22 19:56:55,446 [INFO] ---------------------------------
2019-03-22 19:57:10,400 [INFO] ---------------------------------
2019-03-22 19:57:10,400 [INFO] Summary:
2019-03-22 19:57:10,401 [INFO] Batch 92000, worst loss 0.207186 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:57:10,402 [INFO] Regularization: 1700.335571 * 0.0000010000 = 0.0017003355
2019-03-22 19:57:10,402 [INFO] Sum of grad norms: 0.301510
2019-03-22 19:57:10,403 [INFO] ---------------------------------
2019-03-22 19:57:25,009 [INFO] ---------------------------------
2019-03-22 19:57:25,010 [INFO] Summary:
2019-03-22 19:57:25,011 [INFO] Batch 93000, worst loss 0.247827 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:57:25,011 [INFO] Regularization: 1700.296997 * 0.0000010000 = 0.0017002970
2019-03-22 19:57:25,012 [INFO] Sum of grad norms: 6.134677
2019-03-22 19:57:25,013 [INFO] ---------------------------------
2019-03-22 19:57:39,888 [INFO] ---------------------------------
2019-03-22 19:57:39,889 [INFO] Summary:
2019-03-22 19:57:39,889 [INFO] Batch 94000, worst loss 0.209586 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:57:39,890 [INFO] Regularization: 1700.271973 * 0.0000010000 = 0.0017002720
2019-03-22 19:57:39,890 [INFO] Sum of grad norms: 8.443963
2019-03-22 19:57:39,891 [INFO] ---------------------------------
2019-03-22 19:57:54,247 [INFO] ---------------------------------
2019-03-22 19:57:54,248 [INFO] Summary:
2019-03-22 19:57:54,249 [INFO] Batch 95000, worst loss 0.271874 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:57:54,249 [INFO] Regularization: 1700.240967 * 0.0000010000 = 0.0017002410
2019-03-22 19:57:54,250 [INFO] Sum of grad norms: 21.965055
2019-03-22 19:57:54,250 [INFO] ---------------------------------
2019-03-22 19:58:08,994 [INFO] ---------------------------------
2019-03-22 19:58:08,994 [INFO] Summary:
2019-03-22 19:58:08,995 [INFO] Batch 96000, worst loss 0.201271 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:58:08,996 [INFO] Regularization: 1700.214600 * 0.0000010000 = 0.0017002146
2019-03-22 19:58:08,996 [INFO] Sum of grad norms: 15.594172
2019-03-22 19:58:08,997 [INFO] ---------------------------------
2019-03-22 19:58:22,691 [INFO] ---------------------------------
2019-03-22 19:58:22,692 [INFO] Summary:
2019-03-22 19:58:22,693 [INFO] Batch 97000, worst loss 0.229772 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:58:22,693 [INFO] Regularization: 1700.186157 * 0.0000010000 = 0.0017001862
2019-03-22 19:58:22,694 [INFO] Sum of grad norms: 0.003734
2019-03-22 19:58:22,694 [INFO] ---------------------------------
2019-03-22 19:58:37,096 [INFO] ---------------------------------
2019-03-22 19:58:37,097 [INFO] Summary:
2019-03-22 19:58:37,098 [INFO] Batch 98000, worst loss 0.213126 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:58:37,098 [INFO] Regularization: 1700.151611 * 0.0000010000 = 0.0017001516
2019-03-22 19:58:37,099 [INFO] Sum of grad norms: 1.547561
2019-03-22 19:58:37,100 [INFO] ---------------------------------
2019-03-22 19:58:51,200 [INFO] ---------------------------------
2019-03-22 19:58:51,201 [INFO] Summary:
2019-03-22 19:58:51,202 [INFO] Batch 99000, worst loss 0.200867 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:58:51,202 [INFO] Regularization: 1700.120972 * 0.0000010000 = 0.0017001210
2019-03-22 19:58:51,203 [INFO] Sum of grad norms: 0.017131
2019-03-22 19:58:51,204 [INFO] ---------------------------------
2019-03-22 19:59:05,315 [INFO] ---------------------------------
2019-03-22 19:59:05,316 [INFO] Summary:
2019-03-22 19:59:05,317 [INFO] Batch 100000, worst loss 0.194980 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 19:59:05,317 [INFO] Regularization: 1700.095215 * 0.0000010000 = 0.0017000953
2019-03-22 19:59:05,318 [INFO] Sum of grad norms: 35.720104
2019-03-22 19:59:05,319 [INFO] ---------------------------------
2019-03-22 19:59:08,067 [INFO] ---------------------------------
2019-03-22 19:59:08,067 [INFO] Evaluation:
2019-03-22 19:59:08,068 [INFO] Batch 100000, worst loss 0.235673 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 19:59:08,069 [INFO] ---------------------------------
2019-03-22 19:59:08,070 [INFO] Finished training, saved to file classifier/1553275201/1553281148_3_classifier_final.pth
2019-03-22 19:59:08,231 [INFO] ---------------------------------
2019-03-22 19:59:08,233 [INFO] Training model #4: (8, 64, 2) @ 2
2019-03-22 19:59:23,313 [INFO] ---------------------------------
2019-03-22 19:59:23,314 [INFO] Summary:
2019-03-22 19:59:23,314 [INFO] Batch 1000, worst loss 521.250854 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:59:23,315 [INFO] Regularization: 30146.275391 * 0.0000010000 = 0.0301462747
2019-03-22 19:59:23,315 [INFO] Sum of grad norms: 586.131165
2019-03-22 19:59:23,316 [INFO] ---------------------------------
2019-03-22 19:59:37,914 [INFO] ---------------------------------
2019-03-22 19:59:37,915 [INFO] Summary:
2019-03-22 19:59:37,916 [INFO] Batch 2000, worst loss 11.805379 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:59:37,916 [INFO] Regularization: 25473.773438 * 0.0000010000 = 0.0254737735
2019-03-22 19:59:37,917 [INFO] Sum of grad norms: 271.403900
2019-03-22 19:59:37,917 [INFO] ---------------------------------
2019-03-22 19:59:52,452 [INFO] ---------------------------------
2019-03-22 19:59:52,453 [INFO] Summary:
2019-03-22 19:59:52,453 [INFO] Batch 3000, worst loss 6.465004 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 19:59:52,454 [INFO] Regularization: 17294.117188 * 0.0000010000 = 0.0172941163
2019-03-22 19:59:52,454 [INFO] Sum of grad norms: 166.735565
2019-03-22 19:59:52,455 [INFO] ---------------------------------
2019-03-22 20:00:07,081 [INFO] ---------------------------------
2019-03-22 20:00:07,081 [INFO] Summary:
2019-03-22 20:00:07,082 [INFO] Batch 4000, worst loss 2.062706 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:00:07,083 [INFO] Regularization: 10605.848633 * 0.0000010000 = 0.0106058484
2019-03-22 20:00:07,083 [INFO] Sum of grad norms: 33.679893
2019-03-22 20:00:07,084 [INFO] ---------------------------------
2019-03-22 20:00:22,040 [INFO] ---------------------------------
2019-03-22 20:00:22,040 [INFO] Summary:
2019-03-22 20:00:22,041 [INFO] Batch 5000, worst loss 1.246400 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:00:22,041 [INFO] Regularization: 10457.976562 * 0.0000010000 = 0.0104579767
2019-03-22 20:00:22,042 [INFO] Sum of grad norms: 7.441930
2019-03-22 20:00:22,042 [INFO] ---------------------------------
2019-03-22 20:00:36,819 [INFO] ---------------------------------
2019-03-22 20:00:36,820 [INFO] Summary:
2019-03-22 20:00:36,821 [INFO] Batch 6000, worst loss 0.789368 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:00:36,822 [INFO] Regularization: 10291.841797 * 0.0000010000 = 0.0102918418
2019-03-22 20:00:36,822 [INFO] Sum of grad norms: 6.638955
2019-03-22 20:00:36,823 [INFO] ---------------------------------
2019-03-22 20:00:51,398 [INFO] ---------------------------------
2019-03-22 20:00:51,399 [INFO] Summary:
2019-03-22 20:00:51,400 [INFO] Batch 7000, worst loss 0.701942 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:00:51,400 [INFO] Regularization: 10097.943359 * 0.0000010000 = 0.0100979432
2019-03-22 20:00:51,401 [INFO] Sum of grad norms: 17.136885
2019-03-22 20:00:51,401 [INFO] ---------------------------------
2019-03-22 20:01:06,816 [INFO] ---------------------------------
2019-03-22 20:01:06,817 [INFO] Summary:
2019-03-22 20:01:06,818 [INFO] Batch 8000, worst loss 0.615767 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:01:06,818 [INFO] Regularization: 9894.339844 * 0.0000010000 = 0.0098943394
2019-03-22 20:01:06,819 [INFO] Sum of grad norms: 2.227045
2019-03-22 20:01:06,819 [INFO] ---------------------------------
2019-03-22 20:01:21,099 [INFO] ---------------------------------
2019-03-22 20:01:21,100 [INFO] Summary:
2019-03-22 20:01:21,100 [INFO] Batch 9000, worst loss 0.434378 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:01:21,101 [INFO] Regularization: 9578.687500 * 0.0000010000 = 0.0095786871
2019-03-22 20:01:21,101 [INFO] Sum of grad norms: 18.903883
2019-03-22 20:01:21,102 [INFO] ---------------------------------
2019-03-22 20:01:35,788 [INFO] ---------------------------------
2019-03-22 20:01:35,788 [INFO] Summary:
2019-03-22 20:01:35,789 [INFO] Batch 10000, worst loss 0.414276 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:01:35,790 [INFO] Regularization: 9206.336914 * 0.0000010000 = 0.0092063369
2019-03-22 20:01:35,790 [INFO] Sum of grad norms: 3.937321
2019-03-22 20:01:35,791 [INFO] ---------------------------------
2019-03-22 20:01:38,523 [INFO] ---------------------------------
2019-03-22 20:01:38,523 [INFO] Evaluation:
2019-03-22 20:01:38,524 [INFO] Batch 10000, worst loss 0.367446 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:01:38,525 [INFO] ---------------------------------
2019-03-22 20:01:53,868 [INFO] ---------------------------------
2019-03-22 20:01:53,869 [INFO] Summary:
2019-03-22 20:01:53,870 [INFO] Batch 11000, worst loss 0.388841 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:01:53,870 [INFO] Regularization: 8826.291992 * 0.0000010000 = 0.0088262921
2019-03-22 20:01:53,871 [INFO] Sum of grad norms: 1.377505
2019-03-22 20:01:53,871 [INFO] ---------------------------------
2019-03-22 20:02:08,584 [INFO] ---------------------------------
2019-03-22 20:02:08,585 [INFO] Summary:
2019-03-22 20:02:08,586 [INFO] Batch 12000, worst loss 0.352491 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:02:08,586 [INFO] Regularization: 8431.486328 * 0.0000010000 = 0.0084314859
2019-03-22 20:02:08,587 [INFO] Sum of grad norms: 0.949851
2019-03-22 20:02:08,588 [INFO] ---------------------------------
2019-03-22 20:02:22,930 [INFO] ---------------------------------
2019-03-22 20:02:22,931 [INFO] Summary:
2019-03-22 20:02:22,932 [INFO] Batch 13000, worst loss 0.314907 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:02:22,932 [INFO] Regularization: 7969.608398 * 0.0000010000 = 0.0079696085
2019-03-22 20:02:22,933 [INFO] Sum of grad norms: 7.755176
2019-03-22 20:02:22,933 [INFO] ---------------------------------
2019-03-22 20:02:37,080 [INFO] ---------------------------------
2019-03-22 20:02:37,081 [INFO] Summary:
2019-03-22 20:02:37,081 [INFO] Batch 14000, worst loss 0.356927 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:02:37,082 [INFO] Regularization: 7186.703125 * 0.0000010000 = 0.0071867029
2019-03-22 20:02:37,082 [INFO] Sum of grad norms: 4.472621
2019-03-22 20:02:37,083 [INFO] ---------------------------------
2019-03-22 20:02:51,669 [INFO] ---------------------------------
2019-03-22 20:02:51,670 [INFO] Summary:
2019-03-22 20:02:51,671 [INFO] Batch 15000, worst loss 0.406122 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:02:51,671 [INFO] Regularization: 6570.876465 * 0.0000010000 = 0.0065708766
2019-03-22 20:02:51,672 [INFO] Sum of grad norms: 2.161265
2019-03-22 20:02:51,672 [INFO] ---------------------------------
2019-03-22 20:03:06,018 [INFO] ---------------------------------
2019-03-22 20:03:06,019 [INFO] Summary:
2019-03-22 20:03:06,020 [INFO] Batch 16000, worst loss 0.340459 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:03:06,020 [INFO] Regularization: 6057.810059 * 0.0000010000 = 0.0060578100
2019-03-22 20:03:06,021 [INFO] Sum of grad norms: 19.495327
2019-03-22 20:03:06,021 [INFO] ---------------------------------
2019-03-22 20:03:20,819 [INFO] ---------------------------------
2019-03-22 20:03:20,820 [INFO] Summary:
2019-03-22 20:03:20,821 [INFO] Batch 17000, worst loss 0.387169 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:03:20,822 [INFO] Regularization: 5537.572266 * 0.0000010000 = 0.0055375723
2019-03-22 20:03:20,822 [INFO] Sum of grad norms: 23.319660
2019-03-22 20:03:20,823 [INFO] ---------------------------------
2019-03-22 20:03:35,809 [INFO] ---------------------------------
2019-03-22 20:03:35,810 [INFO] Summary:
2019-03-22 20:03:35,810 [INFO] Batch 18000, worst loss 0.363043 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:03:35,811 [INFO] Regularization: 5017.521973 * 0.0000010000 = 0.0050175218
2019-03-22 20:03:35,812 [INFO] Sum of grad norms: 6.603053
2019-03-22 20:03:35,812 [INFO] ---------------------------------
2019-03-22 20:03:50,026 [INFO] ---------------------------------
2019-03-22 20:03:50,027 [INFO] Summary:
2019-03-22 20:03:50,028 [INFO] Batch 19000, worst loss 0.371294 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:03:50,028 [INFO] Regularization: 4640.900879 * 0.0000010000 = 0.0046409010
2019-03-22 20:03:50,029 [INFO] Sum of grad norms: 0.778842
2019-03-22 20:03:50,029 [INFO] ---------------------------------
2019-03-22 20:04:04,633 [INFO] ---------------------------------
2019-03-22 20:04:04,634 [INFO] Summary:
2019-03-22 20:04:04,635 [INFO] Batch 20000, worst loss 0.330335 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:04:04,635 [INFO] Regularization: 4294.495117 * 0.0000010000 = 0.0042944951
2019-03-22 20:04:04,636 [INFO] Sum of grad norms: 0.825362
2019-03-22 20:04:04,636 [INFO] ---------------------------------
2019-03-22 20:04:07,380 [INFO] ---------------------------------
2019-03-22 20:04:07,381 [INFO] Evaluation:
2019-03-22 20:04:07,382 [INFO] Batch 20000, worst loss 0.339605 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:04:07,382 [INFO] ---------------------------------
2019-03-22 20:04:21,909 [INFO] ---------------------------------
2019-03-22 20:04:21,909 [INFO] Summary:
2019-03-22 20:04:21,910 [INFO] Batch 21000, worst loss 0.338559 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:04:21,910 [INFO] Regularization: 3897.076660 * 0.0000010000 = 0.0038970767
2019-03-22 20:04:21,911 [INFO] Sum of grad norms: 3.583033
2019-03-22 20:04:21,911 [INFO] ---------------------------------
2019-03-22 20:04:36,260 [INFO] ---------------------------------
2019-03-22 20:04:36,261 [INFO] Summary:
2019-03-22 20:04:36,261 [INFO] Batch 22000, worst loss 0.372935 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:04:36,262 [INFO] Regularization: 3530.156982 * 0.0000010000 = 0.0035301570
2019-03-22 20:04:36,263 [INFO] Sum of grad norms: 33.745792
2019-03-22 20:04:36,263 [INFO] ---------------------------------
2019-03-22 20:04:50,738 [INFO] ---------------------------------
2019-03-22 20:04:50,739 [INFO] Summary:
2019-03-22 20:04:50,739 [INFO] Batch 23000, worst loss 0.429962 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:04:50,740 [INFO] Regularization: 3257.356934 * 0.0000010000 = 0.0032573568
2019-03-22 20:04:50,740 [INFO] Sum of grad norms: 9.273841
2019-03-22 20:04:50,741 [INFO] ---------------------------------
2019-03-22 20:05:05,730 [INFO] ---------------------------------
2019-03-22 20:05:05,731 [INFO] Summary:
2019-03-22 20:05:05,732 [INFO] Batch 24000, worst loss 0.346361 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:05:05,732 [INFO] Regularization: 3090.438477 * 0.0000010000 = 0.0030904384
2019-03-22 20:05:05,733 [INFO] Sum of grad norms: 9.304666
2019-03-22 20:05:05,733 [INFO] ---------------------------------
2019-03-22 20:05:20,050 [INFO] ---------------------------------
2019-03-22 20:05:20,051 [INFO] Summary:
2019-03-22 20:05:20,052 [INFO] Batch 25000, worst loss 0.320512 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:05:20,052 [INFO] Regularization: 2867.152344 * 0.0000010000 = 0.0028671524
2019-03-22 20:05:20,053 [INFO] Sum of grad norms: 5.934819
2019-03-22 20:05:20,053 [INFO] ---------------------------------
2019-03-22 20:05:34,463 [INFO] ---------------------------------
2019-03-22 20:05:34,464 [INFO] Summary:
2019-03-22 20:05:34,465 [INFO] Batch 26000, worst loss 0.348165 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:05:34,465 [INFO] Regularization: 2702.725586 * 0.0000010000 = 0.0027027256
2019-03-22 20:05:34,466 [INFO] Sum of grad norms: 4.486594
2019-03-22 20:05:34,466 [INFO] ---------------------------------
2019-03-22 20:05:48,541 [INFO] ---------------------------------
2019-03-22 20:05:48,542 [INFO] Summary:
2019-03-22 20:05:48,542 [INFO] Batch 27000, worst loss 0.307074 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:05:48,543 [INFO] Regularization: 2582.034180 * 0.0000010000 = 0.0025820341
2019-03-22 20:05:48,543 [INFO] Sum of grad norms: 4.065585
2019-03-22 20:05:48,544 [INFO] ---------------------------------
2019-03-22 20:06:03,068 [INFO] ---------------------------------
2019-03-22 20:06:03,069 [INFO] Summary:
2019-03-22 20:06:03,069 [INFO] Batch 28000, worst loss 0.322889 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:06:03,070 [INFO] Regularization: 2447.480469 * 0.0000010000 = 0.0024474806
2019-03-22 20:06:03,071 [INFO] Sum of grad norms: 2.170500
2019-03-22 20:06:03,071 [INFO] ---------------------------------
2019-03-22 20:06:18,020 [INFO] ---------------------------------
2019-03-22 20:06:18,021 [INFO] Summary:
2019-03-22 20:06:18,022 [INFO] Batch 29000, worst loss 0.357712 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:06:18,022 [INFO] Regularization: 2354.150635 * 0.0000010000 = 0.0023541506
2019-03-22 20:06:18,023 [INFO] Sum of grad norms: 8.960387
2019-03-22 20:06:18,023 [INFO] ---------------------------------
2019-03-22 20:06:32,439 [INFO] ---------------------------------
2019-03-22 20:06:32,440 [INFO] Summary:
2019-03-22 20:06:32,441 [INFO] Batch 30000, worst loss 0.364158 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:06:32,441 [INFO] Regularization: 2300.175781 * 0.0000010000 = 0.0023001758
2019-03-22 20:06:32,442 [INFO] Sum of grad norms: 0.182578
2019-03-22 20:06:32,442 [INFO] ---------------------------------
2019-03-22 20:06:35,170 [INFO] ---------------------------------
2019-03-22 20:06:35,171 [INFO] Evaluation:
2019-03-22 20:06:35,172 [INFO] Batch 30000, worst loss 0.335646 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:06:35,173 [INFO] ---------------------------------
2019-03-22 20:06:49,786 [INFO] ---------------------------------
2019-03-22 20:06:49,787 [INFO] Summary:
2019-03-22 20:06:49,787 [INFO] Batch 31000, worst loss 0.559620 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:06:49,788 [INFO] Regularization: 2178.703613 * 0.0000010000 = 0.0021787037
2019-03-22 20:06:49,788 [INFO] Sum of grad norms: 9.478595
2019-03-22 20:06:49,789 [INFO] ---------------------------------
2019-03-22 20:07:05,176 [INFO] ---------------------------------
2019-03-22 20:07:05,177 [INFO] Summary:
2019-03-22 20:07:05,177 [INFO] Batch 32000, worst loss 0.401782 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:07:05,178 [INFO] Regularization: 2226.756592 * 0.0000010000 = 0.0022267567
2019-03-22 20:07:05,179 [INFO] Sum of grad norms: 5.936141
2019-03-22 20:07:05,179 [INFO] ---------------------------------
2019-03-22 20:07:19,804 [INFO] ---------------------------------
2019-03-22 20:07:19,805 [INFO] Summary:
2019-03-22 20:07:19,805 [INFO] Batch 33000, worst loss 0.391747 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:07:19,806 [INFO] Regularization: 2114.579834 * 0.0000010000 = 0.0021145798
2019-03-22 20:07:19,806 [INFO] Sum of grad norms: 8.168036
2019-03-22 20:07:19,807 [INFO] ---------------------------------
2019-03-22 20:07:33,999 [INFO] ---------------------------------
2019-03-22 20:07:34,000 [INFO] Summary:
2019-03-22 20:07:34,000 [INFO] Batch 34000, worst loss 0.308248 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:07:34,001 [INFO] Regularization: 2027.746826 * 0.0000010000 = 0.0020277468
2019-03-22 20:07:34,001 [INFO] Sum of grad norms: 8.768620
2019-03-22 20:07:34,002 [INFO] ---------------------------------
2019-03-22 20:07:48,399 [INFO] ---------------------------------
2019-03-22 20:07:48,400 [INFO] Summary:
2019-03-22 20:07:48,401 [INFO] Batch 35000, worst loss 0.404641 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:07:48,401 [INFO] Regularization: 1926.750732 * 0.0000010000 = 0.0019267508
2019-03-22 20:07:48,402 [INFO] Sum of grad norms: 4.288762
2019-03-22 20:07:48,403 [INFO] ---------------------------------
2019-03-22 20:08:03,393 [INFO] ---------------------------------
2019-03-22 20:08:03,393 [INFO] Summary:
2019-03-22 20:08:03,394 [INFO] Batch 36000, worst loss 0.338728 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:08:03,394 [INFO] Regularization: 1902.958374 * 0.0000010000 = 0.0019029584
2019-03-22 20:08:03,395 [INFO] Sum of grad norms: 0.664356
2019-03-22 20:08:03,395 [INFO] ---------------------------------
2019-03-22 20:08:18,645 [INFO] ---------------------------------
2019-03-22 20:08:18,646 [INFO] Summary:
2019-03-22 20:08:18,646 [INFO] Batch 37000, worst loss 0.385396 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:08:18,647 [INFO] Regularization: 1789.776611 * 0.0000010000 = 0.0017897766
2019-03-22 20:08:18,648 [INFO] Sum of grad norms: 3.454730
2019-03-22 20:08:18,648 [INFO] ---------------------------------
2019-03-22 20:08:33,351 [INFO] ---------------------------------
2019-03-22 20:08:33,352 [INFO] Summary:
2019-03-22 20:08:33,352 [INFO] Batch 38000, worst loss 0.418705 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:08:33,353 [INFO] Regularization: 1746.366577 * 0.0000010000 = 0.0017463666
2019-03-22 20:08:33,353 [INFO] Sum of grad norms: 5.792705
2019-03-22 20:08:33,354 [INFO] ---------------------------------
2019-03-22 20:08:48,140 [INFO] ---------------------------------
2019-03-22 20:08:48,141 [INFO] Summary:
2019-03-22 20:08:48,142 [INFO] Batch 39000, worst loss 0.349345 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:08:48,142 [INFO] Regularization: 1696.045166 * 0.0000010000 = 0.0016960452
2019-03-22 20:08:48,143 [INFO] Sum of grad norms: 16.329115
2019-03-22 20:08:48,143 [INFO] ---------------------------------
2019-03-22 20:09:03,317 [INFO] ---------------------------------
2019-03-22 20:09:03,319 [INFO] Summary:
2019-03-22 20:09:03,319 [INFO] Batch 40000, worst loss 0.350618 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:09:03,320 [INFO] Regularization: 1600.535889 * 0.0000010000 = 0.0016005359
2019-03-22 20:09:03,321 [INFO] Sum of grad norms: 4.393455
2019-03-22 20:09:03,322 [INFO] ---------------------------------
2019-03-22 20:09:06,029 [INFO] ---------------------------------
2019-03-22 20:09:06,029 [INFO] Evaluation:
2019-03-22 20:09:06,030 [INFO] Batch 40000, worst loss 0.271480 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:09:06,031 [INFO] ---------------------------------
2019-03-22 20:09:20,304 [INFO] ---------------------------------
2019-03-22 20:09:20,305 [INFO] Summary:
2019-03-22 20:09:20,306 [INFO] Batch 41000, worst loss 0.302429 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:09:20,307 [INFO] Regularization: 1564.971313 * 0.0000010000 = 0.0015649713
2019-03-22 20:09:20,308 [INFO] Sum of grad norms: 7.786951
2019-03-22 20:09:20,309 [INFO] ---------------------------------
2019-03-22 20:09:34,443 [INFO] ---------------------------------
2019-03-22 20:09:34,443 [INFO] Summary:
2019-03-22 20:09:34,444 [INFO] Batch 42000, worst loss 0.253960 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:09:34,444 [INFO] Regularization: 1492.734253 * 0.0000010000 = 0.0014927343
2019-03-22 20:09:34,445 [INFO] Sum of grad norms: 0.010908
2019-03-22 20:09:34,446 [INFO] ---------------------------------
2019-03-22 20:09:48,647 [INFO] ---------------------------------
2019-03-22 20:09:48,648 [INFO] Summary:
2019-03-22 20:09:48,649 [INFO] Batch 43000, worst loss 0.282754 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:09:48,650 [INFO] Regularization: 1441.311646 * 0.0000010000 = 0.0014413117
2019-03-22 20:09:48,650 [INFO] Sum of grad norms: 17.893297
2019-03-22 20:09:48,651 [INFO] ---------------------------------
2019-03-22 20:10:03,293 [INFO] ---------------------------------
2019-03-22 20:10:03,294 [INFO] Summary:
2019-03-22 20:10:03,294 [INFO] Batch 44000, worst loss 0.231922 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:10:03,295 [INFO] Regularization: 1401.866333 * 0.0000010000 = 0.0014018663
2019-03-22 20:10:03,295 [INFO] Sum of grad norms: 0.769668
2019-03-22 20:10:03,296 [INFO] ---------------------------------
2019-03-22 20:10:17,172 [INFO] ---------------------------------
2019-03-22 20:10:17,173 [INFO] Summary:
2019-03-22 20:10:17,173 [INFO] Batch 45000, worst loss 0.248966 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:10:17,174 [INFO] Regularization: 1374.308594 * 0.0000010000 = 0.0013743086
2019-03-22 20:10:17,174 [INFO] Sum of grad norms: 0.023168
2019-03-22 20:10:17,175 [INFO] ---------------------------------
2019-03-22 20:10:31,363 [INFO] ---------------------------------
2019-03-22 20:10:31,364 [INFO] Summary:
2019-03-22 20:10:31,365 [INFO] Batch 46000, worst loss 0.320468 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:10:31,365 [INFO] Regularization: 1349.485596 * 0.0000010000 = 0.0013494856
2019-03-22 20:10:31,366 [INFO] Sum of grad norms: 0.874442
2019-03-22 20:10:31,367 [INFO] ---------------------------------
2019-03-22 20:10:45,946 [INFO] ---------------------------------
2019-03-22 20:10:45,947 [INFO] Summary:
2019-03-22 20:10:45,948 [INFO] Batch 47000, worst loss 0.251546 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:10:45,948 [INFO] Regularization: 1317.104004 * 0.0000010000 = 0.0013171040
2019-03-22 20:10:45,949 [INFO] Sum of grad norms: 0.017601
2019-03-22 20:10:45,949 [INFO] ---------------------------------
2019-03-22 20:11:00,898 [INFO] ---------------------------------
2019-03-22 20:11:00,899 [INFO] Summary:
2019-03-22 20:11:00,900 [INFO] Batch 48000, worst loss 0.240963 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:11:00,900 [INFO] Regularization: 1301.041016 * 0.0000010000 = 0.0013010410
2019-03-22 20:11:00,901 [INFO] Sum of grad norms: 4.704773
2019-03-22 20:11:00,901 [INFO] ---------------------------------
2019-03-22 20:11:14,773 [INFO] ---------------------------------
2019-03-22 20:11:14,774 [INFO] Summary:
2019-03-22 20:11:14,775 [INFO] Batch 49000, worst loss 0.285847 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:11:14,775 [INFO] Regularization: 1280.463501 * 0.0000010000 = 0.0012804635
2019-03-22 20:11:14,776 [INFO] Sum of grad norms: 0.372484
2019-03-22 20:11:14,777 [INFO] ---------------------------------
2019-03-22 20:11:28,903 [INFO] ---------------------------------
2019-03-22 20:11:28,904 [INFO] Summary:
2019-03-22 20:11:28,905 [INFO] Batch 50000, worst loss 0.315484 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:11:28,905 [INFO] Regularization: 1262.581299 * 0.0000010000 = 0.0012625813
2019-03-22 20:11:28,906 [INFO] Sum of grad norms: 12.735768
2019-03-22 20:11:28,907 [INFO] ---------------------------------
2019-03-22 20:11:31,666 [INFO] ---------------------------------
2019-03-22 20:11:31,666 [INFO] Evaluation:
2019-03-22 20:11:31,667 [INFO] Batch 50000, worst loss 0.238546 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:11:31,668 [INFO] ---------------------------------
2019-03-22 20:11:46,616 [INFO] ---------------------------------
2019-03-22 20:11:46,616 [INFO] Summary:
2019-03-22 20:11:46,617 [INFO] Batch 51000, worst loss 0.235534 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:11:46,618 [INFO] Regularization: 1251.087891 * 0.0000010000 = 0.0012510879
2019-03-22 20:11:46,618 [INFO] Sum of grad norms: 0.666749
2019-03-22 20:11:46,619 [INFO] ---------------------------------
2019-03-22 20:12:00,986 [INFO] ---------------------------------
2019-03-22 20:12:00,987 [INFO] Summary:
2019-03-22 20:12:00,988 [INFO] Batch 52000, worst loss 0.217641 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:12:00,988 [INFO] Regularization: 1241.244507 * 0.0000010000 = 0.0012412445
2019-03-22 20:12:00,989 [INFO] Sum of grad norms: 10.561952
2019-03-22 20:12:00,990 [INFO] ---------------------------------
2019-03-22 20:12:15,819 [INFO] ---------------------------------
2019-03-22 20:12:15,820 [INFO] Summary:
2019-03-22 20:12:15,820 [INFO] Batch 53000, worst loss 0.213545 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:12:15,821 [INFO] Regularization: 1237.839478 * 0.0000010000 = 0.0012378395
2019-03-22 20:12:15,821 [INFO] Sum of grad norms: 0.130955
2019-03-22 20:12:15,822 [INFO] ---------------------------------
2019-03-22 20:12:29,742 [INFO] ---------------------------------
2019-03-22 20:12:29,743 [INFO] Summary:
2019-03-22 20:12:29,744 [INFO] Batch 54000, worst loss 0.183733 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:12:29,745 [INFO] Regularization: 1231.996826 * 0.0000010000 = 0.0012319968
2019-03-22 20:12:29,746 [INFO] Sum of grad norms: 6.565354
2019-03-22 20:12:29,747 [INFO] ---------------------------------
2019-03-22 20:12:44,032 [INFO] ---------------------------------
2019-03-22 20:12:44,033 [INFO] Summary:
2019-03-22 20:12:44,033 [INFO] Batch 55000, worst loss 0.262632 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:12:44,034 [INFO] Regularization: 1228.925903 * 0.0000010000 = 0.0012289259
2019-03-22 20:12:44,035 [INFO] Sum of grad norms: 12.995691
2019-03-22 20:12:44,035 [INFO] ---------------------------------
2019-03-22 20:12:57,808 [INFO] ---------------------------------
2019-03-22 20:12:57,809 [INFO] Summary:
2019-03-22 20:12:57,810 [INFO] Batch 56000, worst loss 0.218665 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:12:57,810 [INFO] Regularization: 1223.354614 * 0.0000010000 = 0.0012233546
2019-03-22 20:12:57,811 [INFO] Sum of grad norms: 6.413031
2019-03-22 20:12:57,811 [INFO] ---------------------------------
2019-03-22 20:13:12,373 [INFO] ---------------------------------
2019-03-22 20:13:12,374 [INFO] Summary:
2019-03-22 20:13:12,374 [INFO] Batch 57000, worst loss 0.327616 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:13:12,375 [INFO] Regularization: 1217.570068 * 0.0000010000 = 0.0012175700
2019-03-22 20:13:12,375 [INFO] Sum of grad norms: 1.828212
2019-03-22 20:13:12,376 [INFO] ---------------------------------
2019-03-22 20:13:26,937 [INFO] ---------------------------------
2019-03-22 20:13:26,938 [INFO] Summary:
2019-03-22 20:13:26,938 [INFO] Batch 58000, worst loss 0.240392 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:13:26,939 [INFO] Regularization: 1214.060059 * 0.0000010000 = 0.0012140600
2019-03-22 20:13:26,940 [INFO] Sum of grad norms: 0.009695
2019-03-22 20:13:26,940 [INFO] ---------------------------------
2019-03-22 20:13:41,841 [INFO] ---------------------------------
2019-03-22 20:13:41,842 [INFO] Summary:
2019-03-22 20:13:41,843 [INFO] Batch 59000, worst loss 0.254217 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:13:41,843 [INFO] Regularization: 1209.111572 * 0.0000010000 = 0.0012091115
2019-03-22 20:13:41,844 [INFO] Sum of grad norms: 8.529047
2019-03-22 20:13:41,844 [INFO] ---------------------------------
2019-03-22 20:13:56,006 [INFO] ---------------------------------
2019-03-22 20:13:56,008 [INFO] Summary:
2019-03-22 20:13:56,008 [INFO] Batch 60000, worst loss 0.255515 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:13:56,009 [INFO] Regularization: 1206.226807 * 0.0000010000 = 0.0012062268
2019-03-22 20:13:56,009 [INFO] Sum of grad norms: 0.002882
2019-03-22 20:13:56,010 [INFO] ---------------------------------
2019-03-22 20:13:58,753 [INFO] ---------------------------------
2019-03-22 20:13:58,754 [INFO] Evaluation:
2019-03-22 20:13:58,755 [INFO] Batch 60000, worst loss 0.204693 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:13:58,756 [INFO] ---------------------------------
2019-03-22 20:14:13,245 [INFO] ---------------------------------
2019-03-22 20:14:13,246 [INFO] Summary:
2019-03-22 20:14:13,246 [INFO] Batch 61000, worst loss 0.268739 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:14:13,247 [INFO] Regularization: 1200.878296 * 0.0000010000 = 0.0012008783
2019-03-22 20:14:13,247 [INFO] Sum of grad norms: 0.186053
2019-03-22 20:14:13,248 [INFO] ---------------------------------
2019-03-22 20:14:27,521 [INFO] ---------------------------------
2019-03-22 20:14:27,521 [INFO] Summary:
2019-03-22 20:14:27,522 [INFO] Batch 62000, worst loss 0.188886 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:14:27,523 [INFO] Regularization: 1198.530640 * 0.0000010000 = 0.0011985307
2019-03-22 20:14:27,523 [INFO] Sum of grad norms: 0.005877
2019-03-22 20:14:27,524 [INFO] ---------------------------------
2019-03-22 20:14:42,432 [INFO] ---------------------------------
2019-03-22 20:14:42,433 [INFO] Summary:
2019-03-22 20:14:42,434 [INFO] Batch 63000, worst loss 0.191976 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:14:42,435 [INFO] Regularization: 1197.480225 * 0.0000010000 = 0.0011974802
2019-03-22 20:14:42,435 [INFO] Sum of grad norms: 9.345432
2019-03-22 20:14:42,436 [INFO] ---------------------------------
2019-03-22 20:14:57,086 [INFO] ---------------------------------
2019-03-22 20:14:57,087 [INFO] Summary:
2019-03-22 20:14:57,087 [INFO] Batch 64000, worst loss 0.219554 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:14:57,088 [INFO] Regularization: 1196.301636 * 0.0000010000 = 0.0011963017
2019-03-22 20:14:57,088 [INFO] Sum of grad norms: 16.931364
2019-03-22 20:14:57,089 [INFO] ---------------------------------
2019-03-22 20:15:12,726 [INFO] ---------------------------------
2019-03-22 20:15:12,727 [INFO] Summary:
2019-03-22 20:15:12,727 [INFO] Batch 65000, worst loss 0.217507 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:15:12,728 [INFO] Regularization: 1195.067017 * 0.0000010000 = 0.0011950670
2019-03-22 20:15:12,728 [INFO] Sum of grad norms: 0.065764
2019-03-22 20:15:12,729 [INFO] ---------------------------------
2019-03-22 20:15:27,682 [INFO] ---------------------------------
2019-03-22 20:15:27,683 [INFO] Summary:
2019-03-22 20:15:27,683 [INFO] Batch 66000, worst loss 0.252625 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:15:27,684 [INFO] Regularization: 1193.458252 * 0.0000010000 = 0.0011934582
2019-03-22 20:15:27,685 [INFO] Sum of grad norms: 0.020747
2019-03-22 20:15:27,685 [INFO] ---------------------------------
2019-03-22 20:15:41,720 [INFO] ---------------------------------
2019-03-22 20:15:41,721 [INFO] Summary:
2019-03-22 20:15:41,721 [INFO] Batch 67000, worst loss 0.237240 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:15:41,722 [INFO] Regularization: 1192.473633 * 0.0000010000 = 0.0011924736
2019-03-22 20:15:41,722 [INFO] Sum of grad norms: 0.095061
2019-03-22 20:15:41,723 [INFO] ---------------------------------
2019-03-22 20:15:56,303 [INFO] ---------------------------------
2019-03-22 20:15:56,304 [INFO] Summary:
2019-03-22 20:15:56,305 [INFO] Batch 68000, worst loss 0.223192 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:15:56,305 [INFO] Regularization: 1190.947510 * 0.0000010000 = 0.0011909475
2019-03-22 20:15:56,306 [INFO] Sum of grad norms: 6.036737
2019-03-22 20:15:56,306 [INFO] ---------------------------------
2019-03-22 20:16:10,924 [INFO] ---------------------------------
2019-03-22 20:16:10,925 [INFO] Summary:
2019-03-22 20:16:10,925 [INFO] Batch 69000, worst loss 0.290035 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:16:10,926 [INFO] Regularization: 1189.784546 * 0.0000010000 = 0.0011897845
2019-03-22 20:16:10,926 [INFO] Sum of grad norms: 0.004234
2019-03-22 20:16:10,927 [INFO] ---------------------------------
2019-03-22 20:16:26,035 [INFO] ---------------------------------
2019-03-22 20:16:26,036 [INFO] Summary:
2019-03-22 20:16:26,037 [INFO] Batch 70000, worst loss 0.200100 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:16:26,037 [INFO] Regularization: 1188.104980 * 0.0000010000 = 0.0011881050
2019-03-22 20:16:26,038 [INFO] Sum of grad norms: 4.956833
2019-03-22 20:16:26,038 [INFO] ---------------------------------
2019-03-22 20:16:28,769 [INFO] ---------------------------------
2019-03-22 20:16:28,770 [INFO] Evaluation:
2019-03-22 20:16:28,770 [INFO] Batch 70000, worst loss 0.227723 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:16:28,772 [INFO] ---------------------------------
2019-03-22 20:16:43,147 [INFO] ---------------------------------
2019-03-22 20:16:43,148 [INFO] Summary:
2019-03-22 20:16:43,148 [INFO] Batch 71000, worst loss 0.227573 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:16:43,149 [INFO] Regularization: 1187.059448 * 0.0000010000 = 0.0011870594
2019-03-22 20:16:43,149 [INFO] Sum of grad norms: 1.705910
2019-03-22 20:16:43,150 [INFO] ---------------------------------
2019-03-22 20:16:57,542 [INFO] ---------------------------------
2019-03-22 20:16:57,543 [INFO] Summary:
2019-03-22 20:16:57,544 [INFO] Batch 72000, worst loss 0.181298 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:16:57,544 [INFO] Regularization: 1186.524170 * 0.0000010000 = 0.0011865242
2019-03-22 20:16:57,545 [INFO] Sum of grad norms: 13.480640
2019-03-22 20:16:57,545 [INFO] ---------------------------------
2019-03-22 20:17:11,968 [INFO] ---------------------------------
2019-03-22 20:17:11,968 [INFO] Summary:
2019-03-22 20:17:11,969 [INFO] Batch 73000, worst loss 0.167772 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:17:11,970 [INFO] Regularization: 1186.162842 * 0.0000010000 = 0.0011861628
2019-03-22 20:17:11,970 [INFO] Sum of grad norms: 12.451936
2019-03-22 20:17:11,971 [INFO] ---------------------------------
2019-03-22 20:17:26,587 [INFO] ---------------------------------
2019-03-22 20:17:26,588 [INFO] Summary:
2019-03-22 20:17:26,589 [INFO] Batch 74000, worst loss 0.215690 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:17:26,589 [INFO] Regularization: 1186.034912 * 0.0000010000 = 0.0011860349
2019-03-22 20:17:26,590 [INFO] Sum of grad norms: 5.435647
2019-03-22 20:17:26,590 [INFO] ---------------------------------
2019-03-22 20:17:41,771 [INFO] ---------------------------------
2019-03-22 20:17:41,772 [INFO] Summary:
2019-03-22 20:17:41,773 [INFO] Batch 75000, worst loss 0.212518 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:17:41,774 [INFO] Regularization: 1185.794556 * 0.0000010000 = 0.0011857946
2019-03-22 20:17:41,774 [INFO] Sum of grad norms: 10.571809
2019-03-22 20:17:41,775 [INFO] ---------------------------------
2019-03-22 20:17:56,591 [INFO] ---------------------------------
2019-03-22 20:17:56,592 [INFO] Summary:
2019-03-22 20:17:56,592 [INFO] Batch 76000, worst loss 0.209938 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:17:56,593 [INFO] Regularization: 1185.254883 * 0.0000010000 = 0.0011852549
2019-03-22 20:17:56,593 [INFO] Sum of grad norms: 0.006998
2019-03-22 20:17:56,594 [INFO] ---------------------------------
2019-03-22 20:18:11,201 [INFO] ---------------------------------
2019-03-22 20:18:11,202 [INFO] Summary:
2019-03-22 20:18:11,203 [INFO] Batch 77000, worst loss 0.226629 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:18:11,203 [INFO] Regularization: 1185.001953 * 0.0000010000 = 0.0011850019
2019-03-22 20:18:11,204 [INFO] Sum of grad norms: 5.254483
2019-03-22 20:18:11,205 [INFO] ---------------------------------
2019-03-22 20:18:25,380 [INFO] ---------------------------------
2019-03-22 20:18:25,381 [INFO] Summary:
2019-03-22 20:18:25,382 [INFO] Batch 78000, worst loss 0.257240 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:18:25,382 [INFO] Regularization: 1184.694092 * 0.0000010000 = 0.0011846941
2019-03-22 20:18:25,383 [INFO] Sum of grad norms: 0.004554
2019-03-22 20:18:25,384 [INFO] ---------------------------------
2019-03-22 20:18:39,878 [INFO] ---------------------------------
2019-03-22 20:18:39,879 [INFO] Summary:
2019-03-22 20:18:39,880 [INFO] Batch 79000, worst loss 0.223983 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:18:39,880 [INFO] Regularization: 1184.396973 * 0.0000010000 = 0.0011843970
2019-03-22 20:18:39,881 [INFO] Sum of grad norms: 29.190359
2019-03-22 20:18:39,881 [INFO] ---------------------------------
2019-03-22 20:18:54,065 [INFO] ---------------------------------
2019-03-22 20:18:54,066 [INFO] Summary:
2019-03-22 20:18:54,066 [INFO] Batch 80000, worst loss 0.221996 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:18:54,067 [INFO] Regularization: 1184.032104 * 0.0000010000 = 0.0011840321
2019-03-22 20:18:54,067 [INFO] Sum of grad norms: 10.610854
2019-03-22 20:18:54,068 [INFO] ---------------------------------
2019-03-22 20:18:56,800 [INFO] ---------------------------------
2019-03-22 20:18:56,801 [INFO] Evaluation:
2019-03-22 20:18:56,802 [INFO] Batch 80000, worst loss 0.216202 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:18:56,802 [INFO] ---------------------------------
2019-03-22 20:19:11,309 [INFO] ---------------------------------
2019-03-22 20:19:11,310 [INFO] Summary:
2019-03-22 20:19:11,311 [INFO] Batch 81000, worst loss 0.261259 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:19:11,311 [INFO] Regularization: 1183.804443 * 0.0000010000 = 0.0011838045
2019-03-22 20:19:11,312 [INFO] Sum of grad norms: 0.001643
2019-03-22 20:19:11,312 [INFO] ---------------------------------
2019-03-22 20:19:26,326 [INFO] ---------------------------------
2019-03-22 20:19:26,327 [INFO] Summary:
2019-03-22 20:19:26,328 [INFO] Batch 82000, worst loss 0.263313 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:19:26,328 [INFO] Regularization: 1183.682983 * 0.0000010000 = 0.0011836829
2019-03-22 20:19:26,329 [INFO] Sum of grad norms: 0.003182
2019-03-22 20:19:26,330 [INFO] ---------------------------------
2019-03-22 20:19:41,335 [INFO] ---------------------------------
2019-03-22 20:19:41,336 [INFO] Summary:
2019-03-22 20:19:41,337 [INFO] Batch 83000, worst loss 0.200268 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:19:41,337 [INFO] Regularization: 1183.558838 * 0.0000010000 = 0.0011835588
2019-03-22 20:19:41,337 [INFO] Sum of grad norms: 10.325855
2019-03-22 20:19:41,338 [INFO] ---------------------------------
2019-03-22 20:19:55,937 [INFO] ---------------------------------
2019-03-22 20:19:55,938 [INFO] Summary:
2019-03-22 20:19:55,939 [INFO] Batch 84000, worst loss 0.262486 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:19:55,939 [INFO] Regularization: 1183.443604 * 0.0000010000 = 0.0011834436
2019-03-22 20:19:55,940 [INFO] Sum of grad norms: 3.689861
2019-03-22 20:19:55,941 [INFO] ---------------------------------
2019-03-22 20:20:10,289 [INFO] ---------------------------------
2019-03-22 20:20:10,290 [INFO] Summary:
2019-03-22 20:20:10,291 [INFO] Batch 85000, worst loss 0.168776 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:20:10,292 [INFO] Regularization: 1183.336304 * 0.0000010000 = 0.0011833363
2019-03-22 20:20:10,292 [INFO] Sum of grad norms: 28.121077
2019-03-22 20:20:10,293 [INFO] ---------------------------------
2019-03-22 20:20:24,413 [INFO] ---------------------------------
2019-03-22 20:20:24,414 [INFO] Summary:
2019-03-22 20:20:24,414 [INFO] Batch 86000, worst loss 0.189277 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:20:24,415 [INFO] Regularization: 1183.254761 * 0.0000010000 = 0.0011832548
2019-03-22 20:20:24,415 [INFO] Sum of grad norms: 0.003393
2019-03-22 20:20:24,416 [INFO] ---------------------------------
2019-03-22 20:20:39,015 [INFO] ---------------------------------
2019-03-22 20:20:39,016 [INFO] Summary:
2019-03-22 20:20:39,017 [INFO] Batch 87000, worst loss 0.280114 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:20:39,017 [INFO] Regularization: 1183.136963 * 0.0000010000 = 0.0011831369
2019-03-22 20:20:39,018 [INFO] Sum of grad norms: 20.088053
2019-03-22 20:20:39,018 [INFO] ---------------------------------
2019-03-22 20:20:52,904 [INFO] ---------------------------------
2019-03-22 20:20:52,905 [INFO] Summary:
2019-03-22 20:20:52,905 [INFO] Batch 88000, worst loss 0.292644 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:20:52,906 [INFO] Regularization: 1183.041870 * 0.0000010000 = 0.0011830418
2019-03-22 20:20:52,906 [INFO] Sum of grad norms: 23.037045
2019-03-22 20:20:52,907 [INFO] ---------------------------------
2019-03-22 20:21:07,180 [INFO] ---------------------------------
2019-03-22 20:21:07,181 [INFO] Summary:
2019-03-22 20:21:07,181 [INFO] Batch 89000, worst loss 0.293486 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:21:07,182 [INFO] Regularization: 1182.954346 * 0.0000010000 = 0.0011829543
2019-03-22 20:21:07,182 [INFO] Sum of grad norms: 0.008006
2019-03-22 20:21:07,183 [INFO] ---------------------------------
2019-03-22 20:21:22,120 [INFO] ---------------------------------
2019-03-22 20:21:22,121 [INFO] Summary:
2019-03-22 20:21:22,122 [INFO] Batch 90000, worst loss 0.177962 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:21:22,122 [INFO] Regularization: 1182.863770 * 0.0000010000 = 0.0011828637
2019-03-22 20:21:22,123 [INFO] Sum of grad norms: 0.693005
2019-03-22 20:21:22,123 [INFO] ---------------------------------
2019-03-22 20:21:24,874 [INFO] ---------------------------------
2019-03-22 20:21:24,874 [INFO] Evaluation:
2019-03-22 20:21:24,876 [INFO] Batch 90000, worst loss 0.177963 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:21:24,878 [INFO] ---------------------------------
2019-03-22 20:21:39,033 [INFO] ---------------------------------
2019-03-22 20:21:39,034 [INFO] Summary:
2019-03-22 20:21:39,034 [INFO] Batch 91000, worst loss 0.215083 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:21:39,035 [INFO] Regularization: 1182.746582 * 0.0000010000 = 0.0011827466
2019-03-22 20:21:39,036 [INFO] Sum of grad norms: 6.629062
2019-03-22 20:21:39,037 [INFO] ---------------------------------
2019-03-22 20:21:54,391 [INFO] ---------------------------------
2019-03-22 20:21:54,392 [INFO] Summary:
2019-03-22 20:21:54,393 [INFO] Batch 92000, worst loss 0.224097 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:21:54,393 [INFO] Regularization: 1182.696899 * 0.0000010000 = 0.0011826969
2019-03-22 20:21:54,394 [INFO] Sum of grad norms: 0.002481
2019-03-22 20:21:54,394 [INFO] ---------------------------------
2019-03-22 20:22:09,271 [INFO] ---------------------------------
2019-03-22 20:22:09,272 [INFO] Summary:
2019-03-22 20:22:09,273 [INFO] Batch 93000, worst loss 0.234805 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:22:09,273 [INFO] Regularization: 1182.671753 * 0.0000010000 = 0.0011826718
2019-03-22 20:22:09,274 [INFO] Sum of grad norms: 0.994702
2019-03-22 20:22:09,274 [INFO] ---------------------------------
2019-03-22 20:22:23,738 [INFO] ---------------------------------
2019-03-22 20:22:23,739 [INFO] Summary:
2019-03-22 20:22:23,740 [INFO] Batch 94000, worst loss 0.206619 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:22:23,740 [INFO] Regularization: 1182.651001 * 0.0000010000 = 0.0011826510
2019-03-22 20:22:23,741 [INFO] Sum of grad norms: 12.189065
2019-03-22 20:22:23,742 [INFO] ---------------------------------
2019-03-22 20:22:38,631 [INFO] ---------------------------------
2019-03-22 20:22:38,632 [INFO] Summary:
2019-03-22 20:22:38,633 [INFO] Batch 95000, worst loss 0.264535 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:22:38,634 [INFO] Regularization: 1182.619263 * 0.0000010000 = 0.0011826192
2019-03-22 20:22:38,634 [INFO] Sum of grad norms: 0.002043
2019-03-22 20:22:38,635 [INFO] ---------------------------------
2019-03-22 20:22:52,807 [INFO] ---------------------------------
2019-03-22 20:22:52,808 [INFO] Summary:
2019-03-22 20:22:52,809 [INFO] Batch 96000, worst loss 0.199691 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:22:52,809 [INFO] Regularization: 1182.603760 * 0.0000010000 = 0.0011826038
2019-03-22 20:22:52,810 [INFO] Sum of grad norms: 0.002838
2019-03-22 20:22:52,811 [INFO] ---------------------------------
2019-03-22 20:23:07,791 [INFO] ---------------------------------
2019-03-22 20:23:07,792 [INFO] Summary:
2019-03-22 20:23:07,793 [INFO] Batch 97000, worst loss 0.218452 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:23:07,793 [INFO] Regularization: 1182.579590 * 0.0000010000 = 0.0011825796
2019-03-22 20:23:07,794 [INFO] Sum of grad norms: 0.003607
2019-03-22 20:23:07,794 [INFO] ---------------------------------
2019-03-22 20:23:22,442 [INFO] ---------------------------------
2019-03-22 20:23:22,443 [INFO] Summary:
2019-03-22 20:23:22,444 [INFO] Batch 98000, worst loss 0.317154 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:23:22,444 [INFO] Regularization: 1182.541992 * 0.0000010000 = 0.0011825419
2019-03-22 20:23:22,445 [INFO] Sum of grad norms: 13.905829
2019-03-22 20:23:22,445 [INFO] ---------------------------------
2019-03-22 20:23:36,932 [INFO] ---------------------------------
2019-03-22 20:23:36,933 [INFO] Summary:
2019-03-22 20:23:36,934 [INFO] Batch 99000, worst loss 0.187651 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:23:36,934 [INFO] Regularization: 1182.524536 * 0.0000010000 = 0.0011825245
2019-03-22 20:23:36,935 [INFO] Sum of grad norms: 0.002747
2019-03-22 20:23:36,935 [INFO] ---------------------------------
2019-03-22 20:23:51,138 [INFO] ---------------------------------
2019-03-22 20:23:51,138 [INFO] Summary:
2019-03-22 20:23:51,139 [INFO] Batch 100000, worst loss 0.188826 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:23:51,139 [INFO] Regularization: 1182.510132 * 0.0000010000 = 0.0011825102
2019-03-22 20:23:51,140 [INFO] Sum of grad norms: 0.051273
2019-03-22 20:23:51,141 [INFO] ---------------------------------
2019-03-22 20:23:53,898 [INFO] ---------------------------------
2019-03-22 20:23:53,898 [INFO] Evaluation:
2019-03-22 20:23:53,899 [INFO] Batch 100000, worst loss 0.221975 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:23:53,901 [INFO] ---------------------------------
2019-03-22 20:23:53,902 [INFO] Finished training, saved to file classifier/1553275201/1553282633_4_classifier_final.pth
2019-03-22 20:23:54,066 [INFO] ---------------------------------
2019-03-22 20:23:54,068 [INFO] Training model #5: (8, 64, 2) @ 2
2019-03-22 20:24:09,244 [INFO] ---------------------------------
2019-03-22 20:24:09,245 [INFO] Summary:
2019-03-22 20:24:09,246 [INFO] Batch 1000, worst loss 1082.489746 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:24:09,247 [INFO] Regularization: 30919.410156 * 0.0000010000 = 0.0309194103
2019-03-22 20:24:09,247 [INFO] Sum of grad norms: 1114.158081
2019-03-22 20:24:09,248 [INFO] ---------------------------------
2019-03-22 20:24:24,049 [INFO] ---------------------------------
2019-03-22 20:24:24,050 [INFO] Summary:
2019-03-22 20:24:24,050 [INFO] Batch 2000, worst loss 35.283081 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:24:24,051 [INFO] Regularization: 29646.757812 * 0.0000010000 = 0.0296467580
2019-03-22 20:24:24,052 [INFO] Sum of grad norms: 1156.314941
2019-03-22 20:24:24,053 [INFO] ---------------------------------
2019-03-22 20:24:38,768 [INFO] ---------------------------------
2019-03-22 20:24:38,769 [INFO] Summary:
2019-03-22 20:24:38,769 [INFO] Batch 3000, worst loss 14.320908 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:24:38,770 [INFO] Regularization: 26887.378906 * 0.0000010000 = 0.0268873796
2019-03-22 20:24:38,770 [INFO] Sum of grad norms: 265.451080
2019-03-22 20:24:38,771 [INFO] ---------------------------------
2019-03-22 20:24:53,626 [INFO] ---------------------------------
2019-03-22 20:24:53,627 [INFO] Summary:
2019-03-22 20:24:53,627 [INFO] Batch 4000, worst loss 10.171766 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:24:53,628 [INFO] Regularization: 22527.509766 * 0.0000010000 = 0.0225275103
2019-03-22 20:24:53,628 [INFO] Sum of grad norms: 397.028320
2019-03-22 20:24:53,629 [INFO] ---------------------------------
2019-03-22 20:25:08,419 [INFO] ---------------------------------
2019-03-22 20:25:08,420 [INFO] Summary:
2019-03-22 20:25:08,421 [INFO] Batch 5000, worst loss 4.551004 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:25:08,421 [INFO] Regularization: 16967.310547 * 0.0000010000 = 0.0169673096
2019-03-22 20:25:08,422 [INFO] Sum of grad norms: 66.371689
2019-03-22 20:25:08,423 [INFO] ---------------------------------
2019-03-22 20:25:23,628 [INFO] ---------------------------------
2019-03-22 20:25:23,629 [INFO] Summary:
2019-03-22 20:25:23,630 [INFO] Batch 6000, worst loss 2.143551 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:25:23,631 [INFO] Regularization: 11492.856445 * 0.0000010000 = 0.0114928568
2019-03-22 20:25:23,631 [INFO] Sum of grad norms: 80.089325
2019-03-22 20:25:23,632 [INFO] ---------------------------------
2019-03-22 20:25:39,302 [INFO] ---------------------------------
2019-03-22 20:25:39,303 [INFO] Summary:
2019-03-22 20:25:39,304 [INFO] Batch 7000, worst loss 2.024316 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:25:39,305 [INFO] Regularization: 9119.249023 * 0.0000010000 = 0.0091192489
2019-03-22 20:25:39,306 [INFO] Sum of grad norms: 60.609020
2019-03-22 20:25:39,307 [INFO] ---------------------------------
2019-03-22 20:25:53,724 [INFO] ---------------------------------
2019-03-22 20:25:53,725 [INFO] Summary:
2019-03-22 20:25:53,726 [INFO] Batch 8000, worst loss 0.923777 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:25:53,727 [INFO] Regularization: 8893.786133 * 0.0000010000 = 0.0088937860
2019-03-22 20:25:53,727 [INFO] Sum of grad norms: 9.816292
2019-03-22 20:25:53,728 [INFO] ---------------------------------
2019-03-22 20:26:08,411 [INFO] ---------------------------------
2019-03-22 20:26:08,412 [INFO] Summary:
2019-03-22 20:26:08,412 [INFO] Batch 9000, worst loss 0.641832 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:26:08,413 [INFO] Regularization: 8633.371094 * 0.0000010000 = 0.0086333714
2019-03-22 20:26:08,413 [INFO] Sum of grad norms: 10.352040
2019-03-22 20:26:08,414 [INFO] ---------------------------------
2019-03-22 20:26:24,106 [INFO] ---------------------------------
2019-03-22 20:26:24,107 [INFO] Summary:
2019-03-22 20:26:24,107 [INFO] Batch 10000, worst loss 0.430597 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:26:24,108 [INFO] Regularization: 8297.196289 * 0.0000010000 = 0.0082971966
2019-03-22 20:26:24,108 [INFO] Sum of grad norms: 2.939957
2019-03-22 20:26:24,109 [INFO] ---------------------------------
2019-03-22 20:26:26,849 [INFO] ---------------------------------
2019-03-22 20:26:26,851 [INFO] Evaluation:
2019-03-22 20:26:26,852 [INFO] Batch 10000, worst loss 0.348407 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:26:26,852 [INFO] ---------------------------------
2019-03-22 20:26:42,070 [INFO] ---------------------------------
2019-03-22 20:26:42,071 [INFO] Summary:
2019-03-22 20:26:42,071 [INFO] Batch 11000, worst loss 0.560396 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:26:42,072 [INFO] Regularization: 7999.082031 * 0.0000010000 = 0.0079990821
2019-03-22 20:26:42,072 [INFO] Sum of grad norms: 7.138239
2019-03-22 20:26:42,073 [INFO] ---------------------------------
2019-03-22 20:26:56,296 [INFO] ---------------------------------
2019-03-22 20:26:56,297 [INFO] Summary:
2019-03-22 20:26:56,298 [INFO] Batch 12000, worst loss 0.407618 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:26:56,299 [INFO] Regularization: 7684.238770 * 0.0000010000 = 0.0076842387
2019-03-22 20:26:56,300 [INFO] Sum of grad norms: 9.711514
2019-03-22 20:26:56,302 [INFO] ---------------------------------
2019-03-22 20:27:10,126 [INFO] ---------------------------------
2019-03-22 20:27:10,126 [INFO] Summary:
2019-03-22 20:27:10,127 [INFO] Batch 13000, worst loss 0.403654 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:27:10,127 [INFO] Regularization: 7382.324219 * 0.0000010000 = 0.0073823244
2019-03-22 20:27:10,128 [INFO] Sum of grad norms: 7.632117
2019-03-22 20:27:10,128 [INFO] ---------------------------------
2019-03-22 20:27:24,679 [INFO] ---------------------------------
2019-03-22 20:27:24,680 [INFO] Summary:
2019-03-22 20:27:24,680 [INFO] Batch 14000, worst loss 0.452688 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:27:24,681 [INFO] Regularization: 7043.695312 * 0.0000010000 = 0.0070436951
2019-03-22 20:27:24,681 [INFO] Sum of grad norms: 7.738899
2019-03-22 20:27:24,682 [INFO] ---------------------------------
2019-03-22 20:27:38,802 [INFO] ---------------------------------
2019-03-22 20:27:38,802 [INFO] Summary:
2019-03-22 20:27:38,803 [INFO] Batch 15000, worst loss 0.377958 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:27:38,803 [INFO] Regularization: 6760.791992 * 0.0000010000 = 0.0067607919
2019-03-22 20:27:38,804 [INFO] Sum of grad norms: 10.372108
2019-03-22 20:27:38,805 [INFO] ---------------------------------
2019-03-22 20:27:52,916 [INFO] ---------------------------------
2019-03-22 20:27:52,917 [INFO] Summary:
2019-03-22 20:27:52,917 [INFO] Batch 16000, worst loss 0.520694 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:27:52,918 [INFO] Regularization: 6437.806641 * 0.0000010000 = 0.0064378064
2019-03-22 20:27:52,918 [INFO] Sum of grad norms: 4.308659
2019-03-22 20:27:52,919 [INFO] ---------------------------------
2019-03-22 20:28:07,356 [INFO] ---------------------------------
2019-03-22 20:28:07,357 [INFO] Summary:
2019-03-22 20:28:07,358 [INFO] Batch 17000, worst loss 0.444692 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:28:07,359 [INFO] Regularization: 6114.893555 * 0.0000010000 = 0.0061148936
2019-03-22 20:28:07,360 [INFO] Sum of grad norms: 4.992108
2019-03-22 20:28:07,360 [INFO] ---------------------------------
2019-03-22 20:28:21,528 [INFO] ---------------------------------
2019-03-22 20:28:21,529 [INFO] Summary:
2019-03-22 20:28:21,530 [INFO] Batch 18000, worst loss 0.425980 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:28:21,530 [INFO] Regularization: 5787.422852 * 0.0000010000 = 0.0057874229
2019-03-22 20:28:21,531 [INFO] Sum of grad norms: 4.159366
2019-03-22 20:28:21,532 [INFO] ---------------------------------
2019-03-22 20:28:36,378 [INFO] ---------------------------------
2019-03-22 20:28:36,379 [INFO] Summary:
2019-03-22 20:28:36,380 [INFO] Batch 19000, worst loss 0.491247 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:28:36,381 [INFO] Regularization: 5466.796387 * 0.0000010000 = 0.0054667965
2019-03-22 20:28:36,382 [INFO] Sum of grad norms: 16.948452
2019-03-22 20:28:36,383 [INFO] ---------------------------------
2019-03-22 20:28:50,646 [INFO] ---------------------------------
2019-03-22 20:28:50,647 [INFO] Summary:
2019-03-22 20:28:50,648 [INFO] Batch 20000, worst loss 0.383303 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:28:50,648 [INFO] Regularization: 5090.250488 * 0.0000010000 = 0.0050902506
2019-03-22 20:28:50,649 [INFO] Sum of grad norms: 6.360898
2019-03-22 20:28:50,649 [INFO] ---------------------------------
2019-03-22 20:28:53,367 [INFO] ---------------------------------
2019-03-22 20:28:53,368 [INFO] Evaluation:
2019-03-22 20:28:53,369 [INFO] Batch 20000, worst loss 0.379037 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:28:53,371 [INFO] ---------------------------------
2019-03-22 20:29:08,305 [INFO] ---------------------------------
2019-03-22 20:29:08,306 [INFO] Summary:
2019-03-22 20:29:08,307 [INFO] Batch 21000, worst loss 0.318885 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:29:08,308 [INFO] Regularization: 4760.453613 * 0.0000010000 = 0.0047604535
2019-03-22 20:29:08,308 [INFO] Sum of grad norms: 5.443916
2019-03-22 20:29:08,309 [INFO] ---------------------------------
2019-03-22 20:29:23,541 [INFO] ---------------------------------
2019-03-22 20:29:23,542 [INFO] Summary:
2019-03-22 20:29:23,542 [INFO] Batch 22000, worst loss 0.436313 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:29:23,543 [INFO] Regularization: 4440.152344 * 0.0000010000 = 0.0044401526
2019-03-22 20:29:23,543 [INFO] Sum of grad norms: 8.388609
2019-03-22 20:29:23,544 [INFO] ---------------------------------
2019-03-22 20:29:37,928 [INFO] ---------------------------------
2019-03-22 20:29:37,929 [INFO] Summary:
2019-03-22 20:29:37,929 [INFO] Batch 23000, worst loss 0.458475 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:29:37,930 [INFO] Regularization: 4104.403320 * 0.0000010000 = 0.0041044033
2019-03-22 20:29:37,931 [INFO] Sum of grad norms: 0.783654
2019-03-22 20:29:37,931 [INFO] ---------------------------------
2019-03-22 20:29:51,862 [INFO] ---------------------------------
2019-03-22 20:29:51,863 [INFO] Summary:
2019-03-22 20:29:51,864 [INFO] Batch 24000, worst loss 0.336774 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:29:51,864 [INFO] Regularization: 3827.816650 * 0.0000010000 = 0.0038278166
2019-03-22 20:29:51,865 [INFO] Sum of grad norms: 4.688655
2019-03-22 20:29:51,865 [INFO] ---------------------------------
2019-03-22 20:30:06,638 [INFO] ---------------------------------
2019-03-22 20:30:06,639 [INFO] Summary:
2019-03-22 20:30:06,639 [INFO] Batch 25000, worst loss 0.350067 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:30:06,640 [INFO] Regularization: 3607.807617 * 0.0000010000 = 0.0036078077
2019-03-22 20:30:06,640 [INFO] Sum of grad norms: 0.606317
2019-03-22 20:30:06,641 [INFO] ---------------------------------
2019-03-22 20:30:21,872 [INFO] ---------------------------------
2019-03-22 20:30:21,873 [INFO] Summary:
2019-03-22 20:30:21,873 [INFO] Batch 26000, worst loss 0.310744 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:30:21,874 [INFO] Regularization: 3389.863525 * 0.0000010000 = 0.0033898635
2019-03-22 20:30:21,874 [INFO] Sum of grad norms: 8.024255
2019-03-22 20:30:21,875 [INFO] ---------------------------------
2019-03-22 20:30:36,030 [INFO] ---------------------------------
2019-03-22 20:30:36,031 [INFO] Summary:
2019-03-22 20:30:36,032 [INFO] Batch 27000, worst loss 0.340472 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:30:36,033 [INFO] Regularization: 3245.136963 * 0.0000010000 = 0.0032451369
2019-03-22 20:30:36,033 [INFO] Sum of grad norms: 18.137793
2019-03-22 20:30:36,034 [INFO] ---------------------------------
2019-03-22 20:30:51,292 [INFO] ---------------------------------
2019-03-22 20:30:51,293 [INFO] Summary:
2019-03-22 20:30:51,293 [INFO] Batch 28000, worst loss 0.325163 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:30:51,294 [INFO] Regularization: 3084.608154 * 0.0000010000 = 0.0030846081
2019-03-22 20:30:51,294 [INFO] Sum of grad norms: 6.731481
2019-03-22 20:30:51,295 [INFO] ---------------------------------
2019-03-22 20:31:05,482 [INFO] ---------------------------------
2019-03-22 20:31:05,483 [INFO] Summary:
2019-03-22 20:31:05,483 [INFO] Batch 29000, worst loss 0.389664 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:31:05,484 [INFO] Regularization: 3024.193848 * 0.0000010000 = 0.0030241939
2019-03-22 20:31:05,484 [INFO] Sum of grad norms: 5.974254
2019-03-22 20:31:05,485 [INFO] ---------------------------------
2019-03-22 20:31:19,505 [INFO] ---------------------------------
2019-03-22 20:31:19,506 [INFO] Summary:
2019-03-22 20:31:19,507 [INFO] Batch 30000, worst loss 0.399830 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:31:19,508 [INFO] Regularization: 2871.384766 * 0.0000010000 = 0.0028713848
2019-03-22 20:31:19,508 [INFO] Sum of grad norms: 8.161958
2019-03-22 20:31:19,509 [INFO] ---------------------------------
2019-03-22 20:31:22,243 [INFO] ---------------------------------
2019-03-22 20:31:22,245 [INFO] Evaluation:
2019-03-22 20:31:22,245 [INFO] Batch 30000, worst loss 0.338131 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:31:22,246 [INFO] ---------------------------------
2019-03-22 20:31:36,525 [INFO] ---------------------------------
2019-03-22 20:31:36,525 [INFO] Summary:
2019-03-22 20:31:36,526 [INFO] Batch 31000, worst loss 0.378837 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:31:36,526 [INFO] Regularization: 2788.420410 * 0.0000010000 = 0.0027884203
2019-03-22 20:31:36,527 [INFO] Sum of grad norms: 4.847061
2019-03-22 20:31:36,528 [INFO] ---------------------------------
2019-03-22 20:31:50,887 [INFO] ---------------------------------
2019-03-22 20:31:50,888 [INFO] Summary:
2019-03-22 20:31:50,888 [INFO] Batch 32000, worst loss 0.424416 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:31:50,889 [INFO] Regularization: 2640.883545 * 0.0000010000 = 0.0026408834
2019-03-22 20:31:50,889 [INFO] Sum of grad norms: 3.754842
2019-03-22 20:31:50,890 [INFO] ---------------------------------
2019-03-22 20:32:05,619 [INFO] ---------------------------------
2019-03-22 20:32:05,620 [INFO] Summary:
2019-03-22 20:32:05,620 [INFO] Batch 33000, worst loss 0.288835 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:32:05,621 [INFO] Regularization: 2541.571045 * 0.0000010000 = 0.0025415709
2019-03-22 20:32:05,621 [INFO] Sum of grad norms: 19.083950
2019-03-22 20:32:05,622 [INFO] ---------------------------------
2019-03-22 20:32:20,024 [INFO] ---------------------------------
2019-03-22 20:32:20,025 [INFO] Summary:
2019-03-22 20:32:20,025 [INFO] Batch 34000, worst loss 0.307881 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:32:20,026 [INFO] Regularization: 2437.068359 * 0.0000010000 = 0.0024370684
2019-03-22 20:32:20,027 [INFO] Sum of grad norms: 0.137837
2019-03-22 20:32:20,027 [INFO] ---------------------------------
2019-03-22 20:32:34,471 [INFO] ---------------------------------
2019-03-22 20:32:34,472 [INFO] Summary:
2019-03-22 20:32:34,472 [INFO] Batch 35000, worst loss 0.366632 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:32:34,473 [INFO] Regularization: 2401.135010 * 0.0000010000 = 0.0024011349
2019-03-22 20:32:34,473 [INFO] Sum of grad norms: 16.434891
2019-03-22 20:32:34,474 [INFO] ---------------------------------
2019-03-22 20:32:48,629 [INFO] ---------------------------------
2019-03-22 20:32:48,630 [INFO] Summary:
2019-03-22 20:32:48,630 [INFO] Batch 36000, worst loss 0.318798 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:32:48,631 [INFO] Regularization: 2290.893311 * 0.0000010000 = 0.0022908933
2019-03-22 20:32:48,631 [INFO] Sum of grad norms: 1.619382
2019-03-22 20:32:48,632 [INFO] ---------------------------------
2019-03-22 20:33:03,310 [INFO] ---------------------------------
2019-03-22 20:33:03,311 [INFO] Summary:
2019-03-22 20:33:03,311 [INFO] Batch 37000, worst loss 0.372829 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:33:03,312 [INFO] Regularization: 2240.291992 * 0.0000010000 = 0.0022402920
2019-03-22 20:33:03,312 [INFO] Sum of grad norms: 24.353928
2019-03-22 20:33:03,313 [INFO] ---------------------------------
2019-03-22 20:33:17,349 [INFO] ---------------------------------
2019-03-22 20:33:17,350 [INFO] Summary:
2019-03-22 20:33:17,351 [INFO] Batch 38000, worst loss 0.373669 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:33:17,351 [INFO] Regularization: 2166.842529 * 0.0000010000 = 0.0021668426
2019-03-22 20:33:17,352 [INFO] Sum of grad norms: 3.275539
2019-03-22 20:33:17,352 [INFO] ---------------------------------
2019-03-22 20:33:32,299 [INFO] ---------------------------------
2019-03-22 20:33:32,300 [INFO] Summary:
2019-03-22 20:33:32,301 [INFO] Batch 39000, worst loss 0.387993 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:33:32,301 [INFO] Regularization: 2119.272705 * 0.0000010000 = 0.0021192727
2019-03-22 20:33:32,302 [INFO] Sum of grad norms: 5.764889
2019-03-22 20:33:32,303 [INFO] ---------------------------------
2019-03-22 20:33:46,243 [INFO] ---------------------------------
2019-03-22 20:33:46,244 [INFO] Summary:
2019-03-22 20:33:46,245 [INFO] Batch 40000, worst loss 0.324367 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:33:46,245 [INFO] Regularization: 2035.746582 * 0.0000010000 = 0.0020357466
2019-03-22 20:33:46,246 [INFO] Sum of grad norms: 20.606195
2019-03-22 20:33:46,246 [INFO] ---------------------------------
2019-03-22 20:33:48,945 [INFO] ---------------------------------
2019-03-22 20:33:48,945 [INFO] Evaluation:
2019-03-22 20:33:48,946 [INFO] Batch 40000, worst loss 0.254874 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:33:48,947 [INFO] ---------------------------------
2019-03-22 20:34:03,778 [INFO] ---------------------------------
2019-03-22 20:34:03,779 [INFO] Summary:
2019-03-22 20:34:03,779 [INFO] Batch 41000, worst loss 0.367867 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:34:03,780 [INFO] Regularization: 1992.833252 * 0.0000010000 = 0.0019928333
2019-03-22 20:34:03,781 [INFO] Sum of grad norms: 7.182041
2019-03-22 20:34:03,781 [INFO] ---------------------------------
2019-03-22 20:34:18,064 [INFO] ---------------------------------
2019-03-22 20:34:18,065 [INFO] Summary:
2019-03-22 20:34:18,066 [INFO] Batch 42000, worst loss 0.251905 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:34:18,067 [INFO] Regularization: 1946.185669 * 0.0000010000 = 0.0019461857
2019-03-22 20:34:18,067 [INFO] Sum of grad norms: 3.222729
2019-03-22 20:34:18,068 [INFO] ---------------------------------
2019-03-22 20:34:32,601 [INFO] ---------------------------------
2019-03-22 20:34:32,602 [INFO] Summary:
2019-03-22 20:34:32,602 [INFO] Batch 43000, worst loss 0.268989 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:34:32,603 [INFO] Regularization: 1905.594360 * 0.0000010000 = 0.0019055944
2019-03-22 20:34:32,604 [INFO] Sum of grad norms: 0.004277
2019-03-22 20:34:32,604 [INFO] ---------------------------------
2019-03-22 20:34:47,568 [INFO] ---------------------------------
2019-03-22 20:34:47,569 [INFO] Summary:
2019-03-22 20:34:47,570 [INFO] Batch 44000, worst loss 0.383134 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:34:47,570 [INFO] Regularization: 1867.987671 * 0.0000010000 = 0.0018679877
2019-03-22 20:34:47,571 [INFO] Sum of grad norms: 12.530918
2019-03-22 20:34:47,571 [INFO] ---------------------------------
2019-03-22 20:35:01,966 [INFO] ---------------------------------
2019-03-22 20:35:01,967 [INFO] Summary:
2019-03-22 20:35:01,968 [INFO] Batch 45000, worst loss 0.239303 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:35:01,969 [INFO] Regularization: 1836.450806 * 0.0000010000 = 0.0018364508
2019-03-22 20:35:01,969 [INFO] Sum of grad norms: 8.857698
2019-03-22 20:35:01,970 [INFO] ---------------------------------
2019-03-22 20:35:15,849 [INFO] ---------------------------------
2019-03-22 20:35:15,850 [INFO] Summary:
2019-03-22 20:35:15,850 [INFO] Batch 46000, worst loss 0.341573 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:35:15,851 [INFO] Regularization: 1812.693848 * 0.0000010000 = 0.0018126939
2019-03-22 20:35:15,851 [INFO] Sum of grad norms: 14.686476
2019-03-22 20:35:15,852 [INFO] ---------------------------------
2019-03-22 20:35:29,954 [INFO] ---------------------------------
2019-03-22 20:35:29,956 [INFO] Summary:
2019-03-22 20:35:29,956 [INFO] Batch 47000, worst loss 0.332085 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:35:29,957 [INFO] Regularization: 1781.649170 * 0.0000010000 = 0.0017816492
2019-03-22 20:35:29,957 [INFO] Sum of grad norms: 0.075441
2019-03-22 20:35:29,958 [INFO] ---------------------------------
2019-03-22 20:35:44,719 [INFO] ---------------------------------
2019-03-22 20:35:44,720 [INFO] Summary:
2019-03-22 20:35:44,721 [INFO] Batch 48000, worst loss 0.249724 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:35:44,721 [INFO] Regularization: 1752.641479 * 0.0000010000 = 0.0017526415
2019-03-22 20:35:44,722 [INFO] Sum of grad norms: 0.004478
2019-03-22 20:35:44,723 [INFO] ---------------------------------
2019-03-22 20:35:58,880 [INFO] ---------------------------------
2019-03-22 20:35:58,881 [INFO] Summary:
2019-03-22 20:35:58,882 [INFO] Batch 49000, worst loss 0.266439 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:35:58,882 [INFO] Regularization: 1733.355103 * 0.0000010000 = 0.0017333551
2019-03-22 20:35:58,883 [INFO] Sum of grad norms: 11.945938
2019-03-22 20:35:58,883 [INFO] ---------------------------------
2019-03-22 20:36:14,064 [INFO] ---------------------------------
2019-03-22 20:36:14,065 [INFO] Summary:
2019-03-22 20:36:14,066 [INFO] Batch 50000, worst loss 0.320450 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:36:14,066 [INFO] Regularization: 1717.243286 * 0.0000010000 = 0.0017172432
2019-03-22 20:36:14,067 [INFO] Sum of grad norms: 0.059884
2019-03-22 20:36:14,067 [INFO] ---------------------------------
2019-03-22 20:36:16,774 [INFO] ---------------------------------
2019-03-22 20:36:16,775 [INFO] Evaluation:
2019-03-22 20:36:16,776 [INFO] Batch 50000, worst loss 0.231352 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:36:16,777 [INFO] ---------------------------------
2019-03-22 20:36:32,018 [INFO] ---------------------------------
2019-03-22 20:36:32,019 [INFO] Summary:
2019-03-22 20:36:32,020 [INFO] Batch 51000, worst loss 0.243392 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:36:32,020 [INFO] Regularization: 1692.119507 * 0.0000010000 = 0.0016921195
2019-03-22 20:36:32,021 [INFO] Sum of grad norms: 21.951168
2019-03-22 20:36:32,021 [INFO] ---------------------------------
2019-03-22 20:36:46,701 [INFO] ---------------------------------
2019-03-22 20:36:46,702 [INFO] Summary:
2019-03-22 20:36:46,702 [INFO] Batch 52000, worst loss 0.233856 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:36:46,703 [INFO] Regularization: 1678.204224 * 0.0000010000 = 0.0016782042
2019-03-22 20:36:46,703 [INFO] Sum of grad norms: 3.076545
2019-03-22 20:36:46,704 [INFO] ---------------------------------
2019-03-22 20:37:00,913 [INFO] ---------------------------------
2019-03-22 20:37:00,914 [INFO] Summary:
2019-03-22 20:37:00,915 [INFO] Batch 53000, worst loss 0.193655 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:37:00,916 [INFO] Regularization: 1670.226685 * 0.0000010000 = 0.0016702267
2019-03-22 20:37:00,917 [INFO] Sum of grad norms: 6.117026
2019-03-22 20:37:00,918 [INFO] ---------------------------------
2019-03-22 20:37:15,284 [INFO] ---------------------------------
2019-03-22 20:37:15,285 [INFO] Summary:
2019-03-22 20:37:15,285 [INFO] Batch 54000, worst loss 0.211839 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:37:15,286 [INFO] Regularization: 1661.687256 * 0.0000010000 = 0.0016616873
2019-03-22 20:37:15,286 [INFO] Sum of grad norms: 24.079321
2019-03-22 20:37:15,287 [INFO] ---------------------------------
2019-03-22 20:37:29,999 [INFO] ---------------------------------
2019-03-22 20:37:30,000 [INFO] Summary:
2019-03-22 20:37:30,001 [INFO] Batch 55000, worst loss 0.239981 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:37:30,002 [INFO] Regularization: 1654.674316 * 0.0000010000 = 0.0016546743
2019-03-22 20:37:30,003 [INFO] Sum of grad norms: 12.847199
2019-03-22 20:37:30,004 [INFO] ---------------------------------
2019-03-22 20:37:44,646 [INFO] ---------------------------------
2019-03-22 20:37:44,647 [INFO] Summary:
2019-03-22 20:37:44,648 [INFO] Batch 56000, worst loss 0.180677 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:37:44,649 [INFO] Regularization: 1645.912231 * 0.0000010000 = 0.0016459122
2019-03-22 20:37:44,649 [INFO] Sum of grad norms: 37.422634
2019-03-22 20:37:44,650 [INFO] ---------------------------------
2019-03-22 20:38:00,092 [INFO] ---------------------------------
2019-03-22 20:38:00,093 [INFO] Summary:
2019-03-22 20:38:00,094 [INFO] Batch 57000, worst loss 0.314168 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:38:00,094 [INFO] Regularization: 1637.578369 * 0.0000010000 = 0.0016375784
2019-03-22 20:38:00,095 [INFO] Sum of grad norms: 26.907816
2019-03-22 20:38:00,096 [INFO] ---------------------------------
2019-03-22 20:38:14,306 [INFO] ---------------------------------
2019-03-22 20:38:14,307 [INFO] Summary:
2019-03-22 20:38:14,308 [INFO] Batch 58000, worst loss 0.265042 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:38:14,309 [INFO] Regularization: 1629.627441 * 0.0000010000 = 0.0016296274
2019-03-22 20:38:14,309 [INFO] Sum of grad norms: 0.060526
2019-03-22 20:38:14,310 [INFO] ---------------------------------
2019-03-22 20:38:28,621 [INFO] ---------------------------------
2019-03-22 20:38:28,622 [INFO] Summary:
2019-03-22 20:38:28,622 [INFO] Batch 59000, worst loss 0.228664 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:38:28,623 [INFO] Regularization: 1624.265503 * 0.0000010000 = 0.0016242655
2019-03-22 20:38:28,623 [INFO] Sum of grad norms: 20.086229
2019-03-22 20:38:28,624 [INFO] ---------------------------------
2019-03-22 20:38:43,577 [INFO] ---------------------------------
2019-03-22 20:38:43,579 [INFO] Summary:
2019-03-22 20:38:43,579 [INFO] Batch 60000, worst loss 0.243825 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 20:38:43,580 [INFO] Regularization: 1616.572266 * 0.0000010000 = 0.0016165723
2019-03-22 20:38:43,580 [INFO] Sum of grad norms: 0.057308
2019-03-22 20:38:43,581 [INFO] ---------------------------------
2019-03-22 20:38:46,289 [INFO] ---------------------------------
2019-03-22 20:38:46,289 [INFO] Evaluation:
2019-03-22 20:38:46,290 [INFO] Batch 60000, worst loss 0.243747 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:38:46,291 [INFO] ---------------------------------
2019-03-22 20:39:01,690 [INFO] ---------------------------------
2019-03-22 20:39:01,691 [INFO] Summary:
2019-03-22 20:39:01,691 [INFO] Batch 61000, worst loss 0.250931 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:39:01,692 [INFO] Regularization: 1609.774414 * 0.0000010000 = 0.0016097744
2019-03-22 20:39:01,692 [INFO] Sum of grad norms: 3.934317
2019-03-22 20:39:01,693 [INFO] ---------------------------------
2019-03-22 20:39:15,733 [INFO] ---------------------------------
2019-03-22 20:39:15,734 [INFO] Summary:
2019-03-22 20:39:15,734 [INFO] Batch 62000, worst loss 0.203970 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:39:15,735 [INFO] Regularization: 1606.832275 * 0.0000010000 = 0.0016068323
2019-03-22 20:39:15,735 [INFO] Sum of grad norms: 11.970565
2019-03-22 20:39:15,736 [INFO] ---------------------------------
2019-03-22 20:39:30,950 [INFO] ---------------------------------
2019-03-22 20:39:30,951 [INFO] Summary:
2019-03-22 20:39:30,952 [INFO] Batch 63000, worst loss 0.203360 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:39:30,953 [INFO] Regularization: 1604.970947 * 0.0000010000 = 0.0016049709
2019-03-22 20:39:30,954 [INFO] Sum of grad norms: 0.020655
2019-03-22 20:39:30,954 [INFO] ---------------------------------
2019-03-22 20:39:45,204 [INFO] ---------------------------------
2019-03-22 20:39:45,204 [INFO] Summary:
2019-03-22 20:39:45,205 [INFO] Batch 64000, worst loss 0.221455 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:39:45,206 [INFO] Regularization: 1603.147705 * 0.0000010000 = 0.0016031477
2019-03-22 20:39:45,206 [INFO] Sum of grad norms: 11.453806
2019-03-22 20:39:45,207 [INFO] ---------------------------------
2019-03-22 20:39:59,863 [INFO] ---------------------------------
2019-03-22 20:39:59,864 [INFO] Summary:
2019-03-22 20:39:59,865 [INFO] Batch 65000, worst loss 0.212722 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:39:59,866 [INFO] Regularization: 1601.325806 * 0.0000010000 = 0.0016013258
2019-03-22 20:39:59,866 [INFO] Sum of grad norms: 7.894598
2019-03-22 20:39:59,867 [INFO] ---------------------------------
2019-03-22 20:40:15,126 [INFO] ---------------------------------
2019-03-22 20:40:15,127 [INFO] Summary:
2019-03-22 20:40:15,127 [INFO] Batch 66000, worst loss 0.254257 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:40:15,128 [INFO] Regularization: 1599.080811 * 0.0000010000 = 0.0015990809
2019-03-22 20:40:15,129 [INFO] Sum of grad norms: 28.936531
2019-03-22 20:40:15,129 [INFO] ---------------------------------
2019-03-22 20:40:30,120 [INFO] ---------------------------------
2019-03-22 20:40:30,121 [INFO] Summary:
2019-03-22 20:40:30,121 [INFO] Batch 67000, worst loss 0.246802 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:40:30,122 [INFO] Regularization: 1597.137939 * 0.0000010000 = 0.0015971379
2019-03-22 20:40:30,122 [INFO] Sum of grad norms: 0.006363
2019-03-22 20:40:30,123 [INFO] ---------------------------------
2019-03-22 20:40:43,913 [INFO] ---------------------------------
2019-03-22 20:40:43,914 [INFO] Summary:
2019-03-22 20:40:43,915 [INFO] Batch 68000, worst loss 0.199061 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:40:43,915 [INFO] Regularization: 1594.578125 * 0.0000010000 = 0.0015945781
2019-03-22 20:40:43,916 [INFO] Sum of grad norms: 21.948692
2019-03-22 20:40:43,916 [INFO] ---------------------------------
2019-03-22 20:40:58,218 [INFO] ---------------------------------
2019-03-22 20:40:58,219 [INFO] Summary:
2019-03-22 20:40:58,219 [INFO] Batch 69000, worst loss 0.305314 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:40:58,220 [INFO] Regularization: 1592.752197 * 0.0000010000 = 0.0015927522
2019-03-22 20:40:58,220 [INFO] Sum of grad norms: 0.006383
2019-03-22 20:40:58,221 [INFO] ---------------------------------
2019-03-22 20:41:12,540 [INFO] ---------------------------------
2019-03-22 20:41:12,541 [INFO] Summary:
2019-03-22 20:41:12,542 [INFO] Batch 70000, worst loss 0.210797 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 20:41:12,542 [INFO] Regularization: 1590.567261 * 0.0000010000 = 0.0015905673
2019-03-22 20:41:12,543 [INFO] Sum of grad norms: 0.003077
2019-03-22 20:41:12,544 [INFO] ---------------------------------
2019-03-22 20:41:15,308 [INFO] ---------------------------------
2019-03-22 20:41:15,309 [INFO] Evaluation:
2019-03-22 20:41:15,310 [INFO] Batch 70000, worst loss 0.209506 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:41:15,311 [INFO] ---------------------------------
2019-03-22 20:41:29,660 [INFO] ---------------------------------
2019-03-22 20:41:29,661 [INFO] Summary:
2019-03-22 20:41:29,662 [INFO] Batch 71000, worst loss 0.211620 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:41:29,662 [INFO] Regularization: 1588.227783 * 0.0000010000 = 0.0015882278
2019-03-22 20:41:29,663 [INFO] Sum of grad norms: 27.430033
2019-03-22 20:41:29,663 [INFO] ---------------------------------
2019-03-22 20:41:45,115 [INFO] ---------------------------------
2019-03-22 20:41:45,115 [INFO] Summary:
2019-03-22 20:41:45,116 [INFO] Batch 72000, worst loss 0.183189 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:41:45,116 [INFO] Regularization: 1587.552734 * 0.0000010000 = 0.0015875527
2019-03-22 20:41:45,117 [INFO] Sum of grad norms: 13.187153
2019-03-22 20:41:45,117 [INFO] ---------------------------------
2019-03-22 20:41:59,891 [INFO] ---------------------------------
2019-03-22 20:41:59,892 [INFO] Summary:
2019-03-22 20:41:59,893 [INFO] Batch 73000, worst loss 0.177979 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:41:59,893 [INFO] Regularization: 1586.824219 * 0.0000010000 = 0.0015868242
2019-03-22 20:41:59,894 [INFO] Sum of grad norms: 13.338657
2019-03-22 20:41:59,894 [INFO] ---------------------------------
2019-03-22 20:42:14,179 [INFO] ---------------------------------
2019-03-22 20:42:14,180 [INFO] Summary:
2019-03-22 20:42:14,180 [INFO] Batch 74000, worst loss 0.213311 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:42:14,181 [INFO] Regularization: 1586.384766 * 0.0000010000 = 0.0015863847
2019-03-22 20:42:14,181 [INFO] Sum of grad norms: 1.619208
2019-03-22 20:42:14,182 [INFO] ---------------------------------
2019-03-22 20:42:28,302 [INFO] ---------------------------------
2019-03-22 20:42:28,303 [INFO] Summary:
2019-03-22 20:42:28,304 [INFO] Batch 75000, worst loss 0.194578 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:42:28,304 [INFO] Regularization: 1585.870850 * 0.0000010000 = 0.0015858709
2019-03-22 20:42:28,305 [INFO] Sum of grad norms: 19.369764
2019-03-22 20:42:28,306 [INFO] ---------------------------------
2019-03-22 20:42:43,978 [INFO] ---------------------------------
2019-03-22 20:42:43,979 [INFO] Summary:
2019-03-22 20:42:43,979 [INFO] Batch 76000, worst loss 0.187535 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:42:43,980 [INFO] Regularization: 1585.455811 * 0.0000010000 = 0.0015854558
2019-03-22 20:42:43,981 [INFO] Sum of grad norms: 0.185453
2019-03-22 20:42:43,981 [INFO] ---------------------------------
2019-03-22 20:42:58,547 [INFO] ---------------------------------
2019-03-22 20:42:58,548 [INFO] Summary:
2019-03-22 20:42:58,548 [INFO] Batch 77000, worst loss 0.243934 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:42:58,549 [INFO] Regularization: 1584.974487 * 0.0000010000 = 0.0015849745
2019-03-22 20:42:58,549 [INFO] Sum of grad norms: 0.002358
2019-03-22 20:42:58,550 [INFO] ---------------------------------
2019-03-22 20:43:13,629 [INFO] ---------------------------------
2019-03-22 20:43:13,630 [INFO] Summary:
2019-03-22 20:43:13,631 [INFO] Batch 78000, worst loss 0.289857 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:43:13,632 [INFO] Regularization: 1584.413696 * 0.0000010000 = 0.0015844137
2019-03-22 20:43:13,632 [INFO] Sum of grad norms: 0.004073
2019-03-22 20:43:13,633 [INFO] ---------------------------------
2019-03-22 20:43:28,248 [INFO] ---------------------------------
2019-03-22 20:43:28,249 [INFO] Summary:
2019-03-22 20:43:28,250 [INFO] Batch 79000, worst loss 0.209181 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:43:28,250 [INFO] Regularization: 1583.942505 * 0.0000010000 = 0.0015839424
2019-03-22 20:43:28,251 [INFO] Sum of grad norms: 0.004855
2019-03-22 20:43:28,252 [INFO] ---------------------------------
2019-03-22 20:43:42,861 [INFO] ---------------------------------
2019-03-22 20:43:42,862 [INFO] Summary:
2019-03-22 20:43:42,862 [INFO] Batch 80000, worst loss 0.223161 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 20:43:42,863 [INFO] Regularization: 1583.432129 * 0.0000010000 = 0.0015834321
2019-03-22 20:43:42,864 [INFO] Sum of grad norms: 0.003466
2019-03-22 20:43:42,864 [INFO] ---------------------------------
2019-03-22 20:43:45,579 [INFO] ---------------------------------
2019-03-22 20:43:45,580 [INFO] Evaluation:
2019-03-22 20:43:45,581 [INFO] Batch 80000, worst loss 0.191419 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:43:45,582 [INFO] ---------------------------------
2019-03-22 20:44:00,819 [INFO] ---------------------------------
2019-03-22 20:44:00,820 [INFO] Summary:
2019-03-22 20:44:00,821 [INFO] Batch 81000, worst loss 0.263814 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:44:00,821 [INFO] Regularization: 1583.010498 * 0.0000010000 = 0.0015830105
2019-03-22 20:44:00,822 [INFO] Sum of grad norms: 44.259907
2019-03-22 20:44:00,823 [INFO] ---------------------------------
2019-03-22 20:44:15,406 [INFO] ---------------------------------
2019-03-22 20:44:15,407 [INFO] Summary:
2019-03-22 20:44:15,407 [INFO] Batch 82000, worst loss 0.258079 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:44:15,408 [INFO] Regularization: 1582.877319 * 0.0000010000 = 0.0015828774
2019-03-22 20:44:15,408 [INFO] Sum of grad norms: 25.124756
2019-03-22 20:44:15,409 [INFO] ---------------------------------
2019-03-22 20:44:29,855 [INFO] ---------------------------------
2019-03-22 20:44:29,856 [INFO] Summary:
2019-03-22 20:44:29,856 [INFO] Batch 83000, worst loss 0.188855 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:44:29,857 [INFO] Regularization: 1582.733521 * 0.0000010000 = 0.0015827335
2019-03-22 20:44:29,858 [INFO] Sum of grad norms: 2.365686
2019-03-22 20:44:29,858 [INFO] ---------------------------------
2019-03-22 20:44:43,926 [INFO] ---------------------------------
2019-03-22 20:44:43,927 [INFO] Summary:
2019-03-22 20:44:43,927 [INFO] Batch 84000, worst loss 0.206883 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:44:43,928 [INFO] Regularization: 1582.572998 * 0.0000010000 = 0.0015825731
2019-03-22 20:44:43,928 [INFO] Sum of grad norms: 0.002223
2019-03-22 20:44:43,929 [INFO] ---------------------------------
2019-03-22 20:44:58,323 [INFO] ---------------------------------
2019-03-22 20:44:58,324 [INFO] Summary:
2019-03-22 20:44:58,324 [INFO] Batch 85000, worst loss 0.180160 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:44:58,325 [INFO] Regularization: 1582.371826 * 0.0000010000 = 0.0015823718
2019-03-22 20:44:58,325 [INFO] Sum of grad norms: 0.245645
2019-03-22 20:44:58,326 [INFO] ---------------------------------
2019-03-22 20:45:12,523 [INFO] ---------------------------------
2019-03-22 20:45:12,524 [INFO] Summary:
2019-03-22 20:45:12,524 [INFO] Batch 86000, worst loss 0.185215 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:45:12,525 [INFO] Regularization: 1582.224609 * 0.0000010000 = 0.0015822246
2019-03-22 20:45:12,525 [INFO] Sum of grad norms: 17.413000
2019-03-22 20:45:12,526 [INFO] ---------------------------------
2019-03-22 20:45:27,452 [INFO] ---------------------------------
2019-03-22 20:45:27,453 [INFO] Summary:
2019-03-22 20:45:27,454 [INFO] Batch 87000, worst loss 0.278694 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:45:27,454 [INFO] Regularization: 1582.065796 * 0.0000010000 = 0.0015820658
2019-03-22 20:45:27,455 [INFO] Sum of grad norms: 0.005096
2019-03-22 20:45:27,455 [INFO] ---------------------------------
2019-03-22 20:45:42,262 [INFO] ---------------------------------
2019-03-22 20:45:42,263 [INFO] Summary:
2019-03-22 20:45:42,263 [INFO] Batch 88000, worst loss 0.314533 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:45:42,264 [INFO] Regularization: 1581.939575 * 0.0000010000 = 0.0015819395
2019-03-22 20:45:42,264 [INFO] Sum of grad norms: 5.927643
2019-03-22 20:45:42,265 [INFO] ---------------------------------
2019-03-22 20:45:56,205 [INFO] ---------------------------------
2019-03-22 20:45:56,206 [INFO] Summary:
2019-03-22 20:45:56,207 [INFO] Batch 89000, worst loss 0.312916 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:45:56,207 [INFO] Regularization: 1581.788940 * 0.0000010000 = 0.0015817890
2019-03-22 20:45:56,208 [INFO] Sum of grad norms: 0.007597
2019-03-22 20:45:56,208 [INFO] ---------------------------------
2019-03-22 20:46:10,572 [INFO] ---------------------------------
2019-03-22 20:46:10,573 [INFO] Summary:
2019-03-22 20:46:10,574 [INFO] Batch 90000, worst loss 0.181938 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 20:46:10,575 [INFO] Regularization: 1581.619751 * 0.0000010000 = 0.0015816197
2019-03-22 20:46:10,576 [INFO] Sum of grad norms: 21.498978
2019-03-22 20:46:10,576 [INFO] ---------------------------------
2019-03-22 20:46:13,303 [INFO] ---------------------------------
2019-03-22 20:46:13,304 [INFO] Evaluation:
2019-03-22 20:46:13,305 [INFO] Batch 90000, worst loss 0.180955 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:46:13,306 [INFO] ---------------------------------
2019-03-22 20:46:27,670 [INFO] ---------------------------------
2019-03-22 20:46:27,671 [INFO] Summary:
2019-03-22 20:46:27,672 [INFO] Batch 91000, worst loss 0.206809 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:46:27,673 [INFO] Regularization: 1581.446899 * 0.0000010000 = 0.0015814468
2019-03-22 20:46:27,674 [INFO] Sum of grad norms: 0.013530
2019-03-22 20:46:27,676 [INFO] ---------------------------------
2019-03-22 20:46:41,486 [INFO] ---------------------------------
2019-03-22 20:46:41,487 [INFO] Summary:
2019-03-22 20:46:41,488 [INFO] Batch 92000, worst loss 0.196065 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:46:41,489 [INFO] Regularization: 1581.376953 * 0.0000010000 = 0.0015813770
2019-03-22 20:46:41,489 [INFO] Sum of grad norms: 0.003422
2019-03-22 20:46:41,490 [INFO] ---------------------------------
2019-03-22 20:46:55,989 [INFO] ---------------------------------
2019-03-22 20:46:55,990 [INFO] Summary:
2019-03-22 20:46:55,991 [INFO] Batch 93000, worst loss 0.205997 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:46:55,992 [INFO] Regularization: 1581.331421 * 0.0000010000 = 0.0015813314
2019-03-22 20:46:55,993 [INFO] Sum of grad norms: 0.002159
2019-03-22 20:46:55,993 [INFO] ---------------------------------
2019-03-22 20:47:10,135 [INFO] ---------------------------------
2019-03-22 20:47:10,136 [INFO] Summary:
2019-03-22 20:47:10,136 [INFO] Batch 94000, worst loss 0.197134 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:47:10,137 [INFO] Regularization: 1581.304321 * 0.0000010000 = 0.0015813044
2019-03-22 20:47:10,137 [INFO] Sum of grad norms: 25.667173
2019-03-22 20:47:10,138 [INFO] ---------------------------------
2019-03-22 20:47:24,811 [INFO] ---------------------------------
2019-03-22 20:47:24,812 [INFO] Summary:
2019-03-22 20:47:24,813 [INFO] Batch 95000, worst loss 0.279665 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:47:24,814 [INFO] Regularization: 1581.264526 * 0.0000010000 = 0.0015812645
2019-03-22 20:47:24,815 [INFO] Sum of grad norms: 0.003330
2019-03-22 20:47:24,815 [INFO] ---------------------------------
2019-03-22 20:47:40,213 [INFO] ---------------------------------
2019-03-22 20:47:40,214 [INFO] Summary:
2019-03-22 20:47:40,214 [INFO] Batch 96000, worst loss 0.191126 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:47:40,215 [INFO] Regularization: 1581.224854 * 0.0000010000 = 0.0015812248
2019-03-22 20:47:40,215 [INFO] Sum of grad norms: 0.099606
2019-03-22 20:47:40,216 [INFO] ---------------------------------
2019-03-22 20:47:54,194 [INFO] ---------------------------------
2019-03-22 20:47:54,195 [INFO] Summary:
2019-03-22 20:47:54,196 [INFO] Batch 97000, worst loss 0.201078 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:47:54,196 [INFO] Regularization: 1581.187012 * 0.0000010000 = 0.0015811870
2019-03-22 20:47:54,196 [INFO] Sum of grad norms: 0.001959
2019-03-22 20:47:54,197 [INFO] ---------------------------------
2019-03-22 20:48:09,072 [INFO] ---------------------------------
2019-03-22 20:48:09,073 [INFO] Summary:
2019-03-22 20:48:09,073 [INFO] Batch 98000, worst loss 0.264353 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:48:09,074 [INFO] Regularization: 1581.148071 * 0.0000010000 = 0.0015811480
2019-03-22 20:48:09,074 [INFO] Sum of grad norms: 0.010080
2019-03-22 20:48:09,075 [INFO] ---------------------------------
2019-03-22 20:48:23,631 [INFO] ---------------------------------
2019-03-22 20:48:23,632 [INFO] Summary:
2019-03-22 20:48:23,633 [INFO] Batch 99000, worst loss 0.193531 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:48:23,633 [INFO] Regularization: 1581.114746 * 0.0000010000 = 0.0015811147
2019-03-22 20:48:23,634 [INFO] Sum of grad norms: 0.016837
2019-03-22 20:48:23,634 [INFO] ---------------------------------
2019-03-22 20:48:37,903 [INFO] ---------------------------------
2019-03-22 20:48:37,904 [INFO] Summary:
2019-03-22 20:48:37,904 [INFO] Batch 100000, worst loss 0.193505 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 20:48:37,905 [INFO] Regularization: 1581.078369 * 0.0000010000 = 0.0015810784
2019-03-22 20:48:37,905 [INFO] Sum of grad norms: 0.004164
2019-03-22 20:48:37,906 [INFO] ---------------------------------
2019-03-22 20:48:40,631 [INFO] ---------------------------------
2019-03-22 20:48:40,632 [INFO] Evaluation:
2019-03-22 20:48:40,633 [INFO] Batch 100000, worst loss 0.225696 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:48:40,635 [INFO] ---------------------------------
2019-03-22 20:48:40,636 [INFO] Finished training, saved to file classifier/1553275201/1553284120_5_classifier_final.pth
2019-03-22 20:48:40,799 [INFO] ---------------------------------
2019-03-22 20:48:40,801 [INFO] Training model #6: (8, 64, 2) @ 2
2019-03-22 20:48:54,795 [INFO] ---------------------------------
2019-03-22 20:48:54,796 [INFO] Summary:
2019-03-22 20:48:54,796 [INFO] Batch 1000, worst loss 776.128113 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:48:54,797 [INFO] Regularization: 30453.740234 * 0.0000010000 = 0.0304537397
2019-03-22 20:48:54,798 [INFO] Sum of grad norms: 526.230713
2019-03-22 20:48:54,798 [INFO] ---------------------------------
2019-03-22 20:49:08,724 [INFO] ---------------------------------
2019-03-22 20:49:08,725 [INFO] Summary:
2019-03-22 20:49:08,725 [INFO] Batch 2000, worst loss 18.703699 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:49:08,726 [INFO] Regularization: 28189.248047 * 0.0000010000 = 0.0281892475
2019-03-22 20:49:08,726 [INFO] Sum of grad norms: 1475.899414
2019-03-22 20:49:08,727 [INFO] ---------------------------------
2019-03-22 20:49:22,966 [INFO] ---------------------------------
2019-03-22 20:49:22,967 [INFO] Summary:
2019-03-22 20:49:22,967 [INFO] Batch 3000, worst loss 10.039248 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:49:22,968 [INFO] Regularization: 24417.128906 * 0.0000010000 = 0.0244171284
2019-03-22 20:49:22,969 [INFO] Sum of grad norms: 192.601242
2019-03-22 20:49:22,969 [INFO] ---------------------------------
2019-03-22 20:49:38,038 [INFO] ---------------------------------
2019-03-22 20:49:38,039 [INFO] Summary:
2019-03-22 20:49:38,040 [INFO] Batch 4000, worst loss 10.645843 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:49:38,040 [INFO] Regularization: 19181.753906 * 0.0000010000 = 0.0191817544
2019-03-22 20:49:38,041 [INFO] Sum of grad norms: 319.184631
2019-03-22 20:49:38,042 [INFO] ---------------------------------
2019-03-22 20:49:52,681 [INFO] ---------------------------------
2019-03-22 20:49:52,682 [INFO] Summary:
2019-03-22 20:49:52,683 [INFO] Batch 5000, worst loss 3.508108 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:49:52,683 [INFO] Regularization: 13170.324219 * 0.0000010000 = 0.0131703243
2019-03-22 20:49:52,684 [INFO] Sum of grad norms: 85.211899
2019-03-22 20:49:52,684 [INFO] ---------------------------------
2019-03-22 20:50:06,984 [INFO] ---------------------------------
2019-03-22 20:50:06,985 [INFO] Summary:
2019-03-22 20:50:06,985 [INFO] Batch 6000, worst loss 2.000507 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:50:06,986 [INFO] Regularization: 9337.361328 * 0.0000010000 = 0.0093373610
2019-03-22 20:50:06,986 [INFO] Sum of grad norms: 30.223112
2019-03-22 20:50:06,987 [INFO] ---------------------------------
2019-03-22 20:50:22,007 [INFO] ---------------------------------
2019-03-22 20:50:22,008 [INFO] Summary:
2019-03-22 20:50:22,008 [INFO] Batch 7000, worst loss 1.130307 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:50:22,009 [INFO] Regularization: 9013.272461 * 0.0000010000 = 0.0090132728
2019-03-22 20:50:22,009 [INFO] Sum of grad norms: 19.379314
2019-03-22 20:50:22,010 [INFO] ---------------------------------
2019-03-22 20:50:36,446 [INFO] ---------------------------------
2019-03-22 20:50:36,447 [INFO] Summary:
2019-03-22 20:50:36,448 [INFO] Batch 8000, worst loss 0.831424 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:50:36,448 [INFO] Regularization: 8827.155273 * 0.0000010000 = 0.0088271555
2019-03-22 20:50:36,449 [INFO] Sum of grad norms: 23.076311
2019-03-22 20:50:36,449 [INFO] ---------------------------------
2019-03-22 20:50:51,063 [INFO] ---------------------------------
2019-03-22 20:50:51,064 [INFO] Summary:
2019-03-22 20:50:51,065 [INFO] Batch 9000, worst loss 0.587645 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:50:51,065 [INFO] Regularization: 8588.588867 * 0.0000010000 = 0.0085885888
2019-03-22 20:50:51,066 [INFO] Sum of grad norms: 3.137183
2019-03-22 20:50:51,066 [INFO] ---------------------------------
2019-03-22 20:51:06,334 [INFO] ---------------------------------
2019-03-22 20:51:06,335 [INFO] Summary:
2019-03-22 20:51:06,336 [INFO] Batch 10000, worst loss 0.529888 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:51:06,336 [INFO] Regularization: 8322.860352 * 0.0000010000 = 0.0083228601
2019-03-22 20:51:06,337 [INFO] Sum of grad norms: 12.461989
2019-03-22 20:51:06,337 [INFO] ---------------------------------
2019-03-22 20:51:09,024 [INFO] ---------------------------------
2019-03-22 20:51:09,025 [INFO] Evaluation:
2019-03-22 20:51:09,026 [INFO] Batch 10000, worst loss 0.361604 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:51:09,028 [INFO] ---------------------------------
2019-03-22 20:51:23,671 [INFO] ---------------------------------
2019-03-22 20:51:23,672 [INFO] Summary:
2019-03-22 20:51:23,673 [INFO] Batch 11000, worst loss 0.447042 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:51:23,673 [INFO] Regularization: 8061.250488 * 0.0000010000 = 0.0080612507
2019-03-22 20:51:23,674 [INFO] Sum of grad norms: 8.286087
2019-03-22 20:51:23,675 [INFO] ---------------------------------
2019-03-22 20:51:37,725 [INFO] ---------------------------------
2019-03-22 20:51:37,726 [INFO] Summary:
2019-03-22 20:51:37,727 [INFO] Batch 12000, worst loss 0.536495 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:51:37,727 [INFO] Regularization: 7707.607422 * 0.0000010000 = 0.0077076075
2019-03-22 20:51:37,728 [INFO] Sum of grad norms: 15.210447
2019-03-22 20:51:37,728 [INFO] ---------------------------------
2019-03-22 20:51:51,833 [INFO] ---------------------------------
2019-03-22 20:51:51,834 [INFO] Summary:
2019-03-22 20:51:51,835 [INFO] Batch 13000, worst loss 0.388428 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:51:51,835 [INFO] Regularization: 7349.338379 * 0.0000010000 = 0.0073493384
2019-03-22 20:51:51,836 [INFO] Sum of grad norms: 6.692771
2019-03-22 20:51:51,836 [INFO] ---------------------------------
2019-03-22 20:52:06,491 [INFO] ---------------------------------
2019-03-22 20:52:06,492 [INFO] Summary:
2019-03-22 20:52:06,492 [INFO] Batch 14000, worst loss 0.511674 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:52:06,493 [INFO] Regularization: 7053.136719 * 0.0000010000 = 0.0070531368
2019-03-22 20:52:06,493 [INFO] Sum of grad norms: 7.744318
2019-03-22 20:52:06,494 [INFO] ---------------------------------
2019-03-22 20:52:20,778 [INFO] ---------------------------------
2019-03-22 20:52:20,779 [INFO] Summary:
2019-03-22 20:52:20,780 [INFO] Batch 15000, worst loss 0.344286 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:52:20,781 [INFO] Regularization: 6721.453125 * 0.0000010000 = 0.0067214533
2019-03-22 20:52:20,781 [INFO] Sum of grad norms: 6.680484
2019-03-22 20:52:20,782 [INFO] ---------------------------------
2019-03-22 20:52:35,488 [INFO] ---------------------------------
2019-03-22 20:52:35,489 [INFO] Summary:
2019-03-22 20:52:35,489 [INFO] Batch 16000, worst loss 0.545535 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:52:35,490 [INFO] Regularization: 6412.765625 * 0.0000010000 = 0.0064127655
2019-03-22 20:52:35,491 [INFO] Sum of grad norms: 12.750001
2019-03-22 20:52:35,491 [INFO] ---------------------------------
2019-03-22 20:52:49,824 [INFO] ---------------------------------
2019-03-22 20:52:49,825 [INFO] Summary:
2019-03-22 20:52:49,826 [INFO] Batch 17000, worst loss 0.471872 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:52:49,826 [INFO] Regularization: 6184.333496 * 0.0000010000 = 0.0061843335
2019-03-22 20:52:49,827 [INFO] Sum of grad norms: 18.360821
2019-03-22 20:52:49,827 [INFO] ---------------------------------
2019-03-22 20:53:04,875 [INFO] ---------------------------------
2019-03-22 20:53:04,876 [INFO] Summary:
2019-03-22 20:53:04,877 [INFO] Batch 18000, worst loss 0.370646 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:53:04,877 [INFO] Regularization: 5974.432129 * 0.0000010000 = 0.0059744320
2019-03-22 20:53:04,877 [INFO] Sum of grad norms: 3.789687
2019-03-22 20:53:04,878 [INFO] ---------------------------------
2019-03-22 20:53:20,752 [INFO] ---------------------------------
2019-03-22 20:53:20,753 [INFO] Summary:
2019-03-22 20:53:20,754 [INFO] Batch 19000, worst loss 0.624641 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:53:20,754 [INFO] Regularization: 5751.466797 * 0.0000010000 = 0.0057514668
2019-03-22 20:53:20,755 [INFO] Sum of grad norms: 1.385016
2019-03-22 20:53:20,755 [INFO] ---------------------------------
2019-03-22 20:53:35,207 [INFO] ---------------------------------
2019-03-22 20:53:35,208 [INFO] Summary:
2019-03-22 20:53:35,208 [INFO] Batch 20000, worst loss 0.430028 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:53:35,209 [INFO] Regularization: 5585.624512 * 0.0000010000 = 0.0055856244
2019-03-22 20:53:35,209 [INFO] Sum of grad norms: 5.934474
2019-03-22 20:53:35,210 [INFO] ---------------------------------
2019-03-22 20:53:37,945 [INFO] ---------------------------------
2019-03-22 20:53:37,946 [INFO] Evaluation:
2019-03-22 20:53:37,947 [INFO] Batch 20000, worst loss 0.329866 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:53:37,948 [INFO] ---------------------------------
2019-03-22 20:53:52,090 [INFO] ---------------------------------
2019-03-22 20:53:52,091 [INFO] Summary:
2019-03-22 20:53:52,092 [INFO] Batch 21000, worst loss 0.343094 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:53:52,092 [INFO] Regularization: 5406.269043 * 0.0000010000 = 0.0054062689
2019-03-22 20:53:52,093 [INFO] Sum of grad norms: 16.268133
2019-03-22 20:53:52,093 [INFO] ---------------------------------
2019-03-22 20:54:06,591 [INFO] ---------------------------------
2019-03-22 20:54:06,592 [INFO] Summary:
2019-03-22 20:54:06,593 [INFO] Batch 22000, worst loss 0.369325 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:54:06,593 [INFO] Regularization: 5203.365723 * 0.0000010000 = 0.0052033658
2019-03-22 20:54:06,594 [INFO] Sum of grad norms: 7.357585
2019-03-22 20:54:06,594 [INFO] ---------------------------------
2019-03-22 20:54:20,717 [INFO] ---------------------------------
2019-03-22 20:54:20,718 [INFO] Summary:
2019-03-22 20:54:20,718 [INFO] Batch 23000, worst loss 0.306696 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:54:20,719 [INFO] Regularization: 5024.219727 * 0.0000010000 = 0.0050242199
2019-03-22 20:54:20,720 [INFO] Sum of grad norms: 12.815516
2019-03-22 20:54:20,720 [INFO] ---------------------------------
2019-03-22 20:54:35,722 [INFO] ---------------------------------
2019-03-22 20:54:35,723 [INFO] Summary:
2019-03-22 20:54:35,723 [INFO] Batch 24000, worst loss 0.418362 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:54:35,724 [INFO] Regularization: 4896.870605 * 0.0000010000 = 0.0048968708
2019-03-22 20:54:35,724 [INFO] Sum of grad norms: 6.144670
2019-03-22 20:54:35,725 [INFO] ---------------------------------
2019-03-22 20:54:51,171 [INFO] ---------------------------------
2019-03-22 20:54:51,172 [INFO] Summary:
2019-03-22 20:54:51,173 [INFO] Batch 25000, worst loss 0.501457 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:54:51,174 [INFO] Regularization: 4791.386719 * 0.0000010000 = 0.0047913869
2019-03-22 20:54:51,174 [INFO] Sum of grad norms: 8.305956
2019-03-22 20:54:51,175 [INFO] ---------------------------------
2019-03-22 20:55:05,583 [INFO] ---------------------------------
2019-03-22 20:55:05,584 [INFO] Summary:
2019-03-22 20:55:05,584 [INFO] Batch 26000, worst loss 0.421328 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:55:05,585 [INFO] Regularization: 4642.903809 * 0.0000010000 = 0.0046429038
2019-03-22 20:55:05,586 [INFO] Sum of grad norms: 5.171639
2019-03-22 20:55:05,586 [INFO] ---------------------------------
2019-03-22 20:55:20,003 [INFO] ---------------------------------
2019-03-22 20:55:20,004 [INFO] Summary:
2019-03-22 20:55:20,004 [INFO] Batch 27000, worst loss 0.396206 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:55:20,005 [INFO] Regularization: 4561.575195 * 0.0000010000 = 0.0045615751
2019-03-22 20:55:20,005 [INFO] Sum of grad norms: 7.368758
2019-03-22 20:55:20,006 [INFO] ---------------------------------
2019-03-22 20:55:35,167 [INFO] ---------------------------------
2019-03-22 20:55:35,168 [INFO] Summary:
2019-03-22 20:55:35,168 [INFO] Batch 28000, worst loss 0.358841 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:55:35,169 [INFO] Regularization: 4497.326172 * 0.0000010000 = 0.0044973260
2019-03-22 20:55:35,169 [INFO] Sum of grad norms: 0.492344
2019-03-22 20:55:35,170 [INFO] ---------------------------------
2019-03-22 20:55:49,323 [INFO] ---------------------------------
2019-03-22 20:55:49,324 [INFO] Summary:
2019-03-22 20:55:49,324 [INFO] Batch 29000, worst loss 0.307136 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:55:49,325 [INFO] Regularization: 4268.223633 * 0.0000010000 = 0.0042682234
2019-03-22 20:55:49,325 [INFO] Sum of grad norms: 3.861429
2019-03-22 20:55:49,326 [INFO] ---------------------------------
2019-03-22 20:56:04,230 [INFO] ---------------------------------
2019-03-22 20:56:04,231 [INFO] Summary:
2019-03-22 20:56:04,232 [INFO] Batch 30000, worst loss 0.318110 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:56:04,233 [INFO] Regularization: 4154.527832 * 0.0000010000 = 0.0041545280
2019-03-22 20:56:04,233 [INFO] Sum of grad norms: 17.125856
2019-03-22 20:56:04,234 [INFO] ---------------------------------
2019-03-22 20:56:07,016 [INFO] ---------------------------------
2019-03-22 20:56:07,017 [INFO] Evaluation:
2019-03-22 20:56:07,018 [INFO] Batch 30000, worst loss 0.254012 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:56:07,019 [INFO] ---------------------------------
2019-03-22 20:56:22,245 [INFO] ---------------------------------
2019-03-22 20:56:22,246 [INFO] Summary:
2019-03-22 20:56:22,246 [INFO] Batch 31000, worst loss 0.340016 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:56:22,247 [INFO] Regularization: 4019.166748 * 0.0000010000 = 0.0040191668
2019-03-22 20:56:22,247 [INFO] Sum of grad norms: 4.274902
2019-03-22 20:56:22,248 [INFO] ---------------------------------
2019-03-22 20:56:36,303 [INFO] ---------------------------------
2019-03-22 20:56:36,304 [INFO] Summary:
2019-03-22 20:56:36,305 [INFO] Batch 32000, worst loss 0.451498 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:56:36,305 [INFO] Regularization: 3931.292725 * 0.0000010000 = 0.0039312928
2019-03-22 20:56:36,306 [INFO] Sum of grad norms: 0.271533
2019-03-22 20:56:36,307 [INFO] ---------------------------------
2019-03-22 20:56:51,336 [INFO] ---------------------------------
2019-03-22 20:56:51,337 [INFO] Summary:
2019-03-22 20:56:51,337 [INFO] Batch 33000, worst loss 0.395948 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:56:51,338 [INFO] Regularization: 3886.023682 * 0.0000010000 = 0.0038860238
2019-03-22 20:56:51,338 [INFO] Sum of grad norms: 6.461792
2019-03-22 20:56:51,339 [INFO] ---------------------------------
2019-03-22 20:57:06,472 [INFO] ---------------------------------
2019-03-22 20:57:06,472 [INFO] Summary:
2019-03-22 20:57:06,473 [INFO] Batch 34000, worst loss 0.383191 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:57:06,474 [INFO] Regularization: 3805.960449 * 0.0000010000 = 0.0038059605
2019-03-22 20:57:06,474 [INFO] Sum of grad norms: 1.270690
2019-03-22 20:57:06,475 [INFO] ---------------------------------
2019-03-22 20:57:21,218 [INFO] ---------------------------------
2019-03-22 20:57:21,219 [INFO] Summary:
2019-03-22 20:57:21,219 [INFO] Batch 35000, worst loss 0.376817 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:57:21,220 [INFO] Regularization: 3663.435303 * 0.0000010000 = 0.0036634353
2019-03-22 20:57:21,220 [INFO] Sum of grad norms: 2.327294
2019-03-22 20:57:21,221 [INFO] ---------------------------------
2019-03-22 20:57:36,504 [INFO] ---------------------------------
2019-03-22 20:57:36,505 [INFO] Summary:
2019-03-22 20:57:36,506 [INFO] Batch 36000, worst loss 0.393668 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:57:36,506 [INFO] Regularization: 3573.003418 * 0.0000010000 = 0.0035730035
2019-03-22 20:57:36,507 [INFO] Sum of grad norms: 0.133518
2019-03-22 20:57:36,508 [INFO] ---------------------------------
2019-03-22 20:57:51,108 [INFO] ---------------------------------
2019-03-22 20:57:51,110 [INFO] Summary:
2019-03-22 20:57:51,111 [INFO] Batch 37000, worst loss 0.387056 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:57:51,112 [INFO] Regularization: 3504.663330 * 0.0000010000 = 0.0035046632
2019-03-22 20:57:51,113 [INFO] Sum of grad norms: 5.605287
2019-03-22 20:57:51,114 [INFO] ---------------------------------
2019-03-22 20:58:05,411 [INFO] ---------------------------------
2019-03-22 20:58:05,412 [INFO] Summary:
2019-03-22 20:58:05,412 [INFO] Batch 38000, worst loss 0.577903 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:58:05,413 [INFO] Regularization: 3416.273926 * 0.0000010000 = 0.0034162740
2019-03-22 20:58:05,413 [INFO] Sum of grad norms: 3.735803
2019-03-22 20:58:05,414 [INFO] ---------------------------------
2019-03-22 20:58:20,282 [INFO] ---------------------------------
2019-03-22 20:58:20,283 [INFO] Summary:
2019-03-22 20:58:20,284 [INFO] Batch 39000, worst loss 0.412232 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:58:20,285 [INFO] Regularization: 3325.856934 * 0.0000010000 = 0.0033258570
2019-03-22 20:58:20,285 [INFO] Sum of grad norms: 3.794251
2019-03-22 20:58:20,286 [INFO] ---------------------------------
2019-03-22 20:58:34,582 [INFO] ---------------------------------
2019-03-22 20:58:34,583 [INFO] Summary:
2019-03-22 20:58:34,584 [INFO] Batch 40000, worst loss 0.323555 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 20:58:34,584 [INFO] Regularization: 3253.034912 * 0.0000010000 = 0.0032530350
2019-03-22 20:58:34,585 [INFO] Sum of grad norms: 5.050622
2019-03-22 20:58:34,585 [INFO] ---------------------------------
2019-03-22 20:58:37,320 [INFO] ---------------------------------
2019-03-22 20:58:37,321 [INFO] Evaluation:
2019-03-22 20:58:37,322 [INFO] Batch 40000, worst loss 0.244290 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 20:58:37,323 [INFO] ---------------------------------
2019-03-22 20:58:51,714 [INFO] ---------------------------------
2019-03-22 20:58:51,715 [INFO] Summary:
2019-03-22 20:58:51,715 [INFO] Batch 41000, worst loss 0.324200 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:58:51,716 [INFO] Regularization: 3164.534912 * 0.0000010000 = 0.0031645349
2019-03-22 20:58:51,716 [INFO] Sum of grad norms: 2.458057
2019-03-22 20:58:51,717 [INFO] ---------------------------------
2019-03-22 20:59:06,195 [INFO] ---------------------------------
2019-03-22 20:59:06,196 [INFO] Summary:
2019-03-22 20:59:06,196 [INFO] Batch 42000, worst loss 0.292621 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:59:06,197 [INFO] Regularization: 3051.601074 * 0.0000010000 = 0.0030516011
2019-03-22 20:59:06,197 [INFO] Sum of grad norms: 0.032794
2019-03-22 20:59:06,198 [INFO] ---------------------------------
2019-03-22 20:59:20,141 [INFO] ---------------------------------
2019-03-22 20:59:20,142 [INFO] Summary:
2019-03-22 20:59:20,142 [INFO] Batch 43000, worst loss 0.257099 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:59:20,143 [INFO] Regularization: 2983.720703 * 0.0000010000 = 0.0029837207
2019-03-22 20:59:20,143 [INFO] Sum of grad norms: 0.137452
2019-03-22 20:59:20,144 [INFO] ---------------------------------
2019-03-22 20:59:34,360 [INFO] ---------------------------------
2019-03-22 20:59:34,361 [INFO] Summary:
2019-03-22 20:59:34,362 [INFO] Batch 44000, worst loss 0.263403 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:59:34,363 [INFO] Regularization: 2916.560791 * 0.0000010000 = 0.0029165607
2019-03-22 20:59:34,363 [INFO] Sum of grad norms: 0.020965
2019-03-22 20:59:34,364 [INFO] ---------------------------------
2019-03-22 20:59:48,874 [INFO] ---------------------------------
2019-03-22 20:59:48,875 [INFO] Summary:
2019-03-22 20:59:48,875 [INFO] Batch 45000, worst loss 0.248025 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 20:59:48,876 [INFO] Regularization: 2853.627686 * 0.0000010000 = 0.0028536278
2019-03-22 20:59:48,876 [INFO] Sum of grad norms: 11.993776
2019-03-22 20:59:48,877 [INFO] ---------------------------------
2019-03-22 21:00:02,730 [INFO] ---------------------------------
2019-03-22 21:00:02,731 [INFO] Summary:
2019-03-22 21:00:02,732 [INFO] Batch 46000, worst loss 0.295035 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:00:02,732 [INFO] Regularization: 2804.589355 * 0.0000010000 = 0.0028045895
2019-03-22 21:00:02,733 [INFO] Sum of grad norms: 5.493708
2019-03-22 21:00:02,733 [INFO] ---------------------------------
2019-03-22 21:00:17,111 [INFO] ---------------------------------
2019-03-22 21:00:17,112 [INFO] Summary:
2019-03-22 21:00:17,113 [INFO] Batch 47000, worst loss 0.274046 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:00:17,113 [INFO] Regularization: 2735.492920 * 0.0000010000 = 0.0027354930
2019-03-22 21:00:17,114 [INFO] Sum of grad norms: 3.802151
2019-03-22 21:00:17,115 [INFO] ---------------------------------
2019-03-22 21:00:32,017 [INFO] ---------------------------------
2019-03-22 21:00:32,018 [INFO] Summary:
2019-03-22 21:00:32,019 [INFO] Batch 48000, worst loss 0.352625 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:00:32,020 [INFO] Regularization: 2688.850098 * 0.0000010000 = 0.0026888500
2019-03-22 21:00:32,020 [INFO] Sum of grad norms: 0.035214
2019-03-22 21:00:32,021 [INFO] ---------------------------------
2019-03-22 21:00:45,741 [INFO] ---------------------------------
2019-03-22 21:00:45,742 [INFO] Summary:
2019-03-22 21:00:45,742 [INFO] Batch 49000, worst loss 0.285949 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:00:45,743 [INFO] Regularization: 2645.328613 * 0.0000010000 = 0.0026453286
2019-03-22 21:00:45,743 [INFO] Sum of grad norms: 0.022599
2019-03-22 21:00:45,744 [INFO] ---------------------------------
2019-03-22 21:00:59,654 [INFO] ---------------------------------
2019-03-22 21:00:59,655 [INFO] Summary:
2019-03-22 21:00:59,655 [INFO] Batch 50000, worst loss 0.277252 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:00:59,656 [INFO] Regularization: 2619.049072 * 0.0000010000 = 0.0026190490
2019-03-22 21:00:59,656 [INFO] Sum of grad norms: 18.838150
2019-03-22 21:00:59,657 [INFO] ---------------------------------
2019-03-22 21:01:02,410 [INFO] ---------------------------------
2019-03-22 21:01:02,411 [INFO] Evaluation:
2019-03-22 21:01:02,412 [INFO] Batch 50000, worst loss 0.267805 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:01:02,415 [INFO] ---------------------------------
2019-03-22 21:01:17,592 [INFO] ---------------------------------
2019-03-22 21:01:17,593 [INFO] Summary:
2019-03-22 21:01:17,594 [INFO] Batch 51000, worst loss 0.248489 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:01:17,594 [INFO] Regularization: 2590.584229 * 0.0000010000 = 0.0025905843
2019-03-22 21:01:17,595 [INFO] Sum of grad norms: 0.031604
2019-03-22 21:01:17,596 [INFO] ---------------------------------
2019-03-22 21:01:32,381 [INFO] ---------------------------------
2019-03-22 21:01:32,382 [INFO] Summary:
2019-03-22 21:01:32,383 [INFO] Batch 52000, worst loss 0.287080 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:01:32,383 [INFO] Regularization: 2571.204102 * 0.0000010000 = 0.0025712040
2019-03-22 21:01:32,384 [INFO] Sum of grad norms: 4.209753
2019-03-22 21:01:32,384 [INFO] ---------------------------------
2019-03-22 21:01:47,021 [INFO] ---------------------------------
2019-03-22 21:01:47,022 [INFO] Summary:
2019-03-22 21:01:47,022 [INFO] Batch 53000, worst loss 0.233395 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:01:47,023 [INFO] Regularization: 2561.167480 * 0.0000010000 = 0.0025611676
2019-03-22 21:01:47,023 [INFO] Sum of grad norms: 6.147273
2019-03-22 21:01:47,024 [INFO] ---------------------------------
2019-03-22 21:02:00,861 [INFO] ---------------------------------
2019-03-22 21:02:00,862 [INFO] Summary:
2019-03-22 21:02:00,863 [INFO] Batch 54000, worst loss 0.186402 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:02:00,863 [INFO] Regularization: 2549.880371 * 0.0000010000 = 0.0025498804
2019-03-22 21:02:00,864 [INFO] Sum of grad norms: 2.734235
2019-03-22 21:02:00,864 [INFO] ---------------------------------
2019-03-22 21:02:14,648 [INFO] ---------------------------------
2019-03-22 21:02:14,649 [INFO] Summary:
2019-03-22 21:02:14,650 [INFO] Batch 55000, worst loss 0.194042 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:02:14,651 [INFO] Regularization: 2540.908936 * 0.0000010000 = 0.0025409090
2019-03-22 21:02:14,652 [INFO] Sum of grad norms: 0.018313
2019-03-22 21:02:14,653 [INFO] ---------------------------------
2019-03-22 21:02:28,495 [INFO] ---------------------------------
2019-03-22 21:02:28,496 [INFO] Summary:
2019-03-22 21:02:28,497 [INFO] Batch 56000, worst loss 0.183976 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:02:28,497 [INFO] Regularization: 2530.688477 * 0.0000010000 = 0.0025306884
2019-03-22 21:02:28,498 [INFO] Sum of grad norms: 0.011468
2019-03-22 21:02:28,499 [INFO] ---------------------------------
2019-03-22 21:02:42,213 [INFO] ---------------------------------
2019-03-22 21:02:42,214 [INFO] Summary:
2019-03-22 21:02:42,214 [INFO] Batch 57000, worst loss 0.354314 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:02:42,215 [INFO] Regularization: 2521.156738 * 0.0000010000 = 0.0025211568
2019-03-22 21:02:42,215 [INFO] Sum of grad norms: 1.936800
2019-03-22 21:02:42,216 [INFO] ---------------------------------
2019-03-22 21:02:56,987 [INFO] ---------------------------------
2019-03-22 21:02:56,988 [INFO] Summary:
2019-03-22 21:02:56,989 [INFO] Batch 58000, worst loss 0.235707 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:02:56,989 [INFO] Regularization: 2510.932129 * 0.0000010000 = 0.0025109320
2019-03-22 21:02:56,990 [INFO] Sum of grad norms: 13.577158
2019-03-22 21:02:56,990 [INFO] ---------------------------------
2019-03-22 21:03:12,017 [INFO] ---------------------------------
2019-03-22 21:03:12,018 [INFO] Summary:
2019-03-22 21:03:12,019 [INFO] Batch 59000, worst loss 0.284406 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:03:12,020 [INFO] Regularization: 2503.355713 * 0.0000010000 = 0.0025033557
2019-03-22 21:03:12,021 [INFO] Sum of grad norms: 13.704000
2019-03-22 21:03:12,021 [INFO] ---------------------------------
2019-03-22 21:03:27,128 [INFO] ---------------------------------
2019-03-22 21:03:27,129 [INFO] Summary:
2019-03-22 21:03:27,129 [INFO] Batch 60000, worst loss 0.257447 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:03:27,130 [INFO] Regularization: 2494.402832 * 0.0000010000 = 0.0024944029
2019-03-22 21:03:27,131 [INFO] Sum of grad norms: 11.434292
2019-03-22 21:03:27,131 [INFO] ---------------------------------
2019-03-22 21:03:29,840 [INFO] ---------------------------------
2019-03-22 21:03:29,841 [INFO] Evaluation:
2019-03-22 21:03:29,841 [INFO] Batch 60000, worst loss 0.250378 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:03:29,842 [INFO] ---------------------------------
2019-03-22 21:03:44,249 [INFO] ---------------------------------
2019-03-22 21:03:44,250 [INFO] Summary:
2019-03-22 21:03:44,251 [INFO] Batch 61000, worst loss 0.258201 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:03:44,251 [INFO] Regularization: 2483.740967 * 0.0000010000 = 0.0024837409
2019-03-22 21:03:44,252 [INFO] Sum of grad norms: 8.354163
2019-03-22 21:03:44,253 [INFO] ---------------------------------
2019-03-22 21:03:59,527 [INFO] ---------------------------------
2019-03-22 21:03:59,528 [INFO] Summary:
2019-03-22 21:03:59,528 [INFO] Batch 62000, worst loss 0.197993 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:03:59,529 [INFO] Regularization: 2479.794678 * 0.0000010000 = 0.0024797947
2019-03-22 21:03:59,529 [INFO] Sum of grad norms: 0.011011
2019-03-22 21:03:59,530 [INFO] ---------------------------------
2019-03-22 21:04:14,155 [INFO] ---------------------------------
2019-03-22 21:04:14,156 [INFO] Summary:
2019-03-22 21:04:14,157 [INFO] Batch 63000, worst loss 0.201956 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:04:14,158 [INFO] Regularization: 2477.465820 * 0.0000010000 = 0.0024774659
2019-03-22 21:04:14,158 [INFO] Sum of grad norms: 2.374720
2019-03-22 21:04:14,159 [INFO] ---------------------------------
2019-03-22 21:04:28,540 [INFO] ---------------------------------
2019-03-22 21:04:28,541 [INFO] Summary:
2019-03-22 21:04:28,541 [INFO] Batch 64000, worst loss 0.191983 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:04:28,542 [INFO] Regularization: 2475.056641 * 0.0000010000 = 0.0024750566
2019-03-22 21:04:28,542 [INFO] Sum of grad norms: 0.006941
2019-03-22 21:04:28,543 [INFO] ---------------------------------
2019-03-22 21:04:42,838 [INFO] ---------------------------------
2019-03-22 21:04:42,839 [INFO] Summary:
2019-03-22 21:04:42,840 [INFO] Batch 65000, worst loss 0.239030 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:04:42,840 [INFO] Regularization: 2472.935791 * 0.0000010000 = 0.0024729357
2019-03-22 21:04:42,840 [INFO] Sum of grad norms: 0.980456
2019-03-22 21:04:42,841 [INFO] ---------------------------------
2019-03-22 21:04:58,015 [INFO] ---------------------------------
2019-03-22 21:04:58,016 [INFO] Summary:
2019-03-22 21:04:58,016 [INFO] Batch 66000, worst loss 0.212789 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:04:58,017 [INFO] Regularization: 2470.188965 * 0.0000010000 = 0.0024701890
2019-03-22 21:04:58,017 [INFO] Sum of grad norms: 17.507250
2019-03-22 21:04:58,018 [INFO] ---------------------------------
2019-03-22 21:05:11,728 [INFO] ---------------------------------
2019-03-22 21:05:11,729 [INFO] Summary:
2019-03-22 21:05:11,729 [INFO] Batch 67000, worst loss 0.248306 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:05:11,730 [INFO] Regularization: 2467.880615 * 0.0000010000 = 0.0024678807
2019-03-22 21:05:11,730 [INFO] Sum of grad norms: 7.185687
2019-03-22 21:05:11,731 [INFO] ---------------------------------
2019-03-22 21:05:26,564 [INFO] ---------------------------------
2019-03-22 21:05:26,565 [INFO] Summary:
2019-03-22 21:05:26,565 [INFO] Batch 68000, worst loss 0.214386 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:05:26,566 [INFO] Regularization: 2465.146729 * 0.0000010000 = 0.0024651468
2019-03-22 21:05:26,566 [INFO] Sum of grad norms: 0.122001
2019-03-22 21:05:26,567 [INFO] ---------------------------------
2019-03-22 21:05:41,011 [INFO] ---------------------------------
2019-03-22 21:05:41,012 [INFO] Summary:
2019-03-22 21:05:41,013 [INFO] Batch 69000, worst loss 0.306560 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:05:41,013 [INFO] Regularization: 2462.516846 * 0.0000010000 = 0.0024625168
2019-03-22 21:05:41,014 [INFO] Sum of grad norms: 19.707991
2019-03-22 21:05:41,014 [INFO] ---------------------------------
2019-03-22 21:05:55,712 [INFO] ---------------------------------
2019-03-22 21:05:55,713 [INFO] Summary:
2019-03-22 21:05:55,714 [INFO] Batch 70000, worst loss 0.184113 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:05:55,714 [INFO] Regularization: 2459.982910 * 0.0000010000 = 0.0024599829
2019-03-22 21:05:55,715 [INFO] Sum of grad norms: 5.027241
2019-03-22 21:05:55,715 [INFO] ---------------------------------
2019-03-22 21:05:58,425 [INFO] ---------------------------------
2019-03-22 21:05:58,426 [INFO] Evaluation:
2019-03-22 21:05:58,426 [INFO] Batch 70000, worst loss 0.206802 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:05:58,427 [INFO] ---------------------------------
2019-03-22 21:06:13,127 [INFO] ---------------------------------
2019-03-22 21:06:13,128 [INFO] Summary:
2019-03-22 21:06:13,128 [INFO] Batch 71000, worst loss 0.213377 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:06:13,129 [INFO] Regularization: 2457.467285 * 0.0000010000 = 0.0024574674
2019-03-22 21:06:13,130 [INFO] Sum of grad norms: 0.787010
2019-03-22 21:06:13,130 [INFO] ---------------------------------
2019-03-22 21:06:27,585 [INFO] ---------------------------------
2019-03-22 21:06:27,586 [INFO] Summary:
2019-03-22 21:06:27,587 [INFO] Batch 72000, worst loss 0.177809 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:06:27,588 [INFO] Regularization: 2456.590088 * 0.0000010000 = 0.0024565901
2019-03-22 21:06:27,588 [INFO] Sum of grad norms: 0.024572
2019-03-22 21:06:27,589 [INFO] ---------------------------------
2019-03-22 21:06:41,789 [INFO] ---------------------------------
2019-03-22 21:06:41,790 [INFO] Summary:
2019-03-22 21:06:41,790 [INFO] Batch 73000, worst loss 0.155853 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:06:41,791 [INFO] Regularization: 2455.936523 * 0.0000010000 = 0.0024559365
2019-03-22 21:06:41,791 [INFO] Sum of grad norms: 0.197848
2019-03-22 21:06:41,792 [INFO] ---------------------------------
2019-03-22 21:06:56,468 [INFO] ---------------------------------
2019-03-22 21:06:56,469 [INFO] Summary:
2019-03-22 21:06:56,470 [INFO] Batch 74000, worst loss 0.216349 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:06:56,471 [INFO] Regularization: 2455.294189 * 0.0000010000 = 0.0024552941
2019-03-22 21:06:56,471 [INFO] Sum of grad norms: 0.187374
2019-03-22 21:06:56,472 [INFO] ---------------------------------
2019-03-22 21:07:11,265 [INFO] ---------------------------------
2019-03-22 21:07:11,266 [INFO] Summary:
2019-03-22 21:07:11,267 [INFO] Batch 75000, worst loss 0.212684 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:07:11,268 [INFO] Regularization: 2454.785645 * 0.0000010000 = 0.0024547856
2019-03-22 21:07:11,268 [INFO] Sum of grad norms: 1.327790
2019-03-22 21:07:11,269 [INFO] ---------------------------------
2019-03-22 21:07:25,983 [INFO] ---------------------------------
2019-03-22 21:07:25,984 [INFO] Summary:
2019-03-22 21:07:25,985 [INFO] Batch 76000, worst loss 0.215024 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:07:25,985 [INFO] Regularization: 2454.154297 * 0.0000010000 = 0.0024541542
2019-03-22 21:07:25,986 [INFO] Sum of grad norms: 7.316697
2019-03-22 21:07:25,986 [INFO] ---------------------------------
2019-03-22 21:07:40,546 [INFO] ---------------------------------
2019-03-22 21:07:40,546 [INFO] Summary:
2019-03-22 21:07:40,547 [INFO] Batch 77000, worst loss 0.237738 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:07:40,548 [INFO] Regularization: 2453.352539 * 0.0000010000 = 0.0024533526
2019-03-22 21:07:40,548 [INFO] Sum of grad norms: 1.576919
2019-03-22 21:07:40,549 [INFO] ---------------------------------
2019-03-22 21:07:54,380 [INFO] ---------------------------------
2019-03-22 21:07:54,381 [INFO] Summary:
2019-03-22 21:07:54,381 [INFO] Batch 78000, worst loss 0.295030 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:07:54,382 [INFO] Regularization: 2452.801270 * 0.0000010000 = 0.0024528012
2019-03-22 21:07:54,383 [INFO] Sum of grad norms: 13.278466
2019-03-22 21:07:54,383 [INFO] ---------------------------------
2019-03-22 21:08:08,906 [INFO] ---------------------------------
2019-03-22 21:08:08,907 [INFO] Summary:
2019-03-22 21:08:08,908 [INFO] Batch 79000, worst loss 0.260458 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:08:08,909 [INFO] Regularization: 2452.241455 * 0.0000010000 = 0.0024522415
2019-03-22 21:08:08,910 [INFO] Sum of grad norms: 0.009332
2019-03-22 21:08:08,910 [INFO] ---------------------------------
2019-03-22 21:08:22,895 [INFO] ---------------------------------
2019-03-22 21:08:22,896 [INFO] Summary:
2019-03-22 21:08:22,896 [INFO] Batch 80000, worst loss 0.258260 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:08:22,897 [INFO] Regularization: 2451.517578 * 0.0000010000 = 0.0024515176
2019-03-22 21:08:22,898 [INFO] Sum of grad norms: 19.029682
2019-03-22 21:08:22,898 [INFO] ---------------------------------
2019-03-22 21:08:25,645 [INFO] ---------------------------------
2019-03-22 21:08:25,646 [INFO] Evaluation:
2019-03-22 21:08:25,647 [INFO] Batch 80000, worst loss 0.217880 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:08:25,647 [INFO] ---------------------------------
2019-03-22 21:08:40,979 [INFO] ---------------------------------
2019-03-22 21:08:40,980 [INFO] Summary:
2019-03-22 21:08:40,980 [INFO] Batch 81000, worst loss 0.212534 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:08:40,981 [INFO] Regularization: 2450.902588 * 0.0000010000 = 0.0024509025
2019-03-22 21:08:40,982 [INFO] Sum of grad norms: 0.072802
2019-03-22 21:08:40,983 [INFO] ---------------------------------
2019-03-22 21:08:55,571 [INFO] ---------------------------------
2019-03-22 21:08:55,571 [INFO] Summary:
2019-03-22 21:08:55,572 [INFO] Batch 82000, worst loss 0.209981 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:08:55,573 [INFO] Regularization: 2450.699951 * 0.0000010000 = 0.0024506999
2019-03-22 21:08:55,573 [INFO] Sum of grad norms: 0.022199
2019-03-22 21:08:55,574 [INFO] ---------------------------------
2019-03-22 21:09:09,975 [INFO] ---------------------------------
2019-03-22 21:09:09,976 [INFO] Summary:
2019-03-22 21:09:09,977 [INFO] Batch 83000, worst loss 0.194628 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:09:09,977 [INFO] Regularization: 2450.477295 * 0.0000010000 = 0.0024504773
2019-03-22 21:09:09,978 [INFO] Sum of grad norms: 9.445543
2019-03-22 21:09:09,979 [INFO] ---------------------------------
2019-03-22 21:09:24,213 [INFO] ---------------------------------
2019-03-22 21:09:24,214 [INFO] Summary:
2019-03-22 21:09:24,214 [INFO] Batch 84000, worst loss 0.192280 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:09:24,215 [INFO] Regularization: 2450.310791 * 0.0000010000 = 0.0024503109
2019-03-22 21:09:24,215 [INFO] Sum of grad norms: 0.327552
2019-03-22 21:09:24,216 [INFO] ---------------------------------
2019-03-22 21:09:39,276 [INFO] ---------------------------------
2019-03-22 21:09:39,277 [INFO] Summary:
2019-03-22 21:09:39,278 [INFO] Batch 85000, worst loss 0.182542 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:09:39,279 [INFO] Regularization: 2450.102295 * 0.0000010000 = 0.0024501022
2019-03-22 21:09:39,279 [INFO] Sum of grad norms: 40.620327
2019-03-22 21:09:39,280 [INFO] ---------------------------------
2019-03-22 21:09:53,217 [INFO] ---------------------------------
2019-03-22 21:09:53,218 [INFO] Summary:
2019-03-22 21:09:53,218 [INFO] Batch 86000, worst loss 0.201624 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:09:53,219 [INFO] Regularization: 2449.922363 * 0.0000010000 = 0.0024499223
2019-03-22 21:09:53,219 [INFO] Sum of grad norms: 9.859434
2019-03-22 21:09:53,220 [INFO] ---------------------------------
2019-03-22 21:10:07,183 [INFO] ---------------------------------
2019-03-22 21:10:07,184 [INFO] Summary:
2019-03-22 21:10:07,184 [INFO] Batch 87000, worst loss 0.239543 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:10:07,185 [INFO] Regularization: 2449.786133 * 0.0000010000 = 0.0024497861
2019-03-22 21:10:07,185 [INFO] Sum of grad norms: 19.337984
2019-03-22 21:10:07,186 [INFO] ---------------------------------
2019-03-22 21:10:20,747 [INFO] ---------------------------------
2019-03-22 21:10:20,748 [INFO] Summary:
2019-03-22 21:10:20,748 [INFO] Batch 88000, worst loss 0.271891 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:10:20,749 [INFO] Regularization: 2449.641357 * 0.0000010000 = 0.0024496412
2019-03-22 21:10:20,749 [INFO] Sum of grad norms: 6.591068
2019-03-22 21:10:20,750 [INFO] ---------------------------------
2019-03-22 21:10:35,518 [INFO] ---------------------------------
2019-03-22 21:10:35,519 [INFO] Summary:
2019-03-22 21:10:35,520 [INFO] Batch 89000, worst loss 0.270878 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:10:35,520 [INFO] Regularization: 2449.477295 * 0.0000010000 = 0.0024494773
2019-03-22 21:10:35,521 [INFO] Sum of grad norms: 26.393330
2019-03-22 21:10:35,521 [INFO] ---------------------------------
2019-03-22 21:10:49,586 [INFO] ---------------------------------
2019-03-22 21:10:49,587 [INFO] Summary:
2019-03-22 21:10:49,587 [INFO] Batch 90000, worst loss 0.175576 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:10:49,588 [INFO] Regularization: 2449.322266 * 0.0000010000 = 0.0024493223
2019-03-22 21:10:49,588 [INFO] Sum of grad norms: 12.002155
2019-03-22 21:10:49,589 [INFO] ---------------------------------
2019-03-22 21:10:52,294 [INFO] ---------------------------------
2019-03-22 21:10:52,295 [INFO] Evaluation:
2019-03-22 21:10:52,296 [INFO] Batch 90000, worst loss 0.193317 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:10:52,298 [INFO] ---------------------------------
2019-03-22 21:11:06,914 [INFO] ---------------------------------
2019-03-22 21:11:06,915 [INFO] Summary:
2019-03-22 21:11:06,915 [INFO] Batch 91000, worst loss 0.208006 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:11:06,916 [INFO] Regularization: 2449.128174 * 0.0000010000 = 0.0024491281
2019-03-22 21:11:06,917 [INFO] Sum of grad norms: 13.694975
2019-03-22 21:11:06,917 [INFO] ---------------------------------
2019-03-22 21:11:21,260 [INFO] ---------------------------------
2019-03-22 21:11:21,261 [INFO] Summary:
2019-03-22 21:11:21,261 [INFO] Batch 92000, worst loss 0.221121 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:11:21,262 [INFO] Regularization: 2449.073975 * 0.0000010000 = 0.0024490741
2019-03-22 21:11:21,262 [INFO] Sum of grad norms: 0.016206
2019-03-22 21:11:21,263 [INFO] ---------------------------------
2019-03-22 21:11:35,085 [INFO] ---------------------------------
2019-03-22 21:11:35,086 [INFO] Summary:
2019-03-22 21:11:35,087 [INFO] Batch 93000, worst loss 0.199659 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:11:35,088 [INFO] Regularization: 2449.024414 * 0.0000010000 = 0.0024490245
2019-03-22 21:11:35,089 [INFO] Sum of grad norms: 13.060925
2019-03-22 21:11:35,089 [INFO] ---------------------------------
2019-03-22 21:11:49,284 [INFO] ---------------------------------
2019-03-22 21:11:49,285 [INFO] Summary:
2019-03-22 21:11:49,286 [INFO] Batch 94000, worst loss 0.229854 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:11:49,287 [INFO] Regularization: 2448.988770 * 0.0000010000 = 0.0024489888
2019-03-22 21:11:49,287 [INFO] Sum of grad norms: 27.237951
2019-03-22 21:11:49,288 [INFO] ---------------------------------
2019-03-22 21:12:03,971 [INFO] ---------------------------------
2019-03-22 21:12:03,972 [INFO] Summary:
2019-03-22 21:12:03,973 [INFO] Batch 95000, worst loss 0.245487 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:12:03,974 [INFO] Regularization: 2448.944336 * 0.0000010000 = 0.0024489444
2019-03-22 21:12:03,974 [INFO] Sum of grad norms: 0.253587
2019-03-22 21:12:03,975 [INFO] ---------------------------------
2019-03-22 21:12:18,794 [INFO] ---------------------------------
2019-03-22 21:12:18,795 [INFO] Summary:
2019-03-22 21:12:18,795 [INFO] Batch 96000, worst loss 0.191793 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:12:18,796 [INFO] Regularization: 2448.912598 * 0.0000010000 = 0.0024489125
2019-03-22 21:12:18,796 [INFO] Sum of grad norms: 0.051134
2019-03-22 21:12:18,797 [INFO] ---------------------------------
2019-03-22 21:12:33,851 [INFO] ---------------------------------
2019-03-22 21:12:33,851 [INFO] Summary:
2019-03-22 21:12:33,852 [INFO] Batch 97000, worst loss 0.196233 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:12:33,853 [INFO] Regularization: 2448.872803 * 0.0000010000 = 0.0024488729
2019-03-22 21:12:33,853 [INFO] Sum of grad norms: 0.797765
2019-03-22 21:12:33,854 [INFO] ---------------------------------
2019-03-22 21:12:48,481 [INFO] ---------------------------------
2019-03-22 21:12:48,482 [INFO] Summary:
2019-03-22 21:12:48,483 [INFO] Batch 98000, worst loss 0.228053 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:12:48,484 [INFO] Regularization: 2448.820557 * 0.0000010000 = 0.0024488205
2019-03-22 21:12:48,485 [INFO] Sum of grad norms: 9.580623
2019-03-22 21:12:48,486 [INFO] ---------------------------------
2019-03-22 21:13:02,971 [INFO] ---------------------------------
2019-03-22 21:13:02,972 [INFO] Summary:
2019-03-22 21:13:02,972 [INFO] Batch 99000, worst loss 0.183024 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:13:02,973 [INFO] Regularization: 2448.789062 * 0.0000010000 = 0.0024487891
2019-03-22 21:13:02,974 [INFO] Sum of grad norms: 7.049403
2019-03-22 21:13:02,974 [INFO] ---------------------------------
2019-03-22 21:13:17,520 [INFO] ---------------------------------
2019-03-22 21:13:17,521 [INFO] Summary:
2019-03-22 21:13:17,522 [INFO] Batch 100000, worst loss 0.210861 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:13:17,523 [INFO] Regularization: 2448.756104 * 0.0000010000 = 0.0024487560
2019-03-22 21:13:17,524 [INFO] Sum of grad norms: 13.405017
2019-03-22 21:13:17,524 [INFO] ---------------------------------
2019-03-22 21:13:20,247 [INFO] ---------------------------------
2019-03-22 21:13:20,248 [INFO] Evaluation:
2019-03-22 21:13:20,249 [INFO] Batch 100000, worst loss 0.232397 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:13:20,251 [INFO] ---------------------------------
2019-03-22 21:13:20,251 [INFO] Finished training, saved to file classifier/1553275201/1553285600_6_classifier_final.pth
2019-03-22 21:13:20,420 [INFO] ---------------------------------
2019-03-22 21:13:20,421 [INFO] Training model #7: (8, 64, 2) @ 2
2019-03-22 21:13:36,017 [INFO] ---------------------------------
2019-03-22 21:13:36,019 [INFO] Summary:
2019-03-22 21:13:36,020 [INFO] Batch 1000, worst loss 1233.151978 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:13:36,021 [INFO] Regularization: 30601.550781 * 0.0000010000 = 0.0306015499
2019-03-22 21:13:36,022 [INFO] Sum of grad norms: 697.763245
2019-03-22 21:13:36,023 [INFO] ---------------------------------
2019-03-22 21:13:50,582 [INFO] ---------------------------------
2019-03-22 21:13:50,583 [INFO] Summary:
2019-03-22 21:13:50,584 [INFO] Batch 2000, worst loss 20.140623 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:13:50,584 [INFO] Regularization: 28321.439453 * 0.0000010000 = 0.0283214394
2019-03-22 21:13:50,585 [INFO] Sum of grad norms: 551.270508
2019-03-22 21:13:50,585 [INFO] ---------------------------------
2019-03-22 21:14:05,619 [INFO] ---------------------------------
2019-03-22 21:14:05,620 [INFO] Summary:
2019-03-22 21:14:05,621 [INFO] Batch 3000, worst loss 9.530305 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:14:05,621 [INFO] Regularization: 24363.191406 * 0.0000010000 = 0.0243631918
2019-03-22 21:14:05,622 [INFO] Sum of grad norms: 893.979004
2019-03-22 21:14:05,622 [INFO] ---------------------------------
2019-03-22 21:14:21,063 [INFO] ---------------------------------
2019-03-22 21:14:21,063 [INFO] Summary:
2019-03-22 21:14:21,064 [INFO] Batch 4000, worst loss 4.725198 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:14:21,065 [INFO] Regularization: 19301.951172 * 0.0000010000 = 0.0193019509
2019-03-22 21:14:21,065 [INFO] Sum of grad norms: 112.466362
2019-03-22 21:14:21,066 [INFO] ---------------------------------
2019-03-22 21:14:36,068 [INFO] ---------------------------------
2019-03-22 21:14:36,069 [INFO] Summary:
2019-03-22 21:14:36,069 [INFO] Batch 5000, worst loss 1.770257 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:14:36,070 [INFO] Regularization: 13754.037109 * 0.0000010000 = 0.0137540372
2019-03-22 21:14:36,070 [INFO] Sum of grad norms: 93.490303
2019-03-22 21:14:36,071 [INFO] ---------------------------------
2019-03-22 21:14:50,165 [INFO] ---------------------------------
2019-03-22 21:14:50,165 [INFO] Summary:
2019-03-22 21:14:50,166 [INFO] Batch 6000, worst loss 1.764660 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:14:50,167 [INFO] Regularization: 10309.950195 * 0.0000010000 = 0.0103099504
2019-03-22 21:14:50,167 [INFO] Sum of grad norms: 28.666981
2019-03-22 21:14:50,168 [INFO] ---------------------------------
2019-03-22 21:15:05,326 [INFO] ---------------------------------
2019-03-22 21:15:05,327 [INFO] Summary:
2019-03-22 21:15:05,328 [INFO] Batch 7000, worst loss 0.985479 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:15:05,329 [INFO] Regularization: 10001.563477 * 0.0000010000 = 0.0100015635
2019-03-22 21:15:05,329 [INFO] Sum of grad norms: 26.703857
2019-03-22 21:15:05,330 [INFO] ---------------------------------
2019-03-22 21:15:19,996 [INFO] ---------------------------------
2019-03-22 21:15:19,997 [INFO] Summary:
2019-03-22 21:15:19,998 [INFO] Batch 8000, worst loss 0.828705 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:15:19,999 [INFO] Regularization: 9853.765625 * 0.0000010000 = 0.0098537654
2019-03-22 21:15:19,999 [INFO] Sum of grad norms: 23.698526
2019-03-22 21:15:20,000 [INFO] ---------------------------------
2019-03-22 21:15:34,471 [INFO] ---------------------------------
2019-03-22 21:15:34,472 [INFO] Summary:
2019-03-22 21:15:34,473 [INFO] Batch 9000, worst loss 0.870664 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:15:34,473 [INFO] Regularization: 9661.508789 * 0.0000010000 = 0.0096615087
2019-03-22 21:15:34,474 [INFO] Sum of grad norms: 15.733909
2019-03-22 21:15:34,474 [INFO] ---------------------------------
2019-03-22 21:15:49,009 [INFO] ---------------------------------
2019-03-22 21:15:49,010 [INFO] Summary:
2019-03-22 21:15:49,011 [INFO] Batch 10000, worst loss 0.605001 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:15:49,012 [INFO] Regularization: 9431.302734 * 0.0000010000 = 0.0094313025
2019-03-22 21:15:49,012 [INFO] Sum of grad norms: 12.755801
2019-03-22 21:15:49,013 [INFO] ---------------------------------
2019-03-22 21:15:51,735 [INFO] ---------------------------------
2019-03-22 21:15:51,736 [INFO] Evaluation:
2019-03-22 21:15:51,737 [INFO] Batch 10000, worst loss 0.545463 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:15:51,738 [INFO] ---------------------------------
2019-03-22 21:16:06,461 [INFO] ---------------------------------
2019-03-22 21:16:06,462 [INFO] Summary:
2019-03-22 21:16:06,462 [INFO] Batch 11000, worst loss 0.606647 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:16:06,463 [INFO] Regularization: 9139.333008 * 0.0000010000 = 0.0091393329
2019-03-22 21:16:06,464 [INFO] Sum of grad norms: 12.771813
2019-03-22 21:16:06,464 [INFO] ---------------------------------
2019-03-22 21:16:20,678 [INFO] ---------------------------------
2019-03-22 21:16:20,679 [INFO] Summary:
2019-03-22 21:16:20,680 [INFO] Batch 12000, worst loss 0.322837 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:16:20,680 [INFO] Regularization: 8744.209961 * 0.0000010000 = 0.0087442100
2019-03-22 21:16:20,681 [INFO] Sum of grad norms: 4.590056
2019-03-22 21:16:20,681 [INFO] ---------------------------------
2019-03-22 21:16:36,030 [INFO] ---------------------------------
2019-03-22 21:16:36,030 [INFO] Summary:
2019-03-22 21:16:36,031 [INFO] Batch 13000, worst loss 0.317841 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:16:36,032 [INFO] Regularization: 8371.915039 * 0.0000010000 = 0.0083719147
2019-03-22 21:16:36,033 [INFO] Sum of grad norms: 5.312461
2019-03-22 21:16:36,033 [INFO] ---------------------------------
2019-03-22 21:16:50,283 [INFO] ---------------------------------
2019-03-22 21:16:50,284 [INFO] Summary:
2019-03-22 21:16:50,284 [INFO] Batch 14000, worst loss 0.446168 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:16:50,285 [INFO] Regularization: 8015.829102 * 0.0000010000 = 0.0080158291
2019-03-22 21:16:50,285 [INFO] Sum of grad norms: 5.439804
2019-03-22 21:16:50,286 [INFO] ---------------------------------
2019-03-22 21:17:05,034 [INFO] ---------------------------------
2019-03-22 21:17:05,035 [INFO] Summary:
2019-03-22 21:17:05,036 [INFO] Batch 15000, worst loss 0.348263 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:17:05,037 [INFO] Regularization: 7731.494141 * 0.0000010000 = 0.0077314940
2019-03-22 21:17:05,038 [INFO] Sum of grad norms: 12.295975
2019-03-22 21:17:05,038 [INFO] ---------------------------------
2019-03-22 21:17:19,535 [INFO] ---------------------------------
2019-03-22 21:17:19,536 [INFO] Summary:
2019-03-22 21:17:19,537 [INFO] Batch 16000, worst loss 0.489725 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:17:19,537 [INFO] Regularization: 7337.337891 * 0.0000010000 = 0.0073373378
2019-03-22 21:17:19,538 [INFO] Sum of grad norms: 2.399059
2019-03-22 21:17:19,539 [INFO] ---------------------------------
2019-03-22 21:17:33,734 [INFO] ---------------------------------
2019-03-22 21:17:33,735 [INFO] Summary:
2019-03-22 21:17:33,736 [INFO] Batch 17000, worst loss 0.348289 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:17:33,737 [INFO] Regularization: 7013.020508 * 0.0000010000 = 0.0070130206
2019-03-22 21:17:33,737 [INFO] Sum of grad norms: 1.855610
2019-03-22 21:17:33,738 [INFO] ---------------------------------
2019-03-22 21:17:48,411 [INFO] ---------------------------------
2019-03-22 21:17:48,412 [INFO] Summary:
2019-03-22 21:17:48,413 [INFO] Batch 18000, worst loss 0.313000 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:17:48,413 [INFO] Regularization: 6619.500977 * 0.0000010000 = 0.0066195009
2019-03-22 21:17:48,414 [INFO] Sum of grad norms: 4.077260
2019-03-22 21:17:48,414 [INFO] ---------------------------------
2019-03-22 21:18:03,361 [INFO] ---------------------------------
2019-03-22 21:18:03,362 [INFO] Summary:
2019-03-22 21:18:03,362 [INFO] Batch 19000, worst loss 0.400366 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:18:03,363 [INFO] Regularization: 6261.716309 * 0.0000010000 = 0.0062617161
2019-03-22 21:18:03,363 [INFO] Sum of grad norms: 4.378482
2019-03-22 21:18:03,364 [INFO] ---------------------------------
2019-03-22 21:18:17,766 [INFO] ---------------------------------
2019-03-22 21:18:17,767 [INFO] Summary:
2019-03-22 21:18:17,768 [INFO] Batch 20000, worst loss 0.316757 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:18:17,768 [INFO] Regularization: 5926.613770 * 0.0000010000 = 0.0059266137
2019-03-22 21:18:17,769 [INFO] Sum of grad norms: 4.519346
2019-03-22 21:18:17,769 [INFO] ---------------------------------
2019-03-22 21:18:20,519 [INFO] ---------------------------------
2019-03-22 21:18:20,520 [INFO] Evaluation:
2019-03-22 21:18:20,521 [INFO] Batch 20000, worst loss 0.431172 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:18:20,521 [INFO] ---------------------------------
2019-03-22 21:18:35,472 [INFO] ---------------------------------
2019-03-22 21:18:35,473 [INFO] Summary:
2019-03-22 21:18:35,474 [INFO] Batch 21000, worst loss 0.295044 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:18:35,474 [INFO] Regularization: 5702.458496 * 0.0000010000 = 0.0057024583
2019-03-22 21:18:35,475 [INFO] Sum of grad norms: 3.320403
2019-03-22 21:18:35,475 [INFO] ---------------------------------
2019-03-22 21:18:49,516 [INFO] ---------------------------------
2019-03-22 21:18:49,517 [INFO] Summary:
2019-03-22 21:18:49,517 [INFO] Batch 22000, worst loss 0.484892 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:18:49,518 [INFO] Regularization: 5516.702637 * 0.0000010000 = 0.0055167028
2019-03-22 21:18:49,518 [INFO] Sum of grad norms: 3.306739
2019-03-22 21:18:49,519 [INFO] ---------------------------------
2019-03-22 21:19:03,322 [INFO] ---------------------------------
2019-03-22 21:19:03,323 [INFO] Summary:
2019-03-22 21:19:03,324 [INFO] Batch 23000, worst loss 0.346134 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:19:03,324 [INFO] Regularization: 5233.232910 * 0.0000010000 = 0.0052332329
2019-03-22 21:19:03,325 [INFO] Sum of grad norms: 11.515011
2019-03-22 21:19:03,325 [INFO] ---------------------------------
2019-03-22 21:19:18,395 [INFO] ---------------------------------
2019-03-22 21:19:18,396 [INFO] Summary:
2019-03-22 21:19:18,397 [INFO] Batch 24000, worst loss 0.353915 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:19:18,397 [INFO] Regularization: 5075.962402 * 0.0000010000 = 0.0050759623
2019-03-22 21:19:18,398 [INFO] Sum of grad norms: 10.492318
2019-03-22 21:19:18,398 [INFO] ---------------------------------
2019-03-22 21:19:33,329 [INFO] ---------------------------------
2019-03-22 21:19:33,330 [INFO] Summary:
2019-03-22 21:19:33,330 [INFO] Batch 25000, worst loss 0.361955 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:19:33,331 [INFO] Regularization: 4971.698242 * 0.0000010000 = 0.0049716984
2019-03-22 21:19:33,331 [INFO] Sum of grad norms: 7.483109
2019-03-22 21:19:33,332 [INFO] ---------------------------------
2019-03-22 21:19:47,740 [INFO] ---------------------------------
2019-03-22 21:19:47,741 [INFO] Summary:
2019-03-22 21:19:47,741 [INFO] Batch 26000, worst loss 0.384851 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:19:47,742 [INFO] Regularization: 4764.082520 * 0.0000010000 = 0.0047640824
2019-03-22 21:19:47,742 [INFO] Sum of grad norms: 2.049184
2019-03-22 21:19:47,743 [INFO] ---------------------------------
2019-03-22 21:20:01,953 [INFO] ---------------------------------
2019-03-22 21:20:01,955 [INFO] Summary:
2019-03-22 21:20:01,955 [INFO] Batch 27000, worst loss 0.403822 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:20:01,956 [INFO] Regularization: 4590.803711 * 0.0000010000 = 0.0045908038
2019-03-22 21:20:01,956 [INFO] Sum of grad norms: 0.506469
2019-03-22 21:20:01,957 [INFO] ---------------------------------
2019-03-22 21:20:16,532 [INFO] ---------------------------------
2019-03-22 21:20:16,533 [INFO] Summary:
2019-03-22 21:20:16,533 [INFO] Batch 28000, worst loss 0.288626 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:20:16,534 [INFO] Regularization: 4446.223145 * 0.0000010000 = 0.0044462229
2019-03-22 21:20:16,534 [INFO] Sum of grad norms: 19.745689
2019-03-22 21:20:16,535 [INFO] ---------------------------------
2019-03-22 21:20:31,617 [INFO] ---------------------------------
2019-03-22 21:20:31,618 [INFO] Summary:
2019-03-22 21:20:31,618 [INFO] Batch 29000, worst loss 0.423724 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:20:31,619 [INFO] Regularization: 4338.722656 * 0.0000010000 = 0.0043387227
2019-03-22 21:20:31,619 [INFO] Sum of grad norms: 1.137066
2019-03-22 21:20:31,620 [INFO] ---------------------------------
2019-03-22 21:20:46,343 [INFO] ---------------------------------
2019-03-22 21:20:46,345 [INFO] Summary:
2019-03-22 21:20:46,345 [INFO] Batch 30000, worst loss 0.354596 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:20:46,346 [INFO] Regularization: 4173.852051 * 0.0000010000 = 0.0041738520
2019-03-22 21:20:46,346 [INFO] Sum of grad norms: 3.557095
2019-03-22 21:20:46,347 [INFO] ---------------------------------
2019-03-22 21:20:49,069 [INFO] ---------------------------------
2019-03-22 21:20:49,070 [INFO] Evaluation:
2019-03-22 21:20:49,070 [INFO] Batch 30000, worst loss 0.347779 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:20:49,071 [INFO] ---------------------------------
2019-03-22 21:21:03,544 [INFO] ---------------------------------
2019-03-22 21:21:03,544 [INFO] Summary:
2019-03-22 21:21:03,545 [INFO] Batch 31000, worst loss 0.497566 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:21:03,546 [INFO] Regularization: 4079.408447 * 0.0000010000 = 0.0040794085
2019-03-22 21:21:03,546 [INFO] Sum of grad norms: 8.398821
2019-03-22 21:21:03,547 [INFO] ---------------------------------
2019-03-22 21:21:18,093 [INFO] ---------------------------------
2019-03-22 21:21:18,094 [INFO] Summary:
2019-03-22 21:21:18,094 [INFO] Batch 32000, worst loss 0.508022 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:21:18,095 [INFO] Regularization: 3958.409180 * 0.0000010000 = 0.0039584092
2019-03-22 21:21:18,095 [INFO] Sum of grad norms: 5.732504
2019-03-22 21:21:18,096 [INFO] ---------------------------------
2019-03-22 21:21:32,253 [INFO] ---------------------------------
2019-03-22 21:21:32,254 [INFO] Summary:
2019-03-22 21:21:32,255 [INFO] Batch 33000, worst loss 0.370590 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:21:32,255 [INFO] Regularization: 3911.807129 * 0.0000010000 = 0.0039118072
2019-03-22 21:21:32,256 [INFO] Sum of grad norms: 2.411384
2019-03-22 21:21:32,257 [INFO] ---------------------------------
2019-03-22 21:21:46,665 [INFO] ---------------------------------
2019-03-22 21:21:46,666 [INFO] Summary:
2019-03-22 21:21:46,666 [INFO] Batch 34000, worst loss 0.300765 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:21:46,667 [INFO] Regularization: 3856.471436 * 0.0000010000 = 0.0038564715
2019-03-22 21:21:46,667 [INFO] Sum of grad norms: 1.346806
2019-03-22 21:21:46,668 [INFO] ---------------------------------
2019-03-22 21:22:00,896 [INFO] ---------------------------------
2019-03-22 21:22:00,897 [INFO] Summary:
2019-03-22 21:22:00,897 [INFO] Batch 35000, worst loss 0.358916 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:22:00,898 [INFO] Regularization: 3736.807129 * 0.0000010000 = 0.0037368070
2019-03-22 21:22:00,898 [INFO] Sum of grad norms: 2.961105
2019-03-22 21:22:00,899 [INFO] ---------------------------------
2019-03-22 21:22:15,538 [INFO] ---------------------------------
2019-03-22 21:22:15,539 [INFO] Summary:
2019-03-22 21:22:15,540 [INFO] Batch 36000, worst loss 0.334412 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:22:15,540 [INFO] Regularization: 3616.418945 * 0.0000010000 = 0.0036164189
2019-03-22 21:22:15,541 [INFO] Sum of grad norms: 0.046754
2019-03-22 21:22:15,542 [INFO] ---------------------------------
2019-03-22 21:22:29,965 [INFO] ---------------------------------
2019-03-22 21:22:29,966 [INFO] Summary:
2019-03-22 21:22:29,967 [INFO] Batch 37000, worst loss 0.288631 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:22:29,968 [INFO] Regularization: 3547.680176 * 0.0000010000 = 0.0035476801
2019-03-22 21:22:29,968 [INFO] Sum of grad norms: 1.926126
2019-03-22 21:22:29,969 [INFO] ---------------------------------
2019-03-22 21:22:44,586 [INFO] ---------------------------------
2019-03-22 21:22:44,587 [INFO] Summary:
2019-03-22 21:22:44,588 [INFO] Batch 38000, worst loss 0.467334 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:22:44,589 [INFO] Regularization: 3449.214355 * 0.0000010000 = 0.0034492144
2019-03-22 21:22:44,590 [INFO] Sum of grad norms: 0.618726
2019-03-22 21:22:44,590 [INFO] ---------------------------------
2019-03-22 21:22:59,474 [INFO] ---------------------------------
2019-03-22 21:22:59,475 [INFO] Summary:
2019-03-22 21:22:59,476 [INFO] Batch 39000, worst loss 0.343617 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:22:59,476 [INFO] Regularization: 3346.811768 * 0.0000010000 = 0.0033468117
2019-03-22 21:22:59,477 [INFO] Sum of grad norms: 4.474759
2019-03-22 21:22:59,477 [INFO] ---------------------------------
2019-03-22 21:23:14,524 [INFO] ---------------------------------
2019-03-22 21:23:14,525 [INFO] Summary:
2019-03-22 21:23:14,526 [INFO] Batch 40000, worst loss 0.355551 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:23:14,526 [INFO] Regularization: 3268.635254 * 0.0000010000 = 0.0032686351
2019-03-22 21:23:14,527 [INFO] Sum of grad norms: 3.570518
2019-03-22 21:23:14,527 [INFO] ---------------------------------
2019-03-22 21:23:17,269 [INFO] ---------------------------------
2019-03-22 21:23:17,270 [INFO] Evaluation:
2019-03-22 21:23:17,271 [INFO] Batch 40000, worst loss 0.305919 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:23:17,272 [INFO] ---------------------------------
2019-03-22 21:23:31,937 [INFO] ---------------------------------
2019-03-22 21:23:31,937 [INFO] Summary:
2019-03-22 21:23:31,938 [INFO] Batch 41000, worst loss 0.437152 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:23:31,938 [INFO] Regularization: 3210.788330 * 0.0000010000 = 0.0032107884
2019-03-22 21:23:31,939 [INFO] Sum of grad norms: 2.353432
2019-03-22 21:23:31,939 [INFO] ---------------------------------
2019-03-22 21:23:47,131 [INFO] ---------------------------------
2019-03-22 21:23:47,132 [INFO] Summary:
2019-03-22 21:23:47,133 [INFO] Batch 42000, worst loss 0.295050 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:23:47,133 [INFO] Regularization: 3144.936768 * 0.0000010000 = 0.0031449369
2019-03-22 21:23:47,134 [INFO] Sum of grad norms: 0.065001
2019-03-22 21:23:47,135 [INFO] ---------------------------------
2019-03-22 21:24:01,838 [INFO] ---------------------------------
2019-03-22 21:24:01,839 [INFO] Summary:
2019-03-22 21:24:01,839 [INFO] Batch 43000, worst loss 0.257832 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:24:01,840 [INFO] Regularization: 3087.947021 * 0.0000010000 = 0.0030879469
2019-03-22 21:24:01,840 [INFO] Sum of grad norms: 26.166082
2019-03-22 21:24:01,841 [INFO] ---------------------------------
2019-03-22 21:24:16,082 [INFO] ---------------------------------
2019-03-22 21:24:16,083 [INFO] Summary:
2019-03-22 21:24:16,084 [INFO] Batch 44000, worst loss 0.242329 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:24:16,084 [INFO] Regularization: 3025.095703 * 0.0000010000 = 0.0030250957
2019-03-22 21:24:16,085 [INFO] Sum of grad norms: 0.014013
2019-03-22 21:24:16,086 [INFO] ---------------------------------
2019-03-22 21:24:31,214 [INFO] ---------------------------------
2019-03-22 21:24:31,215 [INFO] Summary:
2019-03-22 21:24:31,216 [INFO] Batch 45000, worst loss 0.235889 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:24:31,216 [INFO] Regularization: 2962.319336 * 0.0000010000 = 0.0029623194
2019-03-22 21:24:31,217 [INFO] Sum of grad norms: 22.176764
2019-03-22 21:24:31,217 [INFO] ---------------------------------
2019-03-22 21:24:45,201 [INFO] ---------------------------------
2019-03-22 21:24:45,202 [INFO] Summary:
2019-03-22 21:24:45,204 [INFO] Batch 46000, worst loss 0.309619 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:24:45,205 [INFO] Regularization: 2900.838379 * 0.0000010000 = 0.0029008384
2019-03-22 21:24:45,206 [INFO] Sum of grad norms: 0.048666
2019-03-22 21:24:45,207 [INFO] ---------------------------------
2019-03-22 21:25:00,660 [INFO] ---------------------------------
2019-03-22 21:25:00,661 [INFO] Summary:
2019-03-22 21:25:00,661 [INFO] Batch 47000, worst loss 0.258259 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:25:00,662 [INFO] Regularization: 2838.423828 * 0.0000010000 = 0.0028384237
2019-03-22 21:25:00,662 [INFO] Sum of grad norms: 15.961552
2019-03-22 21:25:00,663 [INFO] ---------------------------------
2019-03-22 21:25:15,046 [INFO] ---------------------------------
2019-03-22 21:25:15,047 [INFO] Summary:
2019-03-22 21:25:15,047 [INFO] Batch 48000, worst loss 0.308330 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:25:15,048 [INFO] Regularization: 2789.942383 * 0.0000010000 = 0.0027899423
2019-03-22 21:25:15,048 [INFO] Sum of grad norms: 0.020114
2019-03-22 21:25:15,049 [INFO] ---------------------------------
2019-03-22 21:25:29,136 [INFO] ---------------------------------
2019-03-22 21:25:29,137 [INFO] Summary:
2019-03-22 21:25:29,137 [INFO] Batch 49000, worst loss 0.274982 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:25:29,138 [INFO] Regularization: 2746.130859 * 0.0000010000 = 0.0027461308
2019-03-22 21:25:29,139 [INFO] Sum of grad norms: 4.429811
2019-03-22 21:25:29,139 [INFO] ---------------------------------
2019-03-22 21:25:42,802 [INFO] ---------------------------------
2019-03-22 21:25:42,803 [INFO] Summary:
2019-03-22 21:25:42,804 [INFO] Batch 50000, worst loss 0.408386 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:25:42,804 [INFO] Regularization: 2711.495361 * 0.0000010000 = 0.0027114954
2019-03-22 21:25:42,805 [INFO] Sum of grad norms: 0.432012
2019-03-22 21:25:42,805 [INFO] ---------------------------------
2019-03-22 21:25:45,583 [INFO] ---------------------------------
2019-03-22 21:25:45,584 [INFO] Evaluation:
2019-03-22 21:25:45,585 [INFO] Batch 50000, worst loss 0.235596 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:25:45,586 [INFO] ---------------------------------
2019-03-22 21:25:59,808 [INFO] ---------------------------------
2019-03-22 21:25:59,809 [INFO] Summary:
2019-03-22 21:25:59,809 [INFO] Batch 51000, worst loss 0.289649 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:25:59,810 [INFO] Regularization: 2663.706299 * 0.0000010000 = 0.0026637062
2019-03-22 21:25:59,810 [INFO] Sum of grad norms: 0.661698
2019-03-22 21:25:59,811 [INFO] ---------------------------------
2019-03-22 21:26:13,569 [INFO] ---------------------------------
2019-03-22 21:26:13,570 [INFO] Summary:
2019-03-22 21:26:13,570 [INFO] Batch 52000, worst loss 0.250509 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:26:13,571 [INFO] Regularization: 2644.687012 * 0.0000010000 = 0.0026446870
2019-03-22 21:26:13,571 [INFO] Sum of grad norms: 25.821960
2019-03-22 21:26:13,572 [INFO] ---------------------------------
2019-03-22 21:26:28,093 [INFO] ---------------------------------
2019-03-22 21:26:28,094 [INFO] Summary:
2019-03-22 21:26:28,095 [INFO] Batch 53000, worst loss 0.198230 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:26:28,095 [INFO] Regularization: 2630.110840 * 0.0000010000 = 0.0026301108
2019-03-22 21:26:28,096 [INFO] Sum of grad norms: 0.008974
2019-03-22 21:26:28,096 [INFO] ---------------------------------
2019-03-22 21:26:42,089 [INFO] ---------------------------------
2019-03-22 21:26:42,090 [INFO] Summary:
2019-03-22 21:26:42,090 [INFO] Batch 54000, worst loss 0.195602 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:26:42,091 [INFO] Regularization: 2613.426514 * 0.0000010000 = 0.0026134264
2019-03-22 21:26:42,092 [INFO] Sum of grad norms: 0.096223
2019-03-22 21:26:42,092 [INFO] ---------------------------------
2019-03-22 21:26:56,964 [INFO] ---------------------------------
2019-03-22 21:26:56,965 [INFO] Summary:
2019-03-22 21:26:56,967 [INFO] Batch 55000, worst loss 0.243277 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:26:56,967 [INFO] Regularization: 2599.205566 * 0.0000010000 = 0.0025992056
2019-03-22 21:26:56,968 [INFO] Sum of grad norms: 0.019856
2019-03-22 21:26:56,969 [INFO] ---------------------------------
2019-03-22 21:27:12,104 [INFO] ---------------------------------
2019-03-22 21:27:12,105 [INFO] Summary:
2019-03-22 21:27:12,106 [INFO] Batch 56000, worst loss 0.243781 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:27:12,106 [INFO] Regularization: 2582.232910 * 0.0000010000 = 0.0025822329
2019-03-22 21:27:12,107 [INFO] Sum of grad norms: 0.011423
2019-03-22 21:27:12,107 [INFO] ---------------------------------
2019-03-22 21:27:27,199 [INFO] ---------------------------------
2019-03-22 21:27:27,200 [INFO] Summary:
2019-03-22 21:27:27,201 [INFO] Batch 57000, worst loss 0.252700 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:27:27,201 [INFO] Regularization: 2564.967529 * 0.0000010000 = 0.0025649676
2019-03-22 21:27:27,202 [INFO] Sum of grad norms: 0.079250
2019-03-22 21:27:27,202 [INFO] ---------------------------------
2019-03-22 21:27:41,803 [INFO] ---------------------------------
2019-03-22 21:27:41,804 [INFO] Summary:
2019-03-22 21:27:41,805 [INFO] Batch 58000, worst loss 0.222054 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:27:41,805 [INFO] Regularization: 2549.063232 * 0.0000010000 = 0.0025490632
2019-03-22 21:27:41,806 [INFO] Sum of grad norms: 0.010825
2019-03-22 21:27:41,806 [INFO] ---------------------------------
2019-03-22 21:27:56,697 [INFO] ---------------------------------
2019-03-22 21:27:56,698 [INFO] Summary:
2019-03-22 21:27:56,699 [INFO] Batch 59000, worst loss 0.285733 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:27:56,699 [INFO] Regularization: 2537.242676 * 0.0000010000 = 0.0025372426
2019-03-22 21:27:56,700 [INFO] Sum of grad norms: 40.094872
2019-03-22 21:27:56,700 [INFO] ---------------------------------
2019-03-22 21:28:11,751 [INFO] ---------------------------------
2019-03-22 21:28:11,752 [INFO] Summary:
2019-03-22 21:28:11,753 [INFO] Batch 60000, worst loss 0.195332 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:28:11,753 [INFO] Regularization: 2524.106445 * 0.0000010000 = 0.0025241065
2019-03-22 21:28:11,754 [INFO] Sum of grad norms: 17.922575
2019-03-22 21:28:11,754 [INFO] ---------------------------------
2019-03-22 21:28:14,457 [INFO] ---------------------------------
2019-03-22 21:28:14,458 [INFO] Evaluation:
2019-03-22 21:28:14,458 [INFO] Batch 60000, worst loss 0.256743 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:28:14,459 [INFO] ---------------------------------
2019-03-22 21:28:29,137 [INFO] ---------------------------------
2019-03-22 21:28:29,137 [INFO] Summary:
2019-03-22 21:28:29,138 [INFO] Batch 61000, worst loss 0.271798 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:28:29,138 [INFO] Regularization: 2511.492920 * 0.0000010000 = 0.0025114929
2019-03-22 21:28:29,139 [INFO] Sum of grad norms: 0.379502
2019-03-22 21:28:29,139 [INFO] ---------------------------------
2019-03-22 21:28:43,335 [INFO] ---------------------------------
2019-03-22 21:28:43,336 [INFO] Summary:
2019-03-22 21:28:43,337 [INFO] Batch 62000, worst loss 0.212127 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:28:43,337 [INFO] Regularization: 2506.022949 * 0.0000010000 = 0.0025060230
2019-03-22 21:28:43,338 [INFO] Sum of grad norms: 0.004543
2019-03-22 21:28:43,339 [INFO] ---------------------------------
2019-03-22 21:28:58,258 [INFO] ---------------------------------
2019-03-22 21:28:58,259 [INFO] Summary:
2019-03-22 21:28:58,259 [INFO] Batch 63000, worst loss 0.202371 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:28:58,260 [INFO] Regularization: 2502.281738 * 0.0000010000 = 0.0025022817
2019-03-22 21:28:58,260 [INFO] Sum of grad norms: 0.004031
2019-03-22 21:28:58,261 [INFO] ---------------------------------
2019-03-22 21:29:13,016 [INFO] ---------------------------------
2019-03-22 21:29:13,017 [INFO] Summary:
2019-03-22 21:29:13,018 [INFO] Batch 64000, worst loss 0.191487 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:29:13,018 [INFO] Regularization: 2498.689941 * 0.0000010000 = 0.0024986900
2019-03-22 21:29:13,019 [INFO] Sum of grad norms: 0.005093
2019-03-22 21:29:13,019 [INFO] ---------------------------------
2019-03-22 21:29:27,629 [INFO] ---------------------------------
2019-03-22 21:29:27,629 [INFO] Summary:
2019-03-22 21:29:27,630 [INFO] Batch 65000, worst loss 0.257568 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:29:27,631 [INFO] Regularization: 2496.361816 * 0.0000010000 = 0.0024963617
2019-03-22 21:29:27,631 [INFO] Sum of grad norms: 1.995500
2019-03-22 21:29:27,632 [INFO] ---------------------------------
2019-03-22 21:29:42,483 [INFO] ---------------------------------
2019-03-22 21:29:42,484 [INFO] Summary:
2019-03-22 21:29:42,485 [INFO] Batch 66000, worst loss 0.228670 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:29:42,485 [INFO] Regularization: 2492.152344 * 0.0000010000 = 0.0024921524
2019-03-22 21:29:42,486 [INFO] Sum of grad norms: 0.020205
2019-03-22 21:29:42,486 [INFO] ---------------------------------
2019-03-22 21:29:56,925 [INFO] ---------------------------------
2019-03-22 21:29:56,925 [INFO] Summary:
2019-03-22 21:29:56,926 [INFO] Batch 67000, worst loss 0.245174 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:29:56,926 [INFO] Regularization: 2488.626953 * 0.0000010000 = 0.0024886269
2019-03-22 21:29:56,927 [INFO] Sum of grad norms: 39.721344
2019-03-22 21:29:56,927 [INFO] ---------------------------------
2019-03-22 21:30:10,797 [INFO] ---------------------------------
2019-03-22 21:30:10,798 [INFO] Summary:
2019-03-22 21:30:10,799 [INFO] Batch 68000, worst loss 0.232573 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:30:10,800 [INFO] Regularization: 2484.687500 * 0.0000010000 = 0.0024846876
2019-03-22 21:30:10,800 [INFO] Sum of grad norms: 13.669035
2019-03-22 21:30:10,801 [INFO] ---------------------------------
2019-03-22 21:30:25,344 [INFO] ---------------------------------
2019-03-22 21:30:25,345 [INFO] Summary:
2019-03-22 21:30:25,346 [INFO] Batch 69000, worst loss 0.285241 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:30:25,346 [INFO] Regularization: 2481.314941 * 0.0000010000 = 0.0024813148
2019-03-22 21:30:25,347 [INFO] Sum of grad norms: 8.759375
2019-03-22 21:30:25,347 [INFO] ---------------------------------
2019-03-22 21:30:39,734 [INFO] ---------------------------------
2019-03-22 21:30:39,735 [INFO] Summary:
2019-03-22 21:30:39,736 [INFO] Batch 70000, worst loss 0.238194 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:30:39,736 [INFO] Regularization: 2477.725098 * 0.0000010000 = 0.0024777250
2019-03-22 21:30:39,737 [INFO] Sum of grad norms: 19.331064
2019-03-22 21:30:39,737 [INFO] ---------------------------------
2019-03-22 21:30:42,505 [INFO] ---------------------------------
2019-03-22 21:30:42,506 [INFO] Evaluation:
2019-03-22 21:30:42,506 [INFO] Batch 70000, worst loss 0.207748 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:30:42,507 [INFO] ---------------------------------
2019-03-22 21:30:56,845 [INFO] ---------------------------------
2019-03-22 21:30:56,846 [INFO] Summary:
2019-03-22 21:30:56,847 [INFO] Batch 71000, worst loss 0.213442 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:30:56,847 [INFO] Regularization: 2474.206543 * 0.0000010000 = 0.0024742065
2019-03-22 21:30:56,848 [INFO] Sum of grad norms: 0.444923
2019-03-22 21:30:56,848 [INFO] ---------------------------------
2019-03-22 21:31:11,170 [INFO] ---------------------------------
2019-03-22 21:31:11,171 [INFO] Summary:
2019-03-22 21:31:11,172 [INFO] Batch 72000, worst loss 0.183954 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:31:11,172 [INFO] Regularization: 2472.994873 * 0.0000010000 = 0.0024729948
2019-03-22 21:31:11,173 [INFO] Sum of grad norms: 0.007901
2019-03-22 21:31:11,173 [INFO] ---------------------------------
2019-03-22 21:31:25,639 [INFO] ---------------------------------
2019-03-22 21:31:25,640 [INFO] Summary:
2019-03-22 21:31:25,641 [INFO] Batch 73000, worst loss 0.180021 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:31:25,641 [INFO] Regularization: 2472.060791 * 0.0000010000 = 0.0024720607
2019-03-22 21:31:25,642 [INFO] Sum of grad norms: 19.942249
2019-03-22 21:31:25,643 [INFO] ---------------------------------
2019-03-22 21:31:40,072 [INFO] ---------------------------------
2019-03-22 21:31:40,073 [INFO] Summary:
2019-03-22 21:31:40,074 [INFO] Batch 74000, worst loss 0.195319 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:31:40,074 [INFO] Regularization: 2471.275146 * 0.0000010000 = 0.0024712752
2019-03-22 21:31:40,075 [INFO] Sum of grad norms: 0.003906
2019-03-22 21:31:40,076 [INFO] ---------------------------------
2019-03-22 21:31:53,903 [INFO] ---------------------------------
2019-03-22 21:31:53,904 [INFO] Summary:
2019-03-22 21:31:53,905 [INFO] Batch 75000, worst loss 0.226900 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:31:53,905 [INFO] Regularization: 2470.462646 * 0.0000010000 = 0.0024704626
2019-03-22 21:31:53,906 [INFO] Sum of grad norms: 5.984920
2019-03-22 21:31:53,906 [INFO] ---------------------------------
2019-03-22 21:32:08,611 [INFO] ---------------------------------
2019-03-22 21:32:08,612 [INFO] Summary:
2019-03-22 21:32:08,612 [INFO] Batch 76000, worst loss 0.187832 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:32:08,613 [INFO] Regularization: 2469.627930 * 0.0000010000 = 0.0024696279
2019-03-22 21:32:08,613 [INFO] Sum of grad norms: 3.127621
2019-03-22 21:32:08,614 [INFO] ---------------------------------
2019-03-22 21:32:22,697 [INFO] ---------------------------------
2019-03-22 21:32:22,698 [INFO] Summary:
2019-03-22 21:32:22,699 [INFO] Batch 77000, worst loss 0.229894 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:32:22,699 [INFO] Regularization: 2469.019287 * 0.0000010000 = 0.0024690193
2019-03-22 21:32:22,700 [INFO] Sum of grad norms: 0.078845
2019-03-22 21:32:22,700 [INFO] ---------------------------------
2019-03-22 21:32:37,690 [INFO] ---------------------------------
2019-03-22 21:32:37,691 [INFO] Summary:
2019-03-22 21:32:37,691 [INFO] Batch 78000, worst loss 0.267689 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:32:37,692 [INFO] Regularization: 2468.184082 * 0.0000010000 = 0.0024681841
2019-03-22 21:32:37,692 [INFO] Sum of grad norms: 0.465662
2019-03-22 21:32:37,693 [INFO] ---------------------------------
2019-03-22 21:32:52,694 [INFO] ---------------------------------
2019-03-22 21:32:52,695 [INFO] Summary:
2019-03-22 21:32:52,695 [INFO] Batch 79000, worst loss 0.266084 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:32:52,696 [INFO] Regularization: 2467.399902 * 0.0000010000 = 0.0024673999
2019-03-22 21:32:52,696 [INFO] Sum of grad norms: 10.014533
2019-03-22 21:32:52,697 [INFO] ---------------------------------
2019-03-22 21:33:07,751 [INFO] ---------------------------------
2019-03-22 21:33:07,752 [INFO] Summary:
2019-03-22 21:33:07,752 [INFO] Batch 80000, worst loss 0.265659 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:33:07,753 [INFO] Regularization: 2466.554688 * 0.0000010000 = 0.0024665548
2019-03-22 21:33:07,753 [INFO] Sum of grad norms: 0.005212
2019-03-22 21:33:07,754 [INFO] ---------------------------------
2019-03-22 21:33:10,473 [INFO] ---------------------------------
2019-03-22 21:33:10,473 [INFO] Evaluation:
2019-03-22 21:33:10,474 [INFO] Batch 80000, worst loss 0.220693 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:33:10,475 [INFO] ---------------------------------
2019-03-22 21:33:24,459 [INFO] ---------------------------------
2019-03-22 21:33:24,460 [INFO] Summary:
2019-03-22 21:33:24,461 [INFO] Batch 81000, worst loss 0.184847 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:33:24,461 [INFO] Regularization: 2465.783203 * 0.0000010000 = 0.0024657832
2019-03-22 21:33:24,462 [INFO] Sum of grad norms: 0.061568
2019-03-22 21:33:24,462 [INFO] ---------------------------------
2019-03-22 21:33:39,153 [INFO] ---------------------------------
2019-03-22 21:33:39,154 [INFO] Summary:
2019-03-22 21:33:39,155 [INFO] Batch 82000, worst loss 0.339585 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:33:39,156 [INFO] Regularization: 2465.541748 * 0.0000010000 = 0.0024655417
2019-03-22 21:33:39,157 [INFO] Sum of grad norms: 17.265043
2019-03-22 21:33:39,157 [INFO] ---------------------------------
2019-03-22 21:33:54,035 [INFO] ---------------------------------
2019-03-22 21:33:54,036 [INFO] Summary:
2019-03-22 21:33:54,037 [INFO] Batch 83000, worst loss 0.209922 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:33:54,037 [INFO] Regularization: 2465.295166 * 0.0000010000 = 0.0024652951
2019-03-22 21:33:54,038 [INFO] Sum of grad norms: 3.289971
2019-03-22 21:33:54,039 [INFO] ---------------------------------
2019-03-22 21:34:09,318 [INFO] ---------------------------------
2019-03-22 21:34:09,319 [INFO] Summary:
2019-03-22 21:34:09,320 [INFO] Batch 84000, worst loss 0.271527 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:34:09,321 [INFO] Regularization: 2465.035400 * 0.0000010000 = 0.0024650353
2019-03-22 21:34:09,321 [INFO] Sum of grad norms: 0.011995
2019-03-22 21:34:09,322 [INFO] ---------------------------------
2019-03-22 21:34:23,947 [INFO] ---------------------------------
2019-03-22 21:34:23,948 [INFO] Summary:
2019-03-22 21:34:23,949 [INFO] Batch 85000, worst loss 0.181437 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:34:23,949 [INFO] Regularization: 2464.778076 * 0.0000010000 = 0.0024647780
2019-03-22 21:34:23,950 [INFO] Sum of grad norms: 31.277014
2019-03-22 21:34:23,950 [INFO] ---------------------------------
2019-03-22 21:34:38,883 [INFO] ---------------------------------
2019-03-22 21:34:38,884 [INFO] Summary:
2019-03-22 21:34:38,885 [INFO] Batch 86000, worst loss 0.199165 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:34:38,886 [INFO] Regularization: 2464.551025 * 0.0000010000 = 0.0024645510
2019-03-22 21:34:38,886 [INFO] Sum of grad norms: 0.006729
2019-03-22 21:34:38,887 [INFO] ---------------------------------
2019-03-22 21:34:53,435 [INFO] ---------------------------------
2019-03-22 21:34:53,436 [INFO] Summary:
2019-03-22 21:34:53,436 [INFO] Batch 87000, worst loss 0.307375 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:34:53,437 [INFO] Regularization: 2464.348145 * 0.0000010000 = 0.0024643482
2019-03-22 21:34:53,437 [INFO] Sum of grad norms: 0.005041
2019-03-22 21:34:53,438 [INFO] ---------------------------------
2019-03-22 21:35:07,916 [INFO] ---------------------------------
2019-03-22 21:35:07,917 [INFO] Summary:
2019-03-22 21:35:07,918 [INFO] Batch 88000, worst loss 0.251686 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:35:07,918 [INFO] Regularization: 2464.134033 * 0.0000010000 = 0.0024641340
2019-03-22 21:35:07,919 [INFO] Sum of grad norms: 0.012362
2019-03-22 21:35:07,920 [INFO] ---------------------------------
2019-03-22 21:35:22,451 [INFO] ---------------------------------
2019-03-22 21:35:22,452 [INFO] Summary:
2019-03-22 21:35:22,452 [INFO] Batch 89000, worst loss 0.283572 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:35:22,453 [INFO] Regularization: 2463.910889 * 0.0000010000 = 0.0024639110
2019-03-22 21:35:22,454 [INFO] Sum of grad norms: 2.392475
2019-03-22 21:35:22,454 [INFO] ---------------------------------
2019-03-22 21:35:36,291 [INFO] ---------------------------------
2019-03-22 21:35:36,292 [INFO] Summary:
2019-03-22 21:35:36,292 [INFO] Batch 90000, worst loss 0.179138 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:35:36,293 [INFO] Regularization: 2463.724609 * 0.0000010000 = 0.0024637247
2019-03-22 21:35:36,293 [INFO] Sum of grad norms: 0.762711
2019-03-22 21:35:36,294 [INFO] ---------------------------------
2019-03-22 21:35:39,100 [INFO] ---------------------------------
2019-03-22 21:35:39,101 [INFO] Evaluation:
2019-03-22 21:35:39,102 [INFO] Batch 90000, worst loss 0.183511 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:35:39,103 [INFO] ---------------------------------
2019-03-22 21:35:53,617 [INFO] ---------------------------------
2019-03-22 21:35:53,618 [INFO] Summary:
2019-03-22 21:35:53,619 [INFO] Batch 91000, worst loss 0.208432 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:35:53,619 [INFO] Regularization: 2463.482910 * 0.0000010000 = 0.0024634830
2019-03-22 21:35:53,620 [INFO] Sum of grad norms: 0.003193
2019-03-22 21:35:53,620 [INFO] ---------------------------------
2019-03-22 21:36:08,568 [INFO] ---------------------------------
2019-03-22 21:36:08,569 [INFO] Summary:
2019-03-22 21:36:08,570 [INFO] Batch 92000, worst loss 0.226940 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:36:08,571 [INFO] Regularization: 2463.406006 * 0.0000010000 = 0.0024634060
2019-03-22 21:36:08,571 [INFO] Sum of grad norms: 11.376093
2019-03-22 21:36:08,572 [INFO] ---------------------------------
2019-03-22 21:36:23,636 [INFO] ---------------------------------
2019-03-22 21:36:23,637 [INFO] Summary:
2019-03-22 21:36:23,637 [INFO] Batch 93000, worst loss 0.229984 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:36:23,638 [INFO] Regularization: 2463.349365 * 0.0000010000 = 0.0024633494
2019-03-22 21:36:23,638 [INFO] Sum of grad norms: 0.020650
2019-03-22 21:36:23,639 [INFO] ---------------------------------
2019-03-22 21:36:38,227 [INFO] ---------------------------------
2019-03-22 21:36:38,228 [INFO] Summary:
2019-03-22 21:36:38,229 [INFO] Batch 94000, worst loss 0.217169 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:36:38,229 [INFO] Regularization: 2463.305908 * 0.0000010000 = 0.0024633058
2019-03-22 21:36:38,230 [INFO] Sum of grad norms: 0.002430
2019-03-22 21:36:38,230 [INFO] ---------------------------------
2019-03-22 21:36:53,525 [INFO] ---------------------------------
2019-03-22 21:36:53,526 [INFO] Summary:
2019-03-22 21:36:53,527 [INFO] Batch 95000, worst loss 0.259425 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:36:53,527 [INFO] Regularization: 2463.248291 * 0.0000010000 = 0.0024632483
2019-03-22 21:36:53,528 [INFO] Sum of grad norms: 0.004036
2019-03-22 21:36:53,528 [INFO] ---------------------------------
2019-03-22 21:37:08,085 [INFO] ---------------------------------
2019-03-22 21:37:08,086 [INFO] Summary:
2019-03-22 21:37:08,087 [INFO] Batch 96000, worst loss 0.206340 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:37:08,087 [INFO] Regularization: 2463.193848 * 0.0000010000 = 0.0024631938
2019-03-22 21:37:08,088 [INFO] Sum of grad norms: 22.202572
2019-03-22 21:37:08,089 [INFO] ---------------------------------
2019-03-22 21:37:22,331 [INFO] ---------------------------------
2019-03-22 21:37:22,332 [INFO] Summary:
2019-03-22 21:37:22,333 [INFO] Batch 97000, worst loss 0.236258 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:37:22,333 [INFO] Regularization: 2463.140137 * 0.0000010000 = 0.0024631401
2019-03-22 21:37:22,334 [INFO] Sum of grad norms: 0.820079
2019-03-22 21:37:22,334 [INFO] ---------------------------------
2019-03-22 21:37:36,085 [INFO] ---------------------------------
2019-03-22 21:37:36,086 [INFO] Summary:
2019-03-22 21:37:36,086 [INFO] Batch 98000, worst loss 0.261741 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:37:36,087 [INFO] Regularization: 2463.074463 * 0.0000010000 = 0.0024630744
2019-03-22 21:37:36,087 [INFO] Sum of grad norms: 0.014857
2019-03-22 21:37:36,088 [INFO] ---------------------------------
2019-03-22 21:37:50,365 [INFO] ---------------------------------
2019-03-22 21:37:50,366 [INFO] Summary:
2019-03-22 21:37:50,367 [INFO] Batch 99000, worst loss 0.199529 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:37:50,367 [INFO] Regularization: 2463.025879 * 0.0000010000 = 0.0024630260
2019-03-22 21:37:50,368 [INFO] Sum of grad norms: 25.814356
2019-03-22 21:37:50,369 [INFO] ---------------------------------
2019-03-22 21:38:04,539 [INFO] ---------------------------------
2019-03-22 21:38:04,540 [INFO] Summary:
2019-03-22 21:38:04,541 [INFO] Batch 100000, worst loss 0.199532 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 21:38:04,541 [INFO] Regularization: 2462.975830 * 0.0000010000 = 0.0024629759
2019-03-22 21:38:04,542 [INFO] Sum of grad norms: 23.177258
2019-03-22 21:38:04,543 [INFO] ---------------------------------
2019-03-22 21:38:07,326 [INFO] ---------------------------------
2019-03-22 21:38:07,327 [INFO] Evaluation:
2019-03-22 21:38:07,328 [INFO] Batch 100000, worst loss 0.210456 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:38:07,328 [INFO] ---------------------------------
2019-03-22 21:38:07,329 [INFO] Finished training, saved to file classifier/1553275201/1553287087_7_classifier_final.pth
2019-03-22 21:38:07,491 [INFO] ---------------------------------
2019-03-22 21:38:07,493 [INFO] Training model #8: (8, 64, 2) @ 2
2019-03-22 21:38:22,473 [INFO] ---------------------------------
2019-03-22 21:38:22,474 [INFO] Summary:
2019-03-22 21:38:22,474 [INFO] Batch 1000, worst loss 836.515198 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:38:22,475 [INFO] Regularization: 30674.701172 * 0.0000010000 = 0.0306747016
2019-03-22 21:38:22,475 [INFO] Sum of grad norms: 619.786926
2019-03-22 21:38:22,476 [INFO] ---------------------------------
2019-03-22 21:38:37,594 [INFO] ---------------------------------
2019-03-22 21:38:37,595 [INFO] Summary:
2019-03-22 21:38:37,595 [INFO] Batch 2000, worst loss 23.129187 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:38:37,596 [INFO] Regularization: 28597.265625 * 0.0000010000 = 0.0285972655
2019-03-22 21:38:37,596 [INFO] Sum of grad norms: 546.780823
2019-03-22 21:38:37,597 [INFO] ---------------------------------
2019-03-22 21:38:51,921 [INFO] ---------------------------------
2019-03-22 21:38:51,922 [INFO] Summary:
2019-03-22 21:38:51,923 [INFO] Batch 3000, worst loss 15.245243 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:38:51,923 [INFO] Regularization: 25218.095703 * 0.0000010000 = 0.0252180956
2019-03-22 21:38:51,924 [INFO] Sum of grad norms: 941.978882
2019-03-22 21:38:51,924 [INFO] ---------------------------------
2019-03-22 21:39:06,876 [INFO] ---------------------------------
2019-03-22 21:39:06,877 [INFO] Summary:
2019-03-22 21:39:06,877 [INFO] Batch 4000, worst loss 6.290457 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:39:06,878 [INFO] Regularization: 20050.460938 * 0.0000010000 = 0.0200504605
2019-03-22 21:39:06,879 [INFO] Sum of grad norms: 169.023056
2019-03-22 21:39:06,879 [INFO] ---------------------------------
2019-03-22 21:39:21,375 [INFO] ---------------------------------
2019-03-22 21:39:21,376 [INFO] Summary:
2019-03-22 21:39:21,376 [INFO] Batch 5000, worst loss 2.445731 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:39:21,377 [INFO] Regularization: 14319.611328 * 0.0000010000 = 0.0143196117
2019-03-22 21:39:21,377 [INFO] Sum of grad norms: 27.316254
2019-03-22 21:39:21,378 [INFO] ---------------------------------
2019-03-22 21:39:36,411 [INFO] ---------------------------------
2019-03-22 21:39:36,412 [INFO] Summary:
2019-03-22 21:39:36,412 [INFO] Batch 6000, worst loss 1.810157 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:39:36,413 [INFO] Regularization: 10223.525391 * 0.0000010000 = 0.0102235256
2019-03-22 21:39:36,414 [INFO] Sum of grad norms: 39.782631
2019-03-22 21:39:36,414 [INFO] ---------------------------------
2019-03-22 21:39:51,262 [INFO] ---------------------------------
2019-03-22 21:39:51,263 [INFO] Summary:
2019-03-22 21:39:51,263 [INFO] Batch 7000, worst loss 1.393681 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:39:51,264 [INFO] Regularization: 9242.317383 * 0.0000010000 = 0.0092423176
2019-03-22 21:39:51,265 [INFO] Sum of grad norms: 19.644777
2019-03-22 21:39:51,265 [INFO] ---------------------------------
2019-03-22 21:40:06,151 [INFO] ---------------------------------
2019-03-22 21:40:06,152 [INFO] Summary:
2019-03-22 21:40:06,153 [INFO] Batch 8000, worst loss 0.747622 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:40:06,153 [INFO] Regularization: 9055.718750 * 0.0000010000 = 0.0090557188
2019-03-22 21:40:06,154 [INFO] Sum of grad norms: 51.133617
2019-03-22 21:40:06,155 [INFO] ---------------------------------
2019-03-22 21:40:20,888 [INFO] ---------------------------------
2019-03-22 21:40:20,889 [INFO] Summary:
2019-03-22 21:40:20,890 [INFO] Batch 9000, worst loss 0.691734 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:40:20,890 [INFO] Regularization: 8845.484375 * 0.0000010000 = 0.0088454848
2019-03-22 21:40:20,891 [INFO] Sum of grad norms: 53.224895
2019-03-22 21:40:20,892 [INFO] ---------------------------------
2019-03-22 21:40:36,561 [INFO] ---------------------------------
2019-03-22 21:40:36,563 [INFO] Summary:
2019-03-22 21:40:36,563 [INFO] Batch 10000, worst loss 0.764487 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:40:36,564 [INFO] Regularization: 8530.883789 * 0.0000010000 = 0.0085308840
2019-03-22 21:40:36,565 [INFO] Sum of grad norms: 101.432388
2019-03-22 21:40:36,566 [INFO] ---------------------------------
2019-03-22 21:40:39,323 [INFO] ---------------------------------
2019-03-22 21:40:39,323 [INFO] Evaluation:
2019-03-22 21:40:39,324 [INFO] Batch 10000, worst loss 0.854191 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:40:39,325 [INFO] ---------------------------------
2019-03-22 21:40:53,961 [INFO] ---------------------------------
2019-03-22 21:40:53,962 [INFO] Summary:
2019-03-22 21:40:53,963 [INFO] Batch 11000, worst loss 0.679961 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:40:53,963 [INFO] Regularization: 8260.095703 * 0.0000010000 = 0.0082600955
2019-03-22 21:40:53,964 [INFO] Sum of grad norms: 21.582333
2019-03-22 21:40:53,964 [INFO] ---------------------------------
2019-03-22 21:41:08,438 [INFO] ---------------------------------
2019-03-22 21:41:08,439 [INFO] Summary:
2019-03-22 21:41:08,440 [INFO] Batch 12000, worst loss 0.491465 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:41:08,440 [INFO] Regularization: 7937.865234 * 0.0000010000 = 0.0079378653
2019-03-22 21:41:08,441 [INFO] Sum of grad norms: 16.749344
2019-03-22 21:41:08,441 [INFO] ---------------------------------
2019-03-22 21:41:23,229 [INFO] ---------------------------------
2019-03-22 21:41:23,230 [INFO] Summary:
2019-03-22 21:41:23,231 [INFO] Batch 13000, worst loss 0.519228 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:41:23,231 [INFO] Regularization: 7645.259277 * 0.0000010000 = 0.0076452591
2019-03-22 21:41:23,232 [INFO] Sum of grad norms: 4.506766
2019-03-22 21:41:23,233 [INFO] ---------------------------------
2019-03-22 21:41:38,191 [INFO] ---------------------------------
2019-03-22 21:41:38,192 [INFO] Summary:
2019-03-22 21:41:38,193 [INFO] Batch 14000, worst loss 0.333383 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:41:38,194 [INFO] Regularization: 7199.615234 * 0.0000010000 = 0.0071996152
2019-03-22 21:41:38,194 [INFO] Sum of grad norms: 3.826311
2019-03-22 21:41:38,195 [INFO] ---------------------------------
2019-03-22 21:41:52,471 [INFO] ---------------------------------
2019-03-22 21:41:52,471 [INFO] Summary:
2019-03-22 21:41:52,472 [INFO] Batch 15000, worst loss 0.448133 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:41:52,473 [INFO] Regularization: 6795.899414 * 0.0000010000 = 0.0067958995
2019-03-22 21:41:52,473 [INFO] Sum of grad norms: 12.337368
2019-03-22 21:41:52,474 [INFO] ---------------------------------
2019-03-22 21:42:06,743 [INFO] ---------------------------------
2019-03-22 21:42:06,744 [INFO] Summary:
2019-03-22 21:42:06,745 [INFO] Batch 16000, worst loss 0.331738 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:42:06,746 [INFO] Regularization: 6432.305176 * 0.0000010000 = 0.0064323051
2019-03-22 21:42:06,746 [INFO] Sum of grad norms: 4.218579
2019-03-22 21:42:06,747 [INFO] ---------------------------------
2019-03-22 21:42:21,199 [INFO] ---------------------------------
2019-03-22 21:42:21,199 [INFO] Summary:
2019-03-22 21:42:21,200 [INFO] Batch 17000, worst loss 0.364957 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:42:21,201 [INFO] Regularization: 6075.594727 * 0.0000010000 = 0.0060755946
2019-03-22 21:42:21,201 [INFO] Sum of grad norms: 15.478795
2019-03-22 21:42:21,202 [INFO] ---------------------------------
2019-03-22 21:42:35,700 [INFO] ---------------------------------
2019-03-22 21:42:35,701 [INFO] Summary:
2019-03-22 21:42:35,702 [INFO] Batch 18000, worst loss 0.308467 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:42:35,702 [INFO] Regularization: 5709.006836 * 0.0000010000 = 0.0057090069
2019-03-22 21:42:35,703 [INFO] Sum of grad norms: 8.999639
2019-03-22 21:42:35,704 [INFO] ---------------------------------
2019-03-22 21:42:50,604 [INFO] ---------------------------------
2019-03-22 21:42:50,605 [INFO] Summary:
2019-03-22 21:42:50,606 [INFO] Batch 19000, worst loss 0.351427 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:42:50,606 [INFO] Regularization: 5383.375000 * 0.0000010000 = 0.0053833751
2019-03-22 21:42:50,607 [INFO] Sum of grad norms: 0.950126
2019-03-22 21:42:50,607 [INFO] ---------------------------------
2019-03-22 21:43:04,645 [INFO] ---------------------------------
2019-03-22 21:43:04,646 [INFO] Summary:
2019-03-22 21:43:04,647 [INFO] Batch 20000, worst loss 0.309164 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:43:04,647 [INFO] Regularization: 5042.842773 * 0.0000010000 = 0.0050428426
2019-03-22 21:43:04,648 [INFO] Sum of grad norms: 12.625131
2019-03-22 21:43:04,649 [INFO] ---------------------------------
2019-03-22 21:43:07,356 [INFO] ---------------------------------
2019-03-22 21:43:07,357 [INFO] Evaluation:
2019-03-22 21:43:07,358 [INFO] Batch 20000, worst loss 0.258653 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:43:07,359 [INFO] ---------------------------------
2019-03-22 21:43:22,398 [INFO] ---------------------------------
2019-03-22 21:43:22,399 [INFO] Summary:
2019-03-22 21:43:22,400 [INFO] Batch 21000, worst loss 0.359927 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:43:22,400 [INFO] Regularization: 4809.752930 * 0.0000010000 = 0.0048097530
2019-03-22 21:43:22,401 [INFO] Sum of grad norms: 2.068586
2019-03-22 21:43:22,401 [INFO] ---------------------------------
2019-03-22 21:43:37,826 [INFO] ---------------------------------
2019-03-22 21:43:37,827 [INFO] Summary:
2019-03-22 21:43:37,828 [INFO] Batch 22000, worst loss 0.354492 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:43:37,828 [INFO] Regularization: 4566.422363 * 0.0000010000 = 0.0045664222
2019-03-22 21:43:37,829 [INFO] Sum of grad norms: 29.522335
2019-03-22 21:43:37,829 [INFO] ---------------------------------
2019-03-22 21:43:53,276 [INFO] ---------------------------------
2019-03-22 21:43:53,277 [INFO] Summary:
2019-03-22 21:43:53,278 [INFO] Batch 23000, worst loss 0.373378 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:43:53,278 [INFO] Regularization: 4303.541016 * 0.0000010000 = 0.0043035410
2019-03-22 21:43:53,279 [INFO] Sum of grad norms: 2.889390
2019-03-22 21:43:53,280 [INFO] ---------------------------------
2019-03-22 21:44:07,999 [INFO] ---------------------------------
2019-03-22 21:44:08,000 [INFO] Summary:
2019-03-22 21:44:08,001 [INFO] Batch 24000, worst loss 0.360563 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:44:08,001 [INFO] Regularization: 4117.866211 * 0.0000010000 = 0.0041178660
2019-03-22 21:44:08,002 [INFO] Sum of grad norms: 5.169440
2019-03-22 21:44:08,002 [INFO] ---------------------------------
2019-03-22 21:44:22,829 [INFO] ---------------------------------
2019-03-22 21:44:22,830 [INFO] Summary:
2019-03-22 21:44:22,831 [INFO] Batch 25000, worst loss 0.444278 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:44:22,831 [INFO] Regularization: 3909.486084 * 0.0000010000 = 0.0039094859
2019-03-22 21:44:22,832 [INFO] Sum of grad norms: 1.736109
2019-03-22 21:44:22,833 [INFO] ---------------------------------
2019-03-22 21:44:38,170 [INFO] ---------------------------------
2019-03-22 21:44:38,171 [INFO] Summary:
2019-03-22 21:44:38,171 [INFO] Batch 26000, worst loss 0.536816 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:44:38,172 [INFO] Regularization: 3781.422119 * 0.0000010000 = 0.0037814220
2019-03-22 21:44:38,172 [INFO] Sum of grad norms: 4.189020
2019-03-22 21:44:38,173 [INFO] ---------------------------------
2019-03-22 21:44:52,859 [INFO] ---------------------------------
2019-03-22 21:44:52,860 [INFO] Summary:
2019-03-22 21:44:52,860 [INFO] Batch 27000, worst loss 0.341779 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:44:52,861 [INFO] Regularization: 3647.419678 * 0.0000010000 = 0.0036474196
2019-03-22 21:44:52,861 [INFO] Sum of grad norms: 1.792482
2019-03-22 21:44:52,862 [INFO] ---------------------------------
2019-03-22 21:45:07,173 [INFO] ---------------------------------
2019-03-22 21:45:07,174 [INFO] Summary:
2019-03-22 21:45:07,175 [INFO] Batch 28000, worst loss 0.331118 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:45:07,175 [INFO] Regularization: 3559.521973 * 0.0000010000 = 0.0035595219
2019-03-22 21:45:07,176 [INFO] Sum of grad norms: 6.073578
2019-03-22 21:45:07,176 [INFO] ---------------------------------
2019-03-22 21:45:21,922 [INFO] ---------------------------------
2019-03-22 21:45:21,923 [INFO] Summary:
2019-03-22 21:45:21,924 [INFO] Batch 29000, worst loss 0.412476 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:45:21,925 [INFO] Regularization: 3477.496826 * 0.0000010000 = 0.0034774968
2019-03-22 21:45:21,925 [INFO] Sum of grad norms: 1.865054
2019-03-22 21:45:21,926 [INFO] ---------------------------------
2019-03-22 21:45:37,139 [INFO] ---------------------------------
2019-03-22 21:45:37,140 [INFO] Summary:
2019-03-22 21:45:37,140 [INFO] Batch 30000, worst loss 0.325180 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:45:37,141 [INFO] Regularization: 3305.436523 * 0.0000010000 = 0.0033054366
2019-03-22 21:45:37,142 [INFO] Sum of grad norms: 11.129428
2019-03-22 21:45:37,143 [INFO] ---------------------------------
2019-03-22 21:45:39,903 [INFO] ---------------------------------
2019-03-22 21:45:39,904 [INFO] Evaluation:
2019-03-22 21:45:39,905 [INFO] Batch 30000, worst loss 0.373500 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:45:39,906 [INFO] ---------------------------------
2019-03-22 21:45:54,243 [INFO] ---------------------------------
2019-03-22 21:45:54,243 [INFO] Summary:
2019-03-22 21:45:54,244 [INFO] Batch 31000, worst loss 0.359374 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:45:54,244 [INFO] Regularization: 3195.490967 * 0.0000010000 = 0.0031954909
2019-03-22 21:45:54,245 [INFO] Sum of grad norms: 0.243591
2019-03-22 21:45:54,245 [INFO] ---------------------------------
2019-03-22 21:46:09,278 [INFO] ---------------------------------
2019-03-22 21:46:09,279 [INFO] Summary:
2019-03-22 21:46:09,280 [INFO] Batch 32000, worst loss 0.363479 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:46:09,280 [INFO] Regularization: 2989.977051 * 0.0000010000 = 0.0029899771
2019-03-22 21:46:09,281 [INFO] Sum of grad norms: 19.339104
2019-03-22 21:46:09,281 [INFO] ---------------------------------
2019-03-22 21:46:24,321 [INFO] ---------------------------------
2019-03-22 21:46:24,321 [INFO] Summary:
2019-03-22 21:46:24,322 [INFO] Batch 33000, worst loss 0.373839 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:46:24,322 [INFO] Regularization: 2862.034180 * 0.0000010000 = 0.0028620341
2019-03-22 21:46:24,323 [INFO] Sum of grad norms: 0.062741
2019-03-22 21:46:24,324 [INFO] ---------------------------------
2019-03-22 21:46:38,999 [INFO] ---------------------------------
2019-03-22 21:46:39,000 [INFO] Summary:
2019-03-22 21:46:39,001 [INFO] Batch 34000, worst loss 0.354088 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:46:39,001 [INFO] Regularization: 2795.489014 * 0.0000010000 = 0.0027954890
2019-03-22 21:46:39,002 [INFO] Sum of grad norms: 9.803964
2019-03-22 21:46:39,002 [INFO] ---------------------------------
2019-03-22 21:46:52,863 [INFO] ---------------------------------
2019-03-22 21:46:52,864 [INFO] Summary:
2019-03-22 21:46:52,865 [INFO] Batch 35000, worst loss 0.311414 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:46:52,865 [INFO] Regularization: 2647.410889 * 0.0000010000 = 0.0026474108
2019-03-22 21:46:52,866 [INFO] Sum of grad norms: 0.432738
2019-03-22 21:46:52,866 [INFO] ---------------------------------
2019-03-22 21:47:06,876 [INFO] ---------------------------------
2019-03-22 21:47:06,877 [INFO] Summary:
2019-03-22 21:47:06,878 [INFO] Batch 36000, worst loss 0.273757 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:47:06,878 [INFO] Regularization: 2530.414795 * 0.0000010000 = 0.0025304148
2019-03-22 21:47:06,879 [INFO] Sum of grad norms: 0.094763
2019-03-22 21:47:06,879 [INFO] ---------------------------------
2019-03-22 21:47:20,579 [INFO] ---------------------------------
2019-03-22 21:47:20,580 [INFO] Summary:
2019-03-22 21:47:20,581 [INFO] Batch 37000, worst loss 0.346412 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:47:20,581 [INFO] Regularization: 2437.876709 * 0.0000010000 = 0.0024378768
2019-03-22 21:47:20,582 [INFO] Sum of grad norms: 1.196130
2019-03-22 21:47:20,582 [INFO] ---------------------------------
2019-03-22 21:47:36,057 [INFO] ---------------------------------
2019-03-22 21:47:36,058 [INFO] Summary:
2019-03-22 21:47:36,059 [INFO] Batch 38000, worst loss 0.346620 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:47:36,060 [INFO] Regularization: 2348.194092 * 0.0000010000 = 0.0023481941
2019-03-22 21:47:36,060 [INFO] Sum of grad norms: 0.110665
2019-03-22 21:47:36,061 [INFO] ---------------------------------
2019-03-22 21:47:50,271 [INFO] ---------------------------------
2019-03-22 21:47:50,272 [INFO] Summary:
2019-03-22 21:47:50,273 [INFO] Batch 39000, worst loss 0.365020 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:47:50,274 [INFO] Regularization: 2235.459473 * 0.0000010000 = 0.0022354594
2019-03-22 21:47:50,275 [INFO] Sum of grad norms: 0.018688
2019-03-22 21:47:50,276 [INFO] ---------------------------------
2019-03-22 21:48:04,953 [INFO] ---------------------------------
2019-03-22 21:48:04,954 [INFO] Summary:
2019-03-22 21:48:04,955 [INFO] Batch 40000, worst loss 0.392812 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 21:48:04,955 [INFO] Regularization: 2148.792969 * 0.0000010000 = 0.0021487931
2019-03-22 21:48:04,956 [INFO] Sum of grad norms: 19.585392
2019-03-22 21:48:04,956 [INFO] ---------------------------------
2019-03-22 21:48:07,728 [INFO] ---------------------------------
2019-03-22 21:48:07,729 [INFO] Evaluation:
2019-03-22 21:48:07,730 [INFO] Batch 40000, worst loss 0.226375 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:48:07,730 [INFO] ---------------------------------
2019-03-22 21:48:21,950 [INFO] ---------------------------------
2019-03-22 21:48:21,951 [INFO] Summary:
2019-03-22 21:48:21,952 [INFO] Batch 41000, worst loss 0.294424 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:48:21,952 [INFO] Regularization: 2070.272217 * 0.0000010000 = 0.0020702721
2019-03-22 21:48:21,953 [INFO] Sum of grad norms: 1.285673
2019-03-22 21:48:21,953 [INFO] ---------------------------------
2019-03-22 21:48:36,854 [INFO] ---------------------------------
2019-03-22 21:48:36,855 [INFO] Summary:
2019-03-22 21:48:36,856 [INFO] Batch 42000, worst loss 0.244962 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:48:36,856 [INFO] Regularization: 1992.635620 * 0.0000010000 = 0.0019926357
2019-03-22 21:48:36,857 [INFO] Sum of grad norms: 16.052071
2019-03-22 21:48:36,857 [INFO] ---------------------------------
2019-03-22 21:48:51,389 [INFO] ---------------------------------
2019-03-22 21:48:51,390 [INFO] Summary:
2019-03-22 21:48:51,391 [INFO] Batch 43000, worst loss 0.406084 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:48:51,391 [INFO] Regularization: 1929.119507 * 0.0000010000 = 0.0019291195
2019-03-22 21:48:51,392 [INFO] Sum of grad norms: 0.042327
2019-03-22 21:48:51,392 [INFO] ---------------------------------
2019-03-22 21:49:05,847 [INFO] ---------------------------------
2019-03-22 21:49:05,848 [INFO] Summary:
2019-03-22 21:49:05,849 [INFO] Batch 44000, worst loss 0.297638 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:49:05,849 [INFO] Regularization: 1871.200317 * 0.0000010000 = 0.0018712003
2019-03-22 21:49:05,850 [INFO] Sum of grad norms: 15.678364
2019-03-22 21:49:05,850 [INFO] ---------------------------------
2019-03-22 21:49:20,590 [INFO] ---------------------------------
2019-03-22 21:49:20,591 [INFO] Summary:
2019-03-22 21:49:20,591 [INFO] Batch 45000, worst loss 0.237548 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:49:20,592 [INFO] Regularization: 1825.739136 * 0.0000010000 = 0.0018257392
2019-03-22 21:49:20,592 [INFO] Sum of grad norms: 4.205582
2019-03-22 21:49:20,593 [INFO] ---------------------------------
2019-03-22 21:49:35,449 [INFO] ---------------------------------
2019-03-22 21:49:35,450 [INFO] Summary:
2019-03-22 21:49:35,451 [INFO] Batch 46000, worst loss 0.272019 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:49:35,452 [INFO] Regularization: 1777.931152 * 0.0000010000 = 0.0017779311
2019-03-22 21:49:35,453 [INFO] Sum of grad norms: 0.031695
2019-03-22 21:49:35,454 [INFO] ---------------------------------
2019-03-22 21:49:50,553 [INFO] ---------------------------------
2019-03-22 21:49:50,554 [INFO] Summary:
2019-03-22 21:49:50,554 [INFO] Batch 47000, worst loss 0.282246 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:49:50,555 [INFO] Regularization: 1734.712646 * 0.0000010000 = 0.0017347126
2019-03-22 21:49:50,555 [INFO] Sum of grad norms: 0.013067
2019-03-22 21:49:50,556 [INFO] ---------------------------------
2019-03-22 21:50:04,557 [INFO] ---------------------------------
2019-03-22 21:50:04,558 [INFO] Summary:
2019-03-22 21:50:04,558 [INFO] Batch 48000, worst loss 0.300804 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:50:04,559 [INFO] Regularization: 1695.184570 * 0.0000010000 = 0.0016951846
2019-03-22 21:50:04,560 [INFO] Sum of grad norms: 18.454290
2019-03-22 21:50:04,560 [INFO] ---------------------------------
2019-03-22 21:50:18,543 [INFO] ---------------------------------
2019-03-22 21:50:18,544 [INFO] Summary:
2019-03-22 21:50:18,544 [INFO] Batch 49000, worst loss 0.253819 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:50:18,545 [INFO] Regularization: 1664.548218 * 0.0000010000 = 0.0016645482
2019-03-22 21:50:18,545 [INFO] Sum of grad norms: 16.383890
2019-03-22 21:50:18,546 [INFO] ---------------------------------
2019-03-22 21:50:32,542 [INFO] ---------------------------------
2019-03-22 21:50:32,543 [INFO] Summary:
2019-03-22 21:50:32,544 [INFO] Batch 50000, worst loss 0.390960 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 21:50:32,544 [INFO] Regularization: 1643.803345 * 0.0000010000 = 0.0016438033
2019-03-22 21:50:32,545 [INFO] Sum of grad norms: 0.009148
2019-03-22 21:50:32,546 [INFO] ---------------------------------
2019-03-22 21:50:35,247 [INFO] ---------------------------------
2019-03-22 21:50:35,248 [INFO] Evaluation:
2019-03-22 21:50:35,249 [INFO] Batch 50000, worst loss 0.285914 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:50:35,249 [INFO] ---------------------------------
2019-03-22 21:50:49,190 [INFO] ---------------------------------
2019-03-22 21:50:49,191 [INFO] Summary:
2019-03-22 21:50:49,192 [INFO] Batch 51000, worst loss 0.334510 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:50:49,193 [INFO] Regularization: 1630.991699 * 0.0000010000 = 0.0016309917
2019-03-22 21:50:49,193 [INFO] Sum of grad norms: 0.187762
2019-03-22 21:50:49,194 [INFO] ---------------------------------
2019-03-22 21:51:03,239 [INFO] ---------------------------------
2019-03-22 21:51:03,240 [INFO] Summary:
2019-03-22 21:51:03,240 [INFO] Batch 52000, worst loss 0.221006 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:51:03,241 [INFO] Regularization: 1609.783081 * 0.0000010000 = 0.0016097830
2019-03-22 21:51:03,241 [INFO] Sum of grad norms: 8.587032
2019-03-22 21:51:03,242 [INFO] ---------------------------------
2019-03-22 21:51:17,402 [INFO] ---------------------------------
2019-03-22 21:51:17,403 [INFO] Summary:
2019-03-22 21:51:17,403 [INFO] Batch 53000, worst loss 0.210785 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:51:17,404 [INFO] Regularization: 1598.321045 * 0.0000010000 = 0.0015983210
2019-03-22 21:51:17,404 [INFO] Sum of grad norms: 16.293743
2019-03-22 21:51:17,405 [INFO] ---------------------------------
2019-03-22 21:51:32,145 [INFO] ---------------------------------
2019-03-22 21:51:32,146 [INFO] Summary:
2019-03-22 21:51:32,147 [INFO] Batch 54000, worst loss 0.210291 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:51:32,148 [INFO] Regularization: 1587.012695 * 0.0000010000 = 0.0015870127
2019-03-22 21:51:32,149 [INFO] Sum of grad norms: 0.168334
2019-03-22 21:51:32,150 [INFO] ---------------------------------
2019-03-22 21:51:47,370 [INFO] ---------------------------------
2019-03-22 21:51:47,371 [INFO] Summary:
2019-03-22 21:51:47,371 [INFO] Batch 55000, worst loss 0.260037 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:51:47,372 [INFO] Regularization: 1579.566650 * 0.0000010000 = 0.0015795666
2019-03-22 21:51:47,372 [INFO] Sum of grad norms: 0.675930
2019-03-22 21:51:47,373 [INFO] ---------------------------------
2019-03-22 21:52:01,200 [INFO] ---------------------------------
2019-03-22 21:52:01,201 [INFO] Summary:
2019-03-22 21:52:01,202 [INFO] Batch 56000, worst loss 0.226263 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:52:01,202 [INFO] Regularization: 1569.700684 * 0.0000010000 = 0.0015697007
2019-03-22 21:52:01,203 [INFO] Sum of grad norms: 0.003429
2019-03-22 21:52:01,203 [INFO] ---------------------------------
2019-03-22 21:52:16,573 [INFO] ---------------------------------
2019-03-22 21:52:16,573 [INFO] Summary:
2019-03-22 21:52:16,574 [INFO] Batch 57000, worst loss 0.358213 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:52:16,575 [INFO] Regularization: 1560.185669 * 0.0000010000 = 0.0015601857
2019-03-22 21:52:16,575 [INFO] Sum of grad norms: 7.093577
2019-03-22 21:52:16,576 [INFO] ---------------------------------
2019-03-22 21:52:31,960 [INFO] ---------------------------------
2019-03-22 21:52:31,962 [INFO] Summary:
2019-03-22 21:52:31,963 [INFO] Batch 58000, worst loss 0.224826 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:52:31,964 [INFO] Regularization: 1551.483398 * 0.0000010000 = 0.0015514834
2019-03-22 21:52:31,965 [INFO] Sum of grad norms: 0.010090
2019-03-22 21:52:31,966 [INFO] ---------------------------------
2019-03-22 21:52:47,114 [INFO] ---------------------------------
2019-03-22 21:52:47,115 [INFO] Summary:
2019-03-22 21:52:47,116 [INFO] Batch 59000, worst loss 0.238110 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:52:47,116 [INFO] Regularization: 1544.544556 * 0.0000010000 = 0.0015445446
2019-03-22 21:52:47,117 [INFO] Sum of grad norms: 0.008240
2019-03-22 21:52:47,118 [INFO] ---------------------------------
2019-03-22 21:53:02,205 [INFO] ---------------------------------
2019-03-22 21:53:02,206 [INFO] Summary:
2019-03-22 21:53:02,206 [INFO] Batch 60000, worst loss 0.228079 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 21:53:02,207 [INFO] Regularization: 1538.426025 * 0.0000010000 = 0.0015384260
2019-03-22 21:53:02,207 [INFO] Sum of grad norms: 8.782581
2019-03-22 21:53:02,208 [INFO] ---------------------------------
2019-03-22 21:53:04,969 [INFO] ---------------------------------
2019-03-22 21:53:04,970 [INFO] Evaluation:
2019-03-22 21:53:04,970 [INFO] Batch 60000, worst loss 0.207879 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:53:04,971 [INFO] ---------------------------------
2019-03-22 21:53:19,973 [INFO] ---------------------------------
2019-03-22 21:53:19,974 [INFO] Summary:
2019-03-22 21:53:19,974 [INFO] Batch 61000, worst loss 0.260689 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:53:19,975 [INFO] Regularization: 1530.052246 * 0.0000010000 = 0.0015300523
2019-03-22 21:53:19,975 [INFO] Sum of grad norms: 0.069549
2019-03-22 21:53:19,976 [INFO] ---------------------------------
2019-03-22 21:53:34,457 [INFO] ---------------------------------
2019-03-22 21:53:34,458 [INFO] Summary:
2019-03-22 21:53:34,459 [INFO] Batch 62000, worst loss 0.210278 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:53:34,459 [INFO] Regularization: 1526.902954 * 0.0000010000 = 0.0015269029
2019-03-22 21:53:34,460 [INFO] Sum of grad norms: 26.952843
2019-03-22 21:53:34,461 [INFO] ---------------------------------
2019-03-22 21:53:48,543 [INFO] ---------------------------------
2019-03-22 21:53:48,544 [INFO] Summary:
2019-03-22 21:53:48,544 [INFO] Batch 63000, worst loss 0.222518 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:53:48,545 [INFO] Regularization: 1524.593384 * 0.0000010000 = 0.0015245933
2019-03-22 21:53:48,545 [INFO] Sum of grad norms: 0.480081
2019-03-22 21:53:48,546 [INFO] ---------------------------------
2019-03-22 21:54:02,825 [INFO] ---------------------------------
2019-03-22 21:54:02,826 [INFO] Summary:
2019-03-22 21:54:02,826 [INFO] Batch 64000, worst loss 0.217262 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:54:02,827 [INFO] Regularization: 1522.680664 * 0.0000010000 = 0.0015226806
2019-03-22 21:54:02,827 [INFO] Sum of grad norms: 0.007304
2019-03-22 21:54:02,828 [INFO] ---------------------------------
2019-03-22 21:54:17,606 [INFO] ---------------------------------
2019-03-22 21:54:17,606 [INFO] Summary:
2019-03-22 21:54:17,607 [INFO] Batch 65000, worst loss 0.219155 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:54:17,608 [INFO] Regularization: 1520.818604 * 0.0000010000 = 0.0015208186
2019-03-22 21:54:17,608 [INFO] Sum of grad norms: 0.863169
2019-03-22 21:54:17,609 [INFO] ---------------------------------
2019-03-22 21:54:31,917 [INFO] ---------------------------------
2019-03-22 21:54:31,918 [INFO] Summary:
2019-03-22 21:54:31,919 [INFO] Batch 66000, worst loss 0.287952 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:54:31,919 [INFO] Regularization: 1519.006714 * 0.0000010000 = 0.0015190067
2019-03-22 21:54:31,920 [INFO] Sum of grad norms: 0.053426
2019-03-22 21:54:31,920 [INFO] ---------------------------------
2019-03-22 21:54:46,265 [INFO] ---------------------------------
2019-03-22 21:54:46,266 [INFO] Summary:
2019-03-22 21:54:46,267 [INFO] Batch 67000, worst loss 0.250181 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:54:46,267 [INFO] Regularization: 1516.936768 * 0.0000010000 = 0.0015169368
2019-03-22 21:54:46,268 [INFO] Sum of grad norms: 6.144167
2019-03-22 21:54:46,268 [INFO] ---------------------------------
2019-03-22 21:55:01,023 [INFO] ---------------------------------
2019-03-22 21:55:01,024 [INFO] Summary:
2019-03-22 21:55:01,024 [INFO] Batch 68000, worst loss 0.223456 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:55:01,025 [INFO] Regularization: 1515.070312 * 0.0000010000 = 0.0015150703
2019-03-22 21:55:01,025 [INFO] Sum of grad norms: 0.044148
2019-03-22 21:55:01,026 [INFO] ---------------------------------
2019-03-22 21:55:15,695 [INFO] ---------------------------------
2019-03-22 21:55:15,696 [INFO] Summary:
2019-03-22 21:55:15,697 [INFO] Batch 69000, worst loss 0.301882 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:55:15,697 [INFO] Regularization: 1513.461426 * 0.0000010000 = 0.0015134615
2019-03-22 21:55:15,698 [INFO] Sum of grad norms: 0.014242
2019-03-22 21:55:15,699 [INFO] ---------------------------------
2019-03-22 21:55:30,984 [INFO] ---------------------------------
2019-03-22 21:55:30,985 [INFO] Summary:
2019-03-22 21:55:30,986 [INFO] Batch 70000, worst loss 0.215884 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 21:55:30,986 [INFO] Regularization: 1511.301880 * 0.0000010000 = 0.0015113018
2019-03-22 21:55:30,987 [INFO] Sum of grad norms: 0.025457
2019-03-22 21:55:30,987 [INFO] ---------------------------------
2019-03-22 21:55:33,710 [INFO] ---------------------------------
2019-03-22 21:55:33,711 [INFO] Evaluation:
2019-03-22 21:55:33,711 [INFO] Batch 70000, worst loss 0.260648 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:55:33,712 [INFO] ---------------------------------
2019-03-22 21:55:48,015 [INFO] ---------------------------------
2019-03-22 21:55:48,017 [INFO] Summary:
2019-03-22 21:55:48,017 [INFO] Batch 71000, worst loss 0.229097 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:55:48,018 [INFO] Regularization: 1509.279175 * 0.0000010000 = 0.0015092791
2019-03-22 21:55:48,020 [INFO] Sum of grad norms: 4.426590
2019-03-22 21:55:48,021 [INFO] ---------------------------------
2019-03-22 21:56:01,727 [INFO] ---------------------------------
2019-03-22 21:56:01,728 [INFO] Summary:
2019-03-22 21:56:01,728 [INFO] Batch 72000, worst loss 0.201678 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:56:01,729 [INFO] Regularization: 1508.406494 * 0.0000010000 = 0.0015084065
2019-03-22 21:56:01,729 [INFO] Sum of grad norms: 0.002685
2019-03-22 21:56:01,730 [INFO] ---------------------------------
2019-03-22 21:56:15,964 [INFO] ---------------------------------
2019-03-22 21:56:15,965 [INFO] Summary:
2019-03-22 21:56:15,966 [INFO] Batch 73000, worst loss 0.190148 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:56:15,967 [INFO] Regularization: 1507.824097 * 0.0000010000 = 0.0015078241
2019-03-22 21:56:15,967 [INFO] Sum of grad norms: 0.034229
2019-03-22 21:56:15,968 [INFO] ---------------------------------
2019-03-22 21:56:30,351 [INFO] ---------------------------------
2019-03-22 21:56:30,352 [INFO] Summary:
2019-03-22 21:56:30,352 [INFO] Batch 74000, worst loss 0.217375 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:56:30,353 [INFO] Regularization: 1507.387695 * 0.0000010000 = 0.0015073877
2019-03-22 21:56:30,353 [INFO] Sum of grad norms: 2.290959
2019-03-22 21:56:30,354 [INFO] ---------------------------------
2019-03-22 21:56:44,652 [INFO] ---------------------------------
2019-03-22 21:56:44,653 [INFO] Summary:
2019-03-22 21:56:44,654 [INFO] Batch 75000, worst loss 0.185888 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:56:44,655 [INFO] Regularization: 1507.150757 * 0.0000010000 = 0.0015071507
2019-03-22 21:56:44,655 [INFO] Sum of grad norms: 0.024725
2019-03-22 21:56:44,656 [INFO] ---------------------------------
2019-03-22 21:56:59,478 [INFO] ---------------------------------
2019-03-22 21:56:59,479 [INFO] Summary:
2019-03-22 21:56:59,480 [INFO] Batch 76000, worst loss 0.233916 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:56:59,481 [INFO] Regularization: 1506.459229 * 0.0000010000 = 0.0015064592
2019-03-22 21:56:59,481 [INFO] Sum of grad norms: 0.054833
2019-03-22 21:56:59,482 [INFO] ---------------------------------
2019-03-22 21:57:13,893 [INFO] ---------------------------------
2019-03-22 21:57:13,893 [INFO] Summary:
2019-03-22 21:57:13,894 [INFO] Batch 77000, worst loss 0.230236 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:57:13,894 [INFO] Regularization: 1505.912598 * 0.0000010000 = 0.0015059126
2019-03-22 21:57:13,895 [INFO] Sum of grad norms: 0.003361
2019-03-22 21:57:13,895 [INFO] ---------------------------------
2019-03-22 21:57:28,129 [INFO] ---------------------------------
2019-03-22 21:57:28,130 [INFO] Summary:
2019-03-22 21:57:28,131 [INFO] Batch 78000, worst loss 0.286662 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:57:28,131 [INFO] Regularization: 1505.549194 * 0.0000010000 = 0.0015055492
2019-03-22 21:57:28,132 [INFO] Sum of grad norms: 0.005868
2019-03-22 21:57:28,132 [INFO] ---------------------------------
2019-03-22 21:57:41,801 [INFO] ---------------------------------
2019-03-22 21:57:41,802 [INFO] Summary:
2019-03-22 21:57:41,802 [INFO] Batch 79000, worst loss 0.235104 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:57:41,803 [INFO] Regularization: 1505.093140 * 0.0000010000 = 0.0015050932
2019-03-22 21:57:41,803 [INFO] Sum of grad norms: 0.001691
2019-03-22 21:57:41,804 [INFO] ---------------------------------
2019-03-22 21:57:56,500 [INFO] ---------------------------------
2019-03-22 21:57:56,501 [INFO] Summary:
2019-03-22 21:57:56,501 [INFO] Batch 80000, worst loss 0.247292 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 21:57:56,502 [INFO] Regularization: 1504.563599 * 0.0000010000 = 0.0015045636
2019-03-22 21:57:56,502 [INFO] Sum of grad norms: 0.003910
2019-03-22 21:57:56,503 [INFO] ---------------------------------
2019-03-22 21:57:59,253 [INFO] ---------------------------------
2019-03-22 21:57:59,253 [INFO] Evaluation:
2019-03-22 21:57:59,254 [INFO] Batch 80000, worst loss 0.218978 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 21:57:59,255 [INFO] ---------------------------------
2019-03-22 21:58:13,675 [INFO] ---------------------------------
2019-03-22 21:58:13,676 [INFO] Summary:
2019-03-22 21:58:13,677 [INFO] Batch 81000, worst loss 0.241089 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:58:13,678 [INFO] Regularization: 1504.152466 * 0.0000010000 = 0.0015041524
2019-03-22 21:58:13,679 [INFO] Sum of grad norms: 0.083073
2019-03-22 21:58:13,679 [INFO] ---------------------------------
2019-03-22 21:58:27,450 [INFO] ---------------------------------
2019-03-22 21:58:27,451 [INFO] Summary:
2019-03-22 21:58:27,452 [INFO] Batch 82000, worst loss 0.239277 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:58:27,452 [INFO] Regularization: 1503.994873 * 0.0000010000 = 0.0015039949
2019-03-22 21:58:27,453 [INFO] Sum of grad norms: 22.523363
2019-03-22 21:58:27,453 [INFO] ---------------------------------
2019-03-22 21:58:41,278 [INFO] ---------------------------------
2019-03-22 21:58:41,279 [INFO] Summary:
2019-03-22 21:58:41,279 [INFO] Batch 83000, worst loss 0.206226 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:58:41,280 [INFO] Regularization: 1503.859009 * 0.0000010000 = 0.0015038589
2019-03-22 21:58:41,280 [INFO] Sum of grad norms: 2.023077
2019-03-22 21:58:41,281 [INFO] ---------------------------------
2019-03-22 21:58:55,706 [INFO] ---------------------------------
2019-03-22 21:58:55,707 [INFO] Summary:
2019-03-22 21:58:55,708 [INFO] Batch 84000, worst loss 0.263597 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:58:55,708 [INFO] Regularization: 1503.725952 * 0.0000010000 = 0.0015037260
2019-03-22 21:58:55,709 [INFO] Sum of grad norms: 0.005071
2019-03-22 21:58:55,709 [INFO] ---------------------------------
2019-03-22 21:59:09,780 [INFO] ---------------------------------
2019-03-22 21:59:09,781 [INFO] Summary:
2019-03-22 21:59:09,782 [INFO] Batch 85000, worst loss 0.180539 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:59:09,782 [INFO] Regularization: 1503.587524 * 0.0000010000 = 0.0015035875
2019-03-22 21:59:09,783 [INFO] Sum of grad norms: 0.003796
2019-03-22 21:59:09,783 [INFO] ---------------------------------
2019-03-22 21:59:24,550 [INFO] ---------------------------------
2019-03-22 21:59:24,551 [INFO] Summary:
2019-03-22 21:59:24,552 [INFO] Batch 86000, worst loss 0.197427 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:59:24,552 [INFO] Regularization: 1503.484375 * 0.0000010000 = 0.0015034843
2019-03-22 21:59:24,553 [INFO] Sum of grad norms: 38.560299
2019-03-22 21:59:24,553 [INFO] ---------------------------------
2019-03-22 21:59:38,895 [INFO] ---------------------------------
2019-03-22 21:59:38,896 [INFO] Summary:
2019-03-22 21:59:38,896 [INFO] Batch 87000, worst loss 0.274253 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:59:38,897 [INFO] Regularization: 1503.332764 * 0.0000010000 = 0.0015033328
2019-03-22 21:59:38,897 [INFO] Sum of grad norms: 0.001835
2019-03-22 21:59:38,898 [INFO] ---------------------------------
2019-03-22 21:59:53,307 [INFO] ---------------------------------
2019-03-22 21:59:53,308 [INFO] Summary:
2019-03-22 21:59:53,308 [INFO] Batch 88000, worst loss 0.306876 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 21:59:53,309 [INFO] Regularization: 1503.233398 * 0.0000010000 = 0.0015032334
2019-03-22 21:59:53,309 [INFO] Sum of grad norms: 0.002692
2019-03-22 21:59:53,310 [INFO] ---------------------------------
2019-03-22 22:00:08,200 [INFO] ---------------------------------
2019-03-22 22:00:08,201 [INFO] Summary:
2019-03-22 22:00:08,202 [INFO] Batch 89000, worst loss 0.305057 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:00:08,202 [INFO] Regularization: 1503.132935 * 0.0000010000 = 0.0015031330
2019-03-22 22:00:08,203 [INFO] Sum of grad norms: 1.469982
2019-03-22 22:00:08,203 [INFO] ---------------------------------
2019-03-22 22:00:22,219 [INFO] ---------------------------------
2019-03-22 22:00:22,220 [INFO] Summary:
2019-03-22 22:00:22,220 [INFO] Batch 90000, worst loss 0.178468 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:00:22,221 [INFO] Regularization: 1502.949097 * 0.0000010000 = 0.0015029490
2019-03-22 22:00:22,222 [INFO] Sum of grad norms: 3.660841
2019-03-22 22:00:22,222 [INFO] ---------------------------------
2019-03-22 22:00:24,976 [INFO] ---------------------------------
2019-03-22 22:00:24,977 [INFO] Evaluation:
2019-03-22 22:00:24,978 [INFO] Batch 90000, worst loss 0.185988 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:00:24,978 [INFO] ---------------------------------
2019-03-22 22:00:39,091 [INFO] ---------------------------------
2019-03-22 22:00:39,092 [INFO] Summary:
2019-03-22 22:00:39,092 [INFO] Batch 91000, worst loss 0.227592 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:00:39,093 [INFO] Regularization: 1502.829102 * 0.0000010000 = 0.0015028291
2019-03-22 22:00:39,094 [INFO] Sum of grad norms: 0.008068
2019-03-22 22:00:39,094 [INFO] ---------------------------------
2019-03-22 22:00:53,489 [INFO] ---------------------------------
2019-03-22 22:00:53,490 [INFO] Summary:
2019-03-22 22:00:53,491 [INFO] Batch 92000, worst loss 0.226663 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:00:53,491 [INFO] Regularization: 1502.780029 * 0.0000010000 = 0.0015027800
2019-03-22 22:00:53,492 [INFO] Sum of grad norms: 0.274735
2019-03-22 22:00:53,492 [INFO] ---------------------------------
2019-03-22 22:01:08,431 [INFO] ---------------------------------
2019-03-22 22:01:08,432 [INFO] Summary:
2019-03-22 22:01:08,433 [INFO] Batch 93000, worst loss 0.225138 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:01:08,434 [INFO] Regularization: 1502.746704 * 0.0000010000 = 0.0015027467
2019-03-22 22:01:08,435 [INFO] Sum of grad norms: 21.315905
2019-03-22 22:01:08,436 [INFO] ---------------------------------
2019-03-22 22:01:22,431 [INFO] ---------------------------------
2019-03-22 22:01:22,432 [INFO] Summary:
2019-03-22 22:01:22,432 [INFO] Batch 94000, worst loss 0.227231 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:01:22,433 [INFO] Regularization: 1502.725830 * 0.0000010000 = 0.0015027259
2019-03-22 22:01:22,433 [INFO] Sum of grad norms: 0.001606
2019-03-22 22:01:22,434 [INFO] ---------------------------------
2019-03-22 22:01:36,761 [INFO] ---------------------------------
2019-03-22 22:01:36,762 [INFO] Summary:
2019-03-22 22:01:36,763 [INFO] Batch 95000, worst loss 0.290988 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:01:36,763 [INFO] Regularization: 1502.685791 * 0.0000010000 = 0.0015026858
2019-03-22 22:01:36,764 [INFO] Sum of grad norms: 16.738672
2019-03-22 22:01:36,764 [INFO] ---------------------------------
2019-03-22 22:01:51,527 [INFO] ---------------------------------
2019-03-22 22:01:51,528 [INFO] Summary:
2019-03-22 22:01:51,528 [INFO] Batch 96000, worst loss 0.199620 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:01:51,529 [INFO] Regularization: 1502.660278 * 0.0000010000 = 0.0015026602
2019-03-22 22:01:51,529 [INFO] Sum of grad norms: 0.796385
2019-03-22 22:01:51,530 [INFO] ---------------------------------
2019-03-22 22:02:05,948 [INFO] ---------------------------------
2019-03-22 22:02:05,949 [INFO] Summary:
2019-03-22 22:02:05,950 [INFO] Batch 97000, worst loss 0.229347 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:02:05,951 [INFO] Regularization: 1502.621460 * 0.0000010000 = 0.0015026215
2019-03-22 22:02:05,951 [INFO] Sum of grad norms: 0.003535
2019-03-22 22:02:05,952 [INFO] ---------------------------------
2019-03-22 22:02:20,328 [INFO] ---------------------------------
2019-03-22 22:02:20,329 [INFO] Summary:
2019-03-22 22:02:20,329 [INFO] Batch 98000, worst loss 0.341151 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:02:20,330 [INFO] Regularization: 1502.582642 * 0.0000010000 = 0.0015025827
2019-03-22 22:02:20,330 [INFO] Sum of grad norms: 18.563784
2019-03-22 22:02:20,331 [INFO] ---------------------------------
2019-03-22 22:02:34,569 [INFO] ---------------------------------
2019-03-22 22:02:34,570 [INFO] Summary:
2019-03-22 22:02:34,571 [INFO] Batch 99000, worst loss 0.189080 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:02:34,571 [INFO] Regularization: 1502.546265 * 0.0000010000 = 0.0015025462
2019-03-22 22:02:34,572 [INFO] Sum of grad norms: 11.122920
2019-03-22 22:02:34,573 [INFO] ---------------------------------
2019-03-22 22:02:48,639 [INFO] ---------------------------------
2019-03-22 22:02:48,641 [INFO] Summary:
2019-03-22 22:02:48,641 [INFO] Batch 100000, worst loss 0.210330 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:02:48,642 [INFO] Regularization: 1502.517090 * 0.0000010000 = 0.0015025170
2019-03-22 22:02:48,642 [INFO] Sum of grad norms: 0.004707
2019-03-22 22:02:48,643 [INFO] ---------------------------------
2019-03-22 22:02:51,380 [INFO] ---------------------------------
2019-03-22 22:02:51,380 [INFO] Evaluation:
2019-03-22 22:02:51,382 [INFO] Batch 100000, worst loss 0.234070 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:02:51,384 [INFO] ---------------------------------
2019-03-22 22:02:51,384 [INFO] Finished training, saved to file classifier/1553275201/1553288571_8_classifier_final.pth
2019-03-22 22:02:51,550 [INFO] ---------------------------------
2019-03-22 22:02:51,552 [INFO] Training model #9: (8, 64, 2) @ 2
2019-03-22 22:03:06,130 [INFO] ---------------------------------
2019-03-22 22:03:06,131 [INFO] Summary:
2019-03-22 22:03:06,132 [INFO] Batch 1000, worst loss 612.107544 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:03:06,132 [INFO] Regularization: 30864.994141 * 0.0000010000 = 0.0308649950
2019-03-22 22:03:06,133 [INFO] Sum of grad norms: 1517.627808
2019-03-22 22:03:06,133 [INFO] ---------------------------------
2019-03-22 22:03:20,169 [INFO] ---------------------------------
2019-03-22 22:03:20,170 [INFO] Summary:
2019-03-22 22:03:20,171 [INFO] Batch 2000, worst loss 28.223534 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:03:20,172 [INFO] Regularization: 29004.898438 * 0.0000010000 = 0.0290048979
2019-03-22 22:03:20,172 [INFO] Sum of grad norms: 576.788391
2019-03-22 22:03:20,173 [INFO] ---------------------------------
2019-03-22 22:03:34,274 [INFO] ---------------------------------
2019-03-22 22:03:34,276 [INFO] Summary:
2019-03-22 22:03:34,277 [INFO] Batch 3000, worst loss 13.503646 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:03:34,277 [INFO] Regularization: 25465.376953 * 0.0000010000 = 0.0254653767
2019-03-22 22:03:34,278 [INFO] Sum of grad norms: 249.547623
2019-03-22 22:03:34,279 [INFO] ---------------------------------
2019-03-22 22:03:49,451 [INFO] ---------------------------------
2019-03-22 22:03:49,452 [INFO] Summary:
2019-03-22 22:03:49,453 [INFO] Batch 4000, worst loss 6.055140 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:03:49,453 [INFO] Regularization: 20509.423828 * 0.0000010000 = 0.0205094237
2019-03-22 22:03:49,454 [INFO] Sum of grad norms: 95.908440
2019-03-22 22:03:49,455 [INFO] ---------------------------------
2019-03-22 22:04:04,172 [INFO] ---------------------------------
2019-03-22 22:04:04,172 [INFO] Summary:
2019-03-22 22:04:04,173 [INFO] Batch 5000, worst loss 2.356945 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:04:04,174 [INFO] Regularization: 14801.120117 * 0.0000010000 = 0.0148011204
2019-03-22 22:04:04,175 [INFO] Sum of grad norms: 275.726135
2019-03-22 22:04:04,175 [INFO] ---------------------------------
2019-03-22 22:04:18,271 [INFO] ---------------------------------
2019-03-22 22:04:18,272 [INFO] Summary:
2019-03-22 22:04:18,273 [INFO] Batch 6000, worst loss 1.920926 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:04:18,273 [INFO] Regularization: 10664.728516 * 0.0000010000 = 0.0106647285
2019-03-22 22:04:18,274 [INFO] Sum of grad norms: 40.194508
2019-03-22 22:04:18,274 [INFO] ---------------------------------
2019-03-22 22:04:33,208 [INFO] ---------------------------------
2019-03-22 22:04:33,210 [INFO] Summary:
2019-03-22 22:04:33,210 [INFO] Batch 7000, worst loss 1.074759 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:04:33,211 [INFO] Regularization: 9823.676758 * 0.0000010000 = 0.0098236771
2019-03-22 22:04:33,211 [INFO] Sum of grad norms: 98.011978
2019-03-22 22:04:33,212 [INFO] ---------------------------------
2019-03-22 22:04:47,681 [INFO] ---------------------------------
2019-03-22 22:04:47,682 [INFO] Summary:
2019-03-22 22:04:47,683 [INFO] Batch 8000, worst loss 0.862414 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:04:47,683 [INFO] Regularization: 9662.092773 * 0.0000010000 = 0.0096620927
2019-03-22 22:04:47,684 [INFO] Sum of grad norms: 42.465511
2019-03-22 22:04:47,684 [INFO] ---------------------------------
2019-03-22 22:05:02,215 [INFO] ---------------------------------
2019-03-22 22:05:02,216 [INFO] Summary:
2019-03-22 22:05:02,217 [INFO] Batch 9000, worst loss 0.888563 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:05:02,217 [INFO] Regularization: 9443.980469 * 0.0000010000 = 0.0094439806
2019-03-22 22:05:02,218 [INFO] Sum of grad norms: 73.948273
2019-03-22 22:05:02,218 [INFO] ---------------------------------
2019-03-22 22:05:16,654 [INFO] ---------------------------------
2019-03-22 22:05:16,655 [INFO] Summary:
2019-03-22 22:05:16,656 [INFO] Batch 10000, worst loss 0.718126 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:05:16,658 [INFO] Regularization: 9195.471680 * 0.0000010000 = 0.0091954721
2019-03-22 22:05:16,659 [INFO] Sum of grad norms: 31.039774
2019-03-22 22:05:16,660 [INFO] ---------------------------------
2019-03-22 22:05:19,379 [INFO] ---------------------------------
2019-03-22 22:05:19,380 [INFO] Evaluation:
2019-03-22 22:05:19,380 [INFO] Batch 10000, worst loss 0.463117 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:05:19,382 [INFO] ---------------------------------
2019-03-22 22:05:34,479 [INFO] ---------------------------------
2019-03-22 22:05:34,481 [INFO] Summary:
2019-03-22 22:05:34,481 [INFO] Batch 11000, worst loss 0.726364 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:05:34,482 [INFO] Regularization: 8912.254883 * 0.0000010000 = 0.0089122551
2019-03-22 22:05:34,482 [INFO] Sum of grad norms: 18.299568
2019-03-22 22:05:34,483 [INFO] ---------------------------------
2019-03-22 22:05:48,250 [INFO] ---------------------------------
2019-03-22 22:05:48,251 [INFO] Summary:
2019-03-22 22:05:48,252 [INFO] Batch 12000, worst loss 0.445115 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:05:48,252 [INFO] Regularization: 8576.660156 * 0.0000010000 = 0.0085766604
2019-03-22 22:05:48,253 [INFO] Sum of grad norms: 6.132190
2019-03-22 22:05:48,254 [INFO] ---------------------------------
2019-03-22 22:06:02,940 [INFO] ---------------------------------
2019-03-22 22:06:02,941 [INFO] Summary:
2019-03-22 22:06:02,941 [INFO] Batch 13000, worst loss 0.532939 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:06:02,942 [INFO] Regularization: 8196.987305 * 0.0000010000 = 0.0081969872
2019-03-22 22:06:02,943 [INFO] Sum of grad norms: 2.793272
2019-03-22 22:06:02,943 [INFO] ---------------------------------
2019-03-22 22:06:17,932 [INFO] ---------------------------------
2019-03-22 22:06:17,933 [INFO] Summary:
2019-03-22 22:06:17,933 [INFO] Batch 14000, worst loss 0.379362 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:06:17,935 [INFO] Regularization: 7834.067383 * 0.0000010000 = 0.0078340676
2019-03-22 22:06:17,936 [INFO] Sum of grad norms: 14.860178
2019-03-22 22:06:17,937 [INFO] ---------------------------------
2019-03-22 22:06:32,873 [INFO] ---------------------------------
2019-03-22 22:06:32,874 [INFO] Summary:
2019-03-22 22:06:32,874 [INFO] Batch 15000, worst loss 0.500957 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:06:32,875 [INFO] Regularization: 7520.585938 * 0.0000010000 = 0.0075205858
2019-03-22 22:06:32,875 [INFO] Sum of grad norms: 2.528348
2019-03-22 22:06:32,876 [INFO] ---------------------------------
2019-03-22 22:06:47,640 [INFO] ---------------------------------
2019-03-22 22:06:47,640 [INFO] Summary:
2019-03-22 22:06:47,641 [INFO] Batch 16000, worst loss 0.347068 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:06:47,642 [INFO] Regularization: 7017.029785 * 0.0000010000 = 0.0070170299
2019-03-22 22:06:47,643 [INFO] Sum of grad norms: 8.093672
2019-03-22 22:06:47,643 [INFO] ---------------------------------
2019-03-22 22:07:02,394 [INFO] ---------------------------------
2019-03-22 22:07:02,395 [INFO] Summary:
2019-03-22 22:07:02,396 [INFO] Batch 17000, worst loss 0.355354 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:07:02,396 [INFO] Regularization: 6663.113281 * 0.0000010000 = 0.0066631134
2019-03-22 22:07:02,396 [INFO] Sum of grad norms: 9.969920
2019-03-22 22:07:02,397 [INFO] ---------------------------------
2019-03-22 22:07:17,217 [INFO] ---------------------------------
2019-03-22 22:07:17,217 [INFO] Summary:
2019-03-22 22:07:17,218 [INFO] Batch 18000, worst loss 0.427959 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:07:17,218 [INFO] Regularization: 6262.241211 * 0.0000010000 = 0.0062622414
2019-03-22 22:07:17,219 [INFO] Sum of grad norms: 8.334974
2019-03-22 22:07:17,219 [INFO] ---------------------------------
2019-03-22 22:07:32,631 [INFO] ---------------------------------
2019-03-22 22:07:32,632 [INFO] Summary:
2019-03-22 22:07:32,633 [INFO] Batch 19000, worst loss 0.328455 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:07:32,633 [INFO] Regularization: 5844.458984 * 0.0000010000 = 0.0058444589
2019-03-22 22:07:32,634 [INFO] Sum of grad norms: 3.379501
2019-03-22 22:07:32,634 [INFO] ---------------------------------
2019-03-22 22:07:47,133 [INFO] ---------------------------------
2019-03-22 22:07:47,134 [INFO] Summary:
2019-03-22 22:07:47,135 [INFO] Batch 20000, worst loss 0.491065 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:07:47,135 [INFO] Regularization: 5443.853516 * 0.0000010000 = 0.0054438533
2019-03-22 22:07:47,136 [INFO] Sum of grad norms: 8.885279
2019-03-22 22:07:47,136 [INFO] ---------------------------------
2019-03-22 22:07:49,850 [INFO] ---------------------------------
2019-03-22 22:07:49,851 [INFO] Evaluation:
2019-03-22 22:07:49,852 [INFO] Batch 20000, worst loss 0.382564 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:07:49,853 [INFO] ---------------------------------
2019-03-22 22:08:04,373 [INFO] ---------------------------------
2019-03-22 22:08:04,375 [INFO] Summary:
2019-03-22 22:08:04,376 [INFO] Batch 21000, worst loss 0.363524 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:08:04,377 [INFO] Regularization: 5253.900879 * 0.0000010000 = 0.0052539008
2019-03-22 22:08:04,377 [INFO] Sum of grad norms: 10.361206
2019-03-22 22:08:04,378 [INFO] ---------------------------------
2019-03-22 22:08:19,308 [INFO] ---------------------------------
2019-03-22 22:08:19,309 [INFO] Summary:
2019-03-22 22:08:19,310 [INFO] Batch 22000, worst loss 0.386492 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:08:19,311 [INFO] Regularization: 4889.694824 * 0.0000010000 = 0.0048896950
2019-03-22 22:08:19,311 [INFO] Sum of grad norms: 0.410562
2019-03-22 22:08:19,312 [INFO] ---------------------------------
2019-03-22 22:08:34,021 [INFO] ---------------------------------
2019-03-22 22:08:34,022 [INFO] Summary:
2019-03-22 22:08:34,022 [INFO] Batch 23000, worst loss 0.343965 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:08:34,023 [INFO] Regularization: 4631.363281 * 0.0000010000 = 0.0046313633
2019-03-22 22:08:34,023 [INFO] Sum of grad norms: 4.936750
2019-03-22 22:08:34,024 [INFO] ---------------------------------
2019-03-22 22:08:49,775 [INFO] ---------------------------------
2019-03-22 22:08:49,776 [INFO] Summary:
2019-03-22 22:08:49,777 [INFO] Batch 24000, worst loss 0.311600 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:08:49,777 [INFO] Regularization: 4399.373535 * 0.0000010000 = 0.0043993737
2019-03-22 22:08:49,778 [INFO] Sum of grad norms: 4.930276
2019-03-22 22:08:49,779 [INFO] ---------------------------------
2019-03-22 22:09:04,456 [INFO] ---------------------------------
2019-03-22 22:09:04,457 [INFO] Summary:
2019-03-22 22:09:04,458 [INFO] Batch 25000, worst loss 0.381873 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:09:04,458 [INFO] Regularization: 4281.526367 * 0.0000010000 = 0.0042815264
2019-03-22 22:09:04,459 [INFO] Sum of grad norms: 4.480394
2019-03-22 22:09:04,459 [INFO] ---------------------------------
2019-03-22 22:09:19,497 [INFO] ---------------------------------
2019-03-22 22:09:19,498 [INFO] Summary:
2019-03-22 22:09:19,499 [INFO] Batch 26000, worst loss 0.459279 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:09:19,499 [INFO] Regularization: 4191.638184 * 0.0000010000 = 0.0041916380
2019-03-22 22:09:19,500 [INFO] Sum of grad norms: 1.551604
2019-03-22 22:09:19,501 [INFO] ---------------------------------
2019-03-22 22:09:34,199 [INFO] ---------------------------------
2019-03-22 22:09:34,201 [INFO] Summary:
2019-03-22 22:09:34,201 [INFO] Batch 27000, worst loss 0.328018 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:09:34,202 [INFO] Regularization: 4052.007324 * 0.0000010000 = 0.0040520071
2019-03-22 22:09:34,203 [INFO] Sum of grad norms: 10.272336
2019-03-22 22:09:34,204 [INFO] ---------------------------------
2019-03-22 22:09:48,562 [INFO] ---------------------------------
2019-03-22 22:09:48,563 [INFO] Summary:
2019-03-22 22:09:48,564 [INFO] Batch 28000, worst loss 0.415917 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:09:48,564 [INFO] Regularization: 3909.917480 * 0.0000010000 = 0.0039099175
2019-03-22 22:09:48,565 [INFO] Sum of grad norms: 4.939947
2019-03-22 22:09:48,565 [INFO] ---------------------------------
2019-03-22 22:10:02,848 [INFO] ---------------------------------
2019-03-22 22:10:02,849 [INFO] Summary:
2019-03-22 22:10:02,849 [INFO] Batch 29000, worst loss 0.365809 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:10:02,850 [INFO] Regularization: 3744.999023 * 0.0000010000 = 0.0037449989
2019-03-22 22:10:02,850 [INFO] Sum of grad norms: 13.115112
2019-03-22 22:10:02,851 [INFO] ---------------------------------
2019-03-22 22:10:17,042 [INFO] ---------------------------------
2019-03-22 22:10:17,043 [INFO] Summary:
2019-03-22 22:10:17,044 [INFO] Batch 30000, worst loss 0.453185 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:10:17,044 [INFO] Regularization: 3636.225830 * 0.0000010000 = 0.0036362258
2019-03-22 22:10:17,045 [INFO] Sum of grad norms: 1.197871
2019-03-22 22:10:17,045 [INFO] ---------------------------------
2019-03-22 22:10:19,752 [INFO] ---------------------------------
2019-03-22 22:10:19,752 [INFO] Evaluation:
2019-03-22 22:10:19,753 [INFO] Batch 30000, worst loss 0.282632 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:10:19,753 [INFO] ---------------------------------
2019-03-22 22:10:34,247 [INFO] ---------------------------------
2019-03-22 22:10:34,248 [INFO] Summary:
2019-03-22 22:10:34,248 [INFO] Batch 31000, worst loss 0.476316 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:10:34,249 [INFO] Regularization: 3634.804932 * 0.0000010000 = 0.0036348049
2019-03-22 22:10:34,250 [INFO] Sum of grad norms: 18.526068
2019-03-22 22:10:34,250 [INFO] ---------------------------------
2019-03-22 22:10:48,327 [INFO] ---------------------------------
2019-03-22 22:10:48,328 [INFO] Summary:
2019-03-22 22:10:48,328 [INFO] Batch 32000, worst loss 0.327403 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:10:48,329 [INFO] Regularization: 3520.727783 * 0.0000010000 = 0.0035207279
2019-03-22 22:10:48,329 [INFO] Sum of grad norms: 6.037414
2019-03-22 22:10:48,330 [INFO] ---------------------------------
2019-03-22 22:11:02,836 [INFO] ---------------------------------
2019-03-22 22:11:02,837 [INFO] Summary:
2019-03-22 22:11:02,838 [INFO] Batch 33000, worst loss 0.494868 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:11:02,838 [INFO] Regularization: 3395.027100 * 0.0000010000 = 0.0033950270
2019-03-22 22:11:02,839 [INFO] Sum of grad norms: 0.124182
2019-03-22 22:11:02,839 [INFO] ---------------------------------
2019-03-22 22:11:17,563 [INFO] ---------------------------------
2019-03-22 22:11:17,564 [INFO] Summary:
2019-03-22 22:11:17,565 [INFO] Batch 34000, worst loss 0.390630 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:11:17,565 [INFO] Regularization: 3335.348877 * 0.0000010000 = 0.0033353488
2019-03-22 22:11:17,566 [INFO] Sum of grad norms: 5.059633
2019-03-22 22:11:17,566 [INFO] ---------------------------------
2019-03-22 22:11:32,456 [INFO] ---------------------------------
2019-03-22 22:11:32,457 [INFO] Summary:
2019-03-22 22:11:32,458 [INFO] Batch 35000, worst loss 0.399054 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:11:32,458 [INFO] Regularization: 3287.442139 * 0.0000010000 = 0.0032874420
2019-03-22 22:11:32,459 [INFO] Sum of grad norms: 8.692132
2019-03-22 22:11:32,459 [INFO] ---------------------------------
2019-03-22 22:11:46,867 [INFO] ---------------------------------
2019-03-22 22:11:46,868 [INFO] Summary:
2019-03-22 22:11:46,868 [INFO] Batch 36000, worst loss 0.346022 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:11:46,869 [INFO] Regularization: 3188.581299 * 0.0000010000 = 0.0031885812
2019-03-22 22:11:46,869 [INFO] Sum of grad norms: 7.705560
2019-03-22 22:11:46,870 [INFO] ---------------------------------
2019-03-22 22:12:01,252 [INFO] ---------------------------------
2019-03-22 22:12:01,253 [INFO] Summary:
2019-03-22 22:12:01,253 [INFO] Batch 37000, worst loss 0.311555 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:12:01,254 [INFO] Regularization: 3104.280273 * 0.0000010000 = 0.0031042802
2019-03-22 22:12:01,254 [INFO] Sum of grad norms: 0.643562
2019-03-22 22:12:01,255 [INFO] ---------------------------------
2019-03-22 22:12:15,894 [INFO] ---------------------------------
2019-03-22 22:12:15,895 [INFO] Summary:
2019-03-22 22:12:15,896 [INFO] Batch 38000, worst loss 0.390260 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:12:15,896 [INFO] Regularization: 3098.526123 * 0.0000010000 = 0.0030985260
2019-03-22 22:12:15,897 [INFO] Sum of grad norms: 11.610704
2019-03-22 22:12:15,897 [INFO] ---------------------------------
2019-03-22 22:12:30,093 [INFO] ---------------------------------
2019-03-22 22:12:30,094 [INFO] Summary:
2019-03-22 22:12:30,095 [INFO] Batch 39000, worst loss 0.431799 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:12:30,095 [INFO] Regularization: 2990.266113 * 0.0000010000 = 0.0029902661
2019-03-22 22:12:30,096 [INFO] Sum of grad norms: 0.129411
2019-03-22 22:12:30,096 [INFO] ---------------------------------
2019-03-22 22:12:44,472 [INFO] ---------------------------------
2019-03-22 22:12:44,472 [INFO] Summary:
2019-03-22 22:12:44,473 [INFO] Batch 40000, worst loss 0.372420 (incl. reg.) of 1000 batches, learning rate 0.001000 @cl.-depth 2
2019-03-22 22:12:44,474 [INFO] Regularization: 2887.373535 * 0.0000010000 = 0.0028873736
2019-03-22 22:12:44,474 [INFO] Sum of grad norms: 12.769047
2019-03-22 22:12:44,475 [INFO] ---------------------------------
2019-03-22 22:12:47,246 [INFO] ---------------------------------
2019-03-22 22:12:47,247 [INFO] Evaluation:
2019-03-22 22:12:47,248 [INFO] Batch 40000, worst loss 0.199428 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:12:47,249 [INFO] ---------------------------------
2019-03-22 22:13:02,297 [INFO] ---------------------------------
2019-03-22 22:13:02,298 [INFO] Summary:
2019-03-22 22:13:02,298 [INFO] Batch 41000, worst loss 0.275039 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 22:13:02,299 [INFO] Regularization: 2918.070312 * 0.0000010000 = 0.0029180704
2019-03-22 22:13:02,300 [INFO] Sum of grad norms: 12.135835
2019-03-22 22:13:02,300 [INFO] ---------------------------------
2019-03-22 22:13:16,594 [INFO] ---------------------------------
2019-03-22 22:13:16,595 [INFO] Summary:
2019-03-22 22:13:16,595 [INFO] Batch 42000, worst loss 0.380978 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 22:13:16,596 [INFO] Regularization: 2805.101074 * 0.0000010000 = 0.0028051010
2019-03-22 22:13:16,596 [INFO] Sum of grad norms: 0.811239
2019-03-22 22:13:16,597 [INFO] ---------------------------------
2019-03-22 22:13:30,547 [INFO] ---------------------------------
2019-03-22 22:13:30,548 [INFO] Summary:
2019-03-22 22:13:30,548 [INFO] Batch 43000, worst loss 0.296960 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 22:13:30,549 [INFO] Regularization: 2746.253906 * 0.0000010000 = 0.0027462540
2019-03-22 22:13:30,549 [INFO] Sum of grad norms: 1.599876
2019-03-22 22:13:30,550 [INFO] ---------------------------------
2019-03-22 22:13:44,408 [INFO] ---------------------------------
2019-03-22 22:13:44,409 [INFO] Summary:
2019-03-22 22:13:44,410 [INFO] Batch 44000, worst loss 0.314118 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 22:13:44,410 [INFO] Regularization: 2699.974609 * 0.0000010000 = 0.0026999747
2019-03-22 22:13:44,411 [INFO] Sum of grad norms: 0.062916
2019-03-22 22:13:44,411 [INFO] ---------------------------------
2019-03-22 22:13:58,059 [INFO] ---------------------------------
2019-03-22 22:13:58,060 [INFO] Summary:
2019-03-22 22:13:58,061 [INFO] Batch 45000, worst loss 0.287138 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 22:13:58,061 [INFO] Regularization: 2653.998291 * 0.0000010000 = 0.0026539983
2019-03-22 22:13:58,062 [INFO] Sum of grad norms: 0.141524
2019-03-22 22:13:58,062 [INFO] ---------------------------------
2019-03-22 22:14:12,309 [INFO] ---------------------------------
2019-03-22 22:14:12,310 [INFO] Summary:
2019-03-22 22:14:12,311 [INFO] Batch 46000, worst loss 0.373745 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 22:14:12,311 [INFO] Regularization: 2612.413086 * 0.0000010000 = 0.0026124131
2019-03-22 22:14:12,312 [INFO] Sum of grad norms: 19.791512
2019-03-22 22:14:12,313 [INFO] ---------------------------------
2019-03-22 22:14:26,686 [INFO] ---------------------------------
2019-03-22 22:14:26,687 [INFO] Summary:
2019-03-22 22:14:26,688 [INFO] Batch 47000, worst loss 0.289708 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 22:14:26,688 [INFO] Regularization: 2569.730469 * 0.0000010000 = 0.0025697304
2019-03-22 22:14:26,689 [INFO] Sum of grad norms: 14.993940
2019-03-22 22:14:26,689 [INFO] ---------------------------------
2019-03-22 22:14:41,199 [INFO] ---------------------------------
2019-03-22 22:14:41,200 [INFO] Summary:
2019-03-22 22:14:41,201 [INFO] Batch 48000, worst loss 0.276602 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 22:14:41,201 [INFO] Regularization: 2535.099609 * 0.0000010000 = 0.0025350996
2019-03-22 22:14:41,202 [INFO] Sum of grad norms: 8.257833
2019-03-22 22:14:41,202 [INFO] ---------------------------------
2019-03-22 22:14:55,446 [INFO] ---------------------------------
2019-03-22 22:14:55,447 [INFO] Summary:
2019-03-22 22:14:55,448 [INFO] Batch 49000, worst loss 0.250834 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 22:14:55,448 [INFO] Regularization: 2510.032471 * 0.0000010000 = 0.0025100324
2019-03-22 22:14:55,449 [INFO] Sum of grad norms: 0.206010
2019-03-22 22:14:55,450 [INFO] ---------------------------------
2019-03-22 22:15:09,411 [INFO] ---------------------------------
2019-03-22 22:15:09,412 [INFO] Summary:
2019-03-22 22:15:09,412 [INFO] Batch 50000, worst loss 0.307588 (incl. reg.) of 1000 batches, learning rate 0.000250 @cl.-depth 2
2019-03-22 22:15:09,413 [INFO] Regularization: 2479.666260 * 0.0000010000 = 0.0024796661
2019-03-22 22:15:09,413 [INFO] Sum of grad norms: 30.458616
2019-03-22 22:15:09,414 [INFO] ---------------------------------
2019-03-22 22:15:12,133 [INFO] ---------------------------------
2019-03-22 22:15:12,134 [INFO] Evaluation:
2019-03-22 22:15:12,135 [INFO] Batch 50000, worst loss 0.286275 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:15:12,136 [INFO] ---------------------------------
2019-03-22 22:15:26,942 [INFO] ---------------------------------
2019-03-22 22:15:26,943 [INFO] Summary:
2019-03-22 22:15:26,943 [INFO] Batch 51000, worst loss 0.282826 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 22:15:26,944 [INFO] Regularization: 2446.579834 * 0.0000010000 = 0.0024465797
2019-03-22 22:15:26,945 [INFO] Sum of grad norms: 14.715179
2019-03-22 22:15:26,945 [INFO] ---------------------------------
2019-03-22 22:15:41,048 [INFO] ---------------------------------
2019-03-22 22:15:41,048 [INFO] Summary:
2019-03-22 22:15:41,049 [INFO] Batch 52000, worst loss 0.205108 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 22:15:41,050 [INFO] Regularization: 2433.800781 * 0.0000010000 = 0.0024338008
2019-03-22 22:15:41,050 [INFO] Sum of grad norms: 12.782523
2019-03-22 22:15:41,051 [INFO] ---------------------------------
2019-03-22 22:15:55,904 [INFO] ---------------------------------
2019-03-22 22:15:55,904 [INFO] Summary:
2019-03-22 22:15:55,905 [INFO] Batch 53000, worst loss 0.253872 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 22:15:55,906 [INFO] Regularization: 2423.548340 * 0.0000010000 = 0.0024235484
2019-03-22 22:15:55,906 [INFO] Sum of grad norms: 0.008611
2019-03-22 22:15:55,907 [INFO] ---------------------------------
2019-03-22 22:16:09,798 [INFO] ---------------------------------
2019-03-22 22:16:09,799 [INFO] Summary:
2019-03-22 22:16:09,799 [INFO] Batch 54000, worst loss 0.201168 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 22:16:09,800 [INFO] Regularization: 2412.976807 * 0.0000010000 = 0.0024129767
2019-03-22 22:16:09,801 [INFO] Sum of grad norms: 0.052671
2019-03-22 22:16:09,801 [INFO] ---------------------------------
2019-03-22 22:16:23,821 [INFO] ---------------------------------
2019-03-22 22:16:23,822 [INFO] Summary:
2019-03-22 22:16:23,823 [INFO] Batch 55000, worst loss 0.258857 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 22:16:23,823 [INFO] Regularization: 2404.064697 * 0.0000010000 = 0.0024040646
2019-03-22 22:16:23,824 [INFO] Sum of grad norms: 0.057271
2019-03-22 22:16:23,824 [INFO] ---------------------------------
2019-03-22 22:16:37,698 [INFO] ---------------------------------
2019-03-22 22:16:37,699 [INFO] Summary:
2019-03-22 22:16:37,700 [INFO] Batch 56000, worst loss 0.238223 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 22:16:37,700 [INFO] Regularization: 2394.184082 * 0.0000010000 = 0.0023941840
2019-03-22 22:16:37,701 [INFO] Sum of grad norms: 0.050746
2019-03-22 22:16:37,701 [INFO] ---------------------------------
2019-03-22 22:16:52,776 [INFO] ---------------------------------
2019-03-22 22:16:52,777 [INFO] Summary:
2019-03-22 22:16:52,777 [INFO] Batch 57000, worst loss 0.330481 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 22:16:52,778 [INFO] Regularization: 2385.716309 * 0.0000010000 = 0.0023857162
2019-03-22 22:16:52,778 [INFO] Sum of grad norms: 9.005957
2019-03-22 22:16:52,779 [INFO] ---------------------------------
2019-03-22 22:17:06,417 [INFO] ---------------------------------
2019-03-22 22:17:06,418 [INFO] Summary:
2019-03-22 22:17:06,418 [INFO] Batch 58000, worst loss 0.256124 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 22:17:06,419 [INFO] Regularization: 2377.544434 * 0.0000010000 = 0.0023775445
2019-03-22 22:17:06,419 [INFO] Sum of grad norms: 0.013134
2019-03-22 22:17:06,420 [INFO] ---------------------------------
2019-03-22 22:17:21,293 [INFO] ---------------------------------
2019-03-22 22:17:21,295 [INFO] Summary:
2019-03-22 22:17:21,295 [INFO] Batch 59000, worst loss 0.267565 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 22:17:21,296 [INFO] Regularization: 2370.269775 * 0.0000010000 = 0.0023702697
2019-03-22 22:17:21,296 [INFO] Sum of grad norms: 0.016490
2019-03-22 22:17:21,297 [INFO] ---------------------------------
2019-03-22 22:17:35,870 [INFO] ---------------------------------
2019-03-22 22:17:35,871 [INFO] Summary:
2019-03-22 22:17:35,871 [INFO] Batch 60000, worst loss 0.258484 (incl. reg.) of 1000 batches, learning rate 0.000063 @cl.-depth 2
2019-03-22 22:17:35,872 [INFO] Regularization: 2362.624268 * 0.0000010000 = 0.0023626243
2019-03-22 22:17:35,872 [INFO] Sum of grad norms: 3.841573
2019-03-22 22:17:35,873 [INFO] ---------------------------------
2019-03-22 22:17:38,620 [INFO] ---------------------------------
2019-03-22 22:17:38,621 [INFO] Evaluation:
2019-03-22 22:17:38,621 [INFO] Batch 60000, worst loss 0.250134 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:17:38,622 [INFO] ---------------------------------
2019-03-22 22:17:52,921 [INFO] ---------------------------------
2019-03-22 22:17:52,922 [INFO] Summary:
2019-03-22 22:17:52,923 [INFO] Batch 61000, worst loss 0.260025 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 22:17:52,924 [INFO] Regularization: 2354.530762 * 0.0000010000 = 0.0023545309
2019-03-22 22:17:52,925 [INFO] Sum of grad norms: 15.022896
2019-03-22 22:17:52,926 [INFO] ---------------------------------
2019-03-22 22:18:07,527 [INFO] ---------------------------------
2019-03-22 22:18:07,528 [INFO] Summary:
2019-03-22 22:18:07,529 [INFO] Batch 62000, worst loss 0.216853 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 22:18:07,530 [INFO] Regularization: 2351.717773 * 0.0000010000 = 0.0023517178
2019-03-22 22:18:07,530 [INFO] Sum of grad norms: 0.008031
2019-03-22 22:18:07,531 [INFO] ---------------------------------
2019-03-22 22:18:21,191 [INFO] ---------------------------------
2019-03-22 22:18:21,192 [INFO] Summary:
2019-03-22 22:18:21,193 [INFO] Batch 63000, worst loss 0.196465 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 22:18:21,193 [INFO] Regularization: 2349.141602 * 0.0000010000 = 0.0023491415
2019-03-22 22:18:21,194 [INFO] Sum of grad norms: 0.010167
2019-03-22 22:18:21,194 [INFO] ---------------------------------
2019-03-22 22:18:35,547 [INFO] ---------------------------------
2019-03-22 22:18:35,549 [INFO] Summary:
2019-03-22 22:18:35,549 [INFO] Batch 64000, worst loss 0.205742 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 22:18:35,550 [INFO] Regularization: 2347.051758 * 0.0000010000 = 0.0023470519
2019-03-22 22:18:35,550 [INFO] Sum of grad norms: 10.492105
2019-03-22 22:18:35,551 [INFO] ---------------------------------
2019-03-22 22:18:49,590 [INFO] ---------------------------------
2019-03-22 22:18:49,591 [INFO] Summary:
2019-03-22 22:18:49,592 [INFO] Batch 65000, worst loss 0.229638 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 22:18:49,592 [INFO] Regularization: 2344.974609 * 0.0000010000 = 0.0023449745
2019-03-22 22:18:49,593 [INFO] Sum of grad norms: 11.906775
2019-03-22 22:18:49,593 [INFO] ---------------------------------
2019-03-22 22:19:03,483 [INFO] ---------------------------------
2019-03-22 22:19:03,484 [INFO] Summary:
2019-03-22 22:19:03,485 [INFO] Batch 66000, worst loss 0.302017 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 22:19:03,485 [INFO] Regularization: 2342.134521 * 0.0000010000 = 0.0023421345
2019-03-22 22:19:03,485 [INFO] Sum of grad norms: 28.610556
2019-03-22 22:19:03,486 [INFO] ---------------------------------
2019-03-22 22:19:17,591 [INFO] ---------------------------------
2019-03-22 22:19:17,592 [INFO] Summary:
2019-03-22 22:19:17,592 [INFO] Batch 67000, worst loss 0.244475 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 22:19:17,593 [INFO] Regularization: 2339.771973 * 0.0000010000 = 0.0023397719
2019-03-22 22:19:17,594 [INFO] Sum of grad norms: 0.030408
2019-03-22 22:19:17,594 [INFO] ---------------------------------
2019-03-22 22:19:31,374 [INFO] ---------------------------------
2019-03-22 22:19:31,375 [INFO] Summary:
2019-03-22 22:19:31,375 [INFO] Batch 68000, worst loss 0.225954 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 22:19:31,376 [INFO] Regularization: 2337.125244 * 0.0000010000 = 0.0023371251
2019-03-22 22:19:31,376 [INFO] Sum of grad norms: 13.213816
2019-03-22 22:19:31,377 [INFO] ---------------------------------
2019-03-22 22:19:45,470 [INFO] ---------------------------------
2019-03-22 22:19:45,471 [INFO] Summary:
2019-03-22 22:19:45,471 [INFO] Batch 69000, worst loss 0.275034 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 22:19:45,472 [INFO] Regularization: 2334.802002 * 0.0000010000 = 0.0023348019
2019-03-22 22:19:45,473 [INFO] Sum of grad norms: 0.015838
2019-03-22 22:19:45,473 [INFO] ---------------------------------
2019-03-22 22:20:00,566 [INFO] ---------------------------------
2019-03-22 22:20:00,567 [INFO] Summary:
2019-03-22 22:20:00,568 [INFO] Batch 70000, worst loss 0.204496 (incl. reg.) of 1000 batches, learning rate 0.000016 @cl.-depth 2
2019-03-22 22:20:00,568 [INFO] Regularization: 2332.642334 * 0.0000010000 = 0.0023326424
2019-03-22 22:20:00,568 [INFO] Sum of grad norms: 2.588561
2019-03-22 22:20:00,569 [INFO] ---------------------------------
2019-03-22 22:20:03,265 [INFO] ---------------------------------
2019-03-22 22:20:03,265 [INFO] Evaluation:
2019-03-22 22:20:03,267 [INFO] Batch 70000, worst loss 0.240744 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:20:03,268 [INFO] ---------------------------------
2019-03-22 22:20:17,574 [INFO] ---------------------------------
2019-03-22 22:20:17,575 [INFO] Summary:
2019-03-22 22:20:17,576 [INFO] Batch 71000, worst loss 0.249608 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 22:20:17,576 [INFO] Regularization: 2330.310303 * 0.0000010000 = 0.0023303104
2019-03-22 22:20:17,577 [INFO] Sum of grad norms: 0.103590
2019-03-22 22:20:17,578 [INFO] ---------------------------------
2019-03-22 22:20:32,049 [INFO] ---------------------------------
2019-03-22 22:20:32,050 [INFO] Summary:
2019-03-22 22:20:32,051 [INFO] Batch 72000, worst loss 0.185615 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 22:20:32,051 [INFO] Regularization: 2329.149170 * 0.0000010000 = 0.0023291491
2019-03-22 22:20:32,052 [INFO] Sum of grad norms: 21.708220
2019-03-22 22:20:32,052 [INFO] ---------------------------------
2019-03-22 22:20:46,671 [INFO] ---------------------------------
2019-03-22 22:20:46,672 [INFO] Summary:
2019-03-22 22:20:46,673 [INFO] Batch 73000, worst loss 0.185844 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 22:20:46,673 [INFO] Regularization: 2328.494385 * 0.0000010000 = 0.0023284943
2019-03-22 22:20:46,674 [INFO] Sum of grad norms: 12.796156
2019-03-22 22:20:46,674 [INFO] ---------------------------------
2019-03-22 22:21:01,588 [INFO] ---------------------------------
2019-03-22 22:21:01,589 [INFO] Summary:
2019-03-22 22:21:01,590 [INFO] Batch 74000, worst loss 0.229951 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 22:21:01,590 [INFO] Regularization: 2327.842773 * 0.0000010000 = 0.0023278429
2019-03-22 22:21:01,591 [INFO] Sum of grad norms: 6.663110
2019-03-22 22:21:01,592 [INFO] ---------------------------------
2019-03-22 22:21:16,647 [INFO] ---------------------------------
2019-03-22 22:21:16,649 [INFO] Summary:
2019-03-22 22:21:16,650 [INFO] Batch 75000, worst loss 0.236018 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 22:21:16,651 [INFO] Regularization: 2327.243164 * 0.0000010000 = 0.0023272431
2019-03-22 22:21:16,652 [INFO] Sum of grad norms: 0.645558
2019-03-22 22:21:16,653 [INFO] ---------------------------------
2019-03-22 22:21:30,616 [INFO] ---------------------------------
2019-03-22 22:21:30,617 [INFO] Summary:
2019-03-22 22:21:30,617 [INFO] Batch 76000, worst loss 0.227411 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 22:21:30,618 [INFO] Regularization: 2326.545898 * 0.0000010000 = 0.0023265460
2019-03-22 22:21:30,618 [INFO] Sum of grad norms: 0.020755
2019-03-22 22:21:30,619 [INFO] ---------------------------------
2019-03-22 22:21:45,131 [INFO] ---------------------------------
2019-03-22 22:21:45,132 [INFO] Summary:
2019-03-22 22:21:45,133 [INFO] Batch 77000, worst loss 0.227180 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 22:21:45,133 [INFO] Regularization: 2325.956543 * 0.0000010000 = 0.0023259565
2019-03-22 22:21:45,134 [INFO] Sum of grad norms: 0.157556
2019-03-22 22:21:45,135 [INFO] ---------------------------------
2019-03-22 22:22:00,350 [INFO] ---------------------------------
2019-03-22 22:22:00,351 [INFO] Summary:
2019-03-22 22:22:00,351 [INFO] Batch 78000, worst loss 0.272940 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 22:22:00,352 [INFO] Regularization: 2325.323486 * 0.0000010000 = 0.0023253234
2019-03-22 22:22:00,352 [INFO] Sum of grad norms: 0.004702
2019-03-22 22:22:00,353 [INFO] ---------------------------------
2019-03-22 22:22:14,253 [INFO] ---------------------------------
2019-03-22 22:22:14,254 [INFO] Summary:
2019-03-22 22:22:14,255 [INFO] Batch 79000, worst loss 0.282081 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 22:22:14,255 [INFO] Regularization: 2324.713867 * 0.0000010000 = 0.0023247139
2019-03-22 22:22:14,256 [INFO] Sum of grad norms: 0.009529
2019-03-22 22:22:14,256 [INFO] ---------------------------------
2019-03-22 22:22:29,066 [INFO] ---------------------------------
2019-03-22 22:22:29,067 [INFO] Summary:
2019-03-22 22:22:29,067 [INFO] Batch 80000, worst loss 0.279456 (incl. reg.) of 1000 batches, learning rate 0.000004 @cl.-depth 2
2019-03-22 22:22:29,068 [INFO] Regularization: 2324.063965 * 0.0000010000 = 0.0023240640
2019-03-22 22:22:29,068 [INFO] Sum of grad norms: 0.436717
2019-03-22 22:22:29,069 [INFO] ---------------------------------
2019-03-22 22:22:31,795 [INFO] ---------------------------------
2019-03-22 22:22:31,796 [INFO] Evaluation:
2019-03-22 22:22:31,798 [INFO] Batch 80000, worst loss 0.218438 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:22:31,799 [INFO] ---------------------------------
2019-03-22 22:22:46,455 [INFO] ---------------------------------
2019-03-22 22:22:46,456 [INFO] Summary:
2019-03-22 22:22:46,457 [INFO] Batch 81000, worst loss 0.228112 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:22:46,458 [INFO] Regularization: 2323.534912 * 0.0000010000 = 0.0023235348
2019-03-22 22:22:46,459 [INFO] Sum of grad norms: 0.004236
2019-03-22 22:22:46,460 [INFO] ---------------------------------
2019-03-22 22:23:01,716 [INFO] ---------------------------------
2019-03-22 22:23:01,716 [INFO] Summary:
2019-03-22 22:23:01,717 [INFO] Batch 82000, worst loss 0.224998 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:23:01,717 [INFO] Regularization: 2323.357422 * 0.0000010000 = 0.0023233574
2019-03-22 22:23:01,718 [INFO] Sum of grad norms: 0.004815
2019-03-22 22:23:01,718 [INFO] ---------------------------------
2019-03-22 22:23:16,559 [INFO] ---------------------------------
2019-03-22 22:23:16,560 [INFO] Summary:
2019-03-22 22:23:16,561 [INFO] Batch 83000, worst loss 0.247724 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:23:16,562 [INFO] Regularization: 2323.165039 * 0.0000010000 = 0.0023231651
2019-03-22 22:23:16,563 [INFO] Sum of grad norms: 5.469597
2019-03-22 22:23:16,564 [INFO] ---------------------------------
2019-03-22 22:23:30,495 [INFO] ---------------------------------
2019-03-22 22:23:30,495 [INFO] Summary:
2019-03-22 22:23:30,496 [INFO] Batch 84000, worst loss 0.281563 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:23:30,497 [INFO] Regularization: 2323.024902 * 0.0000010000 = 0.0023230249
2019-03-22 22:23:30,497 [INFO] Sum of grad norms: 0.811838
2019-03-22 22:23:30,498 [INFO] ---------------------------------
2019-03-22 22:23:44,527 [INFO] ---------------------------------
2019-03-22 22:23:44,528 [INFO] Summary:
2019-03-22 22:23:44,529 [INFO] Batch 85000, worst loss 0.217952 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:23:44,529 [INFO] Regularization: 2322.853271 * 0.0000010000 = 0.0023228533
2019-03-22 22:23:44,530 [INFO] Sum of grad norms: 0.104999
2019-03-22 22:23:44,531 [INFO] ---------------------------------
2019-03-22 22:23:59,349 [INFO] ---------------------------------
2019-03-22 22:23:59,349 [INFO] Summary:
2019-03-22 22:23:59,350 [INFO] Batch 86000, worst loss 0.236709 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:23:59,350 [INFO] Regularization: 2322.653076 * 0.0000010000 = 0.0023226531
2019-03-22 22:23:59,351 [INFO] Sum of grad norms: 37.424397
2019-03-22 22:23:59,351 [INFO] ---------------------------------
2019-03-22 22:24:14,452 [INFO] ---------------------------------
2019-03-22 22:24:14,453 [INFO] Summary:
2019-03-22 22:24:14,455 [INFO] Batch 87000, worst loss 0.281050 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:24:14,456 [INFO] Regularization: 2322.469238 * 0.0000010000 = 0.0023224691
2019-03-22 22:24:14,457 [INFO] Sum of grad norms: 11.350451
2019-03-22 22:24:14,458 [INFO] ---------------------------------
2019-03-22 22:24:28,497 [INFO] ---------------------------------
2019-03-22 22:24:28,498 [INFO] Summary:
2019-03-22 22:24:28,499 [INFO] Batch 88000, worst loss 0.285248 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:24:28,499 [INFO] Regularization: 2322.335449 * 0.0000010000 = 0.0023223355
2019-03-22 22:24:28,500 [INFO] Sum of grad norms: 0.002708
2019-03-22 22:24:28,500 [INFO] ---------------------------------
2019-03-22 22:24:42,813 [INFO] ---------------------------------
2019-03-22 22:24:42,814 [INFO] Summary:
2019-03-22 22:24:42,815 [INFO] Batch 89000, worst loss 0.283470 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:24:42,815 [INFO] Regularization: 2322.205566 * 0.0000010000 = 0.0023222056
2019-03-22 22:24:42,816 [INFO] Sum of grad norms: 0.021144
2019-03-22 22:24:42,817 [INFO] ---------------------------------
2019-03-22 22:24:57,750 [INFO] ---------------------------------
2019-03-22 22:24:57,751 [INFO] Summary:
2019-03-22 22:24:57,752 [INFO] Batch 90000, worst loss 0.184371 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 2
2019-03-22 22:24:57,752 [INFO] Regularization: 2322.036133 * 0.0000010000 = 0.0023220361
2019-03-22 22:24:57,753 [INFO] Sum of grad norms: 4.237026
2019-03-22 22:24:57,753 [INFO] ---------------------------------
2019-03-22 22:25:00,500 [INFO] ---------------------------------
2019-03-22 22:25:00,501 [INFO] Evaluation:
2019-03-22 22:25:00,502 [INFO] Batch 90000, worst loss 0.182002 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:25:00,503 [INFO] ---------------------------------
2019-03-22 22:25:14,885 [INFO] ---------------------------------
2019-03-22 22:25:14,885 [INFO] Summary:
2019-03-22 22:25:14,886 [INFO] Batch 91000, worst loss 0.216498 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:25:14,886 [INFO] Regularization: 2321.892090 * 0.0000010000 = 0.0023218922
2019-03-22 22:25:14,887 [INFO] Sum of grad norms: 22.343569
2019-03-22 22:25:14,888 [INFO] ---------------------------------
2019-03-22 22:25:28,993 [INFO] ---------------------------------
2019-03-22 22:25:28,994 [INFO] Summary:
2019-03-22 22:25:28,995 [INFO] Batch 92000, worst loss 0.212395 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:25:28,995 [INFO] Regularization: 2321.837646 * 0.0000010000 = 0.0023218377
2019-03-22 22:25:28,996 [INFO] Sum of grad norms: 31.064604
2019-03-22 22:25:28,996 [INFO] ---------------------------------
2019-03-22 22:25:43,736 [INFO] ---------------------------------
2019-03-22 22:25:43,737 [INFO] Summary:
2019-03-22 22:25:43,737 [INFO] Batch 93000, worst loss 0.208571 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:25:43,738 [INFO] Regularization: 2321.793457 * 0.0000010000 = 0.0023217935
2019-03-22 22:25:43,738 [INFO] Sum of grad norms: 0.004216
2019-03-22 22:25:43,739 [INFO] ---------------------------------
2019-03-22 22:25:57,913 [INFO] ---------------------------------
2019-03-22 22:25:57,914 [INFO] Summary:
2019-03-22 22:25:57,915 [INFO] Batch 94000, worst loss 0.211591 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:25:57,916 [INFO] Regularization: 2321.755371 * 0.0000010000 = 0.0023217553
2019-03-22 22:25:57,916 [INFO] Sum of grad norms: 0.029234
2019-03-22 22:25:57,917 [INFO] ---------------------------------
2019-03-22 22:26:12,830 [INFO] ---------------------------------
2019-03-22 22:26:12,831 [INFO] Summary:
2019-03-22 22:26:12,832 [INFO] Batch 95000, worst loss 0.269555 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:26:12,833 [INFO] Regularization: 2321.713623 * 0.0000010000 = 0.0023217136
2019-03-22 22:26:12,834 [INFO] Sum of grad norms: 3.871556
2019-03-22 22:26:12,834 [INFO] ---------------------------------
2019-03-22 22:26:27,391 [INFO] ---------------------------------
2019-03-22 22:26:27,392 [INFO] Summary:
2019-03-22 22:26:27,392 [INFO] Batch 96000, worst loss 0.207480 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:26:27,393 [INFO] Regularization: 2321.674316 * 0.0000010000 = 0.0023216743
2019-03-22 22:26:27,393 [INFO] Sum of grad norms: 0.289229
2019-03-22 22:26:27,394 [INFO] ---------------------------------
2019-03-22 22:26:42,611 [INFO] ---------------------------------
2019-03-22 22:26:42,612 [INFO] Summary:
2019-03-22 22:26:42,613 [INFO] Batch 97000, worst loss 0.225892 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:26:42,613 [INFO] Regularization: 2321.625732 * 0.0000010000 = 0.0023216258
2019-03-22 22:26:42,614 [INFO] Sum of grad norms: 0.221502
2019-03-22 22:26:42,615 [INFO] ---------------------------------
2019-03-22 22:26:57,801 [INFO] ---------------------------------
2019-03-22 22:26:57,802 [INFO] Summary:
2019-03-22 22:26:57,802 [INFO] Batch 98000, worst loss 0.339286 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:26:57,803 [INFO] Regularization: 2321.578613 * 0.0000010000 = 0.0023215786
2019-03-22 22:26:57,803 [INFO] Sum of grad norms: 0.662917
2019-03-22 22:26:57,804 [INFO] ---------------------------------
2019-03-22 22:27:12,381 [INFO] ---------------------------------
2019-03-22 22:27:12,382 [INFO] Summary:
2019-03-22 22:27:12,384 [INFO] Batch 99000, worst loss 0.210904 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:27:12,384 [INFO] Regularization: 2321.542969 * 0.0000010000 = 0.0023215429
2019-03-22 22:27:12,386 [INFO] Sum of grad norms: 0.016979
2019-03-22 22:27:12,387 [INFO] ---------------------------------
2019-03-22 22:27:27,127 [INFO] ---------------------------------
2019-03-22 22:27:27,128 [INFO] Summary:
2019-03-22 22:27:27,128 [INFO] Batch 100000, worst loss 0.235092 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 2
2019-03-22 22:27:27,129 [INFO] Regularization: 2321.506592 * 0.0000010000 = 0.0023215066
2019-03-22 22:27:27,130 [INFO] Sum of grad norms: 23.811117
2019-03-22 22:27:27,130 [INFO] ---------------------------------
2019-03-22 22:27:29,919 [INFO] ---------------------------------
2019-03-22 22:27:29,919 [INFO] Evaluation:
2019-03-22 22:27:29,920 [INFO] Batch 100000, worst loss 0.218191 (without reg.) of 1000 batches @cl.-depth 2
2019-03-22 22:27:29,921 [INFO] ---------------------------------
2019-03-22 22:27:29,922 [INFO] Finished training, saved to file classifier/1553275201/1553290049_9_classifier_final.pth
