2019-03-18 19:25:39,444 [INFO] learning_rate_initialization: 0.005000, learning_rate_loss_factor: 0.010000, learning_rate_decay_after: 30000, learning_rate_decay_at: 10000, learning_rate_decay_factor: 0.500000
2019-03-18 19:25:39,445 [INFO] regularization factor: 0.0010000000
2019-03-18 19:25:39,545 [INFO] ---------------------------------
2019-03-18 19:25:39,546 [INFO] Training model #0: (1, 64, 201) @ 1
2019-03-18 19:25:55,978 [INFO] ---------------------------------
2019-03-18 19:25:55,979 [INFO] Summary:
2019-03-18 19:25:55,980 [INFO] Batch 1000, worst loss 91.055054 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-18 19:25:55,981 [INFO] Regularization: 838.652344 * 0.0010000000 = 0.8386523724
2019-03-18 19:25:55,981 [INFO] Sum of grad norms: 1.943372
2019-03-18 19:25:55,982 [INFO] ---------------------------------
2019-03-18 19:26:12,553 [INFO] ---------------------------------
2019-03-18 19:26:12,554 [INFO] Summary:
2019-03-18 19:26:12,555 [INFO] Batch 2000, worst loss 1.023419 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-18 19:26:12,555 [INFO] Regularization: 222.467285 * 0.0010000000 = 0.2224672884
2019-03-18 19:26:12,556 [INFO] reducing reg_loss_factor
2019-03-18 19:26:12,556 [INFO] Sum of grad norms: 2.473591
2019-03-18 19:26:12,557 [INFO] ---------------------------------
2019-03-18 19:26:28,419 [INFO] ---------------------------------
2019-03-18 19:26:28,420 [INFO] Summary:
2019-03-18 19:26:28,421 [INFO] Batch 3000, worst loss 0.157204 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-18 19:26:28,421 [INFO] Regularization: 213.225342 * 0.0001000000 = 0.0213225335
2019-03-18 19:26:28,422 [INFO] reducing reg_loss_factor
2019-03-18 19:26:28,422 [INFO] Sum of grad norms: 0.389218
2019-03-18 19:26:28,423 [INFO] ---------------------------------
2019-03-18 19:26:47,258 [INFO] ---------------------------------
2019-03-18 19:26:47,259 [INFO] Summary:
2019-03-18 19:26:47,260 [INFO] Batch 4000, worst loss 0.126347 (incl. reg.) of 1000 batches, learning rate 0.001572 @cl.-depth 1
2019-03-18 19:26:47,260 [INFO] Regularization: 176.133224 * 0.0000100000 = 0.0017613322
2019-03-18 19:26:47,261 [INFO] Sum of grad norms: 0.113752
2019-03-18 19:26:47,262 [INFO] ---------------------------------
2019-03-18 19:27:06,082 [INFO] ---------------------------------
2019-03-18 19:27:06,083 [INFO] Summary:
2019-03-18 19:27:06,084 [INFO] Batch 5000, worst loss 0.119119 (incl. reg.) of 1000 batches, learning rate 0.001263 @cl.-depth 1
2019-03-18 19:27:06,084 [INFO] Regularization: 183.017563 * 0.0000100000 = 0.0018301756
2019-03-18 19:27:06,085 [INFO] Sum of grad norms: 0.185248
2019-03-18 19:27:06,086 [INFO] ---------------------------------
2019-03-18 19:27:25,159 [INFO] ---------------------------------
2019-03-18 19:27:25,160 [INFO] Summary:
2019-03-18 19:27:25,161 [INFO] Batch 6000, worst loss 0.120253 (incl. reg.) of 1000 batches, learning rate 0.001191 @cl.-depth 1
2019-03-18 19:27:25,161 [INFO] Regularization: 190.103989 * 0.0000100000 = 0.0019010399
2019-03-18 19:27:25,162 [INFO] Sum of grad norms: 0.249888
2019-03-18 19:27:25,163 [INFO] ---------------------------------
2019-03-18 19:27:44,308 [INFO] ---------------------------------
2019-03-18 19:27:44,309 [INFO] Summary:
2019-03-18 19:27:44,310 [INFO] Batch 7000, worst loss 0.115411 (incl. reg.) of 1000 batches, learning rate 0.001191 @cl.-depth 1
2019-03-18 19:27:44,310 [INFO] Regularization: 199.782944 * 0.0000100000 = 0.0019978294
2019-03-18 19:27:44,311 [INFO] Sum of grad norms: 0.241953
2019-03-18 19:27:44,311 [INFO] ---------------------------------
2019-03-18 19:28:03,162 [INFO] ---------------------------------
2019-03-18 19:28:03,163 [INFO] Summary:
2019-03-18 19:28:03,164 [INFO] Batch 8000, worst loss 0.118992 (incl. reg.) of 1000 batches, learning rate 0.001154 @cl.-depth 1
2019-03-18 19:28:03,164 [INFO] Regularization: 208.670700 * 0.0000100000 = 0.0020867069
2019-03-18 19:28:03,165 [INFO] Sum of grad norms: 0.112428
2019-03-18 19:28:03,165 [INFO] ---------------------------------
2019-03-18 19:28:21,661 [INFO] ---------------------------------
2019-03-18 19:28:21,662 [INFO] Summary:
2019-03-18 19:28:21,663 [INFO] Batch 9000, worst loss 0.116490 (incl. reg.) of 1000 batches, learning rate 0.001154 @cl.-depth 1
2019-03-18 19:28:21,664 [INFO] Regularization: 215.703186 * 0.0000100000 = 0.0021570318
2019-03-18 19:28:21,664 [INFO] Sum of grad norms: 0.227019
2019-03-18 19:28:21,665 [INFO] ---------------------------------
2019-03-18 19:28:40,464 [INFO] ---------------------------------
2019-03-18 19:28:40,464 [INFO] Summary:
2019-03-18 19:28:40,465 [INFO] Batch 10000, worst loss 0.119233 (incl. reg.) of 1000 batches, learning rate 0.001154 @cl.-depth 1
2019-03-18 19:28:40,466 [INFO] Regularization: 230.429474 * 0.0000100000 = 0.0023042946
2019-03-18 19:28:40,466 [INFO] Sum of grad norms: 0.270735
2019-03-18 19:28:40,467 [INFO] ---------------------------------
2019-03-18 19:28:45,389 [INFO] ---------------------------------
2019-03-18 19:28:45,390 [INFO] Evaluation:
2019-03-18 19:28:45,391 [INFO] Batch 10000, worst loss 0.114864 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 19:28:45,392 [INFO] ---------------------------------
2019-03-18 19:29:04,159 [INFO] ---------------------------------
2019-03-18 19:29:04,160 [INFO] Summary:
2019-03-18 19:29:04,160 [INFO] Batch 11000, worst loss 0.119286 (incl. reg.) of 1000 batches, learning rate 0.001154 @cl.-depth 1
2019-03-18 19:29:04,161 [INFO] Regularization: 234.668762 * 0.0000100000 = 0.0023466875
2019-03-18 19:29:04,161 [INFO] Sum of grad norms: 0.217168
2019-03-18 19:29:04,162 [INFO] ---------------------------------
2019-03-18 19:29:23,009 [INFO] ---------------------------------
2019-03-18 19:29:23,010 [INFO] Summary:
2019-03-18 19:29:23,010 [INFO] Batch 12000, worst loss 0.116683 (incl. reg.) of 1000 batches, learning rate 0.001154 @cl.-depth 1
2019-03-18 19:29:23,011 [INFO] Regularization: 303.919373 * 0.0000100000 = 0.0030391936
2019-03-18 19:29:23,012 [INFO] Sum of grad norms: 0.160389
2019-03-18 19:29:23,012 [INFO] ---------------------------------
2019-03-18 19:29:41,661 [INFO] ---------------------------------
2019-03-18 19:29:41,662 [INFO] Summary:
2019-03-18 19:29:41,662 [INFO] Batch 13000, worst loss 0.105716 (incl. reg.) of 1000 batches, learning rate 0.001154 @cl.-depth 1
2019-03-18 19:29:41,663 [INFO] Regularization: 290.008636 * 0.0000100000 = 0.0029000863
2019-03-18 19:29:41,663 [INFO] Sum of grad norms: 0.337833
2019-03-18 19:29:41,664 [INFO] ---------------------------------
2019-03-18 19:30:00,538 [INFO] ---------------------------------
2019-03-18 19:30:00,539 [INFO] Summary:
2019-03-18 19:30:00,540 [INFO] Batch 14000, worst loss 0.109051 (incl. reg.) of 1000 batches, learning rate 0.001057 @cl.-depth 1
2019-03-18 19:30:00,540 [INFO] Regularization: 301.443512 * 0.0000100000 = 0.0030144351
2019-03-18 19:30:00,541 [INFO] Sum of grad norms: 0.245746
2019-03-18 19:30:00,541 [INFO] ---------------------------------
2019-03-18 19:30:19,248 [INFO] ---------------------------------
2019-03-18 19:30:19,249 [INFO] Summary:
2019-03-18 19:30:19,249 [INFO] Batch 15000, worst loss 0.103199 (incl. reg.) of 1000 batches, learning rate 0.001057 @cl.-depth 1
2019-03-18 19:30:19,250 [INFO] Regularization: 320.333374 * 0.0000100000 = 0.0032033336
2019-03-18 19:30:19,250 [INFO] Sum of grad norms: 0.161287
2019-03-18 19:30:19,251 [INFO] ---------------------------------
2019-03-18 19:30:38,201 [INFO] ---------------------------------
2019-03-18 19:30:38,203 [INFO] Summary:
2019-03-18 19:30:38,204 [INFO] Batch 16000, worst loss 0.095975 (incl. reg.) of 1000 batches, learning rate 0.001032 @cl.-depth 1
2019-03-18 19:30:38,204 [INFO] Regularization: 337.528320 * 0.0000100000 = 0.0033752832
2019-03-18 19:30:38,205 [INFO] Sum of grad norms: 0.172339
2019-03-18 19:30:38,206 [INFO] ---------------------------------
2019-03-18 19:30:56,955 [INFO] ---------------------------------
2019-03-18 19:30:56,957 [INFO] Summary:
2019-03-18 19:30:56,957 [INFO] Batch 17000, worst loss 0.096481 (incl. reg.) of 1000 batches, learning rate 0.000960 @cl.-depth 1
2019-03-18 19:30:56,958 [INFO] Regularization: 343.081268 * 0.0000100000 = 0.0034308126
2019-03-18 19:30:56,958 [INFO] Sum of grad norms: 0.332797
2019-03-18 19:30:56,959 [INFO] ---------------------------------
2019-03-18 19:31:15,635 [INFO] ---------------------------------
2019-03-18 19:31:15,636 [INFO] Summary:
2019-03-18 19:31:15,637 [INFO] Batch 18000, worst loss 0.093230 (incl. reg.) of 1000 batches, learning rate 0.000960 @cl.-depth 1
2019-03-18 19:31:15,637 [INFO] Regularization: 350.375488 * 0.0000100000 = 0.0035037547
2019-03-18 19:31:15,638 [INFO] Sum of grad norms: 0.249783
2019-03-18 19:31:15,638 [INFO] ---------------------------------
2019-03-18 19:31:34,633 [INFO] ---------------------------------
2019-03-18 19:31:34,634 [INFO] Summary:
2019-03-18 19:31:34,634 [INFO] Batch 19000, worst loss 0.090975 (incl. reg.) of 1000 batches, learning rate 0.000932 @cl.-depth 1
2019-03-18 19:31:34,635 [INFO] Regularization: 366.543610 * 0.0000100000 = 0.0036654361
2019-03-18 19:31:34,635 [INFO] Sum of grad norms: 0.217723
2019-03-18 19:31:34,636 [INFO] ---------------------------------
2019-03-18 19:31:53,423 [INFO] ---------------------------------
2019-03-18 19:31:53,424 [INFO] Summary:
2019-03-18 19:31:53,425 [INFO] Batch 20000, worst loss 0.087567 (incl. reg.) of 1000 batches, learning rate 0.000910 @cl.-depth 1
2019-03-18 19:31:53,425 [INFO] Regularization: 378.233063 * 0.0000100000 = 0.0037823305
2019-03-18 19:31:53,426 [INFO] Sum of grad norms: 0.171867
2019-03-18 19:31:53,426 [INFO] ---------------------------------
2019-03-18 19:31:58,336 [INFO] ---------------------------------
2019-03-18 19:31:58,337 [INFO] Evaluation:
2019-03-18 19:31:58,337 [INFO] Batch 20000, worst loss 0.083149 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 19:31:58,338 [INFO] ---------------------------------
2019-03-18 19:32:17,112 [INFO] ---------------------------------
2019-03-18 19:32:17,113 [INFO] Summary:
2019-03-18 19:32:17,114 [INFO] Batch 21000, worst loss 0.085894 (incl. reg.) of 1000 batches, learning rate 0.000876 @cl.-depth 1
2019-03-18 19:32:17,114 [INFO] Regularization: 380.785400 * 0.0000100000 = 0.0038078539
2019-03-18 19:32:17,115 [INFO] Sum of grad norms: 0.182431
2019-03-18 19:32:17,115 [INFO] ---------------------------------
2019-03-18 19:32:35,793 [INFO] ---------------------------------
2019-03-18 19:32:35,794 [INFO] Summary:
2019-03-18 19:32:35,795 [INFO] Batch 22000, worst loss 0.086904 (incl. reg.) of 1000 batches, learning rate 0.000859 @cl.-depth 1
2019-03-18 19:32:35,795 [INFO] Regularization: 382.256317 * 0.0000100000 = 0.0038225630
2019-03-18 19:32:35,796 [INFO] Sum of grad norms: 0.302018
2019-03-18 19:32:35,797 [INFO] ---------------------------------
2019-03-18 19:32:54,535 [INFO] ---------------------------------
2019-03-18 19:32:54,535 [INFO] Summary:
2019-03-18 19:32:54,536 [INFO] Batch 23000, worst loss 0.084810 (incl. reg.) of 1000 batches, learning rate 0.000859 @cl.-depth 1
2019-03-18 19:32:54,536 [INFO] Regularization: 378.898376 * 0.0000100000 = 0.0037889837
2019-03-18 19:32:54,537 [INFO] Sum of grad norms: 0.188255
2019-03-18 19:32:54,537 [INFO] ---------------------------------
2019-03-18 19:33:13,445 [INFO] ---------------------------------
2019-03-18 19:33:13,446 [INFO] Summary:
2019-03-18 19:33:13,446 [INFO] Batch 24000, worst loss 0.086365 (incl. reg.) of 1000 batches, learning rate 0.000848 @cl.-depth 1
2019-03-18 19:33:13,447 [INFO] Regularization: 377.111176 * 0.0000100000 = 0.0037711116
2019-03-18 19:33:13,447 [INFO] Sum of grad norms: 0.298018
2019-03-18 19:33:13,448 [INFO] ---------------------------------
2019-03-18 19:33:32,606 [INFO] ---------------------------------
2019-03-18 19:33:32,607 [INFO] Summary:
2019-03-18 19:33:32,608 [INFO] Batch 25000, worst loss 0.085201 (incl. reg.) of 1000 batches, learning rate 0.000848 @cl.-depth 1
2019-03-18 19:33:32,609 [INFO] Regularization: 372.583435 * 0.0000100000 = 0.0037258342
2019-03-18 19:33:32,609 [INFO] Sum of grad norms: 0.203514
2019-03-18 19:33:32,610 [INFO] ---------------------------------
2019-03-18 19:33:51,347 [INFO] ---------------------------------
2019-03-18 19:33:51,348 [INFO] Summary:
2019-03-18 19:33:51,348 [INFO] Batch 26000, worst loss 0.085134 (incl. reg.) of 1000 batches, learning rate 0.000848 @cl.-depth 1
2019-03-18 19:33:51,349 [INFO] Regularization: 373.022705 * 0.0000100000 = 0.0037302270
2019-03-18 19:33:51,349 [INFO] Sum of grad norms: 0.241939
2019-03-18 19:33:51,350 [INFO] ---------------------------------
2019-03-18 19:34:10,016 [INFO] ---------------------------------
2019-03-18 19:34:10,017 [INFO] Summary:
2019-03-18 19:34:10,018 [INFO] Batch 27000, worst loss 0.086385 (incl. reg.) of 1000 batches, learning rate 0.000848 @cl.-depth 1
2019-03-18 19:34:10,018 [INFO] Regularization: 373.903717 * 0.0000100000 = 0.0037390371
2019-03-18 19:34:10,019 [INFO] Sum of grad norms: 0.164218
2019-03-18 19:34:10,019 [INFO] ---------------------------------
2019-03-18 19:34:28,770 [INFO] ---------------------------------
2019-03-18 19:34:28,771 [INFO] Summary:
2019-03-18 19:34:28,771 [INFO] Batch 28000, worst loss 0.085807 (incl. reg.) of 1000 batches, learning rate 0.000848 @cl.-depth 1
2019-03-18 19:34:28,772 [INFO] Regularization: 368.475830 * 0.0000100000 = 0.0036847582
2019-03-18 19:34:28,773 [INFO] Sum of grad norms: 0.115900
2019-03-18 19:34:28,773 [INFO] ---------------------------------
2019-03-18 19:34:47,605 [INFO] ---------------------------------
2019-03-18 19:34:47,606 [INFO] Summary:
2019-03-18 19:34:47,607 [INFO] Batch 29000, worst loss 0.084811 (incl. reg.) of 1000 batches, learning rate 0.000848 @cl.-depth 1
2019-03-18 19:34:47,607 [INFO] Regularization: 366.997223 * 0.0000100000 = 0.0036699721
2019-03-18 19:34:47,608 [INFO] Sum of grad norms: 0.231639
2019-03-18 19:34:47,608 [INFO] ---------------------------------
2019-03-18 19:35:06,659 [INFO] ---------------------------------
2019-03-18 19:35:06,660 [INFO] Summary:
2019-03-18 19:35:06,661 [INFO] Batch 30000, worst loss 0.084421 (incl. reg.) of 1000 batches, learning rate 0.000848 @cl.-depth 1
2019-03-18 19:35:06,662 [INFO] Regularization: 363.306824 * 0.0000100000 = 0.0036330682
2019-03-18 19:35:06,663 [INFO] Sum of grad norms: 0.195490
2019-03-18 19:35:06,663 [INFO] ---------------------------------
2019-03-18 19:35:11,499 [INFO] ---------------------------------
2019-03-18 19:35:11,500 [INFO] Evaluation:
2019-03-18 19:35:11,501 [INFO] Batch 30000, worst loss 0.082427 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 19:35:11,502 [INFO] ---------------------------------
2019-03-18 19:35:30,326 [INFO] ---------------------------------
2019-03-18 19:35:30,327 [INFO] Summary:
2019-03-18 19:35:30,327 [INFO] Batch 31000, worst loss 0.083646 (incl. reg.) of 1000 batches, learning rate 0.000844 @cl.-depth 1
2019-03-18 19:35:30,328 [INFO] Regularization: 361.790894 * 0.0000100000 = 0.0036179088
2019-03-18 19:35:30,328 [INFO] Sum of grad norms: 0.155685
2019-03-18 19:35:30,329 [INFO] ---------------------------------
2019-03-18 19:35:48,842 [INFO] ---------------------------------
2019-03-18 19:35:48,843 [INFO] Summary:
2019-03-18 19:35:48,844 [INFO] Batch 32000, worst loss 0.084768 (incl. reg.) of 1000 batches, learning rate 0.000836 @cl.-depth 1
2019-03-18 19:35:48,844 [INFO] Regularization: 360.727051 * 0.0000100000 = 0.0036072703
2019-03-18 19:35:48,845 [INFO] Sum of grad norms: 0.125989
2019-03-18 19:35:48,846 [INFO] ---------------------------------
2019-03-18 19:36:07,402 [INFO] ---------------------------------
2019-03-18 19:36:07,403 [INFO] Summary:
2019-03-18 19:36:07,403 [INFO] Batch 33000, worst loss 0.083853 (incl. reg.) of 1000 batches, learning rate 0.000836 @cl.-depth 1
2019-03-18 19:36:07,404 [INFO] Regularization: 355.988220 * 0.0000100000 = 0.0035598821
2019-03-18 19:36:07,404 [INFO] Sum of grad norms: 0.354237
2019-03-18 19:36:07,405 [INFO] ---------------------------------
2019-03-18 19:36:26,283 [INFO] ---------------------------------
2019-03-18 19:36:26,284 [INFO] Summary:
2019-03-18 19:36:26,284 [INFO] Batch 34000, worst loss 0.085658 (incl. reg.) of 1000 batches, learning rate 0.000836 @cl.-depth 1
2019-03-18 19:36:26,285 [INFO] Regularization: 357.703735 * 0.0000100000 = 0.0035770372
2019-03-18 19:36:26,286 [INFO] Sum of grad norms: 0.179955
2019-03-18 19:36:26,286 [INFO] ---------------------------------
2019-03-18 19:36:45,174 [INFO] ---------------------------------
2019-03-18 19:36:45,175 [INFO] Summary:
2019-03-18 19:36:45,175 [INFO] Batch 35000, worst loss 0.084741 (incl. reg.) of 1000 batches, learning rate 0.000836 @cl.-depth 1
2019-03-18 19:36:45,176 [INFO] Regularization: 356.503876 * 0.0000100000 = 0.0035650386
2019-03-18 19:36:45,176 [INFO] Sum of grad norms: 0.216054
2019-03-18 19:36:45,177 [INFO] ---------------------------------
2019-03-18 19:37:04,046 [INFO] ---------------------------------
2019-03-18 19:37:04,047 [INFO] Summary:
2019-03-18 19:37:04,047 [INFO] Batch 36000, worst loss 0.085780 (incl. reg.) of 1000 batches, learning rate 0.000836 @cl.-depth 1
2019-03-18 19:37:04,048 [INFO] Regularization: 352.949951 * 0.0000100000 = 0.0035294995
2019-03-18 19:37:04,049 [INFO] Sum of grad norms: 0.125405
2019-03-18 19:37:04,049 [INFO] ---------------------------------
2019-03-18 19:37:22,497 [INFO] ---------------------------------
2019-03-18 19:37:22,498 [INFO] Summary:
2019-03-18 19:37:22,499 [INFO] Batch 37000, worst loss 0.083977 (incl. reg.) of 1000 batches, learning rate 0.000836 @cl.-depth 1
2019-03-18 19:37:22,499 [INFO] Regularization: 355.335083 * 0.0000100000 = 0.0035533507
2019-03-18 19:37:22,500 [INFO] Sum of grad norms: 0.199510
2019-03-18 19:37:22,501 [INFO] ---------------------------------
2019-03-18 19:37:40,991 [INFO] ---------------------------------
2019-03-18 19:37:40,992 [INFO] Summary:
2019-03-18 19:37:40,993 [INFO] Batch 38000, worst loss 0.084717 (incl. reg.) of 1000 batches, learning rate 0.000836 @cl.-depth 1
2019-03-18 19:37:40,993 [INFO] Regularization: 352.089447 * 0.0000100000 = 0.0035208943
2019-03-18 19:37:40,994 [INFO] Sum of grad norms: 0.186612
2019-03-18 19:37:40,994 [INFO] ---------------------------------
2019-03-18 19:37:59,983 [INFO] ---------------------------------
2019-03-18 19:37:59,984 [INFO] Summary:
2019-03-18 19:37:59,984 [INFO] Batch 39000, worst loss 0.084379 (incl. reg.) of 1000 batches, learning rate 0.000836 @cl.-depth 1
2019-03-18 19:37:59,985 [INFO] Regularization: 348.870483 * 0.0000100000 = 0.0034887048
2019-03-18 19:37:59,985 [INFO] Sum of grad norms: 0.181678
2019-03-18 19:37:59,986 [INFO] ---------------------------------
2019-03-18 19:38:18,768 [INFO] ---------------------------------
2019-03-18 19:38:18,769 [INFO] Summary:
2019-03-18 19:38:18,770 [INFO] Batch 40000, worst loss 0.084179 (incl. reg.) of 1000 batches, learning rate 0.000836 @cl.-depth 1
2019-03-18 19:38:18,770 [INFO] Regularization: 347.952148 * 0.0000100000 = 0.0034795215
2019-03-18 19:38:18,771 [INFO] Sum of grad norms: 0.145515
2019-03-18 19:38:18,771 [INFO] ---------------------------------
2019-03-18 19:38:23,703 [INFO] ---------------------------------
2019-03-18 19:38:23,704 [INFO] Evaluation:
2019-03-18 19:38:23,704 [INFO] Batch 40000, worst loss 0.080424 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 19:38:23,705 [INFO] ---------------------------------
2019-03-18 19:38:42,742 [INFO] ---------------------------------
2019-03-18 19:38:42,742 [INFO] Summary:
2019-03-18 19:38:42,743 [INFO] Batch 41000, worst loss 0.084725 (incl. reg.) of 1000 batches, learning rate 0.000418 @cl.-depth 1
2019-03-18 19:38:42,743 [INFO] Regularization: 347.806915 * 0.0000100000 = 0.0034780691
2019-03-18 19:38:42,744 [INFO] Sum of grad norms: 0.297608
2019-03-18 19:38:42,745 [INFO] ---------------------------------
2019-03-18 19:39:01,697 [INFO] ---------------------------------
2019-03-18 19:39:01,698 [INFO] Summary:
2019-03-18 19:39:01,698 [INFO] Batch 42000, worst loss 0.084788 (incl. reg.) of 1000 batches, learning rate 0.000418 @cl.-depth 1
2019-03-18 19:39:01,699 [INFO] Regularization: 340.198700 * 0.0000100000 = 0.0034019870
2019-03-18 19:39:01,699 [INFO] Sum of grad norms: 0.127007
2019-03-18 19:39:01,700 [INFO] ---------------------------------
2019-03-18 19:39:20,761 [INFO] ---------------------------------
2019-03-18 19:39:20,762 [INFO] Summary:
2019-03-18 19:39:20,763 [INFO] Batch 43000, worst loss 0.083744 (incl. reg.) of 1000 batches, learning rate 0.000418 @cl.-depth 1
2019-03-18 19:39:20,763 [INFO] Regularization: 339.454803 * 0.0000100000 = 0.0033945478
2019-03-18 19:39:20,764 [INFO] Sum of grad norms: 0.101592
2019-03-18 19:39:20,765 [INFO] ---------------------------------
2019-03-18 19:39:39,633 [INFO] ---------------------------------
2019-03-18 19:39:39,634 [INFO] Summary:
2019-03-18 19:39:39,635 [INFO] Batch 44000, worst loss 0.083850 (incl. reg.) of 1000 batches, learning rate 0.000418 @cl.-depth 1
2019-03-18 19:39:39,635 [INFO] Regularization: 338.803192 * 0.0000100000 = 0.0033880319
2019-03-18 19:39:39,636 [INFO] Sum of grad norms: 0.162604
2019-03-18 19:39:39,637 [INFO] ---------------------------------
2019-03-18 19:39:58,331 [INFO] ---------------------------------
2019-03-18 19:39:58,332 [INFO] Summary:
2019-03-18 19:39:58,333 [INFO] Batch 45000, worst loss 0.084429 (incl. reg.) of 1000 batches, learning rate 0.000418 @cl.-depth 1
2019-03-18 19:39:58,333 [INFO] Regularization: 337.661438 * 0.0000100000 = 0.0033766143
2019-03-18 19:39:58,334 [INFO] Sum of grad norms: 0.125688
2019-03-18 19:39:58,334 [INFO] ---------------------------------
2019-03-18 19:40:16,936 [INFO] ---------------------------------
2019-03-18 19:40:16,937 [INFO] Summary:
2019-03-18 19:40:16,938 [INFO] Batch 46000, worst loss 0.084001 (incl. reg.) of 1000 batches, learning rate 0.000418 @cl.-depth 1
2019-03-18 19:40:16,938 [INFO] Regularization: 337.381683 * 0.0000100000 = 0.0033738168
2019-03-18 19:40:16,939 [INFO] Sum of grad norms: 0.143363
2019-03-18 19:40:16,939 [INFO] ---------------------------------
2019-03-18 19:40:35,487 [INFO] ---------------------------------
2019-03-18 19:40:35,488 [INFO] Summary:
2019-03-18 19:40:35,489 [INFO] Batch 47000, worst loss 0.085056 (incl. reg.) of 1000 batches, learning rate 0.000418 @cl.-depth 1
2019-03-18 19:40:35,490 [INFO] Regularization: 335.401398 * 0.0000100000 = 0.0033540139
2019-03-18 19:40:35,490 [INFO] Sum of grad norms: 0.117181
2019-03-18 19:40:35,491 [INFO] ---------------------------------
2019-03-18 19:40:54,502 [INFO] ---------------------------------
2019-03-18 19:40:54,503 [INFO] Summary:
2019-03-18 19:40:54,503 [INFO] Batch 48000, worst loss 0.083055 (incl. reg.) of 1000 batches, learning rate 0.000418 @cl.-depth 1
2019-03-18 19:40:54,504 [INFO] Regularization: 334.408691 * 0.0000100000 = 0.0033440869
2019-03-18 19:40:54,504 [INFO] Sum of grad norms: 0.120168
2019-03-18 19:40:54,505 [INFO] ---------------------------------
2019-03-18 19:41:13,396 [INFO] ---------------------------------
2019-03-18 19:41:13,397 [INFO] Summary:
2019-03-18 19:41:13,397 [INFO] Batch 49000, worst loss 0.083690 (incl. reg.) of 1000 batches, learning rate 0.000418 @cl.-depth 1
2019-03-18 19:41:13,398 [INFO] Regularization: 334.288269 * 0.0000100000 = 0.0033428825
2019-03-18 19:41:13,399 [INFO] Sum of grad norms: 0.128573
2019-03-18 19:41:13,399 [INFO] ---------------------------------
2019-03-18 19:41:31,681 [INFO] ---------------------------------
2019-03-18 19:41:31,682 [INFO] Summary:
2019-03-18 19:41:31,682 [INFO] Batch 50000, worst loss 0.084301 (incl. reg.) of 1000 batches, learning rate 0.000418 @cl.-depth 1
2019-03-18 19:41:31,683 [INFO] Regularization: 334.094849 * 0.0000100000 = 0.0033409484
2019-03-18 19:41:31,683 [INFO] Sum of grad norms: 0.188652
2019-03-18 19:41:31,684 [INFO] ---------------------------------
2019-03-18 19:41:36,637 [INFO] ---------------------------------
2019-03-18 19:41:36,638 [INFO] Evaluation:
2019-03-18 19:41:36,641 [INFO] Batch 50000, worst loss 0.080710 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 19:41:36,643 [INFO] ---------------------------------
2019-03-18 19:41:55,636 [INFO] ---------------------------------
2019-03-18 19:41:55,637 [INFO] Summary:
2019-03-18 19:41:55,638 [INFO] Batch 51000, worst loss 0.083282 (incl. reg.) of 1000 batches, learning rate 0.000209 @cl.-depth 1
2019-03-18 19:41:55,638 [INFO] Regularization: 332.881378 * 0.0000100000 = 0.0033288137
2019-03-18 19:41:55,639 [INFO] Sum of grad norms: 0.099712
2019-03-18 19:41:55,639 [INFO] ---------------------------------
2019-03-18 19:42:14,071 [INFO] ---------------------------------
2019-03-18 19:42:14,072 [INFO] Summary:
2019-03-18 19:42:14,073 [INFO] Batch 52000, worst loss 0.083453 (incl. reg.) of 1000 batches, learning rate 0.000209 @cl.-depth 1
2019-03-18 19:42:14,073 [INFO] Regularization: 329.048676 * 0.0000100000 = 0.0032904868
2019-03-18 19:42:14,074 [INFO] Sum of grad norms: 0.280535
2019-03-18 19:42:14,075 [INFO] ---------------------------------
2019-03-18 19:42:32,991 [INFO] ---------------------------------
2019-03-18 19:42:32,992 [INFO] Summary:
2019-03-18 19:42:32,992 [INFO] Batch 53000, worst loss 0.082766 (incl. reg.) of 1000 batches, learning rate 0.000209 @cl.-depth 1
2019-03-18 19:42:32,993 [INFO] Regularization: 328.593536 * 0.0000100000 = 0.0032859354
2019-03-18 19:42:32,994 [INFO] Sum of grad norms: 0.147441
2019-03-18 19:42:32,995 [INFO] ---------------------------------
2019-03-18 19:42:51,602 [INFO] ---------------------------------
2019-03-18 19:42:51,603 [INFO] Summary:
2019-03-18 19:42:51,603 [INFO] Batch 54000, worst loss 0.084152 (incl. reg.) of 1000 batches, learning rate 0.000209 @cl.-depth 1
2019-03-18 19:42:51,604 [INFO] Regularization: 328.243774 * 0.0000100000 = 0.0032824376
2019-03-18 19:42:51,604 [INFO] Sum of grad norms: 0.105590
2019-03-18 19:42:51,605 [INFO] ---------------------------------
2019-03-18 19:43:10,155 [INFO] ---------------------------------
2019-03-18 19:43:10,156 [INFO] Summary:
2019-03-18 19:43:10,157 [INFO] Batch 55000, worst loss 0.082930 (incl. reg.) of 1000 batches, learning rate 0.000209 @cl.-depth 1
2019-03-18 19:43:10,157 [INFO] Regularization: 327.568512 * 0.0000100000 = 0.0032756850
2019-03-18 19:43:10,158 [INFO] Sum of grad norms: 0.218740
2019-03-18 19:43:10,159 [INFO] ---------------------------------
2019-03-18 19:43:28,959 [INFO] ---------------------------------
2019-03-18 19:43:28,960 [INFO] Summary:
2019-03-18 19:43:28,961 [INFO] Batch 56000, worst loss 0.083030 (incl. reg.) of 1000 batches, learning rate 0.000209 @cl.-depth 1
2019-03-18 19:43:28,961 [INFO] Regularization: 327.289642 * 0.0000100000 = 0.0032728964
2019-03-18 19:43:28,962 [INFO] Sum of grad norms: 0.112581
2019-03-18 19:43:28,962 [INFO] ---------------------------------
2019-03-18 19:43:47,774 [INFO] ---------------------------------
2019-03-18 19:43:47,775 [INFO] Summary:
2019-03-18 19:43:47,776 [INFO] Batch 57000, worst loss 0.083463 (incl. reg.) of 1000 batches, learning rate 0.000209 @cl.-depth 1
2019-03-18 19:43:47,776 [INFO] Regularization: 327.214233 * 0.0000100000 = 0.0032721423
2019-03-18 19:43:47,777 [INFO] Sum of grad norms: 0.217439
2019-03-18 19:43:47,777 [INFO] ---------------------------------
2019-03-18 19:44:06,505 [INFO] ---------------------------------
2019-03-18 19:44:06,506 [INFO] Summary:
2019-03-18 19:44:06,507 [INFO] Batch 58000, worst loss 0.082745 (incl. reg.) of 1000 batches, learning rate 0.000209 @cl.-depth 1
2019-03-18 19:44:06,507 [INFO] Regularization: 326.920197 * 0.0000100000 = 0.0032692018
2019-03-18 19:44:06,508 [INFO] Sum of grad norms: 0.120050
2019-03-18 19:44:06,508 [INFO] ---------------------------------
2019-03-18 19:44:25,411 [INFO] ---------------------------------
2019-03-18 19:44:25,412 [INFO] Summary:
2019-03-18 19:44:25,413 [INFO] Batch 59000, worst loss 0.083159 (incl. reg.) of 1000 batches, learning rate 0.000209 @cl.-depth 1
2019-03-18 19:44:25,413 [INFO] Regularization: 326.518097 * 0.0000100000 = 0.0032651809
2019-03-18 19:44:25,414 [INFO] Sum of grad norms: 0.140464
2019-03-18 19:44:25,414 [INFO] ---------------------------------
2019-03-18 19:44:44,166 [INFO] ---------------------------------
2019-03-18 19:44:44,167 [INFO] Summary:
2019-03-18 19:44:44,167 [INFO] Batch 60000, worst loss 0.085735 (incl. reg.) of 1000 batches, learning rate 0.000209 @cl.-depth 1
2019-03-18 19:44:44,168 [INFO] Regularization: 326.082520 * 0.0000100000 = 0.0032608251
2019-03-18 19:44:44,168 [INFO] Sum of grad norms: 0.152396
2019-03-18 19:44:44,169 [INFO] ---------------------------------
2019-03-18 19:44:49,041 [INFO] ---------------------------------
2019-03-18 19:44:49,042 [INFO] Evaluation:
2019-03-18 19:44:49,043 [INFO] Batch 60000, worst loss 0.079887 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 19:44:49,044 [INFO] ---------------------------------
2019-03-18 19:45:07,968 [INFO] ---------------------------------
2019-03-18 19:45:07,969 [INFO] Summary:
2019-03-18 19:45:07,969 [INFO] Batch 61000, worst loss 0.083984 (incl. reg.) of 1000 batches, learning rate 0.000105 @cl.-depth 1
2019-03-18 19:45:07,970 [INFO] Regularization: 325.858307 * 0.0000100000 = 0.0032585829
2019-03-18 19:45:07,971 [INFO] Sum of grad norms: 0.154543
2019-03-18 19:45:07,972 [INFO] ---------------------------------
2019-03-18 19:45:26,846 [INFO] ---------------------------------
2019-03-18 19:45:26,847 [INFO] Summary:
2019-03-18 19:45:26,848 [INFO] Batch 62000, worst loss 0.083708 (incl. reg.) of 1000 batches, learning rate 0.000105 @cl.-depth 1
2019-03-18 19:45:26,848 [INFO] Regularization: 323.708435 * 0.0000100000 = 0.0032370843
2019-03-18 19:45:26,849 [INFO] Sum of grad norms: 0.150911
2019-03-18 19:45:26,849 [INFO] ---------------------------------
2019-03-18 19:45:45,715 [INFO] ---------------------------------
2019-03-18 19:45:45,716 [INFO] Summary:
2019-03-18 19:45:45,717 [INFO] Batch 63000, worst loss 0.084421 (incl. reg.) of 1000 batches, learning rate 0.000105 @cl.-depth 1
2019-03-18 19:45:45,718 [INFO] Regularization: 323.569214 * 0.0000100000 = 0.0032356922
2019-03-18 19:45:45,719 [INFO] Sum of grad norms: 0.187279
2019-03-18 19:45:45,720 [INFO] ---------------------------------
2019-03-18 19:46:04,701 [INFO] ---------------------------------
2019-03-18 19:46:04,702 [INFO] Summary:
2019-03-18 19:46:04,703 [INFO] Batch 64000, worst loss 0.084122 (incl. reg.) of 1000 batches, learning rate 0.000105 @cl.-depth 1
2019-03-18 19:46:04,704 [INFO] Regularization: 323.332031 * 0.0000100000 = 0.0032333203
2019-03-18 19:46:04,705 [INFO] Sum of grad norms: 0.118373
2019-03-18 19:46:04,705 [INFO] ---------------------------------
2019-03-18 19:46:23,825 [INFO] ---------------------------------
2019-03-18 19:46:23,826 [INFO] Summary:
2019-03-18 19:46:23,827 [INFO] Batch 65000, worst loss 0.082828 (incl. reg.) of 1000 batches, learning rate 0.000105 @cl.-depth 1
2019-03-18 19:46:23,827 [INFO] Regularization: 323.009399 * 0.0000100000 = 0.0032300940
2019-03-18 19:46:23,828 [INFO] Sum of grad norms: 0.178962
2019-03-18 19:46:23,828 [INFO] ---------------------------------
2019-03-18 19:46:42,292 [INFO] ---------------------------------
2019-03-18 19:46:42,293 [INFO] Summary:
2019-03-18 19:46:42,294 [INFO] Batch 66000, worst loss 0.083878 (incl. reg.) of 1000 batches, learning rate 0.000105 @cl.-depth 1
2019-03-18 19:46:42,294 [INFO] Regularization: 322.871613 * 0.0000100000 = 0.0032287161
2019-03-18 19:46:42,295 [INFO] Sum of grad norms: 0.151629
2019-03-18 19:46:42,296 [INFO] ---------------------------------
2019-03-18 19:47:01,275 [INFO] ---------------------------------
2019-03-18 19:47:01,276 [INFO] Summary:
2019-03-18 19:47:01,277 [INFO] Batch 67000, worst loss 0.083918 (incl. reg.) of 1000 batches, learning rate 0.000105 @cl.-depth 1
2019-03-18 19:47:01,278 [INFO] Regularization: 322.683075 * 0.0000100000 = 0.0032268306
2019-03-18 19:47:01,278 [INFO] Sum of grad norms: 0.129766
2019-03-18 19:47:01,279 [INFO] ---------------------------------
2019-03-18 19:47:20,101 [INFO] ---------------------------------
2019-03-18 19:47:20,102 [INFO] Summary:
2019-03-18 19:47:20,103 [INFO] Batch 68000, worst loss 0.083843 (incl. reg.) of 1000 batches, learning rate 0.000105 @cl.-depth 1
2019-03-18 19:47:20,103 [INFO] Regularization: 322.527374 * 0.0000100000 = 0.0032252737
2019-03-18 19:47:20,104 [INFO] Sum of grad norms: 0.121728
2019-03-18 19:47:20,105 [INFO] ---------------------------------
2019-03-18 19:47:39,000 [INFO] ---------------------------------
2019-03-18 19:47:39,001 [INFO] Summary:
2019-03-18 19:47:39,002 [INFO] Batch 69000, worst loss 0.084009 (incl. reg.) of 1000 batches, learning rate 0.000105 @cl.-depth 1
2019-03-18 19:47:39,003 [INFO] Regularization: 322.201935 * 0.0000100000 = 0.0032220192
2019-03-18 19:47:39,004 [INFO] Sum of grad norms: 0.175609
2019-03-18 19:47:39,005 [INFO] ---------------------------------
2019-03-18 19:47:57,554 [INFO] ---------------------------------
2019-03-18 19:47:57,555 [INFO] Summary:
2019-03-18 19:47:57,556 [INFO] Batch 70000, worst loss 0.082885 (incl. reg.) of 1000 batches, learning rate 0.000105 @cl.-depth 1
2019-03-18 19:47:57,556 [INFO] Regularization: 321.708801 * 0.0000100000 = 0.0032170878
2019-03-18 19:47:57,557 [INFO] Sum of grad norms: 0.141091
2019-03-18 19:47:57,558 [INFO] ---------------------------------
2019-03-18 19:48:02,476 [INFO] ---------------------------------
2019-03-18 19:48:02,477 [INFO] Evaluation:
2019-03-18 19:48:02,477 [INFO] Batch 70000, worst loss 0.080223 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 19:48:02,478 [INFO] ---------------------------------
2019-03-18 19:48:21,785 [INFO] ---------------------------------
2019-03-18 19:48:21,786 [INFO] Summary:
2019-03-18 19:48:21,787 [INFO] Batch 71000, worst loss 0.082611 (incl. reg.) of 1000 batches, learning rate 0.000052 @cl.-depth 1
2019-03-18 19:48:21,787 [INFO] Regularization: 321.971161 * 0.0000100000 = 0.0032197116
2019-03-18 19:48:21,788 [INFO] Sum of grad norms: 0.119043
2019-03-18 19:48:21,788 [INFO] ---------------------------------
2019-03-18 19:48:40,376 [INFO] ---------------------------------
2019-03-18 19:48:40,377 [INFO] Summary:
2019-03-18 19:48:40,378 [INFO] Batch 72000, worst loss 0.082289 (incl. reg.) of 1000 batches, learning rate 0.000052 @cl.-depth 1
2019-03-18 19:48:40,378 [INFO] Regularization: 320.693329 * 0.0000100000 = 0.0032069332
2019-03-18 19:48:40,379 [INFO] Sum of grad norms: 0.127993
2019-03-18 19:48:40,380 [INFO] ---------------------------------
2019-03-18 19:48:58,930 [INFO] ---------------------------------
2019-03-18 19:48:58,931 [INFO] Summary:
2019-03-18 19:48:58,931 [INFO] Batch 73000, worst loss 0.085164 (incl. reg.) of 1000 batches, learning rate 0.000052 @cl.-depth 1
2019-03-18 19:48:58,932 [INFO] Regularization: 320.624146 * 0.0000100000 = 0.0032062414
2019-03-18 19:48:58,932 [INFO] Sum of grad norms: 0.106214
2019-03-18 19:48:58,933 [INFO] ---------------------------------
2019-03-18 19:49:17,526 [INFO] ---------------------------------
2019-03-18 19:49:17,527 [INFO] Summary:
2019-03-18 19:49:17,527 [INFO] Batch 74000, worst loss 0.082645 (incl. reg.) of 1000 batches, learning rate 0.000052 @cl.-depth 1
2019-03-18 19:49:17,528 [INFO] Regularization: 320.507690 * 0.0000100000 = 0.0032050768
2019-03-18 19:49:17,528 [INFO] Sum of grad norms: 0.114065
2019-03-18 19:49:17,529 [INFO] ---------------------------------
2019-03-18 19:49:36,608 [INFO] ---------------------------------
2019-03-18 19:49:36,609 [INFO] Summary:
2019-03-18 19:49:36,610 [INFO] Batch 75000, worst loss 0.082301 (incl. reg.) of 1000 batches, learning rate 0.000052 @cl.-depth 1
2019-03-18 19:49:36,610 [INFO] Regularization: 320.402435 * 0.0000100000 = 0.0032040242
2019-03-18 19:49:36,611 [INFO] Sum of grad norms: 0.092725
2019-03-18 19:49:36,612 [INFO] ---------------------------------
2019-03-18 19:49:55,136 [INFO] ---------------------------------
2019-03-18 19:49:55,137 [INFO] Summary:
2019-03-18 19:49:55,137 [INFO] Batch 76000, worst loss 0.081883 (incl. reg.) of 1000 batches, learning rate 0.000052 @cl.-depth 1
2019-03-18 19:49:55,138 [INFO] Regularization: 320.209198 * 0.0000100000 = 0.0032020919
2019-03-18 19:49:55,138 [INFO] Sum of grad norms: 0.082828
2019-03-18 19:49:55,139 [INFO] ---------------------------------
2019-03-18 19:50:13,826 [INFO] ---------------------------------
2019-03-18 19:50:13,827 [INFO] Summary:
2019-03-18 19:50:13,828 [INFO] Batch 77000, worst loss 0.083325 (incl. reg.) of 1000 batches, learning rate 0.000052 @cl.-depth 1
2019-03-18 19:50:13,828 [INFO] Regularization: 319.963928 * 0.0000100000 = 0.0031996393
2019-03-18 19:50:13,829 [INFO] Sum of grad norms: 0.120629
2019-03-18 19:50:13,830 [INFO] ---------------------------------
2019-03-18 19:50:32,242 [INFO] ---------------------------------
2019-03-18 19:50:32,243 [INFO] Summary:
2019-03-18 19:50:32,243 [INFO] Batch 78000, worst loss 0.084360 (incl. reg.) of 1000 batches, learning rate 0.000052 @cl.-depth 1
2019-03-18 19:50:32,244 [INFO] Regularization: 319.952484 * 0.0000100000 = 0.0031995247
2019-03-18 19:50:32,245 [INFO] Sum of grad norms: 0.130635
2019-03-18 19:50:32,245 [INFO] ---------------------------------
2019-03-18 19:50:51,136 [INFO] ---------------------------------
2019-03-18 19:50:51,137 [INFO] Summary:
2019-03-18 19:50:51,138 [INFO] Batch 79000, worst loss 0.082293 (incl. reg.) of 1000 batches, learning rate 0.000052 @cl.-depth 1
2019-03-18 19:50:51,139 [INFO] Regularization: 319.768188 * 0.0000100000 = 0.0031976819
2019-03-18 19:50:51,140 [INFO] Sum of grad norms: 0.129164
2019-03-18 19:50:51,141 [INFO] ---------------------------------
2019-03-18 19:51:10,527 [INFO] ---------------------------------
2019-03-18 19:51:10,528 [INFO] Summary:
2019-03-18 19:51:10,530 [INFO] Batch 80000, worst loss 0.085225 (incl. reg.) of 1000 batches, learning rate 0.000052 @cl.-depth 1
2019-03-18 19:51:10,531 [INFO] Regularization: 319.767090 * 0.0000100000 = 0.0031976709
2019-03-18 19:51:10,531 [INFO] Sum of grad norms: 0.118227
2019-03-18 19:51:10,532 [INFO] ---------------------------------
2019-03-18 19:51:15,432 [INFO] ---------------------------------
2019-03-18 19:51:15,433 [INFO] Evaluation:
2019-03-18 19:51:15,434 [INFO] Batch 80000, worst loss 0.081773 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 19:51:15,435 [INFO] ---------------------------------
2019-03-18 19:51:34,283 [INFO] ---------------------------------
2019-03-18 19:51:34,284 [INFO] Summary:
2019-03-18 19:51:34,284 [INFO] Batch 81000, worst loss 0.082863 (incl. reg.) of 1000 batches, learning rate 0.000026 @cl.-depth 1
2019-03-18 19:51:34,285 [INFO] Regularization: 319.518036 * 0.0000100000 = 0.0031951803
2019-03-18 19:51:34,285 [INFO] Sum of grad norms: 0.095497
2019-03-18 19:51:34,286 [INFO] ---------------------------------
2019-03-18 19:51:53,193 [INFO] ---------------------------------
2019-03-18 19:51:53,194 [INFO] Summary:
2019-03-18 19:51:53,195 [INFO] Batch 82000, worst loss 0.084342 (incl. reg.) of 1000 batches, learning rate 0.000026 @cl.-depth 1
2019-03-18 19:51:53,195 [INFO] Regularization: 319.182800 * 0.0000100000 = 0.0031918280
2019-03-18 19:51:53,196 [INFO] Sum of grad norms: 0.136341
2019-03-18 19:51:53,197 [INFO] ---------------------------------
2019-03-18 19:52:12,220 [INFO] ---------------------------------
2019-03-18 19:52:12,222 [INFO] Summary:
2019-03-18 19:52:12,222 [INFO] Batch 83000, worst loss 0.084179 (incl. reg.) of 1000 batches, learning rate 0.000026 @cl.-depth 1
2019-03-18 19:52:12,223 [INFO] Regularization: 319.167542 * 0.0000100000 = 0.0031916753
2019-03-18 19:52:12,223 [INFO] Sum of grad norms: 0.222601
2019-03-18 19:52:12,224 [INFO] ---------------------------------
2019-03-18 19:52:30,997 [INFO] ---------------------------------
2019-03-18 19:52:30,998 [INFO] Summary:
2019-03-18 19:52:30,999 [INFO] Batch 84000, worst loss 0.083920 (incl. reg.) of 1000 batches, learning rate 0.000026 @cl.-depth 1
2019-03-18 19:52:31,000 [INFO] Regularization: 319.133209 * 0.0000100000 = 0.0031913321
2019-03-18 19:52:31,000 [INFO] Sum of grad norms: 0.095873
2019-03-18 19:52:31,001 [INFO] ---------------------------------
2019-03-18 19:52:49,463 [INFO] ---------------------------------
2019-03-18 19:52:49,464 [INFO] Summary:
2019-03-18 19:52:49,465 [INFO] Batch 85000, worst loss 0.083994 (incl. reg.) of 1000 batches, learning rate 0.000026 @cl.-depth 1
2019-03-18 19:52:49,465 [INFO] Regularization: 319.134430 * 0.0000100000 = 0.0031913442
2019-03-18 19:52:49,466 [INFO] Sum of grad norms: 0.111733
2019-03-18 19:52:49,466 [INFO] ---------------------------------
2019-03-18 19:53:08,571 [INFO] ---------------------------------
2019-03-18 19:53:08,572 [INFO] Summary:
2019-03-18 19:53:08,573 [INFO] Batch 86000, worst loss 0.083604 (incl. reg.) of 1000 batches, learning rate 0.000026 @cl.-depth 1
2019-03-18 19:53:08,573 [INFO] Regularization: 318.976471 * 0.0000100000 = 0.0031897647
2019-03-18 19:53:08,574 [INFO] Sum of grad norms: 0.153390
2019-03-18 19:53:08,574 [INFO] ---------------------------------
2019-03-18 19:53:26,706 [INFO] ---------------------------------
2019-03-18 19:53:26,707 [INFO] Summary:
2019-03-18 19:53:26,708 [INFO] Batch 87000, worst loss 0.083592 (incl. reg.) of 1000 batches, learning rate 0.000026 @cl.-depth 1
2019-03-18 19:53:26,709 [INFO] Regularization: 318.934113 * 0.0000100000 = 0.0031893409
2019-03-18 19:53:26,709 [INFO] Sum of grad norms: 0.082325
2019-03-18 19:53:26,710 [INFO] ---------------------------------
2019-03-18 19:53:45,351 [INFO] ---------------------------------
2019-03-18 19:53:45,352 [INFO] Summary:
2019-03-18 19:53:45,353 [INFO] Batch 88000, worst loss 0.083893 (incl. reg.) of 1000 batches, learning rate 0.000026 @cl.-depth 1
2019-03-18 19:53:45,354 [INFO] Regularization: 318.886108 * 0.0000100000 = 0.0031888611
2019-03-18 19:53:45,354 [INFO] Sum of grad norms: 0.108420
2019-03-18 19:53:45,355 [INFO] ---------------------------------
2019-03-18 19:54:04,103 [INFO] ---------------------------------
2019-03-18 19:54:04,104 [INFO] Summary:
2019-03-18 19:54:04,105 [INFO] Batch 89000, worst loss 0.082303 (incl. reg.) of 1000 batches, learning rate 0.000026 @cl.-depth 1
2019-03-18 19:54:04,105 [INFO] Regularization: 318.786499 * 0.0000100000 = 0.0031878650
2019-03-18 19:54:04,106 [INFO] Sum of grad norms: 0.111767
2019-03-18 19:54:04,106 [INFO] ---------------------------------
2019-03-18 19:54:23,129 [INFO] ---------------------------------
2019-03-18 19:54:23,130 [INFO] Summary:
2019-03-18 19:54:23,130 [INFO] Batch 90000, worst loss 0.083179 (incl. reg.) of 1000 batches, learning rate 0.000026 @cl.-depth 1
2019-03-18 19:54:23,131 [INFO] Regularization: 318.696991 * 0.0000100000 = 0.0031869698
2019-03-18 19:54:23,132 [INFO] Sum of grad norms: 0.134529
2019-03-18 19:54:23,132 [INFO] ---------------------------------
2019-03-18 19:54:28,023 [INFO] ---------------------------------
2019-03-18 19:54:28,024 [INFO] Evaluation:
2019-03-18 19:54:28,025 [INFO] Batch 90000, worst loss 0.079193 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 19:54:28,026 [INFO] ---------------------------------
2019-03-18 19:54:47,000 [INFO] ---------------------------------
2019-03-18 19:54:47,001 [INFO] Summary:
2019-03-18 19:54:47,002 [INFO] Batch 91000, worst loss 0.085388 (incl. reg.) of 1000 batches, learning rate 0.000013 @cl.-depth 1
2019-03-18 19:54:47,002 [INFO] Regularization: 318.664124 * 0.0000100000 = 0.0031866413
2019-03-18 19:54:47,003 [INFO] Sum of grad norms: 0.178780
2019-03-18 19:54:47,003 [INFO] ---------------------------------
2019-03-18 19:55:05,765 [INFO] ---------------------------------
2019-03-18 19:55:05,766 [INFO] Summary:
2019-03-18 19:55:05,767 [INFO] Batch 92000, worst loss 0.081379 (incl. reg.) of 1000 batches, learning rate 0.000013 @cl.-depth 1
2019-03-18 19:55:05,768 [INFO] Regularization: 318.497864 * 0.0000100000 = 0.0031849786
2019-03-18 19:55:05,769 [INFO] Sum of grad norms: 0.154697
2019-03-18 19:55:05,769 [INFO] ---------------------------------
2019-03-18 19:55:24,154 [INFO] ---------------------------------
2019-03-18 19:55:24,155 [INFO] Summary:
2019-03-18 19:55:24,156 [INFO] Batch 93000, worst loss 0.081552 (incl. reg.) of 1000 batches, learning rate 0.000013 @cl.-depth 1
2019-03-18 19:55:24,157 [INFO] Regularization: 318.455750 * 0.0000100000 = 0.0031845574
2019-03-18 19:55:24,158 [INFO] Sum of grad norms: 0.149877
2019-03-18 19:55:24,159 [INFO] ---------------------------------
2019-03-18 19:55:42,403 [INFO] ---------------------------------
2019-03-18 19:55:42,404 [INFO] Summary:
2019-03-18 19:55:42,405 [INFO] Batch 94000, worst loss 0.083925 (incl. reg.) of 1000 batches, learning rate 0.000013 @cl.-depth 1
2019-03-18 19:55:42,406 [INFO] Regularization: 318.361084 * 0.0000100000 = 0.0031836107
2019-03-18 19:55:42,406 [INFO] Sum of grad norms: 0.134775
2019-03-18 19:55:42,407 [INFO] ---------------------------------
2019-03-18 19:56:01,240 [INFO] ---------------------------------
2019-03-18 19:56:01,241 [INFO] Summary:
2019-03-18 19:56:01,242 [INFO] Batch 95000, worst loss 0.082884 (incl. reg.) of 1000 batches, learning rate 0.000013 @cl.-depth 1
2019-03-18 19:56:01,243 [INFO] Regularization: 318.366302 * 0.0000100000 = 0.0031836629
2019-03-18 19:56:01,243 [INFO] Sum of grad norms: 0.143366
2019-03-18 19:56:01,244 [INFO] ---------------------------------
2019-03-18 19:56:20,019 [INFO] ---------------------------------
2019-03-18 19:56:20,020 [INFO] Summary:
2019-03-18 19:56:20,020 [INFO] Batch 96000, worst loss 0.082861 (incl. reg.) of 1000 batches, learning rate 0.000013 @cl.-depth 1
2019-03-18 19:56:20,021 [INFO] Regularization: 318.362549 * 0.0000100000 = 0.0031836254
2019-03-18 19:56:20,021 [INFO] Sum of grad norms: 0.138808
2019-03-18 19:56:20,022 [INFO] ---------------------------------
2019-03-18 19:56:38,575 [INFO] ---------------------------------
2019-03-18 19:56:38,576 [INFO] Summary:
2019-03-18 19:56:38,577 [INFO] Batch 97000, worst loss 0.082156 (incl. reg.) of 1000 batches, learning rate 0.000013 @cl.-depth 1
2019-03-18 19:56:38,577 [INFO] Regularization: 318.310486 * 0.0000100000 = 0.0031831048
2019-03-18 19:56:38,578 [INFO] Sum of grad norms: 0.176148
2019-03-18 19:56:38,578 [INFO] ---------------------------------
2019-03-18 19:56:57,068 [INFO] ---------------------------------
2019-03-18 19:56:57,069 [INFO] Summary:
2019-03-18 19:56:57,069 [INFO] Batch 98000, worst loss 0.085221 (incl. reg.) of 1000 batches, learning rate 0.000013 @cl.-depth 1
2019-03-18 19:56:57,070 [INFO] Regularization: 318.313599 * 0.0000100000 = 0.0031831360
2019-03-18 19:56:57,070 [INFO] Sum of grad norms: 0.125824
2019-03-18 19:56:57,071 [INFO] ---------------------------------
2019-03-18 19:57:15,755 [INFO] ---------------------------------
2019-03-18 19:57:15,756 [INFO] Summary:
2019-03-18 19:57:15,757 [INFO] Batch 99000, worst loss 0.085079 (incl. reg.) of 1000 batches, learning rate 0.000013 @cl.-depth 1
2019-03-18 19:57:15,758 [INFO] Regularization: 318.304199 * 0.0000100000 = 0.0031830419
2019-03-18 19:57:15,758 [INFO] Sum of grad norms: 0.114188
2019-03-18 19:57:15,759 [INFO] ---------------------------------
2019-03-18 19:57:34,259 [INFO] ---------------------------------
2019-03-18 19:57:34,260 [INFO] Summary:
2019-03-18 19:57:34,261 [INFO] Batch 100000, worst loss 0.082553 (incl. reg.) of 1000 batches, learning rate 0.000013 @cl.-depth 1
2019-03-18 19:57:34,262 [INFO] Regularization: 318.286682 * 0.0000100000 = 0.0031828668
2019-03-18 19:57:34,262 [INFO] Sum of grad norms: 0.148733
2019-03-18 19:57:34,263 [INFO] ---------------------------------
2019-03-18 19:57:39,206 [INFO] ---------------------------------
2019-03-18 19:57:39,207 [INFO] Evaluation:
2019-03-18 19:57:39,208 [INFO] Batch 100000, worst loss 0.079849 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 19:57:39,208 [INFO] ---------------------------------
2019-03-18 19:57:58,164 [INFO] ---------------------------------
2019-03-18 19:57:58,165 [INFO] Summary:
2019-03-18 19:57:58,166 [INFO] Batch 101000, worst loss 0.083390 (incl. reg.) of 1000 batches, learning rate 0.000007 @cl.-depth 1
2019-03-18 19:57:58,166 [INFO] Regularization: 318.203217 * 0.0000100000 = 0.0031820321
2019-03-18 19:57:58,167 [INFO] Sum of grad norms: 0.140706
2019-03-18 19:57:58,167 [INFO] ---------------------------------
2019-03-18 19:58:17,236 [INFO] ---------------------------------
2019-03-18 19:58:17,237 [INFO] Summary:
2019-03-18 19:58:17,238 [INFO] Batch 102000, worst loss 0.085893 (incl. reg.) of 1000 batches, learning rate 0.000007 @cl.-depth 1
2019-03-18 19:58:17,238 [INFO] Regularization: 318.095551 * 0.0000100000 = 0.0031809555
2019-03-18 19:58:17,239 [INFO] Sum of grad norms: 0.130042
2019-03-18 19:58:17,239 [INFO] ---------------------------------
2019-03-18 19:58:35,989 [INFO] ---------------------------------
2019-03-18 19:58:35,990 [INFO] Summary:
2019-03-18 19:58:35,991 [INFO] Batch 103000, worst loss 0.083119 (incl. reg.) of 1000 batches, learning rate 0.000007 @cl.-depth 1
2019-03-18 19:58:35,991 [INFO] Regularization: 318.087921 * 0.0000100000 = 0.0031808792
2019-03-18 19:58:35,992 [INFO] Sum of grad norms: 0.160793
2019-03-18 19:58:35,992 [INFO] ---------------------------------
2019-03-18 19:58:54,671 [INFO] ---------------------------------
2019-03-18 19:58:54,672 [INFO] Summary:
2019-03-18 19:58:54,673 [INFO] Batch 104000, worst loss 0.084224 (incl. reg.) of 1000 batches, learning rate 0.000007 @cl.-depth 1
2019-03-18 19:58:54,673 [INFO] Regularization: 318.079437 * 0.0000100000 = 0.0031807942
2019-03-18 19:58:54,674 [INFO] Sum of grad norms: 0.120527
2019-03-18 19:58:54,674 [INFO] ---------------------------------
2019-03-18 19:59:13,186 [INFO] ---------------------------------
2019-03-18 19:59:13,187 [INFO] Summary:
2019-03-18 19:59:13,188 [INFO] Batch 105000, worst loss 0.084207 (incl. reg.) of 1000 batches, learning rate 0.000007 @cl.-depth 1
2019-03-18 19:59:13,188 [INFO] Regularization: 318.077118 * 0.0000100000 = 0.0031807711
2019-03-18 19:59:13,189 [INFO] Sum of grad norms: 0.130187
2019-03-18 19:59:13,189 [INFO] ---------------------------------
2019-03-18 19:59:31,731 [INFO] ---------------------------------
2019-03-18 19:59:31,732 [INFO] Summary:
2019-03-18 19:59:31,733 [INFO] Batch 106000, worst loss 0.084145 (incl. reg.) of 1000 batches, learning rate 0.000007 @cl.-depth 1
2019-03-18 19:59:31,733 [INFO] Regularization: 318.088287 * 0.0000100000 = 0.0031808829
2019-03-18 19:59:31,734 [INFO] Sum of grad norms: 0.125444
2019-03-18 19:59:31,734 [INFO] ---------------------------------
2019-03-18 19:59:50,158 [INFO] ---------------------------------
2019-03-18 19:59:50,160 [INFO] Summary:
2019-03-18 19:59:50,161 [INFO] Batch 107000, worst loss 0.083140 (incl. reg.) of 1000 batches, learning rate 0.000007 @cl.-depth 1
2019-03-18 19:59:50,161 [INFO] Regularization: 318.073669 * 0.0000100000 = 0.0031807367
2019-03-18 19:59:50,162 [INFO] Sum of grad norms: 0.125515
2019-03-18 19:59:50,163 [INFO] ---------------------------------
2019-03-18 20:00:09,345 [INFO] ---------------------------------
2019-03-18 20:00:09,346 [INFO] Summary:
2019-03-18 20:00:09,346 [INFO] Batch 108000, worst loss 0.083088 (incl. reg.) of 1000 batches, learning rate 0.000007 @cl.-depth 1
2019-03-18 20:00:09,347 [INFO] Regularization: 318.021271 * 0.0000100000 = 0.0031802126
2019-03-18 20:00:09,348 [INFO] Sum of grad norms: 0.133931
2019-03-18 20:00:09,348 [INFO] ---------------------------------
2019-03-18 20:00:28,157 [INFO] ---------------------------------
2019-03-18 20:00:28,158 [INFO] Summary:
2019-03-18 20:00:28,158 [INFO] Batch 109000, worst loss 0.082395 (incl. reg.) of 1000 batches, learning rate 0.000007 @cl.-depth 1
2019-03-18 20:00:28,159 [INFO] Regularization: 318.011047 * 0.0000100000 = 0.0031801104
2019-03-18 20:00:28,159 [INFO] Sum of grad norms: 0.149281
2019-03-18 20:00:28,160 [INFO] ---------------------------------
2019-03-18 20:00:47,109 [INFO] ---------------------------------
2019-03-18 20:00:47,110 [INFO] Summary:
2019-03-18 20:00:47,111 [INFO] Batch 110000, worst loss 0.082348 (incl. reg.) of 1000 batches, learning rate 0.000007 @cl.-depth 1
2019-03-18 20:00:47,112 [INFO] Regularization: 318.053925 * 0.0000100000 = 0.0031805392
2019-03-18 20:00:47,112 [INFO] Sum of grad norms: 0.213700
2019-03-18 20:00:47,113 [INFO] ---------------------------------
2019-03-18 20:00:51,982 [INFO] ---------------------------------
2019-03-18 20:00:51,982 [INFO] Evaluation:
2019-03-18 20:00:51,983 [INFO] Batch 110000, worst loss 0.079558 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:00:51,984 [INFO] ---------------------------------
2019-03-18 20:01:11,020 [INFO] ---------------------------------
2019-03-18 20:01:11,021 [INFO] Summary:
2019-03-18 20:01:11,022 [INFO] Batch 111000, worst loss 0.083727 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-18 20:01:11,022 [INFO] Regularization: 317.984253 * 0.0000100000 = 0.0031798424
2019-03-18 20:01:11,023 [INFO] Sum of grad norms: 0.125693
2019-03-18 20:01:11,024 [INFO] ---------------------------------
2019-03-18 20:01:29,725 [INFO] ---------------------------------
2019-03-18 20:01:29,726 [INFO] Summary:
2019-03-18 20:01:29,726 [INFO] Batch 112000, worst loss 0.084387 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-18 20:01:29,727 [INFO] Regularization: 317.938782 * 0.0000100000 = 0.0031793877
2019-03-18 20:01:29,728 [INFO] Sum of grad norms: 0.175187
2019-03-18 20:01:29,728 [INFO] ---------------------------------
2019-03-18 20:01:48,568 [INFO] ---------------------------------
2019-03-18 20:01:48,569 [INFO] Summary:
2019-03-18 20:01:48,569 [INFO] Batch 113000, worst loss 0.083527 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-18 20:01:48,570 [INFO] Regularization: 317.910461 * 0.0000100000 = 0.0031791045
2019-03-18 20:01:48,570 [INFO] Sum of grad norms: 0.127629
2019-03-18 20:01:48,571 [INFO] ---------------------------------
2019-03-18 20:02:07,203 [INFO] ---------------------------------
2019-03-18 20:02:07,204 [INFO] Summary:
2019-03-18 20:02:07,205 [INFO] Batch 114000, worst loss 0.083715 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-18 20:02:07,205 [INFO] Regularization: 317.907135 * 0.0000100000 = 0.0031790712
2019-03-18 20:02:07,206 [INFO] Sum of grad norms: 0.155220
2019-03-18 20:02:07,206 [INFO] ---------------------------------
2019-03-18 20:02:25,843 [INFO] ---------------------------------
2019-03-18 20:02:25,844 [INFO] Summary:
2019-03-18 20:02:25,845 [INFO] Batch 115000, worst loss 0.083723 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-18 20:02:25,845 [INFO] Regularization: 317.899811 * 0.0000100000 = 0.0031789981
2019-03-18 20:02:25,846 [INFO] Sum of grad norms: 0.140203
2019-03-18 20:02:25,847 [INFO] ---------------------------------
2019-03-18 20:02:44,578 [INFO] ---------------------------------
2019-03-18 20:02:44,579 [INFO] Summary:
2019-03-18 20:02:44,579 [INFO] Batch 116000, worst loss 0.084445 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-18 20:02:44,580 [INFO] Regularization: 317.887787 * 0.0000100000 = 0.0031788778
2019-03-18 20:02:44,581 [INFO] Sum of grad norms: 0.117081
2019-03-18 20:02:44,581 [INFO] ---------------------------------
2019-03-18 20:03:03,669 [INFO] ---------------------------------
2019-03-18 20:03:03,670 [INFO] Summary:
2019-03-18 20:03:03,671 [INFO] Batch 117000, worst loss 0.084456 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-18 20:03:03,671 [INFO] Regularization: 317.887604 * 0.0000100000 = 0.0031788759
2019-03-18 20:03:03,672 [INFO] Sum of grad norms: 0.138251
2019-03-18 20:03:03,673 [INFO] ---------------------------------
2019-03-18 20:03:22,178 [INFO] ---------------------------------
2019-03-18 20:03:22,179 [INFO] Summary:
2019-03-18 20:03:22,179 [INFO] Batch 118000, worst loss 0.084189 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-18 20:03:22,180 [INFO] Regularization: 317.885773 * 0.0000100000 = 0.0031788577
2019-03-18 20:03:22,180 [INFO] Sum of grad norms: 0.113770
2019-03-18 20:03:22,181 [INFO] ---------------------------------
2019-03-18 20:03:41,002 [INFO] ---------------------------------
2019-03-18 20:03:41,003 [INFO] Summary:
2019-03-18 20:03:41,004 [INFO] Batch 119000, worst loss 0.085749 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-18 20:03:41,004 [INFO] Regularization: 317.879730 * 0.0000100000 = 0.0031787972
2019-03-18 20:03:41,005 [INFO] Sum of grad norms: 0.146425
2019-03-18 20:03:41,005 [INFO] ---------------------------------
2019-03-18 20:03:59,852 [INFO] ---------------------------------
2019-03-18 20:03:59,853 [INFO] Summary:
2019-03-18 20:03:59,853 [INFO] Batch 120000, worst loss 0.082557 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-18 20:03:59,854 [INFO] Regularization: 317.873199 * 0.0000100000 = 0.0031787320
2019-03-18 20:03:59,854 [INFO] Sum of grad norms: 0.109489
2019-03-18 20:03:59,855 [INFO] ---------------------------------
2019-03-18 20:04:04,846 [INFO] ---------------------------------
2019-03-18 20:04:04,847 [INFO] Evaluation:
2019-03-18 20:04:04,850 [INFO] Batch 120000, worst loss 0.081682 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:04:04,852 [INFO] ---------------------------------
2019-03-18 20:04:23,470 [INFO] ---------------------------------
2019-03-18 20:04:23,471 [INFO] Summary:
2019-03-18 20:04:23,472 [INFO] Batch 121000, worst loss 0.082556 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 20:04:23,473 [INFO] Regularization: 317.871307 * 0.0000100000 = 0.0031787129
2019-03-18 20:04:23,473 [INFO] Sum of grad norms: 0.149825
2019-03-18 20:04:23,474 [INFO] ---------------------------------
2019-03-18 20:04:42,175 [INFO] ---------------------------------
2019-03-18 20:04:42,176 [INFO] Summary:
2019-03-18 20:04:42,177 [INFO] Batch 122000, worst loss 0.083625 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 20:04:42,177 [INFO] Regularization: 317.828278 * 0.0000100000 = 0.0031782826
2019-03-18 20:04:42,178 [INFO] Sum of grad norms: 0.134564
2019-03-18 20:04:42,178 [INFO] ---------------------------------
2019-03-18 20:05:00,683 [INFO] ---------------------------------
2019-03-18 20:05:00,684 [INFO] Summary:
2019-03-18 20:05:00,684 [INFO] Batch 123000, worst loss 0.083158 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 20:05:00,685 [INFO] Regularization: 317.819519 * 0.0000100000 = 0.0031781951
2019-03-18 20:05:00,685 [INFO] Sum of grad norms: 0.137045
2019-03-18 20:05:00,686 [INFO] ---------------------------------
2019-03-18 20:05:19,125 [INFO] ---------------------------------
2019-03-18 20:05:19,126 [INFO] Summary:
2019-03-18 20:05:19,127 [INFO] Batch 124000, worst loss 0.084260 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 20:05:19,128 [INFO] Regularization: 317.816376 * 0.0000100000 = 0.0031781637
2019-03-18 20:05:19,128 [INFO] Sum of grad norms: 0.112069
2019-03-18 20:05:19,129 [INFO] ---------------------------------
2019-03-18 20:05:37,800 [INFO] ---------------------------------
2019-03-18 20:05:37,801 [INFO] Summary:
2019-03-18 20:05:37,802 [INFO] Batch 125000, worst loss 0.084274 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 20:05:37,803 [INFO] Regularization: 317.811554 * 0.0000100000 = 0.0031781155
2019-03-18 20:05:37,804 [INFO] Sum of grad norms: 0.174669
2019-03-18 20:05:37,804 [INFO] ---------------------------------
2019-03-18 20:05:56,816 [INFO] ---------------------------------
2019-03-18 20:05:56,817 [INFO] Summary:
2019-03-18 20:05:56,818 [INFO] Batch 126000, worst loss 0.082752 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 20:05:56,818 [INFO] Regularization: 317.808350 * 0.0000100000 = 0.0031780833
2019-03-18 20:05:56,819 [INFO] Sum of grad norms: 0.157866
2019-03-18 20:05:56,819 [INFO] ---------------------------------
2019-03-18 20:06:15,709 [INFO] ---------------------------------
2019-03-18 20:06:15,710 [INFO] Summary:
2019-03-18 20:06:15,711 [INFO] Batch 127000, worst loss 0.083830 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 20:06:15,711 [INFO] Regularization: 317.800629 * 0.0000100000 = 0.0031780063
2019-03-18 20:06:15,712 [INFO] Sum of grad norms: 0.116672
2019-03-18 20:06:15,712 [INFO] ---------------------------------
2019-03-18 20:06:34,267 [INFO] ---------------------------------
2019-03-18 20:06:34,268 [INFO] Summary:
2019-03-18 20:06:34,269 [INFO] Batch 128000, worst loss 0.081621 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 20:06:34,270 [INFO] Regularization: 317.794678 * 0.0000100000 = 0.0031779467
2019-03-18 20:06:34,270 [INFO] Sum of grad norms: 0.147143
2019-03-18 20:06:34,271 [INFO] ---------------------------------
2019-03-18 20:06:52,899 [INFO] ---------------------------------
2019-03-18 20:06:52,900 [INFO] Summary:
2019-03-18 20:06:52,901 [INFO] Batch 129000, worst loss 0.082821 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 20:06:52,901 [INFO] Regularization: 317.798584 * 0.0000100000 = 0.0031779858
2019-03-18 20:06:52,902 [INFO] Sum of grad norms: 0.112427
2019-03-18 20:06:52,902 [INFO] ---------------------------------
2019-03-18 20:07:11,789 [INFO] ---------------------------------
2019-03-18 20:07:11,791 [INFO] Summary:
2019-03-18 20:07:11,792 [INFO] Batch 130000, worst loss 0.082840 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 20:07:11,792 [INFO] Regularization: 317.790894 * 0.0000100000 = 0.0031779089
2019-03-18 20:07:11,793 [INFO] Sum of grad norms: 0.149615
2019-03-18 20:07:11,794 [INFO] ---------------------------------
2019-03-18 20:07:16,763 [INFO] ---------------------------------
2019-03-18 20:07:16,764 [INFO] Evaluation:
2019-03-18 20:07:16,765 [INFO] Batch 130000, worst loss 0.079656 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:07:16,766 [INFO] ---------------------------------
2019-03-18 20:07:35,367 [INFO] ---------------------------------
2019-03-18 20:07:35,368 [INFO] Summary:
2019-03-18 20:07:35,369 [INFO] Batch 131000, worst loss 0.083674 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 20:07:35,369 [INFO] Regularization: 317.791107 * 0.0000100000 = 0.0031779110
2019-03-18 20:07:35,370 [INFO] Sum of grad norms: 0.114098
2019-03-18 20:07:35,370 [INFO] ---------------------------------
2019-03-18 20:07:54,352 [INFO] ---------------------------------
2019-03-18 20:07:54,353 [INFO] Summary:
2019-03-18 20:07:54,354 [INFO] Batch 132000, worst loss 0.082950 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 20:07:54,354 [INFO] Regularization: 317.777435 * 0.0000100000 = 0.0031777744
2019-03-18 20:07:54,355 [INFO] Sum of grad norms: 0.127538
2019-03-18 20:07:54,355 [INFO] ---------------------------------
2019-03-18 20:08:13,111 [INFO] ---------------------------------
2019-03-18 20:08:13,112 [INFO] Summary:
2019-03-18 20:08:13,112 [INFO] Batch 133000, worst loss 0.083594 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 20:08:13,113 [INFO] Regularization: 317.778076 * 0.0000100000 = 0.0031777807
2019-03-18 20:08:13,113 [INFO] Sum of grad norms: 0.157134
2019-03-18 20:08:13,114 [INFO] ---------------------------------
2019-03-18 20:08:31,416 [INFO] ---------------------------------
2019-03-18 20:08:31,417 [INFO] Summary:
2019-03-18 20:08:31,417 [INFO] Batch 134000, worst loss 0.082285 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 20:08:31,418 [INFO] Regularization: 317.774963 * 0.0000100000 = 0.0031777495
2019-03-18 20:08:31,418 [INFO] Sum of grad norms: 0.139385
2019-03-18 20:08:31,419 [INFO] ---------------------------------
2019-03-18 20:08:50,192 [INFO] ---------------------------------
2019-03-18 20:08:50,193 [INFO] Summary:
2019-03-18 20:08:50,194 [INFO] Batch 135000, worst loss 0.081961 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 20:08:50,195 [INFO] Regularization: 317.770660 * 0.0000100000 = 0.0031777066
2019-03-18 20:08:50,195 [INFO] Sum of grad norms: 0.199192
2019-03-18 20:08:50,196 [INFO] ---------------------------------
2019-03-18 20:09:09,300 [INFO] ---------------------------------
2019-03-18 20:09:09,301 [INFO] Summary:
2019-03-18 20:09:09,301 [INFO] Batch 136000, worst loss 0.082075 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 20:09:09,302 [INFO] Regularization: 317.773560 * 0.0000100000 = 0.0031777355
2019-03-18 20:09:09,302 [INFO] Sum of grad norms: 0.138400
2019-03-18 20:09:09,303 [INFO] ---------------------------------
2019-03-18 20:09:28,009 [INFO] ---------------------------------
2019-03-18 20:09:28,010 [INFO] Summary:
2019-03-18 20:09:28,010 [INFO] Batch 137000, worst loss 0.083359 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 20:09:28,011 [INFO] Regularization: 317.774506 * 0.0000100000 = 0.0031777450
2019-03-18 20:09:28,011 [INFO] Sum of grad norms: 0.232897
2019-03-18 20:09:28,012 [INFO] ---------------------------------
2019-03-18 20:09:46,650 [INFO] ---------------------------------
2019-03-18 20:09:46,651 [INFO] Summary:
2019-03-18 20:09:46,651 [INFO] Batch 138000, worst loss 0.083134 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 20:09:46,652 [INFO] Regularization: 317.777374 * 0.0000100000 = 0.0031777737
2019-03-18 20:09:46,653 [INFO] Sum of grad norms: 0.147277
2019-03-18 20:09:46,654 [INFO] ---------------------------------
2019-03-18 20:10:05,029 [INFO] ---------------------------------
2019-03-18 20:10:05,030 [INFO] Summary:
2019-03-18 20:10:05,031 [INFO] Batch 139000, worst loss 0.082381 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 20:10:05,031 [INFO] Regularization: 317.780945 * 0.0000100000 = 0.0031778093
2019-03-18 20:10:05,032 [INFO] Sum of grad norms: 0.205759
2019-03-18 20:10:05,032 [INFO] ---------------------------------
2019-03-18 20:10:23,339 [INFO] ---------------------------------
2019-03-18 20:10:23,340 [INFO] Summary:
2019-03-18 20:10:23,340 [INFO] Batch 140000, worst loss 0.084712 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 20:10:23,341 [INFO] Regularization: 317.780334 * 0.0000100000 = 0.0031778032
2019-03-18 20:10:23,341 [INFO] Sum of grad norms: 0.128818
2019-03-18 20:10:23,342 [INFO] ---------------------------------
2019-03-18 20:10:28,196 [INFO] ---------------------------------
2019-03-18 20:10:28,197 [INFO] Evaluation:
2019-03-18 20:10:28,198 [INFO] Batch 140000, worst loss 0.080233 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:10:28,199 [INFO] ---------------------------------
2019-03-18 20:10:47,133 [INFO] ---------------------------------
2019-03-18 20:10:47,134 [INFO] Summary:
2019-03-18 20:10:47,135 [INFO] Batch 141000, worst loss 0.083411 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:10:47,135 [INFO] Regularization: 317.777496 * 0.0000100000 = 0.0031777748
2019-03-18 20:10:47,136 [INFO] Sum of grad norms: 0.109839
2019-03-18 20:10:47,137 [INFO] ---------------------------------
2019-03-18 20:11:05,690 [INFO] ---------------------------------
2019-03-18 20:11:05,691 [INFO] Summary:
2019-03-18 20:11:05,691 [INFO] Batch 142000, worst loss 0.084464 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:11:05,692 [INFO] Regularization: 317.768463 * 0.0000100000 = 0.0031776845
2019-03-18 20:11:05,693 [INFO] Sum of grad norms: 0.120723
2019-03-18 20:11:05,694 [INFO] ---------------------------------
2019-03-18 20:11:24,621 [INFO] ---------------------------------
2019-03-18 20:11:24,622 [INFO] Summary:
2019-03-18 20:11:24,622 [INFO] Batch 143000, worst loss 0.083681 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:11:24,623 [INFO] Regularization: 317.767609 * 0.0000100000 = 0.0031776761
2019-03-18 20:11:24,623 [INFO] Sum of grad norms: 0.118300
2019-03-18 20:11:24,624 [INFO] ---------------------------------
2019-03-18 20:11:43,464 [INFO] ---------------------------------
2019-03-18 20:11:43,465 [INFO] Summary:
2019-03-18 20:11:43,466 [INFO] Batch 144000, worst loss 0.083679 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:11:43,466 [INFO] Regularization: 317.766479 * 0.0000100000 = 0.0031776647
2019-03-18 20:11:43,467 [INFO] Sum of grad norms: 0.176643
2019-03-18 20:11:43,467 [INFO] ---------------------------------
2019-03-18 20:12:02,174 [INFO] ---------------------------------
2019-03-18 20:12:02,175 [INFO] Summary:
2019-03-18 20:12:02,176 [INFO] Batch 145000, worst loss 0.082406 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:12:02,176 [INFO] Regularization: 317.764465 * 0.0000100000 = 0.0031776447
2019-03-18 20:12:02,177 [INFO] Sum of grad norms: 0.126208
2019-03-18 20:12:02,178 [INFO] ---------------------------------
2019-03-18 20:12:21,035 [INFO] ---------------------------------
2019-03-18 20:12:21,036 [INFO] Summary:
2019-03-18 20:12:21,036 [INFO] Batch 146000, worst loss 0.082547 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:12:21,037 [INFO] Regularization: 317.765594 * 0.0000100000 = 0.0031776559
2019-03-18 20:12:21,037 [INFO] Sum of grad norms: 0.142018
2019-03-18 20:12:21,038 [INFO] ---------------------------------
2019-03-18 20:12:39,787 [INFO] ---------------------------------
2019-03-18 20:12:39,788 [INFO] Summary:
2019-03-18 20:12:39,789 [INFO] Batch 147000, worst loss 0.083895 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:12:39,790 [INFO] Regularization: 317.764740 * 0.0000100000 = 0.0031776472
2019-03-18 20:12:39,790 [INFO] Sum of grad norms: 0.194204
2019-03-18 20:12:39,791 [INFO] ---------------------------------
2019-03-18 20:12:58,774 [INFO] ---------------------------------
2019-03-18 20:12:58,775 [INFO] Summary:
2019-03-18 20:12:58,776 [INFO] Batch 148000, worst loss 0.083892 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:12:58,776 [INFO] Regularization: 317.764740 * 0.0000100000 = 0.0031776472
2019-03-18 20:12:58,777 [INFO] Sum of grad norms: 0.140088
2019-03-18 20:12:58,778 [INFO] ---------------------------------
2019-03-18 20:13:17,282 [INFO] ---------------------------------
2019-03-18 20:13:17,283 [INFO] Summary:
2019-03-18 20:13:17,284 [INFO] Batch 149000, worst loss 0.083143 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:13:17,284 [INFO] Regularization: 317.763489 * 0.0000100000 = 0.0031776349
2019-03-18 20:13:17,285 [INFO] Sum of grad norms: 0.131920
2019-03-18 20:13:17,285 [INFO] ---------------------------------
2019-03-18 20:13:36,041 [INFO] ---------------------------------
2019-03-18 20:13:36,042 [INFO] Summary:
2019-03-18 20:13:36,043 [INFO] Batch 150000, worst loss 0.083137 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:13:36,043 [INFO] Regularization: 317.763641 * 0.0000100000 = 0.0031776363
2019-03-18 20:13:36,044 [INFO] Sum of grad norms: 0.149542
2019-03-18 20:13:36,045 [INFO] ---------------------------------
2019-03-18 20:13:41,012 [INFO] ---------------------------------
2019-03-18 20:13:41,013 [INFO] Evaluation:
2019-03-18 20:13:41,014 [INFO] Batch 150000, worst loss 0.079510 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:13:41,014 [INFO] ---------------------------------
2019-03-18 20:13:59,802 [INFO] ---------------------------------
2019-03-18 20:13:59,803 [INFO] Summary:
2019-03-18 20:13:59,804 [INFO] Batch 151000, worst loss 0.081914 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:13:59,804 [INFO] Regularization: 317.760651 * 0.0000100000 = 0.0031776065
2019-03-18 20:13:59,805 [INFO] Sum of grad norms: 0.081848
2019-03-18 20:13:59,806 [INFO] ---------------------------------
2019-03-18 20:14:18,725 [INFO] ---------------------------------
2019-03-18 20:14:18,727 [INFO] Summary:
2019-03-18 20:14:18,727 [INFO] Batch 152000, worst loss 0.082539 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:14:18,728 [INFO] Regularization: 317.757782 * 0.0000100000 = 0.0031775776
2019-03-18 20:14:18,728 [INFO] Sum of grad norms: 0.114257
2019-03-18 20:14:18,729 [INFO] ---------------------------------
2019-03-18 20:14:37,607 [INFO] ---------------------------------
2019-03-18 20:14:37,608 [INFO] Summary:
2019-03-18 20:14:37,609 [INFO] Batch 153000, worst loss 0.082998 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:14:37,609 [INFO] Regularization: 317.758179 * 0.0000100000 = 0.0031775818
2019-03-18 20:14:37,610 [INFO] Sum of grad norms: 0.142823
2019-03-18 20:14:37,611 [INFO] ---------------------------------
2019-03-18 20:14:56,071 [INFO] ---------------------------------
2019-03-18 20:14:56,072 [INFO] Summary:
2019-03-18 20:14:56,073 [INFO] Batch 154000, worst loss 0.082997 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:14:56,073 [INFO] Regularization: 317.757355 * 0.0000100000 = 0.0031775734
2019-03-18 20:14:56,074 [INFO] Sum of grad norms: 0.118250
2019-03-18 20:14:56,074 [INFO] ---------------------------------
2019-03-18 20:15:14,457 [INFO] ---------------------------------
2019-03-18 20:15:14,458 [INFO] Summary:
2019-03-18 20:15:14,459 [INFO] Batch 155000, worst loss 0.082281 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:15:14,459 [INFO] Regularization: 317.756836 * 0.0000100000 = 0.0031775683
2019-03-18 20:15:14,460 [INFO] Sum of grad norms: 0.151654
2019-03-18 20:15:14,460 [INFO] ---------------------------------
2019-03-18 20:15:33,333 [INFO] ---------------------------------
2019-03-18 20:15:33,334 [INFO] Summary:
2019-03-18 20:15:33,334 [INFO] Batch 156000, worst loss 0.083934 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:15:33,335 [INFO] Regularization: 317.755188 * 0.0000100000 = 0.0031775518
2019-03-18 20:15:33,335 [INFO] Sum of grad norms: 0.123884
2019-03-18 20:15:33,336 [INFO] ---------------------------------
2019-03-18 20:15:52,017 [INFO] ---------------------------------
2019-03-18 20:15:52,018 [INFO] Summary:
2019-03-18 20:15:52,018 [INFO] Batch 157000, worst loss 0.083227 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:15:52,019 [INFO] Regularization: 317.754120 * 0.0000100000 = 0.0031775411
2019-03-18 20:15:52,019 [INFO] Sum of grad norms: 0.162444
2019-03-18 20:15:52,020 [INFO] ---------------------------------
2019-03-18 20:16:10,892 [INFO] ---------------------------------
2019-03-18 20:16:10,893 [INFO] Summary:
2019-03-18 20:16:10,894 [INFO] Batch 158000, worst loss 0.082285 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:16:10,894 [INFO] Regularization: 317.752899 * 0.0000100000 = 0.0031775290
2019-03-18 20:16:10,895 [INFO] Sum of grad norms: 0.098405
2019-03-18 20:16:10,895 [INFO] ---------------------------------
2019-03-18 20:16:29,311 [INFO] ---------------------------------
2019-03-18 20:16:29,312 [INFO] Summary:
2019-03-18 20:16:29,313 [INFO] Batch 159000, worst loss 0.082994 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:16:29,314 [INFO] Regularization: 317.751801 * 0.0000100000 = 0.0031775180
2019-03-18 20:16:29,314 [INFO] Sum of grad norms: 0.139162
2019-03-18 20:16:29,315 [INFO] ---------------------------------
2019-03-18 20:16:47,783 [INFO] ---------------------------------
2019-03-18 20:16:47,784 [INFO] Summary:
2019-03-18 20:16:47,784 [INFO] Batch 160000, worst loss 0.083526 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:16:47,785 [INFO] Regularization: 317.751282 * 0.0000100000 = 0.0031775127
2019-03-18 20:16:47,785 [INFO] Sum of grad norms: 0.103956
2019-03-18 20:16:47,786 [INFO] ---------------------------------
2019-03-18 20:16:52,621 [INFO] ---------------------------------
2019-03-18 20:16:52,622 [INFO] Evaluation:
2019-03-18 20:16:52,623 [INFO] Batch 160000, worst loss 0.080174 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:16:52,623 [INFO] ---------------------------------
2019-03-18 20:17:11,353 [INFO] ---------------------------------
2019-03-18 20:17:11,353 [INFO] Summary:
2019-03-18 20:17:11,354 [INFO] Batch 161000, worst loss 0.083120 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:17:11,354 [INFO] Regularization: 317.750854 * 0.0000100000 = 0.0031775085
2019-03-18 20:17:11,355 [INFO] Sum of grad norms: 0.124434
2019-03-18 20:17:11,355 [INFO] ---------------------------------
2019-03-18 20:17:30,284 [INFO] ---------------------------------
2019-03-18 20:17:30,285 [INFO] Summary:
2019-03-18 20:17:30,285 [INFO] Batch 162000, worst loss 0.083075 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:17:30,286 [INFO] Regularization: 317.748993 * 0.0000100000 = 0.0031774899
2019-03-18 20:17:30,286 [INFO] Sum of grad norms: 0.158640
2019-03-18 20:17:30,287 [INFO] ---------------------------------
2019-03-18 20:17:49,234 [INFO] ---------------------------------
2019-03-18 20:17:49,234 [INFO] Summary:
2019-03-18 20:17:49,235 [INFO] Batch 163000, worst loss 0.084406 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:17:49,235 [INFO] Regularization: 317.748749 * 0.0000100000 = 0.0031774873
2019-03-18 20:17:49,236 [INFO] Sum of grad norms: 0.168871
2019-03-18 20:17:49,237 [INFO] ---------------------------------
2019-03-18 20:18:07,722 [INFO] ---------------------------------
2019-03-18 20:18:07,723 [INFO] Summary:
2019-03-18 20:18:07,723 [INFO] Batch 164000, worst loss 0.082969 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:18:07,724 [INFO] Regularization: 317.748138 * 0.0000100000 = 0.0031774812
2019-03-18 20:18:07,724 [INFO] Sum of grad norms: 0.202489
2019-03-18 20:18:07,725 [INFO] ---------------------------------
2019-03-18 20:18:26,650 [INFO] ---------------------------------
2019-03-18 20:18:26,651 [INFO] Summary:
2019-03-18 20:18:26,652 [INFO] Batch 165000, worst loss 0.082012 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:18:26,653 [INFO] Regularization: 317.748016 * 0.0000100000 = 0.0031774801
2019-03-18 20:18:26,653 [INFO] Sum of grad norms: 0.105108
2019-03-18 20:18:26,654 [INFO] ---------------------------------
2019-03-18 20:18:45,597 [INFO] ---------------------------------
2019-03-18 20:18:45,598 [INFO] Summary:
2019-03-18 20:18:45,599 [INFO] Batch 166000, worst loss 0.082984 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:18:45,599 [INFO] Regularization: 317.747742 * 0.0000100000 = 0.0031774773
2019-03-18 20:18:45,600 [INFO] Sum of grad norms: 0.157044
2019-03-18 20:18:45,600 [INFO] ---------------------------------
2019-03-18 20:19:04,021 [INFO] ---------------------------------
2019-03-18 20:19:04,022 [INFO] Summary:
2019-03-18 20:19:04,023 [INFO] Batch 167000, worst loss 0.082333 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:19:04,023 [INFO] Regularization: 317.747040 * 0.0000100000 = 0.0031774703
2019-03-18 20:19:04,024 [INFO] Sum of grad norms: 0.187019
2019-03-18 20:19:04,025 [INFO] ---------------------------------
2019-03-18 20:19:22,561 [INFO] ---------------------------------
2019-03-18 20:19:22,562 [INFO] Summary:
2019-03-18 20:19:22,563 [INFO] Batch 168000, worst loss 0.082879 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:19:22,563 [INFO] Regularization: 317.747009 * 0.0000100000 = 0.0031774701
2019-03-18 20:19:22,564 [INFO] Sum of grad norms: 0.139389
2019-03-18 20:19:22,565 [INFO] ---------------------------------
2019-03-18 20:19:41,113 [INFO] ---------------------------------
2019-03-18 20:19:41,114 [INFO] Summary:
2019-03-18 20:19:41,114 [INFO] Batch 169000, worst loss 0.082878 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:19:41,115 [INFO] Regularization: 317.746857 * 0.0000100000 = 0.0031774684
2019-03-18 20:19:41,116 [INFO] Sum of grad norms: 0.142824
2019-03-18 20:19:41,117 [INFO] ---------------------------------
2019-03-18 20:19:59,801 [INFO] ---------------------------------
2019-03-18 20:19:59,802 [INFO] Summary:
2019-03-18 20:19:59,802 [INFO] Batch 170000, worst loss 0.084615 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:19:59,803 [INFO] Regularization: 317.746948 * 0.0000100000 = 0.0031774694
2019-03-18 20:19:59,803 [INFO] Sum of grad norms: 0.292992
2019-03-18 20:19:59,804 [INFO] ---------------------------------
2019-03-18 20:20:04,751 [INFO] ---------------------------------
2019-03-18 20:20:04,752 [INFO] Evaluation:
2019-03-18 20:20:04,753 [INFO] Batch 170000, worst loss 0.081437 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:20:04,753 [INFO] ---------------------------------
2019-03-18 20:20:23,629 [INFO] ---------------------------------
2019-03-18 20:20:23,630 [INFO] Summary:
2019-03-18 20:20:23,631 [INFO] Batch 171000, worst loss 0.082473 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:20:23,632 [INFO] Regularization: 317.746552 * 0.0000100000 = 0.0031774654
2019-03-18 20:20:23,632 [INFO] Sum of grad norms: 0.111803
2019-03-18 20:20:23,633 [INFO] ---------------------------------
2019-03-18 20:20:42,733 [INFO] ---------------------------------
2019-03-18 20:20:42,734 [INFO] Summary:
2019-03-18 20:20:42,734 [INFO] Batch 172000, worst loss 0.083496 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:20:42,735 [INFO] Regularization: 317.745453 * 0.0000100000 = 0.0031774545
2019-03-18 20:20:42,736 [INFO] Sum of grad norms: 0.144758
2019-03-18 20:20:42,736 [INFO] ---------------------------------
2019-03-18 20:21:01,912 [INFO] ---------------------------------
2019-03-18 20:21:01,913 [INFO] Summary:
2019-03-18 20:21:01,913 [INFO] Batch 173000, worst loss 0.083719 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:21:01,914 [INFO] Regularization: 317.745148 * 0.0000100000 = 0.0031774514
2019-03-18 20:21:01,915 [INFO] Sum of grad norms: 0.117941
2019-03-18 20:21:01,915 [INFO] ---------------------------------
2019-03-18 20:21:20,932 [INFO] ---------------------------------
2019-03-18 20:21:20,933 [INFO] Summary:
2019-03-18 20:21:20,933 [INFO] Batch 174000, worst loss 0.083337 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:21:20,934 [INFO] Regularization: 317.744965 * 0.0000100000 = 0.0031774496
2019-03-18 20:21:20,935 [INFO] Sum of grad norms: 0.162792
2019-03-18 20:21:20,935 [INFO] ---------------------------------
2019-03-18 20:21:39,593 [INFO] ---------------------------------
2019-03-18 20:21:39,594 [INFO] Summary:
2019-03-18 20:21:39,595 [INFO] Batch 175000, worst loss 0.084083 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:21:39,595 [INFO] Regularization: 317.744781 * 0.0000100000 = 0.0031774477
2019-03-18 20:21:39,595 [INFO] Sum of grad norms: 0.106554
2019-03-18 20:21:39,596 [INFO] ---------------------------------
2019-03-18 20:21:58,253 [INFO] ---------------------------------
2019-03-18 20:21:58,254 [INFO] Summary:
2019-03-18 20:21:58,255 [INFO] Batch 176000, worst loss 0.084083 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:21:58,255 [INFO] Regularization: 317.744812 * 0.0000100000 = 0.0031774479
2019-03-18 20:21:58,256 [INFO] Sum of grad norms: 0.140647
2019-03-18 20:21:58,256 [INFO] ---------------------------------
2019-03-18 20:22:17,145 [INFO] ---------------------------------
2019-03-18 20:22:17,146 [INFO] Summary:
2019-03-18 20:22:17,146 [INFO] Batch 177000, worst loss 0.082950 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:22:17,147 [INFO] Regularization: 317.744537 * 0.0000100000 = 0.0031774454
2019-03-18 20:22:17,147 [INFO] Sum of grad norms: 0.154009
2019-03-18 20:22:17,148 [INFO] ---------------------------------
2019-03-18 20:22:35,669 [INFO] ---------------------------------
2019-03-18 20:22:35,670 [INFO] Summary:
2019-03-18 20:22:35,671 [INFO] Batch 178000, worst loss 0.082950 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:22:35,671 [INFO] Regularization: 317.744141 * 0.0000100000 = 0.0031774414
2019-03-18 20:22:35,672 [INFO] Sum of grad norms: 0.146905
2019-03-18 20:22:35,672 [INFO] ---------------------------------
2019-03-18 20:22:54,420 [INFO] ---------------------------------
2019-03-18 20:22:54,421 [INFO] Summary:
2019-03-18 20:22:54,422 [INFO] Batch 179000, worst loss 0.086835 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:22:54,422 [INFO] Regularization: 317.744171 * 0.0000100000 = 0.0031774417
2019-03-18 20:22:54,423 [INFO] Sum of grad norms: 0.114179
2019-03-18 20:22:54,424 [INFO] ---------------------------------
2019-03-18 20:23:13,115 [INFO] ---------------------------------
2019-03-18 20:23:13,116 [INFO] Summary:
2019-03-18 20:23:13,117 [INFO] Batch 180000, worst loss 0.086834 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:23:13,117 [INFO] Regularization: 317.743988 * 0.0000100000 = 0.0031774398
2019-03-18 20:23:13,118 [INFO] Sum of grad norms: 0.112693
2019-03-18 20:23:13,119 [INFO] ---------------------------------
2019-03-18 20:23:18,020 [INFO] ---------------------------------
2019-03-18 20:23:18,021 [INFO] Evaluation:
2019-03-18 20:23:18,022 [INFO] Batch 180000, worst loss 0.079030 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:23:18,023 [INFO] ---------------------------------
2019-03-18 20:23:36,763 [INFO] ---------------------------------
2019-03-18 20:23:36,764 [INFO] Summary:
2019-03-18 20:23:36,765 [INFO] Batch 181000, worst loss 0.084055 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:23:36,765 [INFO] Regularization: 317.743622 * 0.0000100000 = 0.0031774361
2019-03-18 20:23:36,766 [INFO] Sum of grad norms: 0.149565
2019-03-18 20:23:36,766 [INFO] ---------------------------------
2019-03-18 20:23:55,483 [INFO] ---------------------------------
2019-03-18 20:23:55,484 [INFO] Summary:
2019-03-18 20:23:55,484 [INFO] Batch 182000, worst loss 0.082471 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:23:55,485 [INFO] Regularization: 317.743317 * 0.0000100000 = 0.0031774330
2019-03-18 20:23:55,485 [INFO] Sum of grad norms: 0.144476
2019-03-18 20:23:55,486 [INFO] ---------------------------------
2019-03-18 20:24:14,002 [INFO] ---------------------------------
2019-03-18 20:24:14,002 [INFO] Summary:
2019-03-18 20:24:14,003 [INFO] Batch 183000, worst loss 0.082904 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:24:14,004 [INFO] Regularization: 317.743286 * 0.0000100000 = 0.0031774328
2019-03-18 20:24:14,004 [INFO] Sum of grad norms: 0.156312
2019-03-18 20:24:14,005 [INFO] ---------------------------------
2019-03-18 20:24:32,442 [INFO] ---------------------------------
2019-03-18 20:24:32,443 [INFO] Summary:
2019-03-18 20:24:32,443 [INFO] Batch 184000, worst loss 0.083145 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:24:32,444 [INFO] Regularization: 317.743225 * 0.0000100000 = 0.0031774321
2019-03-18 20:24:32,444 [INFO] Sum of grad norms: 0.097270
2019-03-18 20:24:32,445 [INFO] ---------------------------------
2019-03-18 20:24:50,849 [INFO] ---------------------------------
2019-03-18 20:24:50,850 [INFO] Summary:
2019-03-18 20:24:50,850 [INFO] Batch 185000, worst loss 0.083145 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:24:50,851 [INFO] Regularization: 317.743164 * 0.0000100000 = 0.0031774316
2019-03-18 20:24:50,851 [INFO] Sum of grad norms: 0.090350
2019-03-18 20:24:50,852 [INFO] ---------------------------------
2019-03-18 20:25:09,684 [INFO] ---------------------------------
2019-03-18 20:25:09,685 [INFO] Summary:
2019-03-18 20:25:09,686 [INFO] Batch 186000, worst loss 0.081932 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:25:09,686 [INFO] Regularization: 317.743073 * 0.0000100000 = 0.0031774307
2019-03-18 20:25:09,687 [INFO] Sum of grad norms: 0.108536
2019-03-18 20:25:09,688 [INFO] ---------------------------------
2019-03-18 20:25:28,592 [INFO] ---------------------------------
2019-03-18 20:25:28,593 [INFO] Summary:
2019-03-18 20:25:28,594 [INFO] Batch 187000, worst loss 0.082610 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:25:28,596 [INFO] Regularization: 317.743011 * 0.0000100000 = 0.0031774300
2019-03-18 20:25:28,597 [INFO] Sum of grad norms: 0.103506
2019-03-18 20:25:28,598 [INFO] ---------------------------------
2019-03-18 20:25:47,546 [INFO] ---------------------------------
2019-03-18 20:25:47,547 [INFO] Summary:
2019-03-18 20:25:47,548 [INFO] Batch 188000, worst loss 0.083237 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:25:47,548 [INFO] Regularization: 317.742798 * 0.0000100000 = 0.0031774279
2019-03-18 20:25:47,549 [INFO] Sum of grad norms: 0.142390
2019-03-18 20:25:47,550 [INFO] ---------------------------------
2019-03-18 20:26:06,256 [INFO] ---------------------------------
2019-03-18 20:26:06,257 [INFO] Summary:
2019-03-18 20:26:06,257 [INFO] Batch 189000, worst loss 0.083500 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:26:06,258 [INFO] Regularization: 317.742737 * 0.0000100000 = 0.0031774272
2019-03-18 20:26:06,258 [INFO] Sum of grad norms: 0.163573
2019-03-18 20:26:06,259 [INFO] ---------------------------------
2019-03-18 20:26:25,162 [INFO] ---------------------------------
2019-03-18 20:26:25,163 [INFO] Summary:
2019-03-18 20:26:25,163 [INFO] Batch 190000, worst loss 0.083500 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:26:25,164 [INFO] Regularization: 317.742676 * 0.0000100000 = 0.0031774268
2019-03-18 20:26:25,164 [INFO] Sum of grad norms: 0.147028
2019-03-18 20:26:25,165 [INFO] ---------------------------------
2019-03-18 20:26:30,056 [INFO] ---------------------------------
2019-03-18 20:26:30,057 [INFO] Evaluation:
2019-03-18 20:26:30,060 [INFO] Batch 190000, worst loss 0.080363 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:26:30,061 [INFO] ---------------------------------
2019-03-18 20:26:49,271 [INFO] ---------------------------------
2019-03-18 20:26:49,272 [INFO] Summary:
2019-03-18 20:26:49,273 [INFO] Batch 191000, worst loss 0.083371 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:26:49,274 [INFO] Regularization: 317.742676 * 0.0000100000 = 0.0031774268
2019-03-18 20:26:49,275 [INFO] Sum of grad norms: 0.141188
2019-03-18 20:26:49,276 [INFO] ---------------------------------
2019-03-18 20:27:07,987 [INFO] ---------------------------------
2019-03-18 20:27:07,988 [INFO] Summary:
2019-03-18 20:27:07,988 [INFO] Batch 192000, worst loss 0.082320 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:27:07,989 [INFO] Regularization: 317.742432 * 0.0000100000 = 0.0031774242
2019-03-18 20:27:07,990 [INFO] Sum of grad norms: 0.098779
2019-03-18 20:27:07,990 [INFO] ---------------------------------
2019-03-18 20:27:26,672 [INFO] ---------------------------------
2019-03-18 20:27:26,673 [INFO] Summary:
2019-03-18 20:27:26,673 [INFO] Batch 193000, worst loss 0.082397 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:27:26,674 [INFO] Regularization: 317.742401 * 0.0000100000 = 0.0031774240
2019-03-18 20:27:26,674 [INFO] Sum of grad norms: 0.159848
2019-03-18 20:27:26,675 [INFO] ---------------------------------
2019-03-18 20:27:45,265 [INFO] ---------------------------------
2019-03-18 20:27:45,266 [INFO] Summary:
2019-03-18 20:27:45,266 [INFO] Batch 194000, worst loss 0.082965 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:27:45,267 [INFO] Regularization: 317.742371 * 0.0000100000 = 0.0031774237
2019-03-18 20:27:45,268 [INFO] Sum of grad norms: 0.105710
2019-03-18 20:27:45,268 [INFO] ---------------------------------
2019-03-18 20:28:04,059 [INFO] ---------------------------------
2019-03-18 20:28:04,060 [INFO] Summary:
2019-03-18 20:28:04,061 [INFO] Batch 195000, worst loss 0.082965 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:28:04,061 [INFO] Regularization: 317.742432 * 0.0000100000 = 0.0031774242
2019-03-18 20:28:04,062 [INFO] Sum of grad norms: 0.146461
2019-03-18 20:28:04,062 [INFO] ---------------------------------
2019-03-18 20:28:22,702 [INFO] ---------------------------------
2019-03-18 20:28:22,703 [INFO] Summary:
2019-03-18 20:28:22,704 [INFO] Batch 196000, worst loss 0.083395 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:28:22,704 [INFO] Regularization: 317.742371 * 0.0000100000 = 0.0031774237
2019-03-18 20:28:22,705 [INFO] Sum of grad norms: 0.104377
2019-03-18 20:28:22,706 [INFO] ---------------------------------
2019-03-18 20:28:41,683 [INFO] ---------------------------------
2019-03-18 20:28:41,684 [INFO] Summary:
2019-03-18 20:28:41,684 [INFO] Batch 197000, worst loss 0.083120 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:28:41,685 [INFO] Regularization: 317.742371 * 0.0000100000 = 0.0031774237
2019-03-18 20:28:41,685 [INFO] Sum of grad norms: 0.106051
2019-03-18 20:28:41,686 [INFO] ---------------------------------
2019-03-18 20:29:00,252 [INFO] ---------------------------------
2019-03-18 20:29:00,253 [INFO] Summary:
2019-03-18 20:29:00,254 [INFO] Batch 198000, worst loss 0.084003 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:29:00,255 [INFO] Regularization: 317.742340 * 0.0000100000 = 0.0031774233
2019-03-18 20:29:00,255 [INFO] Sum of grad norms: 0.141637
2019-03-18 20:29:00,256 [INFO] ---------------------------------
2019-03-18 20:29:19,180 [INFO] ---------------------------------
2019-03-18 20:29:19,181 [INFO] Summary:
2019-03-18 20:29:19,182 [INFO] Batch 199000, worst loss 0.084061 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:29:19,182 [INFO] Regularization: 317.742310 * 0.0000100000 = 0.0031774230
2019-03-18 20:29:19,183 [INFO] Sum of grad norms: 0.200494
2019-03-18 20:29:19,184 [INFO] ---------------------------------
2019-03-18 20:29:37,681 [INFO] ---------------------------------
2019-03-18 20:29:37,683 [INFO] Summary:
2019-03-18 20:29:37,683 [INFO] Batch 200000, worst loss 0.084062 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:29:37,684 [INFO] Regularization: 317.742249 * 0.0000100000 = 0.0031774223
2019-03-18 20:29:37,685 [INFO] Sum of grad norms: 0.119818
2019-03-18 20:29:37,685 [INFO] ---------------------------------
2019-03-18 20:29:42,590 [INFO] ---------------------------------
2019-03-18 20:29:42,592 [INFO] Evaluation:
2019-03-18 20:29:42,592 [INFO] Batch 200000, worst loss 0.080309 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:29:42,593 [INFO] ---------------------------------
2019-03-18 20:30:01,159 [INFO] ---------------------------------
2019-03-18 20:30:01,160 [INFO] Summary:
2019-03-18 20:30:01,160 [INFO] Batch 201000, worst loss 0.085382 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:30:01,161 [INFO] Regularization: 317.742249 * 0.0000100000 = 0.0031774223
2019-03-18 20:30:01,161 [INFO] Sum of grad norms: 0.094735
2019-03-18 20:30:01,162 [INFO] ---------------------------------
2019-03-18 20:30:19,844 [INFO] ---------------------------------
2019-03-18 20:30:19,845 [INFO] Summary:
2019-03-18 20:30:19,845 [INFO] Batch 202000, worst loss 0.085382 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:30:19,846 [INFO] Regularization: 317.742096 * 0.0000100000 = 0.0031774209
2019-03-18 20:30:19,846 [INFO] Sum of grad norms: 0.139562
2019-03-18 20:30:19,847 [INFO] ---------------------------------
2019-03-18 20:30:38,675 [INFO] ---------------------------------
2019-03-18 20:30:38,676 [INFO] Summary:
2019-03-18 20:30:38,677 [INFO] Batch 203000, worst loss 0.083967 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:30:38,677 [INFO] Regularization: 317.742065 * 0.0000100000 = 0.0031774205
2019-03-18 20:30:38,678 [INFO] Sum of grad norms: 0.175429
2019-03-18 20:30:38,678 [INFO] ---------------------------------
2019-03-18 20:30:57,399 [INFO] ---------------------------------
2019-03-18 20:30:57,400 [INFO] Summary:
2019-03-18 20:30:57,401 [INFO] Batch 204000, worst loss 0.083967 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:30:57,402 [INFO] Regularization: 317.742096 * 0.0000100000 = 0.0031774209
2019-03-18 20:30:57,402 [INFO] Sum of grad norms: 0.157359
2019-03-18 20:30:57,403 [INFO] ---------------------------------
2019-03-18 20:31:16,043 [INFO] ---------------------------------
2019-03-18 20:31:16,045 [INFO] Summary:
2019-03-18 20:31:16,045 [INFO] Batch 205000, worst loss 0.084045 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:31:16,046 [INFO] Regularization: 317.742065 * 0.0000100000 = 0.0031774205
2019-03-18 20:31:16,046 [INFO] Sum of grad norms: 0.149763
2019-03-18 20:31:16,047 [INFO] ---------------------------------
2019-03-18 20:31:34,746 [INFO] ---------------------------------
2019-03-18 20:31:34,747 [INFO] Summary:
2019-03-18 20:31:34,747 [INFO] Batch 206000, worst loss 0.083579 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:31:34,748 [INFO] Regularization: 317.742065 * 0.0000100000 = 0.0031774205
2019-03-18 20:31:34,748 [INFO] Sum of grad norms: 0.194673
2019-03-18 20:31:34,749 [INFO] ---------------------------------
2019-03-18 20:31:53,920 [INFO] ---------------------------------
2019-03-18 20:31:53,921 [INFO] Summary:
2019-03-18 20:31:53,921 [INFO] Batch 207000, worst loss 0.083075 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:31:53,922 [INFO] Regularization: 317.742004 * 0.0000100000 = 0.0031774200
2019-03-18 20:31:53,923 [INFO] Sum of grad norms: 0.090316
2019-03-18 20:31:53,923 [INFO] ---------------------------------
2019-03-18 20:32:12,631 [INFO] ---------------------------------
2019-03-18 20:32:12,632 [INFO] Summary:
2019-03-18 20:32:12,632 [INFO] Batch 208000, worst loss 0.081922 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:32:12,633 [INFO] Regularization: 317.742004 * 0.0000100000 = 0.0031774200
2019-03-18 20:32:12,634 [INFO] Sum of grad norms: 0.128292
2019-03-18 20:32:12,634 [INFO] ---------------------------------
2019-03-18 20:32:31,527 [INFO] ---------------------------------
2019-03-18 20:32:31,527 [INFO] Summary:
2019-03-18 20:32:31,528 [INFO] Batch 209000, worst loss 0.083846 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:32:31,528 [INFO] Regularization: 317.742004 * 0.0000100000 = 0.0031774200
2019-03-18 20:32:31,529 [INFO] Sum of grad norms: 0.140680
2019-03-18 20:32:31,529 [INFO] ---------------------------------
2019-03-18 20:32:50,892 [INFO] ---------------------------------
2019-03-18 20:32:50,893 [INFO] Summary:
2019-03-18 20:32:50,894 [INFO] Batch 210000, worst loss 0.083355 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:32:50,894 [INFO] Regularization: 317.742004 * 0.0000100000 = 0.0031774200
2019-03-18 20:32:50,895 [INFO] Sum of grad norms: 0.137274
2019-03-18 20:32:50,896 [INFO] ---------------------------------
2019-03-18 20:32:55,845 [INFO] ---------------------------------
2019-03-18 20:32:55,846 [INFO] Evaluation:
2019-03-18 20:32:55,847 [INFO] Batch 210000, worst loss 0.079881 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:32:55,848 [INFO] ---------------------------------
2019-03-18 20:33:14,399 [INFO] ---------------------------------
2019-03-18 20:33:14,400 [INFO] Summary:
2019-03-18 20:33:14,400 [INFO] Batch 211000, worst loss 0.083213 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:33:14,401 [INFO] Regularization: 317.741974 * 0.0000100000 = 0.0031774198
2019-03-18 20:33:14,401 [INFO] Sum of grad norms: 0.086261
2019-03-18 20:33:14,402 [INFO] ---------------------------------
2019-03-18 20:33:33,205 [INFO] ---------------------------------
2019-03-18 20:33:33,206 [INFO] Summary:
2019-03-18 20:33:33,207 [INFO] Batch 212000, worst loss 0.083213 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:33:33,207 [INFO] Regularization: 317.741943 * 0.0000100000 = 0.0031774193
2019-03-18 20:33:33,208 [INFO] Sum of grad norms: 0.124986
2019-03-18 20:33:33,208 [INFO] ---------------------------------
2019-03-18 20:33:52,060 [INFO] ---------------------------------
2019-03-18 20:33:52,061 [INFO] Summary:
2019-03-18 20:33:52,062 [INFO] Batch 213000, worst loss 0.084728 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:33:52,062 [INFO] Regularization: 317.741943 * 0.0000100000 = 0.0031774193
2019-03-18 20:33:52,063 [INFO] Sum of grad norms: 0.097640
2019-03-18 20:33:52,064 [INFO] ---------------------------------
2019-03-18 20:34:11,044 [INFO] ---------------------------------
2019-03-18 20:34:11,045 [INFO] Summary:
2019-03-18 20:34:11,046 [INFO] Batch 214000, worst loss 0.085301 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:34:11,046 [INFO] Regularization: 317.741913 * 0.0000100000 = 0.0031774191
2019-03-18 20:34:11,047 [INFO] Sum of grad norms: 0.153586
2019-03-18 20:34:11,048 [INFO] ---------------------------------
2019-03-18 20:34:29,675 [INFO] ---------------------------------
2019-03-18 20:34:29,676 [INFO] Summary:
2019-03-18 20:34:29,677 [INFO] Batch 215000, worst loss 0.083987 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:34:29,677 [INFO] Regularization: 317.741943 * 0.0000100000 = 0.0031774193
2019-03-18 20:34:29,678 [INFO] Sum of grad norms: 0.145514
2019-03-18 20:34:29,679 [INFO] ---------------------------------
2019-03-18 20:34:48,816 [INFO] ---------------------------------
2019-03-18 20:34:48,817 [INFO] Summary:
2019-03-18 20:34:48,817 [INFO] Batch 216000, worst loss 0.082694 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:34:48,818 [INFO] Regularization: 317.741913 * 0.0000100000 = 0.0031774191
2019-03-18 20:34:48,818 [INFO] Sum of grad norms: 0.127244
2019-03-18 20:34:48,819 [INFO] ---------------------------------
2019-03-18 20:35:07,598 [INFO] ---------------------------------
2019-03-18 20:35:07,599 [INFO] Summary:
2019-03-18 20:35:07,600 [INFO] Batch 217000, worst loss 0.083960 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:35:07,600 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:35:07,601 [INFO] Sum of grad norms: 0.135937
2019-03-18 20:35:07,602 [INFO] ---------------------------------
2019-03-18 20:35:26,252 [INFO] ---------------------------------
2019-03-18 20:35:26,253 [INFO] Summary:
2019-03-18 20:35:26,254 [INFO] Batch 218000, worst loss 0.083770 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:35:26,255 [INFO] Regularization: 317.741943 * 0.0000100000 = 0.0031774193
2019-03-18 20:35:26,255 [INFO] Sum of grad norms: 0.104748
2019-03-18 20:35:26,256 [INFO] ---------------------------------
2019-03-18 20:35:45,146 [INFO] ---------------------------------
2019-03-18 20:35:45,147 [INFO] Summary:
2019-03-18 20:35:45,147 [INFO] Batch 219000, worst loss 0.083491 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:35:45,148 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:35:45,149 [INFO] Sum of grad norms: 0.094050
2019-03-18 20:35:45,149 [INFO] ---------------------------------
2019-03-18 20:36:04,084 [INFO] ---------------------------------
2019-03-18 20:36:04,084 [INFO] Summary:
2019-03-18 20:36:04,085 [INFO] Batch 220000, worst loss 0.083757 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:36:04,086 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:36:04,086 [INFO] Sum of grad norms: 0.123865
2019-03-18 20:36:04,087 [INFO] ---------------------------------
2019-03-18 20:36:09,061 [INFO] ---------------------------------
2019-03-18 20:36:09,062 [INFO] Evaluation:
2019-03-18 20:36:09,063 [INFO] Batch 220000, worst loss 0.080580 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:36:09,064 [INFO] ---------------------------------
2019-03-18 20:36:28,047 [INFO] ---------------------------------
2019-03-18 20:36:28,048 [INFO] Summary:
2019-03-18 20:36:28,049 [INFO] Batch 221000, worst loss 0.084845 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:36:28,049 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:36:28,050 [INFO] Sum of grad norms: 0.126006
2019-03-18 20:36:28,051 [INFO] ---------------------------------
2019-03-18 20:36:46,970 [INFO] ---------------------------------
2019-03-18 20:36:46,971 [INFO] Summary:
2019-03-18 20:36:46,972 [INFO] Batch 222000, worst loss 0.083303 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:36:46,972 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:36:46,973 [INFO] Sum of grad norms: 0.123644
2019-03-18 20:36:46,973 [INFO] ---------------------------------
2019-03-18 20:37:05,987 [INFO] ---------------------------------
2019-03-18 20:37:05,988 [INFO] Summary:
2019-03-18 20:37:05,989 [INFO] Batch 223000, worst loss 0.083303 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:37:05,989 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:37:05,990 [INFO] Sum of grad norms: 0.123918
2019-03-18 20:37:05,990 [INFO] ---------------------------------
2019-03-18 20:37:24,622 [INFO] ---------------------------------
2019-03-18 20:37:24,623 [INFO] Summary:
2019-03-18 20:37:24,624 [INFO] Batch 224000, worst loss 0.083188 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:37:24,624 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:37:24,625 [INFO] Sum of grad norms: 0.206433
2019-03-18 20:37:24,626 [INFO] ---------------------------------
2019-03-18 20:37:43,741 [INFO] ---------------------------------
2019-03-18 20:37:43,742 [INFO] Summary:
2019-03-18 20:37:43,743 [INFO] Batch 225000, worst loss 0.081665 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:37:43,743 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:37:43,744 [INFO] Sum of grad norms: 0.125130
2019-03-18 20:37:43,745 [INFO] ---------------------------------
2019-03-18 20:38:02,184 [INFO] ---------------------------------
2019-03-18 20:38:02,185 [INFO] Summary:
2019-03-18 20:38:02,186 [INFO] Batch 226000, worst loss 0.083148 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:38:02,186 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:38:02,187 [INFO] Sum of grad norms: 0.156553
2019-03-18 20:38:02,187 [INFO] ---------------------------------
2019-03-18 20:38:21,126 [INFO] ---------------------------------
2019-03-18 20:38:21,127 [INFO] Summary:
2019-03-18 20:38:21,128 [INFO] Batch 227000, worst loss 0.083910 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:38:21,128 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:38:21,129 [INFO] Sum of grad norms: 0.093449
2019-03-18 20:38:21,129 [INFO] ---------------------------------
2019-03-18 20:38:40,024 [INFO] ---------------------------------
2019-03-18 20:38:40,025 [INFO] Summary:
2019-03-18 20:38:40,026 [INFO] Batch 228000, worst loss 0.083566 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:38:40,026 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:38:40,027 [INFO] Sum of grad norms: 0.121904
2019-03-18 20:38:40,027 [INFO] ---------------------------------
2019-03-18 20:38:58,755 [INFO] ---------------------------------
2019-03-18 20:38:58,756 [INFO] Summary:
2019-03-18 20:38:58,757 [INFO] Batch 229000, worst loss 0.083584 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:38:58,757 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:38:58,758 [INFO] Sum of grad norms: 0.106512
2019-03-18 20:38:58,758 [INFO] ---------------------------------
2019-03-18 20:39:17,738 [INFO] ---------------------------------
2019-03-18 20:39:17,739 [INFO] Summary:
2019-03-18 20:39:17,739 [INFO] Batch 230000, worst loss 0.083584 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:39:17,740 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:39:17,740 [INFO] Sum of grad norms: 0.107767
2019-03-18 20:39:17,741 [INFO] ---------------------------------
2019-03-18 20:39:22,590 [INFO] ---------------------------------
2019-03-18 20:39:22,591 [INFO] Evaluation:
2019-03-18 20:39:22,594 [INFO] Batch 230000, worst loss 0.079135 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:39:22,596 [INFO] ---------------------------------
2019-03-18 20:39:40,975 [INFO] ---------------------------------
2019-03-18 20:39:40,976 [INFO] Summary:
2019-03-18 20:39:40,977 [INFO] Batch 231000, worst loss 0.081781 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:39:40,977 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:39:40,978 [INFO] Sum of grad norms: 0.087200
2019-03-18 20:39:40,979 [INFO] ---------------------------------
2019-03-18 20:39:59,776 [INFO] ---------------------------------
2019-03-18 20:39:59,777 [INFO] Summary:
2019-03-18 20:39:59,778 [INFO] Batch 232000, worst loss 0.082766 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:39:59,778 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:39:59,779 [INFO] Sum of grad norms: 0.117542
2019-03-18 20:39:59,779 [INFO] ---------------------------------
2019-03-18 20:40:18,615 [INFO] ---------------------------------
2019-03-18 20:40:18,616 [INFO] Summary:
2019-03-18 20:40:18,616 [INFO] Batch 233000, worst loss 0.083943 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:40:18,617 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:40:18,617 [INFO] Sum of grad norms: 0.190534
2019-03-18 20:40:18,618 [INFO] ---------------------------------
2019-03-18 20:40:37,201 [INFO] ---------------------------------
2019-03-18 20:40:37,202 [INFO] Summary:
2019-03-18 20:40:37,202 [INFO] Batch 234000, worst loss 0.084556 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:40:37,203 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:40:37,203 [INFO] Sum of grad norms: 0.112582
2019-03-18 20:40:37,204 [INFO] ---------------------------------
2019-03-18 20:40:56,266 [INFO] ---------------------------------
2019-03-18 20:40:56,267 [INFO] Summary:
2019-03-18 20:40:56,268 [INFO] Batch 235000, worst loss 0.084556 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:40:56,268 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:40:56,269 [INFO] Sum of grad norms: 0.112039
2019-03-18 20:40:56,269 [INFO] ---------------------------------
2019-03-18 20:41:14,815 [INFO] ---------------------------------
2019-03-18 20:41:14,816 [INFO] Summary:
2019-03-18 20:41:14,816 [INFO] Batch 236000, worst loss 0.083177 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:41:14,817 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:41:14,818 [INFO] Sum of grad norms: 0.108350
2019-03-18 20:41:14,818 [INFO] ---------------------------------
2019-03-18 20:41:33,233 [INFO] ---------------------------------
2019-03-18 20:41:33,234 [INFO] Summary:
2019-03-18 20:41:33,234 [INFO] Batch 237000, worst loss 0.083177 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:41:33,235 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:41:33,235 [INFO] Sum of grad norms: 0.121776
2019-03-18 20:41:33,236 [INFO] ---------------------------------
2019-03-18 20:41:52,791 [INFO] ---------------------------------
2019-03-18 20:41:52,792 [INFO] Summary:
2019-03-18 20:41:52,793 [INFO] Batch 238000, worst loss 0.083042 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:41:52,793 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:41:52,794 [INFO] Sum of grad norms: 0.136339
2019-03-18 20:41:52,794 [INFO] ---------------------------------
2019-03-18 20:42:11,453 [INFO] ---------------------------------
2019-03-18 20:42:11,454 [INFO] Summary:
2019-03-18 20:42:11,454 [INFO] Batch 239000, worst loss 0.083079 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:42:11,455 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:42:11,455 [INFO] Sum of grad norms: 0.179603
2019-03-18 20:42:11,456 [INFO] ---------------------------------
2019-03-18 20:42:30,024 [INFO] ---------------------------------
2019-03-18 20:42:30,025 [INFO] Summary:
2019-03-18 20:42:30,027 [INFO] Batch 240000, worst loss 0.083618 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:42:30,028 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:42:30,028 [INFO] Sum of grad norms: 0.134719
2019-03-18 20:42:30,029 [INFO] ---------------------------------
2019-03-18 20:42:35,053 [INFO] ---------------------------------
2019-03-18 20:42:35,054 [INFO] Evaluation:
2019-03-18 20:42:35,055 [INFO] Batch 240000, worst loss 0.081290 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:42:35,056 [INFO] ---------------------------------
2019-03-18 20:42:53,814 [INFO] ---------------------------------
2019-03-18 20:42:53,815 [INFO] Summary:
2019-03-18 20:42:53,815 [INFO] Batch 241000, worst loss 0.084734 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:42:53,816 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:42:53,816 [INFO] Sum of grad norms: 0.150676
2019-03-18 20:42:53,817 [INFO] ---------------------------------
2019-03-18 20:43:12,783 [INFO] ---------------------------------
2019-03-18 20:43:12,784 [INFO] Summary:
2019-03-18 20:43:12,785 [INFO] Batch 242000, worst loss 0.084734 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:43:12,785 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:43:12,786 [INFO] Sum of grad norms: 0.182246
2019-03-18 20:43:12,787 [INFO] ---------------------------------
2019-03-18 20:43:31,584 [INFO] ---------------------------------
2019-03-18 20:43:31,585 [INFO] Summary:
2019-03-18 20:43:31,586 [INFO] Batch 243000, worst loss 0.082897 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:43:31,586 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:43:31,587 [INFO] Sum of grad norms: 0.115565
2019-03-18 20:43:31,587 [INFO] ---------------------------------
2019-03-18 20:43:50,155 [INFO] ---------------------------------
2019-03-18 20:43:50,156 [INFO] Summary:
2019-03-18 20:43:50,157 [INFO] Batch 244000, worst loss 0.082258 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:43:50,157 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:43:50,158 [INFO] Sum of grad norms: 0.109712
2019-03-18 20:43:50,159 [INFO] ---------------------------------
2019-03-18 20:44:08,829 [INFO] ---------------------------------
2019-03-18 20:44:08,830 [INFO] Summary:
2019-03-18 20:44:08,830 [INFO] Batch 245000, worst loss 0.082932 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:44:08,831 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:44:08,831 [INFO] Sum of grad norms: 0.200563
2019-03-18 20:44:08,832 [INFO] ---------------------------------
2019-03-18 20:44:27,485 [INFO] ---------------------------------
2019-03-18 20:44:27,486 [INFO] Summary:
2019-03-18 20:44:27,486 [INFO] Batch 246000, worst loss 0.085430 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:44:27,487 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:44:27,487 [INFO] Sum of grad norms: 0.131429
2019-03-18 20:44:27,488 [INFO] ---------------------------------
2019-03-18 20:44:45,862 [INFO] ---------------------------------
2019-03-18 20:44:45,864 [INFO] Summary:
2019-03-18 20:44:45,864 [INFO] Batch 247000, worst loss 0.085430 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:44:45,865 [INFO] Regularization: 317.741882 * 0.0000100000 = 0.0031774188
2019-03-18 20:44:45,865 [INFO] Sum of grad norms: 0.162534
2019-03-18 20:44:45,866 [INFO] ---------------------------------
2019-03-18 20:45:04,138 [INFO] ---------------------------------
2019-03-18 20:45:04,138 [INFO] Summary:
2019-03-18 20:45:04,139 [INFO] Batch 248000, worst loss 0.083536 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:45:04,140 [INFO] Regularization: 317.741852 * 0.0000100000 = 0.0031774184
2019-03-18 20:45:04,140 [INFO] Sum of grad norms: 0.141143
2019-03-18 20:45:04,141 [INFO] ---------------------------------
2019-03-18 20:45:23,121 [INFO] ---------------------------------
2019-03-18 20:45:23,122 [INFO] Summary:
2019-03-18 20:45:23,123 [INFO] Batch 249000, worst loss 0.082320 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:45:23,123 [INFO] Regularization: 317.741852 * 0.0000100000 = 0.0031774184
2019-03-18 20:45:23,123 [INFO] Sum of grad norms: 0.130767
2019-03-18 20:45:23,124 [INFO] ---------------------------------
2019-03-18 20:45:41,698 [INFO] ---------------------------------
2019-03-18 20:45:41,699 [INFO] Summary:
2019-03-18 20:45:41,700 [INFO] Batch 250000, worst loss 0.083279 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 20:45:41,700 [INFO] Regularization: 317.741852 * 0.0000100000 = 0.0031774184
2019-03-18 20:45:41,701 [INFO] Sum of grad norms: 0.176758
2019-03-18 20:45:41,702 [INFO] ---------------------------------
2019-03-18 20:45:46,590 [INFO] ---------------------------------
2019-03-18 20:45:46,591 [INFO] Evaluation:
2019-03-18 20:45:46,592 [INFO] Batch 250000, worst loss 0.080358 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:45:46,593 [INFO] ---------------------------------
2019-03-18 20:45:46,594 [INFO] Finished training, saved to file classifier/1552933539/1552938346_0_classifier_final.pth
2019-03-18 20:45:46,767 [INFO] ---------------------------------
2019-03-18 20:45:46,769 [INFO] Training model #1: (1, 64, 201) @ 1
2019-03-18 20:46:02,737 [INFO] ---------------------------------
2019-03-18 20:46:02,739 [INFO] Summary:
2019-03-18 20:46:02,739 [INFO] Batch 1000, worst loss 27.477007 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-18 20:46:02,740 [INFO] Regularization: 8287.344727 * 0.0000100000 = 0.0828734487
2019-03-18 20:46:02,741 [INFO] Sum of grad norms: 1.989206
2019-03-18 20:46:02,742 [INFO] ---------------------------------
2019-03-18 20:46:18,637 [INFO] ---------------------------------
2019-03-18 20:46:18,638 [INFO] Summary:
2019-03-18 20:46:18,639 [INFO] Batch 2000, worst loss 0.284922 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-18 20:46:18,640 [INFO] Regularization: 5437.726562 * 0.0000100000 = 0.0543772653
2019-03-18 20:46:18,640 [INFO] reducing reg_loss_factor
2019-03-18 20:46:18,641 [INFO] Sum of grad norms: 2.069037
2019-03-18 20:46:18,641 [INFO] ---------------------------------
2019-03-18 20:46:37,788 [INFO] ---------------------------------
2019-03-18 20:46:37,789 [INFO] Summary:
2019-03-18 20:46:37,789 [INFO] Batch 3000, worst loss 0.093443 (incl. reg.) of 1000 batches, learning rate 0.002849 @cl.-depth 1
2019-03-18 20:46:37,790 [INFO] Regularization: 5418.779297 * 0.0000010000 = 0.0054187793
2019-03-18 20:46:37,791 [INFO] Sum of grad norms: 1.885466
2019-03-18 20:46:37,791 [INFO] ---------------------------------
2019-03-18 20:46:56,702 [INFO] ---------------------------------
2019-03-18 20:46:56,703 [INFO] Summary:
2019-03-18 20:46:56,704 [INFO] Batch 4000, worst loss 0.075363 (incl. reg.) of 1000 batches, learning rate 0.000934 @cl.-depth 1
2019-03-18 20:46:56,705 [INFO] Regularization: 5333.357422 * 0.0000010000 = 0.0053333575
2019-03-18 20:46:56,705 [INFO] Sum of grad norms: 0.325559
2019-03-18 20:46:56,706 [INFO] ---------------------------------
2019-03-18 20:47:15,621 [INFO] ---------------------------------
2019-03-18 20:47:15,622 [INFO] Summary:
2019-03-18 20:47:15,623 [INFO] Batch 5000, worst loss 0.072274 (incl. reg.) of 1000 batches, learning rate 0.000754 @cl.-depth 1
2019-03-18 20:47:15,623 [INFO] Regularization: 5229.581543 * 0.0000010000 = 0.0052295816
2019-03-18 20:47:15,624 [INFO] Sum of grad norms: 0.089862
2019-03-18 20:47:15,625 [INFO] ---------------------------------
2019-03-18 20:47:34,269 [INFO] ---------------------------------
2019-03-18 20:47:34,270 [INFO] Summary:
2019-03-18 20:47:34,271 [INFO] Batch 6000, worst loss 0.072484 (incl. reg.) of 1000 batches, learning rate 0.000723 @cl.-depth 1
2019-03-18 20:47:34,272 [INFO] Regularization: 5122.270996 * 0.0000010000 = 0.0051222709
2019-03-18 20:47:34,272 [INFO] Sum of grad norms: 0.144551
2019-03-18 20:47:34,273 [INFO] ---------------------------------
2019-03-18 20:47:52,969 [INFO] ---------------------------------
2019-03-18 20:47:52,969 [INFO] Summary:
2019-03-18 20:47:52,970 [INFO] Batch 7000, worst loss 0.073780 (incl. reg.) of 1000 batches, learning rate 0.000723 @cl.-depth 1
2019-03-18 20:47:52,970 [INFO] Regularization: 5011.170410 * 0.0000010000 = 0.0050111702
2019-03-18 20:47:52,971 [INFO] Sum of grad norms: 0.732613
2019-03-18 20:47:52,972 [INFO] ---------------------------------
2019-03-18 20:48:12,137 [INFO] ---------------------------------
2019-03-18 20:48:12,138 [INFO] Summary:
2019-03-18 20:48:12,138 [INFO] Batch 8000, worst loss 0.070472 (incl. reg.) of 1000 batches, learning rate 0.000723 @cl.-depth 1
2019-03-18 20:48:12,139 [INFO] Regularization: 4899.489746 * 0.0000010000 = 0.0048994897
2019-03-18 20:48:12,139 [INFO] Sum of grad norms: 0.416390
2019-03-18 20:48:12,140 [INFO] ---------------------------------
2019-03-18 20:48:30,887 [INFO] ---------------------------------
2019-03-18 20:48:30,888 [INFO] Summary:
2019-03-18 20:48:30,889 [INFO] Batch 9000, worst loss 0.071544 (incl. reg.) of 1000 batches, learning rate 0.000705 @cl.-depth 1
2019-03-18 20:48:30,889 [INFO] Regularization: 4777.938965 * 0.0000010000 = 0.0047779391
2019-03-18 20:48:30,890 [INFO] Sum of grad norms: 0.670745
2019-03-18 20:48:30,891 [INFO] ---------------------------------
2019-03-18 20:48:49,639 [INFO] ---------------------------------
2019-03-18 20:48:49,640 [INFO] Summary:
2019-03-18 20:48:49,641 [INFO] Batch 10000, worst loss 0.071333 (incl. reg.) of 1000 batches, learning rate 0.000705 @cl.-depth 1
2019-03-18 20:48:49,641 [INFO] Regularization: 4674.546387 * 0.0000010000 = 0.0046745464
2019-03-18 20:48:49,642 [INFO] Sum of grad norms: 0.689429
2019-03-18 20:48:49,642 [INFO] ---------------------------------
2019-03-18 20:48:54,592 [INFO] ---------------------------------
2019-03-18 20:48:54,593 [INFO] Evaluation:
2019-03-18 20:48:54,594 [INFO] Batch 10000, worst loss 0.062896 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:48:54,594 [INFO] ---------------------------------
2019-03-18 20:49:13,692 [INFO] ---------------------------------
2019-03-18 20:49:13,692 [INFO] Summary:
2019-03-18 20:49:13,693 [INFO] Batch 11000, worst loss 0.069167 (incl. reg.) of 1000 batches, learning rate 0.000705 @cl.-depth 1
2019-03-18 20:49:13,694 [INFO] Regularization: 4581.807617 * 0.0000010000 = 0.0045818076
2019-03-18 20:49:13,694 [INFO] Sum of grad norms: 0.250059
2019-03-18 20:49:13,695 [INFO] ---------------------------------
2019-03-18 20:49:32,374 [INFO] ---------------------------------
2019-03-18 20:49:32,375 [INFO] Summary:
2019-03-18 20:49:32,376 [INFO] Batch 12000, worst loss 0.069983 (incl. reg.) of 1000 batches, learning rate 0.000692 @cl.-depth 1
2019-03-18 20:49:32,376 [INFO] Regularization: 4486.820312 * 0.0000010000 = 0.0044868202
2019-03-18 20:49:32,377 [INFO] Sum of grad norms: 0.112740
2019-03-18 20:49:32,378 [INFO] ---------------------------------
2019-03-18 20:49:51,338 [INFO] ---------------------------------
2019-03-18 20:49:51,339 [INFO] Summary:
2019-03-18 20:49:51,339 [INFO] Batch 13000, worst loss 0.067190 (incl. reg.) of 1000 batches, learning rate 0.000692 @cl.-depth 1
2019-03-18 20:49:51,340 [INFO] Regularization: 4416.404785 * 0.0000010000 = 0.0044164048
2019-03-18 20:49:51,341 [INFO] Sum of grad norms: 0.102931
2019-03-18 20:49:51,341 [INFO] ---------------------------------
2019-03-18 20:50:10,257 [INFO] ---------------------------------
2019-03-18 20:50:10,258 [INFO] Summary:
2019-03-18 20:50:10,258 [INFO] Batch 14000, worst loss 0.067435 (incl. reg.) of 1000 batches, learning rate 0.000672 @cl.-depth 1
2019-03-18 20:50:10,259 [INFO] Regularization: 4338.338867 * 0.0000010000 = 0.0043383390
2019-03-18 20:50:10,259 [INFO] Sum of grad norms: 0.523689
2019-03-18 20:50:10,260 [INFO] ---------------------------------
2019-03-18 20:50:28,976 [INFO] ---------------------------------
2019-03-18 20:50:28,977 [INFO] Summary:
2019-03-18 20:50:28,978 [INFO] Batch 15000, worst loss 0.068083 (incl. reg.) of 1000 batches, learning rate 0.000672 @cl.-depth 1
2019-03-18 20:50:28,978 [INFO] Regularization: 4279.859375 * 0.0000010000 = 0.0042798594
2019-03-18 20:50:28,979 [INFO] Sum of grad norms: 1.365656
2019-03-18 20:50:28,979 [INFO] ---------------------------------
2019-03-18 20:50:47,782 [INFO] ---------------------------------
2019-03-18 20:50:47,783 [INFO] Summary:
2019-03-18 20:50:47,784 [INFO] Batch 16000, worst loss 0.067761 (incl. reg.) of 1000 batches, learning rate 0.000672 @cl.-depth 1
2019-03-18 20:50:47,785 [INFO] Regularization: 4221.693848 * 0.0000010000 = 0.0042216936
2019-03-18 20:50:47,785 [INFO] Sum of grad norms: 0.406843
2019-03-18 20:50:47,786 [INFO] ---------------------------------
2019-03-18 20:51:06,502 [INFO] ---------------------------------
2019-03-18 20:51:06,503 [INFO] Summary:
2019-03-18 20:51:06,504 [INFO] Batch 17000, worst loss 0.066348 (incl. reg.) of 1000 batches, learning rate 0.000672 @cl.-depth 1
2019-03-18 20:51:06,504 [INFO] Regularization: 4158.973145 * 0.0000010000 = 0.0041589732
2019-03-18 20:51:06,505 [INFO] Sum of grad norms: 0.938160
2019-03-18 20:51:06,505 [INFO] ---------------------------------
2019-03-18 20:51:25,466 [INFO] ---------------------------------
2019-03-18 20:51:25,467 [INFO] Summary:
2019-03-18 20:51:25,467 [INFO] Batch 18000, worst loss 0.065861 (incl. reg.) of 1000 batches, learning rate 0.000663 @cl.-depth 1
2019-03-18 20:51:25,468 [INFO] Regularization: 4114.027832 * 0.0000010000 = 0.0041140276
2019-03-18 20:51:25,468 [INFO] Sum of grad norms: 0.864593
2019-03-18 20:51:25,469 [INFO] ---------------------------------
2019-03-18 20:51:44,267 [INFO] ---------------------------------
2019-03-18 20:51:44,268 [INFO] Summary:
2019-03-18 20:51:44,269 [INFO] Batch 19000, worst loss 0.065079 (incl. reg.) of 1000 batches, learning rate 0.000659 @cl.-depth 1
2019-03-18 20:51:44,269 [INFO] Regularization: 4051.031250 * 0.0000010000 = 0.0040510311
2019-03-18 20:51:44,270 [INFO] Sum of grad norms: 0.344831
2019-03-18 20:51:44,271 [INFO] ---------------------------------
2019-03-18 20:52:02,729 [INFO] ---------------------------------
2019-03-18 20:52:02,730 [INFO] Summary:
2019-03-18 20:52:02,731 [INFO] Batch 20000, worst loss 0.066694 (incl. reg.) of 1000 batches, learning rate 0.000651 @cl.-depth 1
2019-03-18 20:52:02,731 [INFO] Regularization: 4005.605713 * 0.0000010000 = 0.0040056058
2019-03-18 20:52:02,732 [INFO] Sum of grad norms: 0.068043
2019-03-18 20:52:02,733 [INFO] ---------------------------------
2019-03-18 20:52:07,648 [INFO] ---------------------------------
2019-03-18 20:52:07,649 [INFO] Evaluation:
2019-03-18 20:52:07,650 [INFO] Batch 20000, worst loss 0.060424 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:52:07,650 [INFO] ---------------------------------
2019-03-18 20:52:26,552 [INFO] ---------------------------------
2019-03-18 20:52:26,553 [INFO] Summary:
2019-03-18 20:52:26,554 [INFO] Batch 21000, worst loss 0.065093 (incl. reg.) of 1000 batches, learning rate 0.000651 @cl.-depth 1
2019-03-18 20:52:26,554 [INFO] Regularization: 3942.601074 * 0.0000010000 = 0.0039426009
2019-03-18 20:52:26,555 [INFO] Sum of grad norms: 0.091764
2019-03-18 20:52:26,556 [INFO] ---------------------------------
2019-03-18 20:52:45,723 [INFO] ---------------------------------
2019-03-18 20:52:45,724 [INFO] Summary:
2019-03-18 20:52:45,724 [INFO] Batch 22000, worst loss 0.065605 (incl. reg.) of 1000 batches, learning rate 0.000651 @cl.-depth 1
2019-03-18 20:52:45,725 [INFO] Regularization: 3882.765137 * 0.0000010000 = 0.0038827651
2019-03-18 20:52:45,725 [INFO] Sum of grad norms: 0.399100
2019-03-18 20:52:45,726 [INFO] ---------------------------------
2019-03-18 20:53:04,648 [INFO] ---------------------------------
2019-03-18 20:53:04,649 [INFO] Summary:
2019-03-18 20:53:04,650 [INFO] Batch 23000, worst loss 0.065179 (incl. reg.) of 1000 batches, learning rate 0.000651 @cl.-depth 1
2019-03-18 20:53:04,650 [INFO] Regularization: 3834.363037 * 0.0000010000 = 0.0038343631
2019-03-18 20:53:04,651 [INFO] Sum of grad norms: 0.352978
2019-03-18 20:53:04,652 [INFO] ---------------------------------
2019-03-18 20:53:23,820 [INFO] ---------------------------------
2019-03-18 20:53:23,821 [INFO] Summary:
2019-03-18 20:53:23,822 [INFO] Batch 24000, worst loss 0.064407 (incl. reg.) of 1000 batches, learning rate 0.000651 @cl.-depth 1
2019-03-18 20:53:23,822 [INFO] Regularization: 3777.734619 * 0.0000010000 = 0.0037777347
2019-03-18 20:53:23,823 [INFO] Sum of grad norms: 0.104228
2019-03-18 20:53:23,823 [INFO] ---------------------------------
2019-03-18 20:53:42,435 [INFO] ---------------------------------
2019-03-18 20:53:42,436 [INFO] Summary:
2019-03-18 20:53:42,437 [INFO] Batch 25000, worst loss 0.071050 (incl. reg.) of 1000 batches, learning rate 0.000644 @cl.-depth 1
2019-03-18 20:53:42,437 [INFO] Regularization: 3730.120117 * 0.0000010000 = 0.0037301201
2019-03-18 20:53:42,438 [INFO] Sum of grad norms: 0.073837
2019-03-18 20:53:42,438 [INFO] ---------------------------------
2019-03-18 20:54:01,348 [INFO] ---------------------------------
2019-03-18 20:54:01,349 [INFO] Summary:
2019-03-18 20:54:01,350 [INFO] Batch 26000, worst loss 0.064571 (incl. reg.) of 1000 batches, learning rate 0.000644 @cl.-depth 1
2019-03-18 20:54:01,350 [INFO] Regularization: 3673.761963 * 0.0000010000 = 0.0036737619
2019-03-18 20:54:01,351 [INFO] Sum of grad norms: 0.137011
2019-03-18 20:54:01,351 [INFO] ---------------------------------
2019-03-18 20:54:20,789 [INFO] ---------------------------------
2019-03-18 20:54:20,790 [INFO] Summary:
2019-03-18 20:54:20,790 [INFO] Batch 27000, worst loss 0.065176 (incl. reg.) of 1000 batches, learning rate 0.000644 @cl.-depth 1
2019-03-18 20:54:20,791 [INFO] Regularization: 3621.493896 * 0.0000010000 = 0.0036214939
2019-03-18 20:54:20,791 [INFO] Sum of grad norms: 0.070958
2019-03-18 20:54:20,792 [INFO] ---------------------------------
2019-03-18 20:54:39,717 [INFO] ---------------------------------
2019-03-18 20:54:39,718 [INFO] Summary:
2019-03-18 20:54:39,719 [INFO] Batch 28000, worst loss 0.064311 (incl. reg.) of 1000 batches, learning rate 0.000644 @cl.-depth 1
2019-03-18 20:54:39,719 [INFO] Regularization: 3572.229736 * 0.0000010000 = 0.0035722298
2019-03-18 20:54:39,720 [INFO] Sum of grad norms: 0.064738
2019-03-18 20:54:39,720 [INFO] ---------------------------------
2019-03-18 20:54:58,967 [INFO] ---------------------------------
2019-03-18 20:54:58,969 [INFO] Summary:
2019-03-18 20:54:58,969 [INFO] Batch 29000, worst loss 0.065061 (incl. reg.) of 1000 batches, learning rate 0.000643 @cl.-depth 1
2019-03-18 20:54:58,970 [INFO] Regularization: 3526.729248 * 0.0000010000 = 0.0035267293
2019-03-18 20:54:58,970 [INFO] Sum of grad norms: 0.157184
2019-03-18 20:54:58,971 [INFO] ---------------------------------
2019-03-18 20:55:18,012 [INFO] ---------------------------------
2019-03-18 20:55:18,013 [INFO] Summary:
2019-03-18 20:55:18,014 [INFO] Batch 30000, worst loss 0.064979 (incl. reg.) of 1000 batches, learning rate 0.000643 @cl.-depth 1
2019-03-18 20:55:18,014 [INFO] Regularization: 3478.539795 * 0.0000010000 = 0.0034785399
2019-03-18 20:55:18,015 [INFO] Sum of grad norms: 0.575739
2019-03-18 20:55:18,016 [INFO] ---------------------------------
2019-03-18 20:55:22,942 [INFO] ---------------------------------
2019-03-18 20:55:22,943 [INFO] Evaluation:
2019-03-18 20:55:22,944 [INFO] Batch 30000, worst loss 0.059596 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:55:22,945 [INFO] New best loss 0.059596, saved to file classifier/1552933539/1552938922_1_classifier_30000.pth
2019-03-18 20:55:22,957 [INFO] Target
2019-03-18 20:55:22,957 [INFO] [[0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-18 20:55:22,960 [INFO] Classifier output
2019-03-18 20:55:22,961 [INFO] [[0.009887 0.010323 0.011625 ... 0.990655 0.990005 0.990451]
 [0.989519 0.990197 0.991375 ... 0.016541 0.016853 0.020412]
 [0.984711 0.988879 0.98835  ... 0.013797 0.010806 0.011421]
 ...
 [0.98936  0.988585 0.98926  ... 0.013547 0.007166 0.009642]
 [0.990985 0.991152 0.991921 ... 0.989461 0.989302 0.990889]
 [0.990825 0.991697 0.991573 ... 0.990823 0.991838 0.993336]]
2019-03-18 20:55:22,962 [INFO] ---------------------------------
2019-03-18 20:55:42,070 [INFO] ---------------------------------
2019-03-18 20:55:42,071 [INFO] Summary:
2019-03-18 20:55:42,071 [INFO] Batch 31000, worst loss 0.065258 (incl. reg.) of 1000 batches, learning rate 0.000643 @cl.-depth 1
2019-03-18 20:55:42,072 [INFO] Regularization: 3428.858887 * 0.0000010000 = 0.0034288589
2019-03-18 20:55:42,073 [INFO] Sum of grad norms: 0.272077
2019-03-18 20:55:42,073 [INFO] ---------------------------------
2019-03-18 20:56:00,944 [INFO] ---------------------------------
2019-03-18 20:56:00,945 [INFO] Summary:
2019-03-18 20:56:00,946 [INFO] Batch 32000, worst loss 0.065775 (incl. reg.) of 1000 batches, learning rate 0.000643 @cl.-depth 1
2019-03-18 20:56:00,947 [INFO] Regularization: 3386.139648 * 0.0000010000 = 0.0033861396
2019-03-18 20:56:00,947 [INFO] Sum of grad norms: 0.134096
2019-03-18 20:56:00,948 [INFO] ---------------------------------
2019-03-18 20:56:20,029 [INFO] ---------------------------------
2019-03-18 20:56:20,030 [INFO] Summary:
2019-03-18 20:56:20,031 [INFO] Batch 33000, worst loss 0.063186 (incl. reg.) of 1000 batches, learning rate 0.000643 @cl.-depth 1
2019-03-18 20:56:20,031 [INFO] Regularization: 3336.939697 * 0.0000010000 = 0.0033369397
2019-03-18 20:56:20,032 [INFO] Sum of grad norms: 0.063959
2019-03-18 20:56:20,033 [INFO] ---------------------------------
2019-03-18 20:56:38,962 [INFO] ---------------------------------
2019-03-18 20:56:38,963 [INFO] Summary:
2019-03-18 20:56:38,963 [INFO] Batch 34000, worst loss 0.064383 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-18 20:56:38,964 [INFO] Regularization: 3293.692871 * 0.0000010000 = 0.0032936928
2019-03-18 20:56:38,964 [INFO] Sum of grad norms: 0.050730
2019-03-18 20:56:38,965 [INFO] ---------------------------------
2019-03-18 20:56:57,693 [INFO] ---------------------------------
2019-03-18 20:56:57,694 [INFO] Summary:
2019-03-18 20:56:57,695 [INFO] Batch 35000, worst loss 0.063568 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-18 20:56:57,696 [INFO] Regularization: 3261.126465 * 0.0000010000 = 0.0032611263
2019-03-18 20:56:57,696 [INFO] Sum of grad norms: 0.206469
2019-03-18 20:56:57,697 [INFO] ---------------------------------
2019-03-18 20:57:16,964 [INFO] ---------------------------------
2019-03-18 20:57:16,965 [INFO] Summary:
2019-03-18 20:57:16,965 [INFO] Batch 36000, worst loss 0.063348 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-18 20:57:16,966 [INFO] Regularization: 3218.426514 * 0.0000010000 = 0.0032184266
2019-03-18 20:57:16,967 [INFO] Sum of grad norms: 0.113583
2019-03-18 20:57:16,967 [INFO] ---------------------------------
2019-03-18 20:57:35,889 [INFO] ---------------------------------
2019-03-18 20:57:35,890 [INFO] Summary:
2019-03-18 20:57:35,891 [INFO] Batch 37000, worst loss 0.064321 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-18 20:57:35,892 [INFO] Regularization: 3178.102539 * 0.0000010000 = 0.0031781024
2019-03-18 20:57:35,892 [INFO] Sum of grad norms: 0.869538
2019-03-18 20:57:35,893 [INFO] ---------------------------------
2019-03-18 20:57:55,206 [INFO] ---------------------------------
2019-03-18 20:57:55,206 [INFO] Summary:
2019-03-18 20:57:55,207 [INFO] Batch 38000, worst loss 0.062636 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-18 20:57:55,207 [INFO] Regularization: 3142.035645 * 0.0000010000 = 0.0031420356
2019-03-18 20:57:55,208 [INFO] Sum of grad norms: 0.150462
2019-03-18 20:57:55,208 [INFO] ---------------------------------
2019-03-18 20:58:14,115 [INFO] ---------------------------------
2019-03-18 20:58:14,116 [INFO] Summary:
2019-03-18 20:58:14,116 [INFO] Batch 39000, worst loss 0.063360 (incl. reg.) of 1000 batches, learning rate 0.000626 @cl.-depth 1
2019-03-18 20:58:14,117 [INFO] Regularization: 3111.419922 * 0.0000010000 = 0.0031114200
2019-03-18 20:58:14,118 [INFO] Sum of grad norms: 0.677511
2019-03-18 20:58:14,118 [INFO] ---------------------------------
2019-03-18 20:58:32,742 [INFO] ---------------------------------
2019-03-18 20:58:32,743 [INFO] Summary:
2019-03-18 20:58:32,744 [INFO] Batch 40000, worst loss 0.062923 (incl. reg.) of 1000 batches, learning rate 0.000626 @cl.-depth 1
2019-03-18 20:58:32,744 [INFO] Regularization: 3082.773438 * 0.0000010000 = 0.0030827734
2019-03-18 20:58:32,745 [INFO] Sum of grad norms: 0.244525
2019-03-18 20:58:32,745 [INFO] ---------------------------------
2019-03-18 20:58:37,717 [INFO] ---------------------------------
2019-03-18 20:58:37,718 [INFO] Evaluation:
2019-03-18 20:58:37,721 [INFO] Batch 40000, worst loss 0.059053 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 20:58:37,722 [INFO] New best loss 0.059053, saved to file classifier/1552933539/1552939117_1_classifier_40000.pth
2019-03-18 20:58:37,735 [INFO] Target
2019-03-18 20:58:37,736 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 ...
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]]
2019-03-18 20:58:37,738 [INFO] Classifier output
2019-03-18 20:58:37,739 [INFO] [[0.990621 0.990321 0.989531 ... 0.013212 0.006743 0.007933]
 [0.009246 0.012079 0.010621 ... 0.990328 0.988673 0.990711]
 [0.988951 0.989526 0.989341 ... 0.990909 0.99174  0.991732]
 ...
 [0.009725 0.009491 0.009116 ... 0.991812 0.992103 0.991055]
 [0.988948 0.989525 0.989345 ... 0.990853 0.991672 0.991697]
 [0.986652 0.988106 0.988528 ... 0.000034 0.000043 0.000051]]
2019-03-18 20:58:37,741 [INFO] ---------------------------------
2019-03-18 20:58:56,888 [INFO] ---------------------------------
2019-03-18 20:58:56,889 [INFO] Summary:
2019-03-18 20:58:56,890 [INFO] Batch 41000, worst loss 0.063264 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 20:58:56,891 [INFO] Regularization: 3051.081299 * 0.0000010000 = 0.0030510812
2019-03-18 20:58:56,891 [INFO] Sum of grad norms: 0.366611
2019-03-18 20:58:56,892 [INFO] ---------------------------------
2019-03-18 20:59:15,780 [INFO] ---------------------------------
2019-03-18 20:59:15,781 [INFO] Summary:
2019-03-18 20:59:15,782 [INFO] Batch 42000, worst loss 0.062057 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 20:59:15,783 [INFO] Regularization: 3022.191406 * 0.0000010000 = 0.0030221913
2019-03-18 20:59:15,783 [INFO] Sum of grad norms: 0.306721
2019-03-18 20:59:15,784 [INFO] ---------------------------------
2019-03-18 20:59:34,689 [INFO] ---------------------------------
2019-03-18 20:59:34,690 [INFO] Summary:
2019-03-18 20:59:34,691 [INFO] Batch 43000, worst loss 0.062240 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 20:59:34,691 [INFO] Regularization: 3004.038574 * 0.0000010000 = 0.0030040385
2019-03-18 20:59:34,692 [INFO] Sum of grad norms: 0.364260
2019-03-18 20:59:34,692 [INFO] ---------------------------------
2019-03-18 20:59:53,605 [INFO] ---------------------------------
2019-03-18 20:59:53,606 [INFO] Summary:
2019-03-18 20:59:53,606 [INFO] Batch 44000, worst loss 0.061905 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 20:59:53,607 [INFO] Regularization: 2982.423340 * 0.0000010000 = 0.0029824234
2019-03-18 20:59:53,607 [INFO] Sum of grad norms: 0.608318
2019-03-18 20:59:53,608 [INFO] ---------------------------------
2019-03-18 21:00:12,469 [INFO] ---------------------------------
2019-03-18 21:00:12,470 [INFO] Summary:
2019-03-18 21:00:12,470 [INFO] Batch 45000, worst loss 0.061935 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 21:00:12,471 [INFO] Regularization: 2963.816406 * 0.0000010000 = 0.0029638165
2019-03-18 21:00:12,471 [INFO] Sum of grad norms: 0.163101
2019-03-18 21:00:12,472 [INFO] ---------------------------------
2019-03-18 21:00:31,521 [INFO] ---------------------------------
2019-03-18 21:00:31,522 [INFO] Summary:
2019-03-18 21:00:31,523 [INFO] Batch 46000, worst loss 0.061970 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 21:00:31,524 [INFO] Regularization: 2946.749268 * 0.0000010000 = 0.0029467493
2019-03-18 21:00:31,524 [INFO] Sum of grad norms: 0.286540
2019-03-18 21:00:31,525 [INFO] ---------------------------------
2019-03-18 21:00:50,585 [INFO] ---------------------------------
2019-03-18 21:00:50,586 [INFO] Summary:
2019-03-18 21:00:50,586 [INFO] Batch 47000, worst loss 0.062399 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 21:00:50,587 [INFO] Regularization: 2929.966309 * 0.0000010000 = 0.0029299662
2019-03-18 21:00:50,588 [INFO] Sum of grad norms: 0.254984
2019-03-18 21:00:50,588 [INFO] ---------------------------------
2019-03-18 21:01:09,523 [INFO] ---------------------------------
2019-03-18 21:01:09,524 [INFO] Summary:
2019-03-18 21:01:09,525 [INFO] Batch 48000, worst loss 0.061703 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 21:01:09,525 [INFO] Regularization: 2912.690430 * 0.0000010000 = 0.0029126904
2019-03-18 21:01:09,526 [INFO] Sum of grad norms: 0.381195
2019-03-18 21:01:09,526 [INFO] ---------------------------------
2019-03-18 21:01:28,042 [INFO] ---------------------------------
2019-03-18 21:01:28,043 [INFO] Summary:
2019-03-18 21:01:28,043 [INFO] Batch 49000, worst loss 0.062164 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 21:01:28,044 [INFO] Regularization: 2898.396973 * 0.0000010000 = 0.0028983969
2019-03-18 21:01:28,044 [INFO] Sum of grad norms: 0.263721
2019-03-18 21:01:28,045 [INFO] ---------------------------------
2019-03-18 21:01:47,106 [INFO] ---------------------------------
2019-03-18 21:01:47,107 [INFO] Summary:
2019-03-18 21:01:47,108 [INFO] Batch 50000, worst loss 0.062130 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 21:01:47,108 [INFO] Regularization: 2882.220459 * 0.0000010000 = 0.0028822205
2019-03-18 21:01:47,109 [INFO] Sum of grad norms: 0.206638
2019-03-18 21:01:47,109 [INFO] ---------------------------------
2019-03-18 21:01:52,029 [INFO] ---------------------------------
2019-03-18 21:01:52,030 [INFO] Evaluation:
2019-03-18 21:01:52,031 [INFO] Batch 50000, worst loss 0.058681 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:01:52,032 [INFO] New best loss 0.058681, saved to file classifier/1552933539/1552939312_1_classifier_50000.pth
2019-03-18 21:01:52,043 [INFO] Target
2019-03-18 21:01:52,044 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-18 21:01:52,046 [INFO] Classifier output
2019-03-18 21:01:52,047 [INFO] [[0.991682 0.99026  0.990547 ... 0.007817 0.007675 0.006135]
 [0.990283 0.990326 0.990185 ... 0.98595  0.988342 0.989503]
 [0.989863 0.989676 0.989991 ... 0.005707 0.005008 0.005188]
 ...
 [0.989863 0.989676 0.989991 ... 0.005707 0.005008 0.005188]
 [0.991031 0.990774 0.990582 ... 0.004327 0.001523 0.00075 ]
 [0.990139 0.990276 0.990089 ... 0.988771 0.990242 0.99054 ]]
2019-03-18 21:01:52,049 [INFO] ---------------------------------
2019-03-18 21:02:10,789 [INFO] ---------------------------------
2019-03-18 21:02:10,790 [INFO] Summary:
2019-03-18 21:02:10,791 [INFO] Batch 51000, worst loss 0.061551 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 21:02:10,791 [INFO] Regularization: 2866.862793 * 0.0000010000 = 0.0028668628
2019-03-18 21:02:10,792 [INFO] Sum of grad norms: 0.044135
2019-03-18 21:02:10,792 [INFO] ---------------------------------
2019-03-18 21:02:29,779 [INFO] ---------------------------------
2019-03-18 21:02:29,780 [INFO] Summary:
2019-03-18 21:02:29,780 [INFO] Batch 52000, worst loss 0.061545 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 21:02:29,781 [INFO] Regularization: 2854.368896 * 0.0000010000 = 0.0028543689
2019-03-18 21:02:29,781 [INFO] Sum of grad norms: 0.068259
2019-03-18 21:02:29,782 [INFO] ---------------------------------
2019-03-18 21:02:48,630 [INFO] ---------------------------------
2019-03-18 21:02:48,631 [INFO] Summary:
2019-03-18 21:02:48,632 [INFO] Batch 53000, worst loss 0.061575 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 21:02:48,632 [INFO] Regularization: 2844.920410 * 0.0000010000 = 0.0028449204
2019-03-18 21:02:48,633 [INFO] Sum of grad norms: 0.179024
2019-03-18 21:02:48,633 [INFO] ---------------------------------
2019-03-18 21:03:07,893 [INFO] ---------------------------------
2019-03-18 21:03:07,894 [INFO] Summary:
2019-03-18 21:03:07,895 [INFO] Batch 54000, worst loss 0.061736 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 21:03:07,895 [INFO] Regularization: 2835.452881 * 0.0000010000 = 0.0028354528
2019-03-18 21:03:07,896 [INFO] Sum of grad norms: 0.139972
2019-03-18 21:03:07,896 [INFO] ---------------------------------
2019-03-18 21:03:26,625 [INFO] ---------------------------------
2019-03-18 21:03:26,626 [INFO] Summary:
2019-03-18 21:03:26,627 [INFO] Batch 55000, worst loss 0.061315 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 21:03:26,627 [INFO] Regularization: 2827.112793 * 0.0000010000 = 0.0028271128
2019-03-18 21:03:26,628 [INFO] Sum of grad norms: 0.034872
2019-03-18 21:03:26,628 [INFO] ---------------------------------
2019-03-18 21:03:45,148 [INFO] ---------------------------------
2019-03-18 21:03:45,149 [INFO] Summary:
2019-03-18 21:03:45,150 [INFO] Batch 56000, worst loss 0.061580 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 21:03:45,150 [INFO] Regularization: 2819.310303 * 0.0000010000 = 0.0028193104
2019-03-18 21:03:45,151 [INFO] Sum of grad norms: 0.236091
2019-03-18 21:03:45,152 [INFO] ---------------------------------
2019-03-18 21:04:03,925 [INFO] ---------------------------------
2019-03-18 21:04:03,926 [INFO] Summary:
2019-03-18 21:04:03,927 [INFO] Batch 57000, worst loss 0.061867 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 21:04:03,927 [INFO] Regularization: 2811.030762 * 0.0000010000 = 0.0028110307
2019-03-18 21:04:03,927 [INFO] Sum of grad norms: 0.049831
2019-03-18 21:04:03,928 [INFO] ---------------------------------
2019-03-18 21:04:22,986 [INFO] ---------------------------------
2019-03-18 21:04:22,987 [INFO] Summary:
2019-03-18 21:04:22,988 [INFO] Batch 58000, worst loss 0.061446 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 21:04:22,989 [INFO] Regularization: 2802.557129 * 0.0000010000 = 0.0028025571
2019-03-18 21:04:22,989 [INFO] Sum of grad norms: 0.096143
2019-03-18 21:04:22,990 [INFO] ---------------------------------
2019-03-18 21:04:41,919 [INFO] ---------------------------------
2019-03-18 21:04:41,920 [INFO] Summary:
2019-03-18 21:04:41,921 [INFO] Batch 59000, worst loss 0.061350 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 21:04:41,922 [INFO] Regularization: 2794.731445 * 0.0000010000 = 0.0027947314
2019-03-18 21:04:41,922 [INFO] Sum of grad norms: 0.113106
2019-03-18 21:04:41,923 [INFO] ---------------------------------
2019-03-18 21:05:00,859 [INFO] ---------------------------------
2019-03-18 21:05:00,860 [INFO] Summary:
2019-03-18 21:05:00,861 [INFO] Batch 60000, worst loss 0.061859 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 21:05:00,861 [INFO] Regularization: 2787.440918 * 0.0000010000 = 0.0027874410
2019-03-18 21:05:00,862 [INFO] Sum of grad norms: 0.058094
2019-03-18 21:05:00,862 [INFO] ---------------------------------
2019-03-18 21:05:05,801 [INFO] ---------------------------------
2019-03-18 21:05:05,802 [INFO] Evaluation:
2019-03-18 21:05:05,802 [INFO] Batch 60000, worst loss 0.058336 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:05:05,803 [INFO] New best loss 0.058336, saved to file classifier/1552933539/1552939505_1_classifier_60000.pth
2019-03-18 21:05:05,815 [INFO] Target
2019-03-18 21:05:05,815 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]]
2019-03-18 21:05:05,817 [INFO] Classifier output
2019-03-18 21:05:05,818 [INFO] [[0.989058 0.991161 0.990255 ... 0.013412 0.013275 0.011261]
 [0.009181 0.010072 0.010805 ... 0.989819 0.98985  0.9919  ]
 [0.009854 0.009712 0.010043 ... 0.989743 0.990156 0.989375]
 ...
 [0.990069 0.989765 0.990153 ... 0.009203 0.005716 0.004823]
 [0.989275 0.990291 0.98982  ... 0.008661 0.009521 0.006793]
 [0.989857 0.991028 0.990502 ... 0.012427 0.010514 0.012482]]
2019-03-18 21:05:05,821 [INFO] ---------------------------------
2019-03-18 21:05:24,778 [INFO] ---------------------------------
2019-03-18 21:05:24,779 [INFO] Summary:
2019-03-18 21:05:24,780 [INFO] Batch 61000, worst loss 0.061363 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 21:05:24,780 [INFO] Regularization: 2779.625244 * 0.0000010000 = 0.0027796251
2019-03-18 21:05:24,781 [INFO] Sum of grad norms: 0.036378
2019-03-18 21:05:24,782 [INFO] ---------------------------------
2019-03-18 21:05:43,341 [INFO] ---------------------------------
2019-03-18 21:05:43,342 [INFO] Summary:
2019-03-18 21:05:43,343 [INFO] Batch 62000, worst loss 0.061414 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 21:05:43,344 [INFO] Regularization: 2774.020752 * 0.0000010000 = 0.0027740207
2019-03-18 21:05:43,344 [INFO] Sum of grad norms: 0.247827
2019-03-18 21:05:43,345 [INFO] ---------------------------------
2019-03-18 21:06:02,208 [INFO] ---------------------------------
2019-03-18 21:06:02,209 [INFO] Summary:
2019-03-18 21:06:02,210 [INFO] Batch 63000, worst loss 0.061408 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 21:06:02,210 [INFO] Regularization: 2769.542236 * 0.0000010000 = 0.0027695422
2019-03-18 21:06:02,211 [INFO] Sum of grad norms: 0.091553
2019-03-18 21:06:02,211 [INFO] ---------------------------------
2019-03-18 21:06:20,921 [INFO] ---------------------------------
2019-03-18 21:06:20,922 [INFO] Summary:
2019-03-18 21:06:20,922 [INFO] Batch 64000, worst loss 0.061313 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 21:06:20,923 [INFO] Regularization: 2765.522949 * 0.0000010000 = 0.0027655230
2019-03-18 21:06:20,924 [INFO] Sum of grad norms: 0.238591
2019-03-18 21:06:20,924 [INFO] ---------------------------------
2019-03-18 21:06:39,596 [INFO] ---------------------------------
2019-03-18 21:06:39,597 [INFO] Summary:
2019-03-18 21:06:39,598 [INFO] Batch 65000, worst loss 0.061451 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 21:06:39,598 [INFO] Regularization: 2761.292725 * 0.0000010000 = 0.0027612927
2019-03-18 21:06:39,599 [INFO] Sum of grad norms: 0.069352
2019-03-18 21:06:39,600 [INFO] ---------------------------------
2019-03-18 21:06:58,474 [INFO] ---------------------------------
2019-03-18 21:06:58,475 [INFO] Summary:
2019-03-18 21:06:58,475 [INFO] Batch 66000, worst loss 0.061364 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 21:06:58,476 [INFO] Regularization: 2758.014160 * 0.0000010000 = 0.0027580142
2019-03-18 21:06:58,476 [INFO] Sum of grad norms: 0.151977
2019-03-18 21:06:58,477 [INFO] ---------------------------------
2019-03-18 21:07:17,231 [INFO] ---------------------------------
2019-03-18 21:07:17,232 [INFO] Summary:
2019-03-18 21:07:17,232 [INFO] Batch 67000, worst loss 0.061257 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 21:07:17,233 [INFO] Regularization: 2754.337891 * 0.0000010000 = 0.0027543379
2019-03-18 21:07:17,234 [INFO] Sum of grad norms: 0.183982
2019-03-18 21:07:17,234 [INFO] ---------------------------------
2019-03-18 21:07:35,918 [INFO] ---------------------------------
2019-03-18 21:07:35,919 [INFO] Summary:
2019-03-18 21:07:35,920 [INFO] Batch 68000, worst loss 0.061534 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 21:07:35,920 [INFO] Regularization: 2750.291992 * 0.0000010000 = 0.0027502920
2019-03-18 21:07:35,921 [INFO] Sum of grad norms: 0.247727
2019-03-18 21:07:35,922 [INFO] ---------------------------------
2019-03-18 21:07:54,631 [INFO] ---------------------------------
2019-03-18 21:07:54,632 [INFO] Summary:
2019-03-18 21:07:54,633 [INFO] Batch 69000, worst loss 0.061185 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 21:07:54,633 [INFO] Regularization: 2746.390625 * 0.0000010000 = 0.0027463906
2019-03-18 21:07:54,634 [INFO] Sum of grad norms: 0.128162
2019-03-18 21:07:54,634 [INFO] ---------------------------------
2019-03-18 21:08:13,593 [INFO] ---------------------------------
2019-03-18 21:08:13,594 [INFO] Summary:
2019-03-18 21:08:13,594 [INFO] Batch 70000, worst loss 0.061465 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 21:08:13,595 [INFO] Regularization: 2742.840576 * 0.0000010000 = 0.0027428407
2019-03-18 21:08:13,596 [INFO] Sum of grad norms: 0.151285
2019-03-18 21:08:13,596 [INFO] ---------------------------------
2019-03-18 21:08:18,541 [INFO] ---------------------------------
2019-03-18 21:08:18,542 [INFO] Evaluation:
2019-03-18 21:08:18,543 [INFO] Batch 70000, worst loss 0.058503 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:08:18,544 [INFO] ---------------------------------
2019-03-18 21:08:37,576 [INFO] ---------------------------------
2019-03-18 21:08:37,577 [INFO] Summary:
2019-03-18 21:08:37,578 [INFO] Batch 71000, worst loss 0.061242 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 21:08:37,578 [INFO] Regularization: 2738.250732 * 0.0000010000 = 0.0027382507
2019-03-18 21:08:37,579 [INFO] Sum of grad norms: 0.085922
2019-03-18 21:08:37,579 [INFO] ---------------------------------
2019-03-18 21:08:56,280 [INFO] ---------------------------------
2019-03-18 21:08:56,281 [INFO] Summary:
2019-03-18 21:08:56,282 [INFO] Batch 72000, worst loss 0.061087 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 21:08:56,282 [INFO] Regularization: 2735.503662 * 0.0000010000 = 0.0027355037
2019-03-18 21:08:56,283 [INFO] Sum of grad norms: 0.054440
2019-03-18 21:08:56,284 [INFO] ---------------------------------
2019-03-18 21:09:15,455 [INFO] ---------------------------------
2019-03-18 21:09:15,456 [INFO] Summary:
2019-03-18 21:09:15,457 [INFO] Batch 73000, worst loss 0.061233 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 21:09:15,457 [INFO] Regularization: 2733.260498 * 0.0000010000 = 0.0027332604
2019-03-18 21:09:15,458 [INFO] Sum of grad norms: 0.088950
2019-03-18 21:09:15,458 [INFO] ---------------------------------
2019-03-18 21:09:34,133 [INFO] ---------------------------------
2019-03-18 21:09:34,134 [INFO] Summary:
2019-03-18 21:09:34,134 [INFO] Batch 74000, worst loss 0.061142 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 21:09:34,135 [INFO] Regularization: 2730.959229 * 0.0000010000 = 0.0027309593
2019-03-18 21:09:34,135 [INFO] Sum of grad norms: 0.039230
2019-03-18 21:09:34,136 [INFO] ---------------------------------
2019-03-18 21:09:52,963 [INFO] ---------------------------------
2019-03-18 21:09:52,964 [INFO] Summary:
2019-03-18 21:09:52,964 [INFO] Batch 75000, worst loss 0.061330 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 21:09:52,965 [INFO] Regularization: 2728.655029 * 0.0000010000 = 0.0027286550
2019-03-18 21:09:52,965 [INFO] Sum of grad norms: 0.095894
2019-03-18 21:09:52,966 [INFO] ---------------------------------
2019-03-18 21:10:11,625 [INFO] ---------------------------------
2019-03-18 21:10:11,627 [INFO] Summary:
2019-03-18 21:10:11,627 [INFO] Batch 76000, worst loss 0.061037 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 21:10:11,628 [INFO] Regularization: 2726.531494 * 0.0000010000 = 0.0027265316
2019-03-18 21:10:11,628 [INFO] Sum of grad norms: 0.135148
2019-03-18 21:10:11,629 [INFO] ---------------------------------
2019-03-18 21:10:30,043 [INFO] ---------------------------------
2019-03-18 21:10:30,044 [INFO] Summary:
2019-03-18 21:10:30,045 [INFO] Batch 77000, worst loss 0.061021 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 21:10:30,046 [INFO] Regularization: 2724.286377 * 0.0000010000 = 0.0027242864
2019-03-18 21:10:30,046 [INFO] Sum of grad norms: 0.101954
2019-03-18 21:10:30,047 [INFO] ---------------------------------
2019-03-18 21:10:48,476 [INFO] ---------------------------------
2019-03-18 21:10:48,477 [INFO] Summary:
2019-03-18 21:10:48,478 [INFO] Batch 78000, worst loss 0.061037 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 21:10:48,479 [INFO] Regularization: 2722.338623 * 0.0000010000 = 0.0027223385
2019-03-18 21:10:48,479 [INFO] Sum of grad norms: 0.147816
2019-03-18 21:10:48,480 [INFO] ---------------------------------
2019-03-18 21:11:07,186 [INFO] ---------------------------------
2019-03-18 21:11:07,187 [INFO] Summary:
2019-03-18 21:11:07,187 [INFO] Batch 79000, worst loss 0.061116 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 21:11:07,188 [INFO] Regularization: 2720.335693 * 0.0000010000 = 0.0027203357
2019-03-18 21:11:07,188 [INFO] Sum of grad norms: 0.162478
2019-03-18 21:11:07,189 [INFO] ---------------------------------
2019-03-18 21:11:26,516 [INFO] ---------------------------------
2019-03-18 21:11:26,517 [INFO] Summary:
2019-03-18 21:11:26,518 [INFO] Batch 80000, worst loss 0.061215 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 21:11:26,518 [INFO] Regularization: 2717.920166 * 0.0000010000 = 0.0027179201
2019-03-18 21:11:26,519 [INFO] Sum of grad norms: 0.047175
2019-03-18 21:11:26,519 [INFO] ---------------------------------
2019-03-18 21:11:31,432 [INFO] ---------------------------------
2019-03-18 21:11:31,433 [INFO] Evaluation:
2019-03-18 21:11:31,434 [INFO] Batch 80000, worst loss 0.058249 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:11:31,435 [INFO] New best loss 0.058249, saved to file classifier/1552933539/1552939891_1_classifier_80000.pth
2019-03-18 21:11:31,446 [INFO] Target
2019-03-18 21:11:31,447 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 ...
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]]
2019-03-18 21:11:31,448 [INFO] Classifier output
2019-03-18 21:11:31,449 [INFO] [[0.990045 0.990105 0.989788 ... 0.009242 0.007756 0.008139]
 [0.990046 0.98994  0.989897 ... 0.98752  0.987863 0.988256]
 [0.989782 0.989739 0.989727 ... 0.989383 0.989531 0.989788]
 ...
 [0.00962  0.012036 0.011138 ... 0.982463 0.98519  0.990298]
 [0.991048 0.989813 0.990731 ... 0.004576 0.001457 0.000804]
 [0.009877 0.009768 0.009943 ... 0.989995 0.990736 0.989846]]
2019-03-18 21:11:31,451 [INFO] ---------------------------------
2019-03-18 21:11:50,172 [INFO] ---------------------------------
2019-03-18 21:11:50,173 [INFO] Summary:
2019-03-18 21:11:50,174 [INFO] Batch 81000, worst loss 0.060906 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 21:11:50,175 [INFO] Regularization: 2715.906982 * 0.0000010000 = 0.0027159071
2019-03-18 21:11:50,175 [INFO] Sum of grad norms: 0.051323
2019-03-18 21:11:50,176 [INFO] ---------------------------------
2019-03-18 21:12:09,126 [INFO] ---------------------------------
2019-03-18 21:12:09,127 [INFO] Summary:
2019-03-18 21:12:09,127 [INFO] Batch 82000, worst loss 0.061252 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 21:12:09,128 [INFO] Regularization: 2714.255859 * 0.0000010000 = 0.0027142558
2019-03-18 21:12:09,129 [INFO] Sum of grad norms: 0.089924
2019-03-18 21:12:09,129 [INFO] ---------------------------------
2019-03-18 21:12:27,737 [INFO] ---------------------------------
2019-03-18 21:12:27,738 [INFO] Summary:
2019-03-18 21:12:27,739 [INFO] Batch 83000, worst loss 0.061034 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 21:12:27,739 [INFO] Regularization: 2713.150879 * 0.0000010000 = 0.0027131508
2019-03-18 21:12:27,740 [INFO] Sum of grad norms: 0.146995
2019-03-18 21:12:27,740 [INFO] ---------------------------------
2019-03-18 21:12:46,834 [INFO] ---------------------------------
2019-03-18 21:12:46,835 [INFO] Summary:
2019-03-18 21:12:46,835 [INFO] Batch 84000, worst loss 0.061108 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 21:12:46,836 [INFO] Regularization: 2711.913086 * 0.0000010000 = 0.0027119131
2019-03-18 21:12:46,836 [INFO] Sum of grad norms: 0.049230
2019-03-18 21:12:46,837 [INFO] ---------------------------------
2019-03-18 21:13:05,746 [INFO] ---------------------------------
2019-03-18 21:13:05,747 [INFO] Summary:
2019-03-18 21:13:05,748 [INFO] Batch 85000, worst loss 0.061110 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 21:13:05,748 [INFO] Regularization: 2710.811279 * 0.0000010000 = 0.0027108113
2019-03-18 21:13:05,749 [INFO] Sum of grad norms: 0.098027
2019-03-18 21:13:05,749 [INFO] ---------------------------------
2019-03-18 21:13:24,675 [INFO] ---------------------------------
2019-03-18 21:13:24,676 [INFO] Summary:
2019-03-18 21:13:24,677 [INFO] Batch 86000, worst loss 0.061010 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 21:13:24,677 [INFO] Regularization: 2709.555908 * 0.0000010000 = 0.0027095559
2019-03-18 21:13:24,678 [INFO] Sum of grad norms: 0.084748
2019-03-18 21:13:24,678 [INFO] ---------------------------------
2019-03-18 21:13:43,605 [INFO] ---------------------------------
2019-03-18 21:13:43,606 [INFO] Summary:
2019-03-18 21:13:43,607 [INFO] Batch 87000, worst loss 0.061324 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 21:13:43,607 [INFO] Regularization: 2708.517334 * 0.0000010000 = 0.0027085172
2019-03-18 21:13:43,608 [INFO] Sum of grad norms: 0.122500
2019-03-18 21:13:43,609 [INFO] ---------------------------------
2019-03-18 21:14:02,404 [INFO] ---------------------------------
2019-03-18 21:14:02,405 [INFO] Summary:
2019-03-18 21:14:02,406 [INFO] Batch 88000, worst loss 0.061358 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 21:14:02,406 [INFO] Regularization: 2707.421387 * 0.0000010000 = 0.0027074213
2019-03-18 21:14:02,407 [INFO] Sum of grad norms: 0.050020
2019-03-18 21:14:02,407 [INFO] ---------------------------------
2019-03-18 21:14:21,153 [INFO] ---------------------------------
2019-03-18 21:14:21,154 [INFO] Summary:
2019-03-18 21:14:21,155 [INFO] Batch 89000, worst loss 0.061048 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 21:14:21,155 [INFO] Regularization: 2706.030762 * 0.0000010000 = 0.0027060308
2019-03-18 21:14:21,156 [INFO] Sum of grad norms: 0.076757
2019-03-18 21:14:21,156 [INFO] ---------------------------------
2019-03-18 21:14:39,678 [INFO] ---------------------------------
2019-03-18 21:14:39,679 [INFO] Summary:
2019-03-18 21:14:39,679 [INFO] Batch 90000, worst loss 0.060950 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 21:14:39,680 [INFO] Regularization: 2705.189453 * 0.0000010000 = 0.0027051894
2019-03-18 21:14:39,680 [INFO] Sum of grad norms: 0.125397
2019-03-18 21:14:39,681 [INFO] ---------------------------------
2019-03-18 21:14:44,587 [INFO] ---------------------------------
2019-03-18 21:14:44,588 [INFO] Evaluation:
2019-03-18 21:14:44,589 [INFO] Batch 90000, worst loss 0.058276 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:14:44,590 [INFO] ---------------------------------
2019-03-18 21:15:03,266 [INFO] ---------------------------------
2019-03-18 21:15:03,267 [INFO] Summary:
2019-03-18 21:15:03,268 [INFO] Batch 91000, worst loss 0.061276 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 21:15:03,268 [INFO] Regularization: 2704.080078 * 0.0000010000 = 0.0027040800
2019-03-18 21:15:03,269 [INFO] Sum of grad norms: 0.097201
2019-03-18 21:15:03,269 [INFO] ---------------------------------
2019-03-18 21:15:21,806 [INFO] ---------------------------------
2019-03-18 21:15:21,807 [INFO] Summary:
2019-03-18 21:15:21,808 [INFO] Batch 92000, worst loss 0.060930 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 21:15:21,808 [INFO] Regularization: 2703.257324 * 0.0000010000 = 0.0027032574
2019-03-18 21:15:21,809 [INFO] Sum of grad norms: 0.125884
2019-03-18 21:15:21,809 [INFO] ---------------------------------
2019-03-18 21:15:40,358 [INFO] ---------------------------------
2019-03-18 21:15:40,359 [INFO] Summary:
2019-03-18 21:15:40,360 [INFO] Batch 93000, worst loss 0.060970 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 21:15:40,361 [INFO] Regularization: 2702.703125 * 0.0000010000 = 0.0027027032
2019-03-18 21:15:40,362 [INFO] Sum of grad norms: 0.168042
2019-03-18 21:15:40,363 [INFO] ---------------------------------
2019-03-18 21:15:59,717 [INFO] ---------------------------------
2019-03-18 21:15:59,718 [INFO] Summary:
2019-03-18 21:15:59,719 [INFO] Batch 94000, worst loss 0.061030 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 21:15:59,720 [INFO] Regularization: 2702.187500 * 0.0000010000 = 0.0027021875
2019-03-18 21:15:59,720 [INFO] Sum of grad norms: 0.089068
2019-03-18 21:15:59,721 [INFO] ---------------------------------
2019-03-18 21:16:18,710 [INFO] ---------------------------------
2019-03-18 21:16:18,711 [INFO] Summary:
2019-03-18 21:16:18,712 [INFO] Batch 95000, worst loss 0.061088 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 21:16:18,712 [INFO] Regularization: 2701.634521 * 0.0000010000 = 0.0027016345
2019-03-18 21:16:18,713 [INFO] Sum of grad norms: 0.077481
2019-03-18 21:16:18,713 [INFO] ---------------------------------
2019-03-18 21:16:37,616 [INFO] ---------------------------------
2019-03-18 21:16:37,617 [INFO] Summary:
2019-03-18 21:16:37,617 [INFO] Batch 96000, worst loss 0.061092 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 21:16:37,618 [INFO] Regularization: 2700.941895 * 0.0000010000 = 0.0027009419
2019-03-18 21:16:37,618 [INFO] Sum of grad norms: 0.112234
2019-03-18 21:16:37,619 [INFO] ---------------------------------
2019-03-18 21:16:56,331 [INFO] ---------------------------------
2019-03-18 21:16:56,332 [INFO] Summary:
2019-03-18 21:16:56,332 [INFO] Batch 97000, worst loss 0.060906 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 21:16:56,333 [INFO] Regularization: 2700.475098 * 0.0000010000 = 0.0027004750
2019-03-18 21:16:56,334 [INFO] Sum of grad norms: 0.048611
2019-03-18 21:16:56,334 [INFO] ---------------------------------
2019-03-18 21:17:15,014 [INFO] ---------------------------------
2019-03-18 21:17:15,015 [INFO] Summary:
2019-03-18 21:17:15,015 [INFO] Batch 98000, worst loss 0.061058 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 21:17:15,016 [INFO] Regularization: 2699.863770 * 0.0000010000 = 0.0026998639
2019-03-18 21:17:15,016 [INFO] Sum of grad norms: 0.178095
2019-03-18 21:17:15,017 [INFO] ---------------------------------
2019-03-18 21:17:33,640 [INFO] ---------------------------------
2019-03-18 21:17:33,641 [INFO] Summary:
2019-03-18 21:17:33,641 [INFO] Batch 99000, worst loss 0.061023 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 21:17:33,642 [INFO] Regularization: 2699.303711 * 0.0000010000 = 0.0026993037
2019-03-18 21:17:33,642 [INFO] Sum of grad norms: 0.041056
2019-03-18 21:17:33,643 [INFO] ---------------------------------
2019-03-18 21:17:52,742 [INFO] ---------------------------------
2019-03-18 21:17:52,743 [INFO] Summary:
2019-03-18 21:17:52,745 [INFO] Batch 100000, worst loss 0.060927 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 21:17:52,745 [INFO] Regularization: 2698.799805 * 0.0000010000 = 0.0026987998
2019-03-18 21:17:52,746 [INFO] Sum of grad norms: 0.065925
2019-03-18 21:17:52,746 [INFO] ---------------------------------
2019-03-18 21:17:57,713 [INFO] ---------------------------------
2019-03-18 21:17:57,714 [INFO] Evaluation:
2019-03-18 21:17:57,715 [INFO] Batch 100000, worst loss 0.058406 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:17:57,716 [INFO] ---------------------------------
2019-03-18 21:18:16,329 [INFO] ---------------------------------
2019-03-18 21:18:16,330 [INFO] Summary:
2019-03-18 21:18:16,331 [INFO] Batch 101000, worst loss 0.061251 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 21:18:16,331 [INFO] Regularization: 2698.257080 * 0.0000010000 = 0.0026982571
2019-03-18 21:18:16,332 [INFO] Sum of grad norms: 0.070049
2019-03-18 21:18:16,333 [INFO] ---------------------------------
2019-03-18 21:18:34,998 [INFO] ---------------------------------
2019-03-18 21:18:34,999 [INFO] Summary:
2019-03-18 21:18:35,000 [INFO] Batch 102000, worst loss 0.061235 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 21:18:35,000 [INFO] Regularization: 2697.786377 * 0.0000010000 = 0.0026977863
2019-03-18 21:18:35,001 [INFO] Sum of grad norms: 0.026074
2019-03-18 21:18:35,001 [INFO] ---------------------------------
2019-03-18 21:18:53,652 [INFO] ---------------------------------
2019-03-18 21:18:53,653 [INFO] Summary:
2019-03-18 21:18:53,653 [INFO] Batch 103000, worst loss 0.061074 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 21:18:53,654 [INFO] Regularization: 2697.476318 * 0.0000010000 = 0.0026974764
2019-03-18 21:18:53,654 [INFO] Sum of grad norms: 0.041551
2019-03-18 21:18:53,655 [INFO] ---------------------------------
2019-03-18 21:19:12,395 [INFO] ---------------------------------
2019-03-18 21:19:12,396 [INFO] Summary:
2019-03-18 21:19:12,396 [INFO] Batch 104000, worst loss 0.061084 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 21:19:12,397 [INFO] Regularization: 2697.182373 * 0.0000010000 = 0.0026971824
2019-03-18 21:19:12,397 [INFO] Sum of grad norms: 0.071240
2019-03-18 21:19:12,398 [INFO] ---------------------------------
2019-03-18 21:19:31,694 [INFO] ---------------------------------
2019-03-18 21:19:31,695 [INFO] Summary:
2019-03-18 21:19:31,696 [INFO] Batch 105000, worst loss 0.060982 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 21:19:31,696 [INFO] Regularization: 2696.955566 * 0.0000010000 = 0.0026969556
2019-03-18 21:19:31,697 [INFO] Sum of grad norms: 0.058128
2019-03-18 21:19:31,698 [INFO] ---------------------------------
2019-03-18 21:19:50,511 [INFO] ---------------------------------
2019-03-18 21:19:50,511 [INFO] Summary:
2019-03-18 21:19:50,512 [INFO] Batch 106000, worst loss 0.061109 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 21:19:50,513 [INFO] Regularization: 2696.652344 * 0.0000010000 = 0.0026966524
2019-03-18 21:19:50,513 [INFO] Sum of grad norms: 0.056333
2019-03-18 21:19:50,514 [INFO] ---------------------------------
2019-03-18 21:20:09,359 [INFO] ---------------------------------
2019-03-18 21:20:09,360 [INFO] Summary:
2019-03-18 21:20:09,361 [INFO] Batch 107000, worst loss 0.061121 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 21:20:09,361 [INFO] Regularization: 2696.371094 * 0.0000010000 = 0.0026963712
2019-03-18 21:20:09,362 [INFO] Sum of grad norms: 0.056956
2019-03-18 21:20:09,363 [INFO] ---------------------------------
2019-03-18 21:20:28,196 [INFO] ---------------------------------
2019-03-18 21:20:28,197 [INFO] Summary:
2019-03-18 21:20:28,198 [INFO] Batch 108000, worst loss 0.060962 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 21:20:28,199 [INFO] Regularization: 2696.138428 * 0.0000010000 = 0.0026961383
2019-03-18 21:20:28,199 [INFO] Sum of grad norms: 0.060782
2019-03-18 21:20:28,200 [INFO] ---------------------------------
2019-03-18 21:20:46,709 [INFO] ---------------------------------
2019-03-18 21:20:46,710 [INFO] Summary:
2019-03-18 21:20:46,712 [INFO] Batch 109000, worst loss 0.060910 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 21:20:46,712 [INFO] Regularization: 2695.864014 * 0.0000010000 = 0.0026958641
2019-03-18 21:20:46,713 [INFO] Sum of grad norms: 0.052862
2019-03-18 21:20:46,714 [INFO] ---------------------------------
2019-03-18 21:21:05,085 [INFO] ---------------------------------
2019-03-18 21:21:05,086 [INFO] Summary:
2019-03-18 21:21:05,087 [INFO] Batch 110000, worst loss 0.061011 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 21:21:05,087 [INFO] Regularization: 2695.750000 * 0.0000010000 = 0.0026957500
2019-03-18 21:21:05,088 [INFO] Sum of grad norms: 0.044672
2019-03-18 21:21:05,088 [INFO] ---------------------------------
2019-03-18 21:21:10,041 [INFO] ---------------------------------
2019-03-18 21:21:10,042 [INFO] Evaluation:
2019-03-18 21:21:10,043 [INFO] Batch 110000, worst loss 0.058500 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:21:10,043 [INFO] ---------------------------------
2019-03-18 21:21:28,851 [INFO] ---------------------------------
2019-03-18 21:21:28,852 [INFO] Summary:
2019-03-18 21:21:28,853 [INFO] Batch 111000, worst loss 0.061189 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 21:21:28,854 [INFO] Regularization: 2695.417480 * 0.0000010000 = 0.0026954175
2019-03-18 21:21:28,855 [INFO] Sum of grad norms: 0.036508
2019-03-18 21:21:28,856 [INFO] ---------------------------------
2019-03-18 21:21:47,740 [INFO] ---------------------------------
2019-03-18 21:21:47,741 [INFO] Summary:
2019-03-18 21:21:47,742 [INFO] Batch 112000, worst loss 0.061138 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 21:21:47,743 [INFO] Regularization: 2695.185059 * 0.0000010000 = 0.0026951851
2019-03-18 21:21:47,743 [INFO] Sum of grad norms: 0.071216
2019-03-18 21:21:47,744 [INFO] ---------------------------------
2019-03-18 21:22:06,682 [INFO] ---------------------------------
2019-03-18 21:22:06,683 [INFO] Summary:
2019-03-18 21:22:06,684 [INFO] Batch 113000, worst loss 0.061132 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 21:22:06,685 [INFO] Regularization: 2694.997803 * 0.0000010000 = 0.0026949977
2019-03-18 21:22:06,685 [INFO] Sum of grad norms: 0.076606
2019-03-18 21:22:06,686 [INFO] ---------------------------------
2019-03-18 21:22:25,509 [INFO] ---------------------------------
2019-03-18 21:22:25,510 [INFO] Summary:
2019-03-18 21:22:25,510 [INFO] Batch 114000, worst loss 0.061141 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 21:22:25,511 [INFO] Regularization: 2694.882568 * 0.0000010000 = 0.0026948825
2019-03-18 21:22:25,511 [INFO] Sum of grad norms: 0.098000
2019-03-18 21:22:25,512 [INFO] ---------------------------------
2019-03-18 21:22:43,979 [INFO] ---------------------------------
2019-03-18 21:22:43,980 [INFO] Summary:
2019-03-18 21:22:43,980 [INFO] Batch 115000, worst loss 0.061211 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 21:22:43,981 [INFO] Regularization: 2694.763184 * 0.0000010000 = 0.0026947632
2019-03-18 21:22:43,981 [INFO] Sum of grad norms: 0.027216
2019-03-18 21:22:43,982 [INFO] ---------------------------------
2019-03-18 21:23:02,965 [INFO] ---------------------------------
2019-03-18 21:23:02,966 [INFO] Summary:
2019-03-18 21:23:02,967 [INFO] Batch 116000, worst loss 0.060854 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 21:23:02,968 [INFO] Regularization: 2694.613770 * 0.0000010000 = 0.0026946138
2019-03-18 21:23:02,968 [INFO] Sum of grad norms: 0.037602
2019-03-18 21:23:02,969 [INFO] ---------------------------------
2019-03-18 21:23:21,805 [INFO] ---------------------------------
2019-03-18 21:23:21,806 [INFO] Summary:
2019-03-18 21:23:21,806 [INFO] Batch 117000, worst loss 0.061158 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 21:23:21,807 [INFO] Regularization: 2694.433838 * 0.0000010000 = 0.0026944338
2019-03-18 21:23:21,807 [INFO] Sum of grad norms: 0.028653
2019-03-18 21:23:21,808 [INFO] ---------------------------------
2019-03-18 21:23:40,410 [INFO] ---------------------------------
2019-03-18 21:23:40,411 [INFO] Summary:
2019-03-18 21:23:40,411 [INFO] Batch 118000, worst loss 0.061145 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 21:23:40,412 [INFO] Regularization: 2694.325928 * 0.0000010000 = 0.0026943260
2019-03-18 21:23:40,412 [INFO] Sum of grad norms: 0.070980
2019-03-18 21:23:40,413 [INFO] ---------------------------------
2019-03-18 21:23:59,443 [INFO] ---------------------------------
2019-03-18 21:23:59,444 [INFO] Summary:
2019-03-18 21:23:59,445 [INFO] Batch 119000, worst loss 0.060919 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 21:23:59,445 [INFO] Regularization: 2694.204590 * 0.0000010000 = 0.0026942047
2019-03-18 21:23:59,446 [INFO] Sum of grad norms: 0.030002
2019-03-18 21:23:59,447 [INFO] ---------------------------------
2019-03-18 21:24:18,103 [INFO] ---------------------------------
2019-03-18 21:24:18,104 [INFO] Summary:
2019-03-18 21:24:18,106 [INFO] Batch 120000, worst loss 0.060848 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 21:24:18,106 [INFO] Regularization: 2694.039795 * 0.0000010000 = 0.0026940398
2019-03-18 21:24:18,107 [INFO] Sum of grad norms: 0.030453
2019-03-18 21:24:18,107 [INFO] ---------------------------------
2019-03-18 21:24:23,026 [INFO] ---------------------------------
2019-03-18 21:24:23,027 [INFO] Evaluation:
2019-03-18 21:24:23,030 [INFO] Batch 120000, worst loss 0.058363 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:24:23,031 [INFO] ---------------------------------
2019-03-18 21:24:41,713 [INFO] ---------------------------------
2019-03-18 21:24:41,714 [INFO] Summary:
2019-03-18 21:24:41,714 [INFO] Batch 121000, worst loss 0.060943 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:24:41,715 [INFO] Regularization: 2693.856934 * 0.0000010000 = 0.0026938568
2019-03-18 21:24:41,715 [INFO] Sum of grad norms: 0.049887
2019-03-18 21:24:41,716 [INFO] ---------------------------------
2019-03-18 21:25:00,384 [INFO] ---------------------------------
2019-03-18 21:25:00,385 [INFO] Summary:
2019-03-18 21:25:00,385 [INFO] Batch 122000, worst loss 0.060943 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:25:00,386 [INFO] Regularization: 2693.723145 * 0.0000010000 = 0.0026937232
2019-03-18 21:25:00,386 [INFO] Sum of grad norms: 0.114927
2019-03-18 21:25:00,387 [INFO] ---------------------------------
2019-03-18 21:25:19,245 [INFO] ---------------------------------
2019-03-18 21:25:19,246 [INFO] Summary:
2019-03-18 21:25:19,247 [INFO] Batch 123000, worst loss 0.060980 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:25:19,248 [INFO] Regularization: 2693.664551 * 0.0000010000 = 0.0026936645
2019-03-18 21:25:19,248 [INFO] Sum of grad norms: 0.075215
2019-03-18 21:25:19,249 [INFO] ---------------------------------
2019-03-18 21:25:38,031 [INFO] ---------------------------------
2019-03-18 21:25:38,032 [INFO] Summary:
2019-03-18 21:25:38,032 [INFO] Batch 124000, worst loss 0.060973 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:25:38,033 [INFO] Regularization: 2693.554932 * 0.0000010000 = 0.0026935549
2019-03-18 21:25:38,033 [INFO] Sum of grad norms: 0.062485
2019-03-18 21:25:38,034 [INFO] ---------------------------------
2019-03-18 21:25:56,868 [INFO] ---------------------------------
2019-03-18 21:25:56,869 [INFO] Summary:
2019-03-18 21:25:56,869 [INFO] Batch 125000, worst loss 0.060835 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:25:56,870 [INFO] Regularization: 2693.493408 * 0.0000010000 = 0.0026934934
2019-03-18 21:25:56,870 [INFO] Sum of grad norms: 0.061607
2019-03-18 21:25:56,871 [INFO] ---------------------------------
2019-03-18 21:26:15,517 [INFO] ---------------------------------
2019-03-18 21:26:15,518 [INFO] Summary:
2019-03-18 21:26:15,518 [INFO] Batch 126000, worst loss 0.061191 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:26:15,519 [INFO] Regularization: 2693.432373 * 0.0000010000 = 0.0026934324
2019-03-18 21:26:15,519 [INFO] Sum of grad norms: 0.092700
2019-03-18 21:26:15,520 [INFO] ---------------------------------
2019-03-18 21:26:34,109 [INFO] ---------------------------------
2019-03-18 21:26:34,109 [INFO] Summary:
2019-03-18 21:26:34,110 [INFO] Batch 127000, worst loss 0.061199 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:26:34,110 [INFO] Regularization: 2693.337158 * 0.0000010000 = 0.0026933372
2019-03-18 21:26:34,111 [INFO] Sum of grad norms: 0.081385
2019-03-18 21:26:34,111 [INFO] ---------------------------------
2019-03-18 21:26:52,632 [INFO] ---------------------------------
2019-03-18 21:26:52,632 [INFO] Summary:
2019-03-18 21:26:52,633 [INFO] Batch 128000, worst loss 0.061201 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:26:52,634 [INFO] Regularization: 2693.283447 * 0.0000010000 = 0.0026932834
2019-03-18 21:26:52,634 [INFO] Sum of grad norms: 0.030235
2019-03-18 21:26:52,635 [INFO] ---------------------------------
2019-03-18 21:27:11,314 [INFO] ---------------------------------
2019-03-18 21:27:11,315 [INFO] Summary:
2019-03-18 21:27:11,316 [INFO] Batch 129000, worst loss 0.061090 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:27:11,316 [INFO] Regularization: 2693.200195 * 0.0000010000 = 0.0026932003
2019-03-18 21:27:11,317 [INFO] Sum of grad norms: 0.034766
2019-03-18 21:27:11,318 [INFO] ---------------------------------
2019-03-18 21:27:30,234 [INFO] ---------------------------------
2019-03-18 21:27:30,235 [INFO] Summary:
2019-03-18 21:27:30,235 [INFO] Batch 130000, worst loss 0.060987 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:27:30,236 [INFO] Regularization: 2693.127930 * 0.0000010000 = 0.0026931278
2019-03-18 21:27:30,236 [INFO] Sum of grad norms: 0.081229
2019-03-18 21:27:30,237 [INFO] ---------------------------------
2019-03-18 21:27:35,109 [INFO] ---------------------------------
2019-03-18 21:27:35,109 [INFO] Evaluation:
2019-03-18 21:27:35,110 [INFO] Batch 130000, worst loss 0.058291 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:27:35,111 [INFO] ---------------------------------
2019-03-18 21:27:53,891 [INFO] ---------------------------------
2019-03-18 21:27:53,892 [INFO] Summary:
2019-03-18 21:27:53,892 [INFO] Batch 131000, worst loss 0.060994 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:27:53,893 [INFO] Regularization: 2693.043945 * 0.0000010000 = 0.0026930440
2019-03-18 21:27:53,894 [INFO] Sum of grad norms: 0.107083
2019-03-18 21:27:53,895 [INFO] ---------------------------------
2019-03-18 21:28:12,517 [INFO] ---------------------------------
2019-03-18 21:28:12,518 [INFO] Summary:
2019-03-18 21:28:12,519 [INFO] Batch 132000, worst loss 0.060972 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:28:12,519 [INFO] Regularization: 2692.993896 * 0.0000010000 = 0.0026929940
2019-03-18 21:28:12,520 [INFO] Sum of grad norms: 0.042251
2019-03-18 21:28:12,520 [INFO] ---------------------------------
2019-03-18 21:28:31,080 [INFO] ---------------------------------
2019-03-18 21:28:31,082 [INFO] Summary:
2019-03-18 21:28:31,082 [INFO] Batch 133000, worst loss 0.060972 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:28:31,083 [INFO] Regularization: 2692.947754 * 0.0000010000 = 0.0026929479
2019-03-18 21:28:31,083 [INFO] Sum of grad norms: 0.027804
2019-03-18 21:28:31,084 [INFO] ---------------------------------
2019-03-18 21:28:49,855 [INFO] ---------------------------------
2019-03-18 21:28:49,856 [INFO] Summary:
2019-03-18 21:28:49,857 [INFO] Batch 134000, worst loss 0.060942 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:28:49,857 [INFO] Regularization: 2692.909180 * 0.0000010000 = 0.0026929092
2019-03-18 21:28:49,858 [INFO] Sum of grad norms: 0.026419
2019-03-18 21:28:49,858 [INFO] ---------------------------------
2019-03-18 21:29:08,500 [INFO] ---------------------------------
2019-03-18 21:29:08,501 [INFO] Summary:
2019-03-18 21:29:08,502 [INFO] Batch 135000, worst loss 0.061106 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:29:08,502 [INFO] Regularization: 2692.860596 * 0.0000010000 = 0.0026928606
2019-03-18 21:29:08,503 [INFO] Sum of grad norms: 0.027635
2019-03-18 21:29:08,503 [INFO] ---------------------------------
2019-03-18 21:29:27,028 [INFO] ---------------------------------
2019-03-18 21:29:27,028 [INFO] Summary:
2019-03-18 21:29:27,029 [INFO] Batch 136000, worst loss 0.061107 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:29:27,030 [INFO] Regularization: 2692.820801 * 0.0000010000 = 0.0026928207
2019-03-18 21:29:27,030 [INFO] Sum of grad norms: 0.071777
2019-03-18 21:29:27,031 [INFO] ---------------------------------
2019-03-18 21:29:46,008 [INFO] ---------------------------------
2019-03-18 21:29:46,009 [INFO] Summary:
2019-03-18 21:29:46,010 [INFO] Batch 137000, worst loss 0.061359 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:29:46,010 [INFO] Regularization: 2692.794434 * 0.0000010000 = 0.0026927944
2019-03-18 21:29:46,011 [INFO] Sum of grad norms: 0.100533
2019-03-18 21:29:46,012 [INFO] ---------------------------------
2019-03-18 21:30:04,627 [INFO] ---------------------------------
2019-03-18 21:30:04,628 [INFO] Summary:
2019-03-18 21:30:04,628 [INFO] Batch 138000, worst loss 0.061353 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:30:04,629 [INFO] Regularization: 2692.761963 * 0.0000010000 = 0.0026927621
2019-03-18 21:30:04,630 [INFO] Sum of grad norms: 0.057412
2019-03-18 21:30:04,630 [INFO] ---------------------------------
2019-03-18 21:30:23,543 [INFO] ---------------------------------
2019-03-18 21:30:23,544 [INFO] Summary:
2019-03-18 21:30:23,544 [INFO] Batch 139000, worst loss 0.060889 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:30:23,545 [INFO] Regularization: 2692.734619 * 0.0000010000 = 0.0026927346
2019-03-18 21:30:23,546 [INFO] Sum of grad norms: 0.086521
2019-03-18 21:30:23,547 [INFO] ---------------------------------
2019-03-18 21:30:42,229 [INFO] ---------------------------------
2019-03-18 21:30:42,230 [INFO] Summary:
2019-03-18 21:30:42,230 [INFO] Batch 140000, worst loss 0.060875 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 21:30:42,231 [INFO] Regularization: 2692.697266 * 0.0000010000 = 0.0026926973
2019-03-18 21:30:42,231 [INFO] Sum of grad norms: 0.034155
2019-03-18 21:30:42,232 [INFO] ---------------------------------
2019-03-18 21:30:47,100 [INFO] ---------------------------------
2019-03-18 21:30:47,101 [INFO] Evaluation:
2019-03-18 21:30:47,102 [INFO] Batch 140000, worst loss 0.058307 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:30:47,105 [INFO] ---------------------------------
2019-03-18 21:31:05,800 [INFO] ---------------------------------
2019-03-18 21:31:05,801 [INFO] Summary:
2019-03-18 21:31:05,801 [INFO] Batch 141000, worst loss 0.061010 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:31:05,802 [INFO] Regularization: 2692.658691 * 0.0000010000 = 0.0026926587
2019-03-18 21:31:05,802 [INFO] Sum of grad norms: 0.072353
2019-03-18 21:31:05,803 [INFO] ---------------------------------
2019-03-18 21:31:24,701 [INFO] ---------------------------------
2019-03-18 21:31:24,702 [INFO] Summary:
2019-03-18 21:31:24,703 [INFO] Batch 142000, worst loss 0.061052 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:31:24,703 [INFO] Regularization: 2692.638184 * 0.0000010000 = 0.0026926382
2019-03-18 21:31:24,704 [INFO] Sum of grad norms: 0.036485
2019-03-18 21:31:24,705 [INFO] ---------------------------------
2019-03-18 21:31:43,271 [INFO] ---------------------------------
2019-03-18 21:31:43,272 [INFO] Summary:
2019-03-18 21:31:43,272 [INFO] Batch 143000, worst loss 0.061108 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:31:43,273 [INFO] Regularization: 2692.616943 * 0.0000010000 = 0.0026926170
2019-03-18 21:31:43,273 [INFO] Sum of grad norms: 0.035265
2019-03-18 21:31:43,274 [INFO] ---------------------------------
2019-03-18 21:32:02,022 [INFO] ---------------------------------
2019-03-18 21:32:02,023 [INFO] Summary:
2019-03-18 21:32:02,023 [INFO] Batch 144000, worst loss 0.061109 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:32:02,024 [INFO] Regularization: 2692.587158 * 0.0000010000 = 0.0026925872
2019-03-18 21:32:02,025 [INFO] Sum of grad norms: 0.057108
2019-03-18 21:32:02,025 [INFO] ---------------------------------
2019-03-18 21:32:21,149 [INFO] ---------------------------------
2019-03-18 21:32:21,150 [INFO] Summary:
2019-03-18 21:32:21,150 [INFO] Batch 145000, worst loss 0.061095 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:32:21,151 [INFO] Regularization: 2692.564209 * 0.0000010000 = 0.0026925642
2019-03-18 21:32:21,151 [INFO] Sum of grad norms: 0.051467
2019-03-18 21:32:21,152 [INFO] ---------------------------------
2019-03-18 21:32:39,939 [INFO] ---------------------------------
2019-03-18 21:32:39,940 [INFO] Summary:
2019-03-18 21:32:39,941 [INFO] Batch 146000, worst loss 0.061210 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:32:39,941 [INFO] Regularization: 2692.542236 * 0.0000010000 = 0.0026925423
2019-03-18 21:32:39,942 [INFO] Sum of grad norms: 0.027112
2019-03-18 21:32:39,942 [INFO] ---------------------------------
2019-03-18 21:32:58,461 [INFO] ---------------------------------
2019-03-18 21:32:58,462 [INFO] Summary:
2019-03-18 21:32:58,463 [INFO] Batch 147000, worst loss 0.061210 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:32:58,463 [INFO] Regularization: 2692.524658 * 0.0000010000 = 0.0026925246
2019-03-18 21:32:58,464 [INFO] Sum of grad norms: 0.025409
2019-03-18 21:32:58,464 [INFO] ---------------------------------
2019-03-18 21:33:17,310 [INFO] ---------------------------------
2019-03-18 21:33:17,311 [INFO] Summary:
2019-03-18 21:33:17,312 [INFO] Batch 148000, worst loss 0.060862 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:33:17,312 [INFO] Regularization: 2692.507080 * 0.0000010000 = 0.0026925071
2019-03-18 21:33:17,313 [INFO] Sum of grad norms: 0.138859
2019-03-18 21:33:17,313 [INFO] ---------------------------------
2019-03-18 21:33:36,499 [INFO] ---------------------------------
2019-03-18 21:33:36,500 [INFO] Summary:
2019-03-18 21:33:36,501 [INFO] Batch 149000, worst loss 0.060852 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:33:36,501 [INFO] Regularization: 2692.487549 * 0.0000010000 = 0.0026924876
2019-03-18 21:33:36,502 [INFO] Sum of grad norms: 0.060147
2019-03-18 21:33:36,502 [INFO] ---------------------------------
2019-03-18 21:33:55,022 [INFO] ---------------------------------
2019-03-18 21:33:55,023 [INFO] Summary:
2019-03-18 21:33:55,024 [INFO] Batch 150000, worst loss 0.060963 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:33:55,024 [INFO] Regularization: 2692.471924 * 0.0000010000 = 0.0026924720
2019-03-18 21:33:55,025 [INFO] Sum of grad norms: 0.035036
2019-03-18 21:33:55,026 [INFO] ---------------------------------
2019-03-18 21:34:00,012 [INFO] ---------------------------------
2019-03-18 21:34:00,013 [INFO] Evaluation:
2019-03-18 21:34:00,014 [INFO] Batch 150000, worst loss 0.058268 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:34:00,014 [INFO] ---------------------------------
2019-03-18 21:34:18,942 [INFO] ---------------------------------
2019-03-18 21:34:18,943 [INFO] Summary:
2019-03-18 21:34:18,944 [INFO] Batch 151000, worst loss 0.060978 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:34:18,944 [INFO] Regularization: 2692.446533 * 0.0000010000 = 0.0026924466
2019-03-18 21:34:18,945 [INFO] Sum of grad norms: 0.111665
2019-03-18 21:34:18,945 [INFO] ---------------------------------
2019-03-18 21:34:37,392 [INFO] ---------------------------------
2019-03-18 21:34:37,393 [INFO] Summary:
2019-03-18 21:34:37,393 [INFO] Batch 152000, worst loss 0.061118 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:34:37,394 [INFO] Regularization: 2692.432373 * 0.0000010000 = 0.0026924324
2019-03-18 21:34:37,394 [INFO] Sum of grad norms: 0.046447
2019-03-18 21:34:37,395 [INFO] ---------------------------------
2019-03-18 21:34:56,157 [INFO] ---------------------------------
2019-03-18 21:34:56,158 [INFO] Summary:
2019-03-18 21:34:56,159 [INFO] Batch 153000, worst loss 0.061117 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:34:56,159 [INFO] Regularization: 2692.419189 * 0.0000010000 = 0.0026924191
2019-03-18 21:34:56,160 [INFO] Sum of grad norms: 0.057609
2019-03-18 21:34:56,161 [INFO] ---------------------------------
2019-03-18 21:35:14,916 [INFO] ---------------------------------
2019-03-18 21:35:14,917 [INFO] Summary:
2019-03-18 21:35:14,918 [INFO] Batch 154000, worst loss 0.060980 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:35:14,918 [INFO] Regularization: 2692.409180 * 0.0000010000 = 0.0026924091
2019-03-18 21:35:14,919 [INFO] Sum of grad norms: 0.040743
2019-03-18 21:35:14,919 [INFO] ---------------------------------
2019-03-18 21:35:33,937 [INFO] ---------------------------------
2019-03-18 21:35:33,937 [INFO] Summary:
2019-03-18 21:35:33,938 [INFO] Batch 155000, worst loss 0.061115 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:35:33,938 [INFO] Regularization: 2692.402344 * 0.0000010000 = 0.0026924023
2019-03-18 21:35:33,939 [INFO] Sum of grad norms: 0.044671
2019-03-18 21:35:33,940 [INFO] ---------------------------------
2019-03-18 21:35:52,567 [INFO] ---------------------------------
2019-03-18 21:35:52,568 [INFO] Summary:
2019-03-18 21:35:52,569 [INFO] Batch 156000, worst loss 0.061114 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:35:52,569 [INFO] Regularization: 2692.389893 * 0.0000010000 = 0.0026923900
2019-03-18 21:35:52,570 [INFO] Sum of grad norms: 0.043603
2019-03-18 21:35:52,570 [INFO] ---------------------------------
2019-03-18 21:36:11,350 [INFO] ---------------------------------
2019-03-18 21:36:11,351 [INFO] Summary:
2019-03-18 21:36:11,351 [INFO] Batch 157000, worst loss 0.060970 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:36:11,352 [INFO] Regularization: 2692.381104 * 0.0000010000 = 0.0026923812
2019-03-18 21:36:11,352 [INFO] Sum of grad norms: 0.035119
2019-03-18 21:36:11,353 [INFO] ---------------------------------
2019-03-18 21:36:30,279 [INFO] ---------------------------------
2019-03-18 21:36:30,280 [INFO] Summary:
2019-03-18 21:36:30,281 [INFO] Batch 158000, worst loss 0.060925 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:36:30,281 [INFO] Regularization: 2692.368164 * 0.0000010000 = 0.0026923681
2019-03-18 21:36:30,281 [INFO] Sum of grad norms: 0.039857
2019-03-18 21:36:30,282 [INFO] ---------------------------------
2019-03-18 21:36:48,925 [INFO] ---------------------------------
2019-03-18 21:36:48,926 [INFO] Summary:
2019-03-18 21:36:48,927 [INFO] Batch 159000, worst loss 0.060925 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:36:48,927 [INFO] Regularization: 2692.354248 * 0.0000010000 = 0.0026923541
2019-03-18 21:36:48,928 [INFO] Sum of grad norms: 0.127956
2019-03-18 21:36:48,928 [INFO] ---------------------------------
2019-03-18 21:37:07,563 [INFO] ---------------------------------
2019-03-18 21:37:07,564 [INFO] Summary:
2019-03-18 21:37:07,565 [INFO] Batch 160000, worst loss 0.060832 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:37:07,565 [INFO] Regularization: 2692.345459 * 0.0000010000 = 0.0026923455
2019-03-18 21:37:07,566 [INFO] Sum of grad norms: 0.035382
2019-03-18 21:37:07,566 [INFO] ---------------------------------
2019-03-18 21:37:12,544 [INFO] ---------------------------------
2019-03-18 21:37:12,545 [INFO] Evaluation:
2019-03-18 21:37:12,546 [INFO] Batch 160000, worst loss 0.058258 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:37:12,546 [INFO] ---------------------------------
2019-03-18 21:37:31,570 [INFO] ---------------------------------
2019-03-18 21:37:31,571 [INFO] Summary:
2019-03-18 21:37:31,572 [INFO] Batch 161000, worst loss 0.060951 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:37:31,572 [INFO] Regularization: 2692.333496 * 0.0000010000 = 0.0026923334
2019-03-18 21:37:31,573 [INFO] Sum of grad norms: 0.032307
2019-03-18 21:37:31,573 [INFO] ---------------------------------
2019-03-18 21:37:49,946 [INFO] ---------------------------------
2019-03-18 21:37:49,947 [INFO] Summary:
2019-03-18 21:37:49,948 [INFO] Batch 162000, worst loss 0.060911 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:37:49,948 [INFO] Regularization: 2692.323730 * 0.0000010000 = 0.0026923236
2019-03-18 21:37:49,949 [INFO] Sum of grad norms: 0.039489
2019-03-18 21:37:49,949 [INFO] ---------------------------------
2019-03-18 21:38:08,577 [INFO] ---------------------------------
2019-03-18 21:38:08,578 [INFO] Summary:
2019-03-18 21:38:08,579 [INFO] Batch 163000, worst loss 0.061207 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:38:08,579 [INFO] Regularization: 2692.316162 * 0.0000010000 = 0.0026923162
2019-03-18 21:38:08,580 [INFO] Sum of grad norms: 0.053515
2019-03-18 21:38:08,580 [INFO] ---------------------------------
2019-03-18 21:38:27,364 [INFO] ---------------------------------
2019-03-18 21:38:27,365 [INFO] Summary:
2019-03-18 21:38:27,365 [INFO] Batch 164000, worst loss 0.061144 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:38:27,366 [INFO] Regularization: 2692.310547 * 0.0000010000 = 0.0026923106
2019-03-18 21:38:27,366 [INFO] Sum of grad norms: 0.042565
2019-03-18 21:38:27,367 [INFO] ---------------------------------
2019-03-18 21:38:46,199 [INFO] ---------------------------------
2019-03-18 21:38:46,200 [INFO] Summary:
2019-03-18 21:38:46,200 [INFO] Batch 165000, worst loss 0.060900 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:38:46,201 [INFO] Regularization: 2692.305664 * 0.0000010000 = 0.0026923057
2019-03-18 21:38:46,201 [INFO] Sum of grad norms: 0.029280
2019-03-18 21:38:46,202 [INFO] ---------------------------------
2019-03-18 21:39:04,671 [INFO] ---------------------------------
2019-03-18 21:39:04,672 [INFO] Summary:
2019-03-18 21:39:04,673 [INFO] Batch 166000, worst loss 0.061098 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:39:04,673 [INFO] Regularization: 2692.300781 * 0.0000010000 = 0.0026923008
2019-03-18 21:39:04,674 [INFO] Sum of grad norms: 0.029196
2019-03-18 21:39:04,674 [INFO] ---------------------------------
2019-03-18 21:39:23,528 [INFO] ---------------------------------
2019-03-18 21:39:23,529 [INFO] Summary:
2019-03-18 21:39:23,530 [INFO] Batch 167000, worst loss 0.061098 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:39:23,530 [INFO] Regularization: 2692.296387 * 0.0000010000 = 0.0026922964
2019-03-18 21:39:23,531 [INFO] Sum of grad norms: 0.062791
2019-03-18 21:39:23,532 [INFO] ---------------------------------
2019-03-18 21:39:42,549 [INFO] ---------------------------------
2019-03-18 21:39:42,550 [INFO] Summary:
2019-03-18 21:39:42,550 [INFO] Batch 168000, worst loss 0.061007 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:39:42,551 [INFO] Regularization: 2692.291260 * 0.0000010000 = 0.0026922913
2019-03-18 21:39:42,551 [INFO] Sum of grad norms: 0.024595
2019-03-18 21:39:42,552 [INFO] ---------------------------------
2019-03-18 21:40:01,151 [INFO] ---------------------------------
2019-03-18 21:40:01,152 [INFO] Summary:
2019-03-18 21:40:01,153 [INFO] Batch 169000, worst loss 0.061007 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:40:01,154 [INFO] Regularization: 2692.286621 * 0.0000010000 = 0.0026922866
2019-03-18 21:40:01,154 [INFO] Sum of grad norms: 0.042405
2019-03-18 21:40:01,155 [INFO] ---------------------------------
2019-03-18 21:40:20,106 [INFO] ---------------------------------
2019-03-18 21:40:20,107 [INFO] Summary:
2019-03-18 21:40:20,107 [INFO] Batch 170000, worst loss 0.061293 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:40:20,108 [INFO] Regularization: 2692.281494 * 0.0000010000 = 0.0026922815
2019-03-18 21:40:20,108 [INFO] Sum of grad norms: 0.071500
2019-03-18 21:40:20,109 [INFO] ---------------------------------
2019-03-18 21:40:24,962 [INFO] ---------------------------------
2019-03-18 21:40:24,962 [INFO] Evaluation:
2019-03-18 21:40:24,963 [INFO] Batch 170000, worst loss 0.058601 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:40:24,964 [INFO] ---------------------------------
2019-03-18 21:40:43,520 [INFO] ---------------------------------
2019-03-18 21:40:43,521 [INFO] Summary:
2019-03-18 21:40:43,521 [INFO] Batch 171000, worst loss 0.061122 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:40:43,522 [INFO] Regularization: 2692.277100 * 0.0000010000 = 0.0026922771
2019-03-18 21:40:43,523 [INFO] Sum of grad norms: 0.030305
2019-03-18 21:40:43,523 [INFO] ---------------------------------
2019-03-18 21:41:02,464 [INFO] ---------------------------------
2019-03-18 21:41:02,465 [INFO] Summary:
2019-03-18 21:41:02,466 [INFO] Batch 172000, worst loss 0.061122 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:41:02,467 [INFO] Regularization: 2692.274658 * 0.0000010000 = 0.0026922747
2019-03-18 21:41:02,467 [INFO] Sum of grad norms: 0.111431
2019-03-18 21:41:02,468 [INFO] ---------------------------------
2019-03-18 21:41:21,039 [INFO] ---------------------------------
2019-03-18 21:41:21,040 [INFO] Summary:
2019-03-18 21:41:21,041 [INFO] Batch 173000, worst loss 0.060877 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:41:21,041 [INFO] Regularization: 2692.272461 * 0.0000010000 = 0.0026922724
2019-03-18 21:41:21,042 [INFO] Sum of grad norms: 0.043885
2019-03-18 21:41:21,043 [INFO] ---------------------------------
2019-03-18 21:41:39,900 [INFO] ---------------------------------
2019-03-18 21:41:39,901 [INFO] Summary:
2019-03-18 21:41:39,902 [INFO] Batch 174000, worst loss 0.060903 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:41:39,902 [INFO] Regularization: 2692.271484 * 0.0000010000 = 0.0026922715
2019-03-18 21:41:39,903 [INFO] Sum of grad norms: 0.024261
2019-03-18 21:41:39,903 [INFO] ---------------------------------
2019-03-18 21:41:58,639 [INFO] ---------------------------------
2019-03-18 21:41:58,640 [INFO] Summary:
2019-03-18 21:41:58,640 [INFO] Batch 175000, worst loss 0.060903 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:41:58,641 [INFO] Regularization: 2692.270264 * 0.0000010000 = 0.0026922703
2019-03-18 21:41:58,641 [INFO] Sum of grad norms: 0.078235
2019-03-18 21:41:58,642 [INFO] ---------------------------------
2019-03-18 21:42:17,355 [INFO] ---------------------------------
2019-03-18 21:42:17,356 [INFO] Summary:
2019-03-18 21:42:17,357 [INFO] Batch 176000, worst loss 0.060891 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:42:17,357 [INFO] Regularization: 2692.269531 * 0.0000010000 = 0.0026922696
2019-03-18 21:42:17,358 [INFO] Sum of grad norms: 0.051498
2019-03-18 21:42:17,358 [INFO] ---------------------------------
2019-03-18 21:42:36,238 [INFO] ---------------------------------
2019-03-18 21:42:36,239 [INFO] Summary:
2019-03-18 21:42:36,239 [INFO] Batch 177000, worst loss 0.060891 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:42:36,240 [INFO] Regularization: 2692.269287 * 0.0000010000 = 0.0026922694
2019-03-18 21:42:36,241 [INFO] Sum of grad norms: 0.037806
2019-03-18 21:42:36,241 [INFO] ---------------------------------
2019-03-18 21:42:55,111 [INFO] ---------------------------------
2019-03-18 21:42:55,112 [INFO] Summary:
2019-03-18 21:42:55,113 [INFO] Batch 178000, worst loss 0.060978 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:42:55,114 [INFO] Regularization: 2692.266846 * 0.0000010000 = 0.0026922668
2019-03-18 21:42:55,114 [INFO] Sum of grad norms: 0.032414
2019-03-18 21:42:55,115 [INFO] ---------------------------------
2019-03-18 21:43:14,208 [INFO] ---------------------------------
2019-03-18 21:43:14,209 [INFO] Summary:
2019-03-18 21:43:14,210 [INFO] Batch 179000, worst loss 0.061094 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:43:14,211 [INFO] Regularization: 2692.265137 * 0.0000010000 = 0.0026922652
2019-03-18 21:43:14,211 [INFO] Sum of grad norms: 0.031266
2019-03-18 21:43:14,212 [INFO] ---------------------------------
2019-03-18 21:43:32,930 [INFO] ---------------------------------
2019-03-18 21:43:32,931 [INFO] Summary:
2019-03-18 21:43:32,932 [INFO] Batch 180000, worst loss 0.061094 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:43:32,932 [INFO] Regularization: 2692.263184 * 0.0000010000 = 0.0026922631
2019-03-18 21:43:32,933 [INFO] Sum of grad norms: 0.073305
2019-03-18 21:43:32,933 [INFO] ---------------------------------
2019-03-18 21:43:37,957 [INFO] ---------------------------------
2019-03-18 21:43:37,958 [INFO] Evaluation:
2019-03-18 21:43:37,959 [INFO] Batch 180000, worst loss 0.058218 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:43:37,960 [INFO] New best loss 0.058218, saved to file classifier/1552933539/1552941817_1_classifier_180000.pth
2019-03-18 21:43:37,972 [INFO] Target
2019-03-18 21:43:37,972 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 ...
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-18 21:43:37,974 [INFO] Classifier output
2019-03-18 21:43:37,978 [INFO] [[0.990155 0.990201 0.989885 ... 0.009819 0.007894 0.008248]
 [0.009641 0.010164 0.009719 ... 0.988434 0.988676 0.989967]
 [0.020871 0.020937 0.021442 ... 0.986794 0.990414 0.988438]
 ...
 [0.990243 0.99017  0.990166 ... 0.988791 0.989424 0.989885]
 [0.010571 0.009855 0.009666 ... 0.989065 0.988895 0.989146]
 [0.989665 0.989771 0.989751 ... 0.990777 0.990495 0.990262]]
2019-03-18 21:43:37,980 [INFO] ---------------------------------
2019-03-18 21:43:56,346 [INFO] ---------------------------------
2019-03-18 21:43:56,347 [INFO] Summary:
2019-03-18 21:43:56,348 [INFO] Batch 181000, worst loss 0.060864 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:43:56,348 [INFO] Regularization: 2692.261719 * 0.0000010000 = 0.0026922617
2019-03-18 21:43:56,349 [INFO] Sum of grad norms: 0.037690
2019-03-18 21:43:56,350 [INFO] ---------------------------------
2019-03-18 21:44:15,227 [INFO] ---------------------------------
2019-03-18 21:44:15,228 [INFO] Summary:
2019-03-18 21:44:15,228 [INFO] Batch 182000, worst loss 0.061123 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:44:15,229 [INFO] Regularization: 2692.260742 * 0.0000010000 = 0.0026922608
2019-03-18 21:44:15,229 [INFO] Sum of grad norms: 0.096050
2019-03-18 21:44:15,230 [INFO] ---------------------------------
2019-03-18 21:44:33,977 [INFO] ---------------------------------
2019-03-18 21:44:33,978 [INFO] Summary:
2019-03-18 21:44:33,980 [INFO] Batch 183000, worst loss 0.061123 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:44:33,980 [INFO] Regularization: 2692.260254 * 0.0000010000 = 0.0026922603
2019-03-18 21:44:33,982 [INFO] Sum of grad norms: 0.024355
2019-03-18 21:44:33,984 [INFO] ---------------------------------
2019-03-18 21:44:52,502 [INFO] ---------------------------------
2019-03-18 21:44:52,503 [INFO] Summary:
2019-03-18 21:44:52,504 [INFO] Batch 184000, worst loss 0.060876 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:44:52,504 [INFO] Regularization: 2692.259766 * 0.0000010000 = 0.0026922598
2019-03-18 21:44:52,505 [INFO] Sum of grad norms: 0.031674
2019-03-18 21:44:52,506 [INFO] ---------------------------------
2019-03-18 21:45:11,220 [INFO] ---------------------------------
2019-03-18 21:45:11,221 [INFO] Summary:
2019-03-18 21:45:11,222 [INFO] Batch 185000, worst loss 0.060923 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:45:11,223 [INFO] Regularization: 2692.259766 * 0.0000010000 = 0.0026922598
2019-03-18 21:45:11,224 [INFO] Sum of grad norms: 0.084986
2019-03-18 21:45:11,224 [INFO] ---------------------------------
2019-03-18 21:45:30,408 [INFO] ---------------------------------
2019-03-18 21:45:30,409 [INFO] Summary:
2019-03-18 21:45:30,409 [INFO] Batch 186000, worst loss 0.060923 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:45:30,410 [INFO] Regularization: 2692.259766 * 0.0000010000 = 0.0026922598
2019-03-18 21:45:30,411 [INFO] Sum of grad norms: 0.037632
2019-03-18 21:45:30,412 [INFO] ---------------------------------
2019-03-18 21:45:49,052 [INFO] ---------------------------------
2019-03-18 21:45:49,053 [INFO] Summary:
2019-03-18 21:45:49,054 [INFO] Batch 187000, worst loss 0.061045 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:45:49,055 [INFO] Regularization: 2692.259277 * 0.0000010000 = 0.0026922594
2019-03-18 21:45:49,055 [INFO] Sum of grad norms: 0.032793
2019-03-18 21:45:49,056 [INFO] ---------------------------------
2019-03-18 21:46:07,849 [INFO] ---------------------------------
2019-03-18 21:46:07,850 [INFO] Summary:
2019-03-18 21:46:07,851 [INFO] Batch 188000, worst loss 0.061045 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:46:07,851 [INFO] Regularization: 2692.259033 * 0.0000010000 = 0.0026922589
2019-03-18 21:46:07,852 [INFO] Sum of grad norms: 0.032160
2019-03-18 21:46:07,852 [INFO] ---------------------------------
2019-03-18 21:46:26,538 [INFO] ---------------------------------
2019-03-18 21:46:26,539 [INFO] Summary:
2019-03-18 21:46:26,539 [INFO] Batch 189000, worst loss 0.060922 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:46:26,540 [INFO] Regularization: 2692.258057 * 0.0000010000 = 0.0026922580
2019-03-18 21:46:26,540 [INFO] Sum of grad norms: 0.037232
2019-03-18 21:46:26,541 [INFO] ---------------------------------
2019-03-18 21:46:45,299 [INFO] ---------------------------------
2019-03-18 21:46:45,300 [INFO] Summary:
2019-03-18 21:46:45,300 [INFO] Batch 190000, worst loss 0.061050 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:46:45,301 [INFO] Regularization: 2692.257324 * 0.0000010000 = 0.0026922573
2019-03-18 21:46:45,301 [INFO] Sum of grad norms: 0.070119
2019-03-18 21:46:45,302 [INFO] ---------------------------------
2019-03-18 21:46:50,216 [INFO] ---------------------------------
2019-03-18 21:46:50,217 [INFO] Evaluation:
2019-03-18 21:46:50,218 [INFO] Batch 190000, worst loss 0.058358 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:46:50,219 [INFO] ---------------------------------
2019-03-18 21:47:09,033 [INFO] ---------------------------------
2019-03-18 21:47:09,034 [INFO] Summary:
2019-03-18 21:47:09,034 [INFO] Batch 191000, worst loss 0.061098 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:47:09,035 [INFO] Regularization: 2692.256836 * 0.0000010000 = 0.0026922568
2019-03-18 21:47:09,035 [INFO] Sum of grad norms: 0.139480
2019-03-18 21:47:09,036 [INFO] ---------------------------------
2019-03-18 21:47:27,992 [INFO] ---------------------------------
2019-03-18 21:47:27,993 [INFO] Summary:
2019-03-18 21:47:27,993 [INFO] Batch 192000, worst loss 0.061098 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:47:27,994 [INFO] Regularization: 2692.256348 * 0.0000010000 = 0.0026922564
2019-03-18 21:47:27,995 [INFO] Sum of grad norms: 0.090631
2019-03-18 21:47:27,995 [INFO] ---------------------------------
2019-03-18 21:47:46,880 [INFO] ---------------------------------
2019-03-18 21:47:46,881 [INFO] Summary:
2019-03-18 21:47:46,882 [INFO] Batch 193000, worst loss 0.060998 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:47:46,882 [INFO] Regularization: 2692.255859 * 0.0000010000 = 0.0026922559
2019-03-18 21:47:46,883 [INFO] Sum of grad norms: 0.023833
2019-03-18 21:47:46,884 [INFO] ---------------------------------
2019-03-18 21:48:05,543 [INFO] ---------------------------------
2019-03-18 21:48:05,544 [INFO] Summary:
2019-03-18 21:48:05,545 [INFO] Batch 194000, worst loss 0.061124 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:48:05,545 [INFO] Regularization: 2692.255615 * 0.0000010000 = 0.0026922557
2019-03-18 21:48:05,546 [INFO] Sum of grad norms: 0.079377
2019-03-18 21:48:05,546 [INFO] ---------------------------------
2019-03-18 21:48:24,705 [INFO] ---------------------------------
2019-03-18 21:48:24,706 [INFO] Summary:
2019-03-18 21:48:24,706 [INFO] Batch 195000, worst loss 0.061141 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:48:24,707 [INFO] Regularization: 2692.255371 * 0.0000010000 = 0.0026922554
2019-03-18 21:48:24,707 [INFO] Sum of grad norms: 0.072642
2019-03-18 21:48:24,708 [INFO] ---------------------------------
2019-03-18 21:48:43,601 [INFO] ---------------------------------
2019-03-18 21:48:43,602 [INFO] Summary:
2019-03-18 21:48:43,602 [INFO] Batch 196000, worst loss 0.061141 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:48:43,603 [INFO] Regularization: 2692.254639 * 0.0000010000 = 0.0026922547
2019-03-18 21:48:43,603 [INFO] Sum of grad norms: 0.077468
2019-03-18 21:48:43,604 [INFO] ---------------------------------
2019-03-18 21:49:02,338 [INFO] ---------------------------------
2019-03-18 21:49:02,339 [INFO] Summary:
2019-03-18 21:49:02,340 [INFO] Batch 197000, worst loss 0.060963 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:49:02,340 [INFO] Regularization: 2692.254395 * 0.0000010000 = 0.0026922545
2019-03-18 21:49:02,341 [INFO] Sum of grad norms: 0.096842
2019-03-18 21:49:02,341 [INFO] ---------------------------------
2019-03-18 21:49:20,991 [INFO] ---------------------------------
2019-03-18 21:49:20,992 [INFO] Summary:
2019-03-18 21:49:20,992 [INFO] Batch 198000, worst loss 0.060963 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:49:20,993 [INFO] Regularization: 2692.254395 * 0.0000010000 = 0.0026922545
2019-03-18 21:49:20,993 [INFO] Sum of grad norms: 0.075721
2019-03-18 21:49:20,994 [INFO] ---------------------------------
2019-03-18 21:49:39,858 [INFO] ---------------------------------
2019-03-18 21:49:39,859 [INFO] Summary:
2019-03-18 21:49:39,860 [INFO] Batch 199000, worst loss 0.061117 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:49:39,860 [INFO] Regularization: 2692.253906 * 0.0000010000 = 0.0026922538
2019-03-18 21:49:39,861 [INFO] Sum of grad norms: 0.070516
2019-03-18 21:49:39,861 [INFO] ---------------------------------
2019-03-18 21:49:58,605 [INFO] ---------------------------------
2019-03-18 21:49:58,606 [INFO] Summary:
2019-03-18 21:49:58,607 [INFO] Batch 200000, worst loss 0.061117 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:49:58,607 [INFO] Regularization: 2692.253906 * 0.0000010000 = 0.0026922538
2019-03-18 21:49:58,608 [INFO] Sum of grad norms: 0.072337
2019-03-18 21:49:58,608 [INFO] ---------------------------------
2019-03-18 21:50:03,562 [INFO] ---------------------------------
2019-03-18 21:50:03,562 [INFO] Evaluation:
2019-03-18 21:50:03,563 [INFO] Batch 200000, worst loss 0.058249 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:50:03,564 [INFO] ---------------------------------
2019-03-18 21:50:22,459 [INFO] ---------------------------------
2019-03-18 21:50:22,460 [INFO] Summary:
2019-03-18 21:50:22,460 [INFO] Batch 201000, worst loss 0.061129 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:50:22,461 [INFO] Regularization: 2692.253906 * 0.0000010000 = 0.0026922538
2019-03-18 21:50:22,461 [INFO] Sum of grad norms: 0.030193
2019-03-18 21:50:22,462 [INFO] ---------------------------------
2019-03-18 21:50:41,372 [INFO] ---------------------------------
2019-03-18 21:50:41,373 [INFO] Summary:
2019-03-18 21:50:41,374 [INFO] Batch 202000, worst loss 0.061129 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:50:41,374 [INFO] Regularization: 2692.253906 * 0.0000010000 = 0.0026922538
2019-03-18 21:50:41,375 [INFO] Sum of grad norms: 0.077546
2019-03-18 21:50:41,375 [INFO] ---------------------------------
2019-03-18 21:50:59,957 [INFO] ---------------------------------
2019-03-18 21:50:59,958 [INFO] Summary:
2019-03-18 21:50:59,958 [INFO] Batch 203000, worst loss 0.060984 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:50:59,959 [INFO] Regularization: 2692.253906 * 0.0000010000 = 0.0026922538
2019-03-18 21:50:59,959 [INFO] Sum of grad norms: 0.045572
2019-03-18 21:50:59,960 [INFO] ---------------------------------
2019-03-18 21:51:18,680 [INFO] ---------------------------------
2019-03-18 21:51:18,681 [INFO] Summary:
2019-03-18 21:51:18,682 [INFO] Batch 204000, worst loss 0.060939 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:51:18,682 [INFO] Regularization: 2692.253906 * 0.0000010000 = 0.0026922538
2019-03-18 21:51:18,683 [INFO] Sum of grad norms: 0.035415
2019-03-18 21:51:18,683 [INFO] ---------------------------------
2019-03-18 21:51:37,457 [INFO] ---------------------------------
2019-03-18 21:51:37,458 [INFO] Summary:
2019-03-18 21:51:37,458 [INFO] Batch 205000, worst loss 0.061111 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:51:37,459 [INFO] Regularization: 2692.253662 * 0.0000010000 = 0.0026922536
2019-03-18 21:51:37,459 [INFO] Sum of grad norms: 0.122421
2019-03-18 21:51:37,460 [INFO] ---------------------------------
2019-03-18 21:51:56,258 [INFO] ---------------------------------
2019-03-18 21:51:56,259 [INFO] Summary:
2019-03-18 21:51:56,260 [INFO] Batch 206000, worst loss 0.061111 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:51:56,260 [INFO] Regularization: 2692.253418 * 0.0000010000 = 0.0026922533
2019-03-18 21:51:56,261 [INFO] Sum of grad norms: 0.023567
2019-03-18 21:51:56,261 [INFO] ---------------------------------
2019-03-18 21:52:15,096 [INFO] ---------------------------------
2019-03-18 21:52:15,097 [INFO] Summary:
2019-03-18 21:52:15,098 [INFO] Batch 207000, worst loss 0.061096 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:52:15,099 [INFO] Regularization: 2692.253662 * 0.0000010000 = 0.0026922536
2019-03-18 21:52:15,100 [INFO] Sum of grad norms: 0.052163
2019-03-18 21:52:15,100 [INFO] ---------------------------------
2019-03-18 21:52:34,558 [INFO] ---------------------------------
2019-03-18 21:52:34,559 [INFO] Summary:
2019-03-18 21:52:34,560 [INFO] Batch 208000, worst loss 0.061096 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:52:34,560 [INFO] Regularization: 2692.253418 * 0.0000010000 = 0.0026922533
2019-03-18 21:52:34,561 [INFO] Sum of grad norms: 0.034398
2019-03-18 21:52:34,562 [INFO] ---------------------------------
2019-03-18 21:52:53,368 [INFO] ---------------------------------
2019-03-18 21:52:53,369 [INFO] Summary:
2019-03-18 21:52:53,370 [INFO] Batch 209000, worst loss 0.061194 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:52:53,370 [INFO] Regularization: 2692.253174 * 0.0000010000 = 0.0026922531
2019-03-18 21:52:53,371 [INFO] Sum of grad norms: 0.052738
2019-03-18 21:52:53,371 [INFO] ---------------------------------
2019-03-18 21:53:11,880 [INFO] ---------------------------------
2019-03-18 21:53:11,881 [INFO] Summary:
2019-03-18 21:53:11,882 [INFO] Batch 210000, worst loss 0.061194 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:53:11,882 [INFO] Regularization: 2692.253174 * 0.0000010000 = 0.0026922531
2019-03-18 21:53:11,883 [INFO] Sum of grad norms: 0.037430
2019-03-18 21:53:11,883 [INFO] ---------------------------------
2019-03-18 21:53:16,860 [INFO] ---------------------------------
2019-03-18 21:53:16,860 [INFO] Evaluation:
2019-03-18 21:53:16,861 [INFO] Batch 210000, worst loss 0.058277 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:53:16,863 [INFO] ---------------------------------
2019-03-18 21:53:35,554 [INFO] ---------------------------------
2019-03-18 21:53:35,555 [INFO] Summary:
2019-03-18 21:53:35,556 [INFO] Batch 211000, worst loss 0.061386 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:53:35,556 [INFO] Regularization: 2692.252930 * 0.0000010000 = 0.0026922529
2019-03-18 21:53:35,557 [INFO] Sum of grad norms: 0.121184
2019-03-18 21:53:35,557 [INFO] ---------------------------------
2019-03-18 21:53:54,335 [INFO] ---------------------------------
2019-03-18 21:53:54,336 [INFO] Summary:
2019-03-18 21:53:54,337 [INFO] Batch 212000, worst loss 0.061386 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:53:54,338 [INFO] Regularization: 2692.252930 * 0.0000010000 = 0.0026922529
2019-03-18 21:53:54,338 [INFO] Sum of grad norms: 0.067462
2019-03-18 21:53:54,339 [INFO] ---------------------------------
2019-03-18 21:54:13,506 [INFO] ---------------------------------
2019-03-18 21:54:13,507 [INFO] Summary:
2019-03-18 21:54:13,508 [INFO] Batch 213000, worst loss 0.061386 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:54:13,509 [INFO] Regularization: 2692.252930 * 0.0000010000 = 0.0026922529
2019-03-18 21:54:13,509 [INFO] Sum of grad norms: 0.023846
2019-03-18 21:54:13,510 [INFO] ---------------------------------
2019-03-18 21:54:32,195 [INFO] ---------------------------------
2019-03-18 21:54:32,195 [INFO] Summary:
2019-03-18 21:54:32,196 [INFO] Batch 214000, worst loss 0.060976 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:54:32,197 [INFO] Regularization: 2692.252930 * 0.0000010000 = 0.0026922529
2019-03-18 21:54:32,197 [INFO] Sum of grad norms: 0.031621
2019-03-18 21:54:32,198 [INFO] ---------------------------------
2019-03-18 21:54:51,114 [INFO] ---------------------------------
2019-03-18 21:54:51,115 [INFO] Summary:
2019-03-18 21:54:51,116 [INFO] Batch 215000, worst loss 0.061424 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:54:51,116 [INFO] Regularization: 2692.252930 * 0.0000010000 = 0.0026922529
2019-03-18 21:54:51,117 [INFO] Sum of grad norms: 0.037806
2019-03-18 21:54:51,117 [INFO] ---------------------------------
2019-03-18 21:55:10,019 [INFO] ---------------------------------
2019-03-18 21:55:10,020 [INFO] Summary:
2019-03-18 21:55:10,020 [INFO] Batch 216000, worst loss 0.061424 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:55:10,021 [INFO] Regularization: 2692.252930 * 0.0000010000 = 0.0026922529
2019-03-18 21:55:10,021 [INFO] Sum of grad norms: 0.059496
2019-03-18 21:55:10,022 [INFO] ---------------------------------
2019-03-18 21:55:28,884 [INFO] ---------------------------------
2019-03-18 21:55:28,885 [INFO] Summary:
2019-03-18 21:55:28,886 [INFO] Batch 217000, worst loss 0.061424 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:55:28,886 [INFO] Regularization: 2692.252930 * 0.0000010000 = 0.0026922529
2019-03-18 21:55:28,887 [INFO] Sum of grad norms: 0.057183
2019-03-18 21:55:28,887 [INFO] ---------------------------------
2019-03-18 21:55:47,562 [INFO] ---------------------------------
2019-03-18 21:55:47,563 [INFO] Summary:
2019-03-18 21:55:47,563 [INFO] Batch 218000, worst loss 0.060934 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:55:47,564 [INFO] Regularization: 2692.252930 * 0.0000010000 = 0.0026922529
2019-03-18 21:55:47,564 [INFO] Sum of grad norms: 0.048892
2019-03-18 21:55:47,565 [INFO] ---------------------------------
2019-03-18 21:56:06,147 [INFO] ---------------------------------
2019-03-18 21:56:06,148 [INFO] Summary:
2019-03-18 21:56:06,148 [INFO] Batch 219000, worst loss 0.061481 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:56:06,149 [INFO] Regularization: 2692.252686 * 0.0000010000 = 0.0026922526
2019-03-18 21:56:06,150 [INFO] Sum of grad norms: 0.059125
2019-03-18 21:56:06,150 [INFO] ---------------------------------
2019-03-18 21:56:25,264 [INFO] ---------------------------------
2019-03-18 21:56:25,265 [INFO] Summary:
2019-03-18 21:56:25,265 [INFO] Batch 220000, worst loss 0.061481 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:56:25,266 [INFO] Regularization: 2692.252686 * 0.0000010000 = 0.0026922526
2019-03-18 21:56:25,266 [INFO] Sum of grad norms: 0.052274
2019-03-18 21:56:25,267 [INFO] ---------------------------------
2019-03-18 21:56:30,211 [INFO] ---------------------------------
2019-03-18 21:56:30,213 [INFO] Evaluation:
2019-03-18 21:56:30,214 [INFO] Batch 220000, worst loss 0.058789 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:56:30,214 [INFO] ---------------------------------
2019-03-18 21:56:49,098 [INFO] ---------------------------------
2019-03-18 21:56:49,100 [INFO] Summary:
2019-03-18 21:56:49,100 [INFO] Batch 221000, worst loss 0.061001 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:56:49,101 [INFO] Regularization: 2692.252686 * 0.0000010000 = 0.0026922526
2019-03-18 21:56:49,101 [INFO] Sum of grad norms: 0.158825
2019-03-18 21:56:49,102 [INFO] ---------------------------------
2019-03-18 21:57:07,845 [INFO] ---------------------------------
2019-03-18 21:57:07,846 [INFO] Summary:
2019-03-18 21:57:07,847 [INFO] Batch 222000, worst loss 0.061151 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:57:07,847 [INFO] Regularization: 2692.252686 * 0.0000010000 = 0.0026922526
2019-03-18 21:57:07,848 [INFO] Sum of grad norms: 0.052151
2019-03-18 21:57:07,848 [INFO] ---------------------------------
2019-03-18 21:57:26,472 [INFO] ---------------------------------
2019-03-18 21:57:26,473 [INFO] Summary:
2019-03-18 21:57:26,473 [INFO] Batch 223000, worst loss 0.061263 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:57:26,474 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 21:57:26,475 [INFO] Sum of grad norms: 0.060072
2019-03-18 21:57:26,475 [INFO] ---------------------------------
2019-03-18 21:57:45,357 [INFO] ---------------------------------
2019-03-18 21:57:45,357 [INFO] Summary:
2019-03-18 21:57:45,358 [INFO] Batch 224000, worst loss 0.061263 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:57:45,359 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 21:57:45,359 [INFO] Sum of grad norms: 0.038772
2019-03-18 21:57:45,360 [INFO] ---------------------------------
2019-03-18 21:58:04,135 [INFO] ---------------------------------
2019-03-18 21:58:04,136 [INFO] Summary:
2019-03-18 21:58:04,137 [INFO] Batch 225000, worst loss 0.061260 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:58:04,137 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 21:58:04,138 [INFO] Sum of grad norms: 0.028969
2019-03-18 21:58:04,138 [INFO] ---------------------------------
2019-03-18 21:58:23,005 [INFO] ---------------------------------
2019-03-18 21:58:23,006 [INFO] Summary:
2019-03-18 21:58:23,006 [INFO] Batch 226000, worst loss 0.061151 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:58:23,007 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 21:58:23,007 [INFO] Sum of grad norms: 0.027627
2019-03-18 21:58:23,008 [INFO] ---------------------------------
2019-03-18 21:58:41,541 [INFO] ---------------------------------
2019-03-18 21:58:41,542 [INFO] Summary:
2019-03-18 21:58:41,542 [INFO] Batch 227000, worst loss 0.061062 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:58:41,543 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 21:58:41,544 [INFO] Sum of grad norms: 0.102346
2019-03-18 21:58:41,545 [INFO] ---------------------------------
2019-03-18 21:59:00,133 [INFO] ---------------------------------
2019-03-18 21:59:00,134 [INFO] Summary:
2019-03-18 21:59:00,134 [INFO] Batch 228000, worst loss 0.061204 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:59:00,135 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 21:59:00,135 [INFO] Sum of grad norms: 0.044163
2019-03-18 21:59:00,136 [INFO] ---------------------------------
2019-03-18 21:59:19,255 [INFO] ---------------------------------
2019-03-18 21:59:19,256 [INFO] Summary:
2019-03-18 21:59:19,257 [INFO] Batch 229000, worst loss 0.061204 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:59:19,257 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 21:59:19,258 [INFO] Sum of grad norms: 0.065933
2019-03-18 21:59:19,258 [INFO] ---------------------------------
2019-03-18 21:59:38,096 [INFO] ---------------------------------
2019-03-18 21:59:38,097 [INFO] Summary:
2019-03-18 21:59:38,098 [INFO] Batch 230000, worst loss 0.061204 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 21:59:38,098 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 21:59:38,099 [INFO] Sum of grad norms: 0.049106
2019-03-18 21:59:38,099 [INFO] ---------------------------------
2019-03-18 21:59:43,003 [INFO] ---------------------------------
2019-03-18 21:59:43,004 [INFO] Evaluation:
2019-03-18 21:59:43,006 [INFO] Batch 230000, worst loss 0.058474 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 21:59:43,007 [INFO] ---------------------------------
2019-03-18 22:00:01,491 [INFO] ---------------------------------
2019-03-18 22:00:01,492 [INFO] Summary:
2019-03-18 22:00:01,492 [INFO] Batch 231000, worst loss 0.061166 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:00:01,493 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:00:01,493 [INFO] Sum of grad norms: 0.039311
2019-03-18 22:00:01,494 [INFO] ---------------------------------
2019-03-18 22:00:20,460 [INFO] ---------------------------------
2019-03-18 22:00:20,461 [INFO] Summary:
2019-03-18 22:00:20,462 [INFO] Batch 232000, worst loss 0.061004 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:00:20,462 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:00:20,463 [INFO] Sum of grad norms: 0.089764
2019-03-18 22:00:20,463 [INFO] ---------------------------------
2019-03-18 22:00:39,229 [INFO] ---------------------------------
2019-03-18 22:00:39,230 [INFO] Summary:
2019-03-18 22:00:39,231 [INFO] Batch 233000, worst loss 0.061004 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:00:39,231 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:00:39,232 [INFO] Sum of grad norms: 0.051671
2019-03-18 22:00:39,232 [INFO] ---------------------------------
2019-03-18 22:00:58,074 [INFO] ---------------------------------
2019-03-18 22:00:58,076 [INFO] Summary:
2019-03-18 22:00:58,076 [INFO] Batch 234000, worst loss 0.060933 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:00:58,077 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:00:58,077 [INFO] Sum of grad norms: 0.108842
2019-03-18 22:00:58,078 [INFO] ---------------------------------
2019-03-18 22:01:16,780 [INFO] ---------------------------------
2019-03-18 22:01:16,782 [INFO] Summary:
2019-03-18 22:01:16,782 [INFO] Batch 235000, worst loss 0.061170 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:01:16,783 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:01:16,784 [INFO] Sum of grad norms: 0.028512
2019-03-18 22:01:16,785 [INFO] ---------------------------------
2019-03-18 22:01:35,341 [INFO] ---------------------------------
2019-03-18 22:01:35,342 [INFO] Summary:
2019-03-18 22:01:35,343 [INFO] Batch 236000, worst loss 0.061170 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:01:35,343 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:01:35,344 [INFO] Sum of grad norms: 0.060805
2019-03-18 22:01:35,344 [INFO] ---------------------------------
2019-03-18 22:01:54,467 [INFO] ---------------------------------
2019-03-18 22:01:54,468 [INFO] Summary:
2019-03-18 22:01:54,469 [INFO] Batch 237000, worst loss 0.061170 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:01:54,469 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:01:54,470 [INFO] Sum of grad norms: 0.056696
2019-03-18 22:01:54,471 [INFO] ---------------------------------
2019-03-18 22:02:13,226 [INFO] ---------------------------------
2019-03-18 22:02:13,227 [INFO] Summary:
2019-03-18 22:02:13,228 [INFO] Batch 238000, worst loss 0.061096 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:02:13,229 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:02:13,229 [INFO] Sum of grad norms: 0.036641
2019-03-18 22:02:13,230 [INFO] ---------------------------------
2019-03-18 22:02:31,944 [INFO] ---------------------------------
2019-03-18 22:02:31,945 [INFO] Summary:
2019-03-18 22:02:31,945 [INFO] Batch 239000, worst loss 0.061096 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:02:31,946 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:02:31,946 [INFO] Sum of grad norms: 0.065979
2019-03-18 22:02:31,947 [INFO] ---------------------------------
2019-03-18 22:02:50,842 [INFO] ---------------------------------
2019-03-18 22:02:50,843 [INFO] Summary:
2019-03-18 22:02:50,844 [INFO] Batch 240000, worst loss 0.061096 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:02:50,844 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:02:50,844 [INFO] Sum of grad norms: 0.023084
2019-03-18 22:02:50,845 [INFO] ---------------------------------
2019-03-18 22:02:55,743 [INFO] ---------------------------------
2019-03-18 22:02:55,744 [INFO] Evaluation:
2019-03-18 22:02:55,744 [INFO] Batch 240000, worst loss 0.058187 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:02:55,747 [INFO] New best loss 0.058187, saved to file classifier/1552933539/1552942975_1_classifier_240000.pth
2019-03-18 22:02:55,758 [INFO] Target
2019-03-18 22:02:55,759 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]]
2019-03-18 22:02:55,761 [INFO] Classifier output
2019-03-18 22:02:55,762 [INFO] [[0.990336 0.989648 0.990235 ... 0.009776 0.007586 0.008056]
 [0.010163 0.01058  0.009789 ... 0.987679 0.988275 0.990157]
 [0.990151 0.990196 0.989882 ... 0.009802 0.007879 0.008232]
 ...
 [0.990151 0.990196 0.989882 ... 0.009802 0.007879 0.008232]
 [0.989383 0.989937 0.989366 ... 0.000007 0.000008 0.000004]
 [0.009205 0.009704 0.009632 ... 0.992746 0.991046 0.988537]]
2019-03-18 22:02:55,763 [INFO] ---------------------------------
2019-03-18 22:03:14,172 [INFO] ---------------------------------
2019-03-18 22:03:14,173 [INFO] Summary:
2019-03-18 22:03:14,173 [INFO] Batch 241000, worst loss 0.060994 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:03:14,174 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:03:14,174 [INFO] Sum of grad norms: 0.055834
2019-03-18 22:03:14,175 [INFO] ---------------------------------
2019-03-18 22:03:32,835 [INFO] ---------------------------------
2019-03-18 22:03:32,836 [INFO] Summary:
2019-03-18 22:03:32,836 [INFO] Batch 242000, worst loss 0.061224 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:03:32,837 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:03:32,837 [INFO] Sum of grad norms: 0.053425
2019-03-18 22:03:32,838 [INFO] ---------------------------------
2019-03-18 22:03:51,399 [INFO] ---------------------------------
2019-03-18 22:03:51,400 [INFO] Summary:
2019-03-18 22:03:51,400 [INFO] Batch 243000, worst loss 0.061224 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:03:51,401 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:03:51,401 [INFO] Sum of grad norms: 0.059002
2019-03-18 22:03:51,402 [INFO] ---------------------------------
2019-03-18 22:04:10,457 [INFO] ---------------------------------
2019-03-18 22:04:10,458 [INFO] Summary:
2019-03-18 22:04:10,458 [INFO] Batch 244000, worst loss 0.061224 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:04:10,459 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:04:10,459 [INFO] Sum of grad norms: 0.037904
2019-03-18 22:04:10,460 [INFO] ---------------------------------
2019-03-18 22:04:29,256 [INFO] ---------------------------------
2019-03-18 22:04:29,257 [INFO] Summary:
2019-03-18 22:04:29,258 [INFO] Batch 245000, worst loss 0.060999 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:04:29,258 [INFO] Regularization: 2692.252441 * 0.0000010000 = 0.0026922524
2019-03-18 22:04:29,259 [INFO] Sum of grad norms: 0.048281
2019-03-18 22:04:29,259 [INFO] ---------------------------------
2019-03-18 22:04:48,096 [INFO] ---------------------------------
2019-03-18 22:04:48,097 [INFO] Summary:
2019-03-18 22:04:48,097 [INFO] Batch 246000, worst loss 0.060999 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:04:48,098 [INFO] Regularization: 2692.252197 * 0.0000010000 = 0.0026922522
2019-03-18 22:04:48,098 [INFO] Sum of grad norms: 0.093496
2019-03-18 22:04:48,099 [INFO] ---------------------------------
2019-03-18 22:05:07,007 [INFO] ---------------------------------
2019-03-18 22:05:07,008 [INFO] Summary:
2019-03-18 22:05:07,009 [INFO] Batch 247000, worst loss 0.060999 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:05:07,010 [INFO] Regularization: 2692.252197 * 0.0000010000 = 0.0026922522
2019-03-18 22:05:07,010 [INFO] Sum of grad norms: 0.035344
2019-03-18 22:05:07,011 [INFO] ---------------------------------
2019-03-18 22:05:25,790 [INFO] ---------------------------------
2019-03-18 22:05:25,791 [INFO] Summary:
2019-03-18 22:05:25,791 [INFO] Batch 248000, worst loss 0.060957 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:05:25,792 [INFO] Regularization: 2692.252197 * 0.0000010000 = 0.0026922522
2019-03-18 22:05:25,792 [INFO] Sum of grad norms: 0.038301
2019-03-18 22:05:25,793 [INFO] ---------------------------------
2019-03-18 22:05:44,504 [INFO] ---------------------------------
2019-03-18 22:05:44,505 [INFO] Summary:
2019-03-18 22:05:44,506 [INFO] Batch 249000, worst loss 0.061121 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:05:44,507 [INFO] Regularization: 2692.252197 * 0.0000010000 = 0.0026922522
2019-03-18 22:05:44,507 [INFO] Sum of grad norms: 0.094653
2019-03-18 22:05:44,508 [INFO] ---------------------------------
2019-03-18 22:06:03,088 [INFO] ---------------------------------
2019-03-18 22:06:03,089 [INFO] Summary:
2019-03-18 22:06:03,090 [INFO] Batch 250000, worst loss 0.061121 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:06:03,091 [INFO] Regularization: 2692.252197 * 0.0000010000 = 0.0026922522
2019-03-18 22:06:03,091 [INFO] Sum of grad norms: 0.029468
2019-03-18 22:06:03,092 [INFO] ---------------------------------
2019-03-18 22:06:08,093 [INFO] ---------------------------------
2019-03-18 22:06:08,094 [INFO] Evaluation:
2019-03-18 22:06:08,095 [INFO] Batch 250000, worst loss 0.058229 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:06:08,096 [INFO] ---------------------------------
2019-03-18 22:06:08,097 [INFO] Finished training, saved to file classifier/1552933539/1552943168_1_classifier_final.pth
2019-03-18 22:06:08,261 [INFO] ---------------------------------
2019-03-18 22:06:08,263 [INFO] Training model #2: (1, 64, 201) @ 1
2019-03-18 22:06:24,293 [INFO] ---------------------------------
2019-03-18 22:06:24,294 [INFO] Summary:
2019-03-18 22:06:24,295 [INFO] Batch 1000, worst loss 18.753939 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-18 22:06:24,295 [INFO] Regularization: 10196.164062 * 0.0000010000 = 0.0101961643
2019-03-18 22:06:24,296 [INFO] Sum of grad norms: 2.871184
2019-03-18 22:06:24,297 [INFO] ---------------------------------
2019-03-18 22:06:39,947 [INFO] ---------------------------------
2019-03-18 22:06:39,948 [INFO] Summary:
2019-03-18 22:06:39,948 [INFO] Batch 2000, worst loss 0.151203 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-18 22:06:39,949 [INFO] Regularization: 8856.326172 * 0.0000010000 = 0.0088563263
2019-03-18 22:06:39,949 [INFO] Sum of grad norms: 3.480185
2019-03-18 22:06:39,950 [INFO] ---------------------------------
2019-03-18 22:06:59,046 [INFO] ---------------------------------
2019-03-18 22:06:59,047 [INFO] Summary:
2019-03-18 22:06:59,047 [INFO] Batch 3000, worst loss 0.089968 (incl. reg.) of 1000 batches, learning rate 0.001512 @cl.-depth 1
2019-03-18 22:06:59,048 [INFO] Regularization: 7809.753418 * 0.0000010000 = 0.0078097535
2019-03-18 22:06:59,049 [INFO] Sum of grad norms: 0.673756
2019-03-18 22:06:59,049 [INFO] ---------------------------------
2019-03-18 22:07:17,888 [INFO] ---------------------------------
2019-03-18 22:07:17,889 [INFO] Summary:
2019-03-18 22:07:17,889 [INFO] Batch 4000, worst loss 0.079927 (incl. reg.) of 1000 batches, learning rate 0.000900 @cl.-depth 1
2019-03-18 22:07:17,890 [INFO] Regularization: 7281.048828 * 0.0000010000 = 0.0072810487
2019-03-18 22:07:17,890 [INFO] Sum of grad norms: 0.623007
2019-03-18 22:07:17,891 [INFO] ---------------------------------
2019-03-18 22:07:36,776 [INFO] ---------------------------------
2019-03-18 22:07:36,777 [INFO] Summary:
2019-03-18 22:07:36,778 [INFO] Batch 5000, worst loss 0.076776 (incl. reg.) of 1000 batches, learning rate 0.000799 @cl.-depth 1
2019-03-18 22:07:36,778 [INFO] Regularization: 6860.397461 * 0.0000010000 = 0.0068603973
2019-03-18 22:07:36,779 [INFO] Sum of grad norms: 0.395890
2019-03-18 22:07:36,779 [INFO] ---------------------------------
2019-03-18 22:07:55,764 [INFO] ---------------------------------
2019-03-18 22:07:55,765 [INFO] Summary:
2019-03-18 22:07:55,765 [INFO] Batch 6000, worst loss 0.074334 (incl. reg.) of 1000 batches, learning rate 0.000768 @cl.-depth 1
2019-03-18 22:07:55,766 [INFO] Regularization: 6553.930176 * 0.0000010000 = 0.0065539302
2019-03-18 22:07:55,766 [INFO] Sum of grad norms: 0.163815
2019-03-18 22:07:55,767 [INFO] ---------------------------------
2019-03-18 22:08:14,699 [INFO] ---------------------------------
2019-03-18 22:08:14,700 [INFO] Summary:
2019-03-18 22:08:14,701 [INFO] Batch 7000, worst loss 0.080967 (incl. reg.) of 1000 batches, learning rate 0.000743 @cl.-depth 1
2019-03-18 22:08:14,701 [INFO] Regularization: 6263.804199 * 0.0000010000 = 0.0062638042
2019-03-18 22:08:14,702 [INFO] Sum of grad norms: 0.142381
2019-03-18 22:08:14,702 [INFO] ---------------------------------
2019-03-18 22:08:33,548 [INFO] ---------------------------------
2019-03-18 22:08:33,549 [INFO] Summary:
2019-03-18 22:08:33,550 [INFO] Batch 8000, worst loss 0.073183 (incl. reg.) of 1000 batches, learning rate 0.000743 @cl.-depth 1
2019-03-18 22:08:33,550 [INFO] Regularization: 6021.207031 * 0.0000010000 = 0.0060212072
2019-03-18 22:08:33,551 [INFO] Sum of grad norms: 0.423138
2019-03-18 22:08:33,551 [INFO] ---------------------------------
2019-03-18 22:08:52,639 [INFO] ---------------------------------
2019-03-18 22:08:52,640 [INFO] Summary:
2019-03-18 22:08:52,641 [INFO] Batch 9000, worst loss 0.075034 (incl. reg.) of 1000 batches, learning rate 0.000732 @cl.-depth 1
2019-03-18 22:08:52,642 [INFO] Regularization: 5853.205078 * 0.0000010000 = 0.0058532050
2019-03-18 22:08:52,643 [INFO] Sum of grad norms: 1.126407
2019-03-18 22:08:52,644 [INFO] ---------------------------------
2019-03-18 22:09:11,888 [INFO] ---------------------------------
2019-03-18 22:09:11,889 [INFO] Summary:
2019-03-18 22:09:11,889 [INFO] Batch 10000, worst loss 0.073083 (incl. reg.) of 1000 batches, learning rate 0.000732 @cl.-depth 1
2019-03-18 22:09:11,890 [INFO] Regularization: 5649.518555 * 0.0000010000 = 0.0056495187
2019-03-18 22:09:11,891 [INFO] Sum of grad norms: 0.108720
2019-03-18 22:09:11,891 [INFO] ---------------------------------
2019-03-18 22:09:16,791 [INFO] ---------------------------------
2019-03-18 22:09:16,793 [INFO] Evaluation:
2019-03-18 22:09:16,802 [INFO] Batch 10000, worst loss 0.064074 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:09:16,803 [INFO] ---------------------------------
2019-03-18 22:09:35,832 [INFO] ---------------------------------
2019-03-18 22:09:35,833 [INFO] Summary:
2019-03-18 22:09:35,834 [INFO] Batch 11000, worst loss 0.072917 (incl. reg.) of 1000 batches, learning rate 0.000731 @cl.-depth 1
2019-03-18 22:09:35,834 [INFO] Regularization: 5505.079102 * 0.0000010000 = 0.0055050789
2019-03-18 22:09:35,835 [INFO] Sum of grad norms: 0.745602
2019-03-18 22:09:35,836 [INFO] ---------------------------------
2019-03-18 22:09:54,865 [INFO] ---------------------------------
2019-03-18 22:09:54,866 [INFO] Summary:
2019-03-18 22:09:54,866 [INFO] Batch 12000, worst loss 0.072667 (incl. reg.) of 1000 batches, learning rate 0.000729 @cl.-depth 1
2019-03-18 22:09:54,867 [INFO] Regularization: 5382.096680 * 0.0000010000 = 0.0053820969
2019-03-18 22:09:54,867 [INFO] Sum of grad norms: 1.264376
2019-03-18 22:09:54,868 [INFO] ---------------------------------
2019-03-18 22:10:13,659 [INFO] ---------------------------------
2019-03-18 22:10:13,660 [INFO] Summary:
2019-03-18 22:10:13,661 [INFO] Batch 13000, worst loss 0.069205 (incl. reg.) of 1000 batches, learning rate 0.000727 @cl.-depth 1
2019-03-18 22:10:13,661 [INFO] Regularization: 5269.861816 * 0.0000010000 = 0.0052698618
2019-03-18 22:10:13,662 [INFO] Sum of grad norms: 0.357415
2019-03-18 22:10:13,663 [INFO] ---------------------------------
2019-03-18 22:10:32,457 [INFO] ---------------------------------
2019-03-18 22:10:32,458 [INFO] Summary:
2019-03-18 22:10:32,459 [INFO] Batch 14000, worst loss 0.071228 (incl. reg.) of 1000 batches, learning rate 0.000692 @cl.-depth 1
2019-03-18 22:10:32,459 [INFO] Regularization: 5167.969238 * 0.0000010000 = 0.0051679690
2019-03-18 22:10:32,460 [INFO] Sum of grad norms: 1.722787
2019-03-18 22:10:32,460 [INFO] ---------------------------------
2019-03-18 22:10:51,646 [INFO] ---------------------------------
2019-03-18 22:10:51,647 [INFO] Summary:
2019-03-18 22:10:51,647 [INFO] Batch 15000, worst loss 0.068121 (incl. reg.) of 1000 batches, learning rate 0.000692 @cl.-depth 1
2019-03-18 22:10:51,648 [INFO] Regularization: 5082.543457 * 0.0000010000 = 0.0050825435
2019-03-18 22:10:51,649 [INFO] Sum of grad norms: 0.144987
2019-03-18 22:10:51,649 [INFO] ---------------------------------
2019-03-18 22:11:10,778 [INFO] ---------------------------------
2019-03-18 22:11:10,779 [INFO] Summary:
2019-03-18 22:11:10,779 [INFO] Batch 16000, worst loss 0.069705 (incl. reg.) of 1000 batches, learning rate 0.000681 @cl.-depth 1
2019-03-18 22:11:10,780 [INFO] Regularization: 4995.373535 * 0.0000010000 = 0.0049953735
2019-03-18 22:11:10,780 [INFO] Sum of grad norms: 0.156329
2019-03-18 22:11:10,781 [INFO] ---------------------------------
2019-03-18 22:11:29,614 [INFO] ---------------------------------
2019-03-18 22:11:29,615 [INFO] Summary:
2019-03-18 22:11:29,616 [INFO] Batch 17000, worst loss 0.067871 (incl. reg.) of 1000 batches, learning rate 0.000681 @cl.-depth 1
2019-03-18 22:11:29,616 [INFO] Regularization: 4921.197266 * 0.0000010000 = 0.0049211974
2019-03-18 22:11:29,617 [INFO] Sum of grad norms: 0.176244
2019-03-18 22:11:29,617 [INFO] ---------------------------------
2019-03-18 22:11:48,491 [INFO] ---------------------------------
2019-03-18 22:11:48,492 [INFO] Summary:
2019-03-18 22:11:48,492 [INFO] Batch 18000, worst loss 0.068214 (incl. reg.) of 1000 batches, learning rate 0.000679 @cl.-depth 1
2019-03-18 22:11:48,493 [INFO] Regularization: 4849.187500 * 0.0000010000 = 0.0048491876
2019-03-18 22:11:48,493 [INFO] Sum of grad norms: 1.157457
2019-03-18 22:11:48,494 [INFO] ---------------------------------
2019-03-18 22:12:07,812 [INFO] ---------------------------------
2019-03-18 22:12:07,813 [INFO] Summary:
2019-03-18 22:12:07,814 [INFO] Batch 19000, worst loss 0.067662 (incl. reg.) of 1000 batches, learning rate 0.000679 @cl.-depth 1
2019-03-18 22:12:07,814 [INFO] Regularization: 4776.895996 * 0.0000010000 = 0.0047768960
2019-03-18 22:12:07,815 [INFO] Sum of grad norms: 0.746757
2019-03-18 22:12:07,816 [INFO] ---------------------------------
2019-03-18 22:12:26,801 [INFO] ---------------------------------
2019-03-18 22:12:26,802 [INFO] Summary:
2019-03-18 22:12:26,803 [INFO] Batch 20000, worst loss 0.066252 (incl. reg.) of 1000 batches, learning rate 0.000677 @cl.-depth 1
2019-03-18 22:12:26,803 [INFO] Regularization: 4711.687500 * 0.0000010000 = 0.0047116876
2019-03-18 22:12:26,804 [INFO] Sum of grad norms: 0.283582
2019-03-18 22:12:26,804 [INFO] ---------------------------------
2019-03-18 22:12:31,722 [INFO] ---------------------------------
2019-03-18 22:12:31,722 [INFO] Evaluation:
2019-03-18 22:12:31,723 [INFO] Batch 20000, worst loss 0.060600 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:12:31,723 [INFO] ---------------------------------
2019-03-18 22:12:50,581 [INFO] ---------------------------------
2019-03-18 22:12:50,582 [INFO] Summary:
2019-03-18 22:12:50,582 [INFO] Batch 21000, worst loss 0.068076 (incl. reg.) of 1000 batches, learning rate 0.000663 @cl.-depth 1
2019-03-18 22:12:50,583 [INFO] Regularization: 4644.451660 * 0.0000010000 = 0.0046444517
2019-03-18 22:12:50,583 [INFO] Sum of grad norms: 0.079620
2019-03-18 22:12:50,584 [INFO] ---------------------------------
2019-03-18 22:13:09,368 [INFO] ---------------------------------
2019-03-18 22:13:09,369 [INFO] Summary:
2019-03-18 22:13:09,370 [INFO] Batch 22000, worst loss 0.067081 (incl. reg.) of 1000 batches, learning rate 0.000663 @cl.-depth 1
2019-03-18 22:13:09,370 [INFO] Regularization: 4577.134766 * 0.0000010000 = 0.0045771347
2019-03-18 22:13:09,371 [INFO] Sum of grad norms: 0.319514
2019-03-18 22:13:09,372 [INFO] ---------------------------------
2019-03-18 22:13:28,180 [INFO] ---------------------------------
2019-03-18 22:13:28,181 [INFO] Summary:
2019-03-18 22:13:28,182 [INFO] Batch 23000, worst loss 0.067006 (incl. reg.) of 1000 batches, learning rate 0.000663 @cl.-depth 1
2019-03-18 22:13:28,183 [INFO] Regularization: 4505.781250 * 0.0000010000 = 0.0045057815
2019-03-18 22:13:28,184 [INFO] Sum of grad norms: 0.060382
2019-03-18 22:13:28,184 [INFO] ---------------------------------
2019-03-18 22:13:47,485 [INFO] ---------------------------------
2019-03-18 22:13:47,486 [INFO] Summary:
2019-03-18 22:13:47,486 [INFO] Batch 24000, worst loss 0.066566 (incl. reg.) of 1000 batches, learning rate 0.000663 @cl.-depth 1
2019-03-18 22:13:47,487 [INFO] Regularization: 4444.598145 * 0.0000010000 = 0.0044445982
2019-03-18 22:13:47,488 [INFO] Sum of grad norms: 0.237544
2019-03-18 22:13:47,488 [INFO] ---------------------------------
2019-03-18 22:14:06,252 [INFO] ---------------------------------
2019-03-18 22:14:06,253 [INFO] Summary:
2019-03-18 22:14:06,254 [INFO] Batch 25000, worst loss 0.069275 (incl. reg.) of 1000 batches, learning rate 0.000663 @cl.-depth 1
2019-03-18 22:14:06,254 [INFO] Regularization: 4408.452148 * 0.0000010000 = 0.0044084522
2019-03-18 22:14:06,255 [INFO] Sum of grad norms: 0.264826
2019-03-18 22:14:06,255 [INFO] ---------------------------------
2019-03-18 22:14:25,312 [INFO] ---------------------------------
2019-03-18 22:14:25,313 [INFO] Summary:
2019-03-18 22:14:25,314 [INFO] Batch 26000, worst loss 0.064877 (incl. reg.) of 1000 batches, learning rate 0.000663 @cl.-depth 1
2019-03-18 22:14:25,314 [INFO] Regularization: 4339.987305 * 0.0000010000 = 0.0043399874
2019-03-18 22:14:25,315 [INFO] Sum of grad norms: 0.457387
2019-03-18 22:14:25,315 [INFO] ---------------------------------
2019-03-18 22:14:43,882 [INFO] ---------------------------------
2019-03-18 22:14:43,883 [INFO] Summary:
2019-03-18 22:14:43,884 [INFO] Batch 27000, worst loss 0.065008 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-18 22:14:43,885 [INFO] Regularization: 4281.875977 * 0.0000010000 = 0.0042818761
2019-03-18 22:14:43,885 [INFO] Sum of grad norms: 0.835256
2019-03-18 22:14:43,886 [INFO] ---------------------------------
2019-03-18 22:15:03,024 [INFO] ---------------------------------
2019-03-18 22:15:03,025 [INFO] Summary:
2019-03-18 22:15:03,026 [INFO] Batch 28000, worst loss 0.065833 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-18 22:15:03,027 [INFO] Regularization: 4224.877441 * 0.0000010000 = 0.0042248773
2019-03-18 22:15:03,027 [INFO] Sum of grad norms: 1.390298
2019-03-18 22:15:03,028 [INFO] ---------------------------------
2019-03-18 22:15:21,873 [INFO] ---------------------------------
2019-03-18 22:15:21,874 [INFO] Summary:
2019-03-18 22:15:21,875 [INFO] Batch 29000, worst loss 0.065287 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-18 22:15:21,875 [INFO] Regularization: 4175.057617 * 0.0000010000 = 0.0041750576
2019-03-18 22:15:21,876 [INFO] Sum of grad norms: 0.296132
2019-03-18 22:15:21,876 [INFO] ---------------------------------
2019-03-18 22:15:40,615 [INFO] ---------------------------------
2019-03-18 22:15:40,616 [INFO] Summary:
2019-03-18 22:15:40,617 [INFO] Batch 30000, worst loss 0.064855 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-18 22:15:40,617 [INFO] Regularization: 4126.953125 * 0.0000010000 = 0.0041269530
2019-03-18 22:15:40,618 [INFO] Sum of grad norms: 0.252128
2019-03-18 22:15:40,618 [INFO] ---------------------------------
2019-03-18 22:15:45,507 [INFO] ---------------------------------
2019-03-18 22:15:45,508 [INFO] Evaluation:
2019-03-18 22:15:45,509 [INFO] Batch 30000, worst loss 0.059192 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:15:45,511 [INFO] ---------------------------------
2019-03-18 22:16:04,140 [INFO] ---------------------------------
2019-03-18 22:16:04,141 [INFO] Summary:
2019-03-18 22:16:04,142 [INFO] Batch 31000, worst loss 0.063653 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-18 22:16:04,142 [INFO] Regularization: 4073.718506 * 0.0000010000 = 0.0040737186
2019-03-18 22:16:04,143 [INFO] Sum of grad norms: 0.097906
2019-03-18 22:16:04,143 [INFO] ---------------------------------
2019-03-18 22:16:23,076 [INFO] ---------------------------------
2019-03-18 22:16:23,077 [INFO] Summary:
2019-03-18 22:16:23,078 [INFO] Batch 32000, worst loss 0.066728 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-18 22:16:23,078 [INFO] Regularization: 4012.926270 * 0.0000010000 = 0.0040129265
2019-03-18 22:16:23,079 [INFO] Sum of grad norms: 0.852550
2019-03-18 22:16:23,080 [INFO] ---------------------------------
2019-03-18 22:16:42,272 [INFO] ---------------------------------
2019-03-18 22:16:42,273 [INFO] Summary:
2019-03-18 22:16:42,273 [INFO] Batch 33000, worst loss 0.064804 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-18 22:16:42,274 [INFO] Regularization: 3963.098877 * 0.0000010000 = 0.0039630989
2019-03-18 22:16:42,274 [INFO] Sum of grad norms: 0.109910
2019-03-18 22:16:42,275 [INFO] ---------------------------------
2019-03-18 22:17:01,209 [INFO] ---------------------------------
2019-03-18 22:17:01,210 [INFO] Summary:
2019-03-18 22:17:01,211 [INFO] Batch 34000, worst loss 0.063606 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-18 22:17:01,212 [INFO] Regularization: 3914.460449 * 0.0000010000 = 0.0039144605
2019-03-18 22:17:01,212 [INFO] Sum of grad norms: 0.936248
2019-03-18 22:17:01,213 [INFO] ---------------------------------
2019-03-18 22:17:20,156 [INFO] ---------------------------------
2019-03-18 22:17:20,157 [INFO] Summary:
2019-03-18 22:17:20,158 [INFO] Batch 35000, worst loss 0.063367 (incl. reg.) of 1000 batches, learning rate 0.000636 @cl.-depth 1
2019-03-18 22:17:20,159 [INFO] Regularization: 3862.848145 * 0.0000010000 = 0.0038628480
2019-03-18 22:17:20,159 [INFO] Sum of grad norms: 0.752295
2019-03-18 22:17:20,160 [INFO] ---------------------------------
2019-03-18 22:17:39,412 [INFO] ---------------------------------
2019-03-18 22:17:39,413 [INFO] Summary:
2019-03-18 22:17:39,413 [INFO] Batch 36000, worst loss 0.063405 (incl. reg.) of 1000 batches, learning rate 0.000634 @cl.-depth 1
2019-03-18 22:17:39,414 [INFO] Regularization: 3815.309814 * 0.0000010000 = 0.0038153098
2019-03-18 22:17:39,414 [INFO] Sum of grad norms: 0.073911
2019-03-18 22:17:39,415 [INFO] ---------------------------------
2019-03-18 22:17:58,111 [INFO] ---------------------------------
2019-03-18 22:17:58,112 [INFO] Summary:
2019-03-18 22:17:58,113 [INFO] Batch 37000, worst loss 0.064933 (incl. reg.) of 1000 batches, learning rate 0.000634 @cl.-depth 1
2019-03-18 22:17:58,113 [INFO] Regularization: 3775.039551 * 0.0000010000 = 0.0037750395
2019-03-18 22:17:58,114 [INFO] Sum of grad norms: 0.551055
2019-03-18 22:17:58,114 [INFO] ---------------------------------
2019-03-18 22:18:17,156 [INFO] ---------------------------------
2019-03-18 22:18:17,157 [INFO] Summary:
2019-03-18 22:18:17,158 [INFO] Batch 38000, worst loss 0.064451 (incl. reg.) of 1000 batches, learning rate 0.000634 @cl.-depth 1
2019-03-18 22:18:17,159 [INFO] Regularization: 3727.867188 * 0.0000010000 = 0.0037278673
2019-03-18 22:18:17,159 [INFO] Sum of grad norms: 0.261581
2019-03-18 22:18:17,160 [INFO] ---------------------------------
2019-03-18 22:18:35,596 [INFO] ---------------------------------
2019-03-18 22:18:35,596 [INFO] Summary:
2019-03-18 22:18:35,597 [INFO] Batch 39000, worst loss 0.063183 (incl. reg.) of 1000 batches, learning rate 0.000634 @cl.-depth 1
2019-03-18 22:18:35,598 [INFO] Regularization: 3681.923584 * 0.0000010000 = 0.0036819235
2019-03-18 22:18:35,598 [INFO] Sum of grad norms: 0.073341
2019-03-18 22:18:35,599 [INFO] ---------------------------------
2019-03-18 22:18:54,607 [INFO] ---------------------------------
2019-03-18 22:18:54,608 [INFO] Summary:
2019-03-18 22:18:54,609 [INFO] Batch 40000, worst loss 0.062661 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-18 22:18:54,609 [INFO] Regularization: 3635.277588 * 0.0000010000 = 0.0036352775
2019-03-18 22:18:54,610 [INFO] Sum of grad norms: 0.045247
2019-03-18 22:18:54,610 [INFO] ---------------------------------
2019-03-18 22:18:59,580 [INFO] ---------------------------------
2019-03-18 22:18:59,581 [INFO] Evaluation:
2019-03-18 22:18:59,582 [INFO] Batch 40000, worst loss 0.058572 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:18:59,583 [INFO] ---------------------------------
2019-03-18 22:19:18,363 [INFO] ---------------------------------
2019-03-18 22:19:18,364 [INFO] Summary:
2019-03-18 22:19:18,365 [INFO] Batch 41000, worst loss 0.063206 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 22:19:18,365 [INFO] Regularization: 3597.755371 * 0.0000010000 = 0.0035977555
2019-03-18 22:19:18,366 [INFO] Sum of grad norms: 0.128885
2019-03-18 22:19:18,366 [INFO] ---------------------------------
2019-03-18 22:19:37,578 [INFO] ---------------------------------
2019-03-18 22:19:37,579 [INFO] Summary:
2019-03-18 22:19:37,579 [INFO] Batch 42000, worst loss 0.061998 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 22:19:37,580 [INFO] Regularization: 3555.395752 * 0.0000010000 = 0.0035553956
2019-03-18 22:19:37,580 [INFO] Sum of grad norms: 0.129617
2019-03-18 22:19:37,581 [INFO] ---------------------------------
2019-03-18 22:19:56,704 [INFO] ---------------------------------
2019-03-18 22:19:56,704 [INFO] Summary:
2019-03-18 22:19:56,705 [INFO] Batch 43000, worst loss 0.061897 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 22:19:56,706 [INFO] Regularization: 3529.832031 * 0.0000010000 = 0.0035298320
2019-03-18 22:19:56,707 [INFO] Sum of grad norms: 0.416514
2019-03-18 22:19:56,707 [INFO] ---------------------------------
2019-03-18 22:20:15,332 [INFO] ---------------------------------
2019-03-18 22:20:15,333 [INFO] Summary:
2019-03-18 22:20:15,334 [INFO] Batch 44000, worst loss 0.061587 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 22:20:15,334 [INFO] Regularization: 3501.867676 * 0.0000010000 = 0.0035018676
2019-03-18 22:20:15,335 [INFO] Sum of grad norms: 0.157324
2019-03-18 22:20:15,335 [INFO] ---------------------------------
2019-03-18 22:20:34,238 [INFO] ---------------------------------
2019-03-18 22:20:34,239 [INFO] Summary:
2019-03-18 22:20:34,239 [INFO] Batch 45000, worst loss 0.061782 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 22:20:34,240 [INFO] Regularization: 3474.826660 * 0.0000010000 = 0.0034748267
2019-03-18 22:20:34,240 [INFO] Sum of grad norms: 0.251378
2019-03-18 22:20:34,241 [INFO] ---------------------------------
2019-03-18 22:20:53,384 [INFO] ---------------------------------
2019-03-18 22:20:53,385 [INFO] Summary:
2019-03-18 22:20:53,385 [INFO] Batch 46000, worst loss 0.061866 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 22:20:53,386 [INFO] Regularization: 3448.974854 * 0.0000010000 = 0.0034489748
2019-03-18 22:20:53,386 [INFO] Sum of grad norms: 0.334494
2019-03-18 22:20:53,387 [INFO] ---------------------------------
2019-03-18 22:21:12,222 [INFO] ---------------------------------
2019-03-18 22:21:12,223 [INFO] Summary:
2019-03-18 22:21:12,224 [INFO] Batch 47000, worst loss 0.061929 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 22:21:12,225 [INFO] Regularization: 3421.713867 * 0.0000010000 = 0.0034217138
2019-03-18 22:21:12,225 [INFO] Sum of grad norms: 0.449917
2019-03-18 22:21:12,226 [INFO] ---------------------------------
2019-03-18 22:21:31,037 [INFO] ---------------------------------
2019-03-18 22:21:31,038 [INFO] Summary:
2019-03-18 22:21:31,039 [INFO] Batch 48000, worst loss 0.061543 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 22:21:31,039 [INFO] Regularization: 3400.460449 * 0.0000010000 = 0.0034004604
2019-03-18 22:21:31,040 [INFO] Sum of grad norms: 0.220782
2019-03-18 22:21:31,040 [INFO] ---------------------------------
2019-03-18 22:21:50,156 [INFO] ---------------------------------
2019-03-18 22:21:50,157 [INFO] Summary:
2019-03-18 22:21:50,157 [INFO] Batch 49000, worst loss 0.061528 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 22:21:50,158 [INFO] Regularization: 3373.855957 * 0.0000010000 = 0.0033738560
2019-03-18 22:21:50,158 [INFO] Sum of grad norms: 0.117540
2019-03-18 22:21:50,159 [INFO] ---------------------------------
2019-03-18 22:22:09,016 [INFO] ---------------------------------
2019-03-18 22:22:09,017 [INFO] Summary:
2019-03-18 22:22:09,018 [INFO] Batch 50000, worst loss 0.062443 (incl. reg.) of 1000 batches, learning rate 0.000313 @cl.-depth 1
2019-03-18 22:22:09,018 [INFO] Regularization: 3349.186523 * 0.0000010000 = 0.0033491866
2019-03-18 22:22:09,019 [INFO] Sum of grad norms: 0.411721
2019-03-18 22:22:09,020 [INFO] ---------------------------------
2019-03-18 22:22:13,949 [INFO] ---------------------------------
2019-03-18 22:22:13,950 [INFO] Evaluation:
2019-03-18 22:22:13,951 [INFO] Batch 50000, worst loss 0.058029 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:22:13,952 [INFO] New best loss 0.058029, saved to file classifier/1552933539/1552944133_2_classifier_50000.pth
2019-03-18 22:22:13,963 [INFO] Target
2019-03-18 22:22:13,964 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-18 22:22:13,966 [INFO] Classifier output
2019-03-18 22:22:13,967 [INFO] [[0.990715 0.990222 0.989961 ... 0.016834 0.019676 0.018988]
 [0.989934 0.990269 0.990226 ... 0.008989 0.007543 0.007669]
 [0.991259 0.990151 0.990184 ... 0.008377 0.007019 0.00669 ]
 ...
 [0.989975 0.990396 0.990251 ... 0.009747 0.008377 0.007496]
 [0.990206 0.990277 0.990313 ... 0.989506 0.989369 0.989675]
 [0.990216 0.990285 0.990327 ... 0.989435 0.989302 0.989591]]
2019-03-18 22:22:13,969 [INFO] ---------------------------------
2019-03-18 22:22:32,790 [INFO] ---------------------------------
2019-03-18 22:22:32,791 [INFO] Summary:
2019-03-18 22:22:32,792 [INFO] Batch 51000, worst loss 0.061204 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 22:22:32,792 [INFO] Regularization: 3323.549072 * 0.0000010000 = 0.0033235492
2019-03-18 22:22:32,793 [INFO] Sum of grad norms: 0.123383
2019-03-18 22:22:32,793 [INFO] ---------------------------------
2019-03-18 22:22:51,604 [INFO] ---------------------------------
2019-03-18 22:22:51,605 [INFO] Summary:
2019-03-18 22:22:51,606 [INFO] Batch 52000, worst loss 0.061308 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 22:22:51,607 [INFO] Regularization: 3306.607910 * 0.0000010000 = 0.0033066079
2019-03-18 22:22:51,607 [INFO] Sum of grad norms: 0.029291
2019-03-18 22:22:51,608 [INFO] ---------------------------------
2019-03-18 22:23:10,536 [INFO] ---------------------------------
2019-03-18 22:23:10,537 [INFO] Summary:
2019-03-18 22:23:10,537 [INFO] Batch 53000, worst loss 0.061176 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 22:23:10,538 [INFO] Regularization: 3291.532959 * 0.0000010000 = 0.0032915329
2019-03-18 22:23:10,539 [INFO] Sum of grad norms: 0.074921
2019-03-18 22:23:10,539 [INFO] ---------------------------------
2019-03-18 22:23:29,553 [INFO] ---------------------------------
2019-03-18 22:23:29,554 [INFO] Summary:
2019-03-18 22:23:29,554 [INFO] Batch 54000, worst loss 0.061198 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 22:23:29,555 [INFO] Regularization: 3276.906250 * 0.0000010000 = 0.0032769062
2019-03-18 22:23:29,556 [INFO] Sum of grad norms: 0.027710
2019-03-18 22:23:29,556 [INFO] ---------------------------------
2019-03-18 22:23:48,056 [INFO] ---------------------------------
2019-03-18 22:23:48,057 [INFO] Summary:
2019-03-18 22:23:48,058 [INFO] Batch 55000, worst loss 0.061047 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 22:23:48,059 [INFO] Regularization: 3262.990234 * 0.0000010000 = 0.0032629902
2019-03-18 22:23:48,059 [INFO] Sum of grad norms: 0.116181
2019-03-18 22:23:48,060 [INFO] ---------------------------------
2019-03-18 22:24:06,773 [INFO] ---------------------------------
2019-03-18 22:24:06,775 [INFO] Summary:
2019-03-18 22:24:06,776 [INFO] Batch 56000, worst loss 0.061300 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 22:24:06,776 [INFO] Regularization: 3249.089111 * 0.0000010000 = 0.0032490890
2019-03-18 22:24:06,777 [INFO] Sum of grad norms: 0.107884
2019-03-18 22:24:06,778 [INFO] ---------------------------------
2019-03-18 22:24:25,460 [INFO] ---------------------------------
2019-03-18 22:24:25,461 [INFO] Summary:
2019-03-18 22:24:25,461 [INFO] Batch 57000, worst loss 0.061168 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 22:24:25,462 [INFO] Regularization: 3234.937012 * 0.0000010000 = 0.0032349371
2019-03-18 22:24:25,462 [INFO] Sum of grad norms: 0.035952
2019-03-18 22:24:25,463 [INFO] ---------------------------------
2019-03-18 22:24:44,033 [INFO] ---------------------------------
2019-03-18 22:24:44,034 [INFO] Summary:
2019-03-18 22:24:44,035 [INFO] Batch 58000, worst loss 0.061147 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 22:24:44,035 [INFO] Regularization: 3222.661133 * 0.0000010000 = 0.0032226611
2019-03-18 22:24:44,036 [INFO] Sum of grad norms: 0.315603
2019-03-18 22:24:44,036 [INFO] ---------------------------------
2019-03-18 22:25:03,172 [INFO] ---------------------------------
2019-03-18 22:25:03,173 [INFO] Summary:
2019-03-18 22:25:03,174 [INFO] Batch 59000, worst loss 0.061227 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 22:25:03,174 [INFO] Regularization: 3209.262939 * 0.0000010000 = 0.0032092629
2019-03-18 22:25:03,175 [INFO] Sum of grad norms: 0.169135
2019-03-18 22:25:03,175 [INFO] ---------------------------------
2019-03-18 22:25:22,309 [INFO] ---------------------------------
2019-03-18 22:25:22,310 [INFO] Summary:
2019-03-18 22:25:22,311 [INFO] Batch 60000, worst loss 0.061332 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-18 22:25:22,312 [INFO] Regularization: 3196.514160 * 0.0000010000 = 0.0031965142
2019-03-18 22:25:22,312 [INFO] Sum of grad norms: 0.043966
2019-03-18 22:25:22,313 [INFO] ---------------------------------
2019-03-18 22:25:27,236 [INFO] ---------------------------------
2019-03-18 22:25:27,237 [INFO] Evaluation:
2019-03-18 22:25:27,237 [INFO] Batch 60000, worst loss 0.057776 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:25:27,238 [INFO] New best loss 0.057776, saved to file classifier/1552933539/1552944327_2_classifier_60000.pth
2019-03-18 22:25:27,253 [INFO] Target
2019-03-18 22:25:27,254 [INFO] [[0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-18 22:25:27,256 [INFO] Classifier output
2019-03-18 22:25:27,258 [INFO] [[0.010319 0.010037 0.009336 ... 0.989557 0.988539 0.988946]
 [0.990063 0.990088 0.990214 ... 0.98793  0.989092 0.988632]
 [0.990018 0.990054 0.990155 ... 0.988589 0.989536 0.98928 ]
 ...
 [0.990509 0.990264 0.990553 ... 0.011338 0.014186 0.012661]
 [0.989927 0.989987 0.990036 ... 0.989802 0.99037  0.990467]
 [0.990008 0.990085 0.990247 ... 0.987785 0.989119 0.988761]]
2019-03-18 22:25:27,260 [INFO] ---------------------------------
2019-03-18 22:25:45,938 [INFO] ---------------------------------
2019-03-18 22:25:45,939 [INFO] Summary:
2019-03-18 22:25:45,940 [INFO] Batch 61000, worst loss 0.061132 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 22:25:45,940 [INFO] Regularization: 3183.280273 * 0.0000010000 = 0.0031832804
2019-03-18 22:25:45,940 [INFO] Sum of grad norms: 0.028858
2019-03-18 22:25:45,941 [INFO] ---------------------------------
2019-03-18 22:26:04,603 [INFO] ---------------------------------
2019-03-18 22:26:04,604 [INFO] Summary:
2019-03-18 22:26:04,605 [INFO] Batch 62000, worst loss 0.060988 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 22:26:04,606 [INFO] Regularization: 3175.296875 * 0.0000010000 = 0.0031752968
2019-03-18 22:26:04,606 [INFO] Sum of grad norms: 0.110451
2019-03-18 22:26:04,607 [INFO] ---------------------------------
2019-03-18 22:26:23,424 [INFO] ---------------------------------
2019-03-18 22:26:23,425 [INFO] Summary:
2019-03-18 22:26:23,425 [INFO] Batch 63000, worst loss 0.061019 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 22:26:23,426 [INFO] Regularization: 3168.027832 * 0.0000010000 = 0.0031680278
2019-03-18 22:26:23,426 [INFO] Sum of grad norms: 0.070147
2019-03-18 22:26:23,427 [INFO] ---------------------------------
2019-03-18 22:26:42,314 [INFO] ---------------------------------
2019-03-18 22:26:42,316 [INFO] Summary:
2019-03-18 22:26:42,316 [INFO] Batch 64000, worst loss 0.060866 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 22:26:42,317 [INFO] Regularization: 3161.236328 * 0.0000010000 = 0.0031612364
2019-03-18 22:26:42,317 [INFO] Sum of grad norms: 0.067312
2019-03-18 22:26:42,318 [INFO] ---------------------------------
2019-03-18 22:27:00,624 [INFO] ---------------------------------
2019-03-18 22:27:00,625 [INFO] Summary:
2019-03-18 22:27:00,625 [INFO] Batch 65000, worst loss 0.060966 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 22:27:00,626 [INFO] Regularization: 3153.945557 * 0.0000010000 = 0.0031539456
2019-03-18 22:27:00,627 [INFO] Sum of grad norms: 0.061744
2019-03-18 22:27:00,627 [INFO] ---------------------------------
2019-03-18 22:27:19,578 [INFO] ---------------------------------
2019-03-18 22:27:19,579 [INFO] Summary:
2019-03-18 22:27:19,580 [INFO] Batch 66000, worst loss 0.060933 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 22:27:19,580 [INFO] Regularization: 3147.159912 * 0.0000010000 = 0.0031471599
2019-03-18 22:27:19,581 [INFO] Sum of grad norms: 0.180648
2019-03-18 22:27:19,581 [INFO] ---------------------------------
2019-03-18 22:27:38,160 [INFO] ---------------------------------
2019-03-18 22:27:38,161 [INFO] Summary:
2019-03-18 22:27:38,162 [INFO] Batch 67000, worst loss 0.060927 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 22:27:38,162 [INFO] Regularization: 3140.488037 * 0.0000010000 = 0.0031404879
2019-03-18 22:27:38,163 [INFO] Sum of grad norms: 0.106947
2019-03-18 22:27:38,163 [INFO] ---------------------------------
2019-03-18 22:27:56,592 [INFO] ---------------------------------
2019-03-18 22:27:56,593 [INFO] Summary:
2019-03-18 22:27:56,594 [INFO] Batch 68000, worst loss 0.060974 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 22:27:56,595 [INFO] Regularization: 3133.232910 * 0.0000010000 = 0.0031332329
2019-03-18 22:27:56,595 [INFO] Sum of grad norms: 0.145847
2019-03-18 22:27:56,596 [INFO] ---------------------------------
2019-03-18 22:28:15,006 [INFO] ---------------------------------
2019-03-18 22:28:15,007 [INFO] Summary:
2019-03-18 22:28:15,008 [INFO] Batch 69000, worst loss 0.060938 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 22:28:15,008 [INFO] Regularization: 3126.032715 * 0.0000010000 = 0.0031260327
2019-03-18 22:28:15,009 [INFO] Sum of grad norms: 0.019856
2019-03-18 22:28:15,009 [INFO] ---------------------------------
2019-03-18 22:28:33,861 [INFO] ---------------------------------
2019-03-18 22:28:33,862 [INFO] Summary:
2019-03-18 22:28:33,863 [INFO] Batch 70000, worst loss 0.060878 (incl. reg.) of 1000 batches, learning rate 0.000078 @cl.-depth 1
2019-03-18 22:28:33,863 [INFO] Regularization: 3118.857422 * 0.0000010000 = 0.0031188575
2019-03-18 22:28:33,864 [INFO] Sum of grad norms: 0.108645
2019-03-18 22:28:33,864 [INFO] ---------------------------------
2019-03-18 22:28:38,772 [INFO] ---------------------------------
2019-03-18 22:28:38,773 [INFO] Evaluation:
2019-03-18 22:28:38,774 [INFO] Batch 70000, worst loss 0.057859 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:28:38,775 [INFO] ---------------------------------
2019-03-18 22:28:57,508 [INFO] ---------------------------------
2019-03-18 22:28:57,509 [INFO] Summary:
2019-03-18 22:28:57,510 [INFO] Batch 71000, worst loss 0.060895 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 22:28:57,510 [INFO] Regularization: 3111.622803 * 0.0000010000 = 0.0031116228
2019-03-18 22:28:57,511 [INFO] Sum of grad norms: 0.066188
2019-03-18 22:28:57,511 [INFO] ---------------------------------
2019-03-18 22:29:16,185 [INFO] ---------------------------------
2019-03-18 22:29:16,186 [INFO] Summary:
2019-03-18 22:29:16,186 [INFO] Batch 72000, worst loss 0.060828 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 22:29:16,187 [INFO] Regularization: 3107.091553 * 0.0000010000 = 0.0031070916
2019-03-18 22:29:16,187 [INFO] Sum of grad norms: 0.032275
2019-03-18 22:29:16,188 [INFO] ---------------------------------
2019-03-18 22:29:35,112 [INFO] ---------------------------------
2019-03-18 22:29:35,112 [INFO] Summary:
2019-03-18 22:29:35,113 [INFO] Batch 73000, worst loss 0.060809 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 22:29:35,113 [INFO] Regularization: 3103.132812 * 0.0000010000 = 0.0031031328
2019-03-18 22:29:35,114 [INFO] Sum of grad norms: 0.047699
2019-03-18 22:29:35,114 [INFO] ---------------------------------
2019-03-18 22:29:54,054 [INFO] ---------------------------------
2019-03-18 22:29:54,055 [INFO] Summary:
2019-03-18 22:29:54,056 [INFO] Batch 74000, worst loss 0.060923 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 22:29:54,057 [INFO] Regularization: 3099.001953 * 0.0000010000 = 0.0030990019
2019-03-18 22:29:54,058 [INFO] Sum of grad norms: 0.052817
2019-03-18 22:29:54,058 [INFO] ---------------------------------
2019-03-18 22:30:12,462 [INFO] ---------------------------------
2019-03-18 22:30:12,463 [INFO] Summary:
2019-03-18 22:30:12,464 [INFO] Batch 75000, worst loss 0.061083 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 22:30:12,464 [INFO] Regularization: 3095.035889 * 0.0000010000 = 0.0030950359
2019-03-18 22:30:12,465 [INFO] Sum of grad norms: 0.072396
2019-03-18 22:30:12,465 [INFO] ---------------------------------
2019-03-18 22:30:31,672 [INFO] ---------------------------------
2019-03-18 22:30:31,673 [INFO] Summary:
2019-03-18 22:30:31,674 [INFO] Batch 76000, worst loss 0.060658 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 22:30:31,675 [INFO] Regularization: 3091.089355 * 0.0000010000 = 0.0030910894
2019-03-18 22:30:31,676 [INFO] Sum of grad norms: 0.040310
2019-03-18 22:30:31,676 [INFO] ---------------------------------
2019-03-18 22:30:50,330 [INFO] ---------------------------------
2019-03-18 22:30:50,331 [INFO] Summary:
2019-03-18 22:30:50,331 [INFO] Batch 77000, worst loss 0.060785 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 22:30:50,332 [INFO] Regularization: 3087.190186 * 0.0000010000 = 0.0030871902
2019-03-18 22:30:50,332 [INFO] Sum of grad norms: 0.095586
2019-03-18 22:30:50,333 [INFO] ---------------------------------
2019-03-18 22:31:09,108 [INFO] ---------------------------------
2019-03-18 22:31:09,109 [INFO] Summary:
2019-03-18 22:31:09,110 [INFO] Batch 78000, worst loss 0.060915 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 22:31:09,111 [INFO] Regularization: 3083.290527 * 0.0000010000 = 0.0030832905
2019-03-18 22:31:09,112 [INFO] Sum of grad norms: 0.304984
2019-03-18 22:31:09,112 [INFO] ---------------------------------
2019-03-18 22:31:28,138 [INFO] ---------------------------------
2019-03-18 22:31:28,139 [INFO] Summary:
2019-03-18 22:31:28,139 [INFO] Batch 79000, worst loss 0.060857 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 22:31:28,140 [INFO] Regularization: 3079.386963 * 0.0000010000 = 0.0030793869
2019-03-18 22:31:28,140 [INFO] Sum of grad norms: 0.029133
2019-03-18 22:31:28,141 [INFO] ---------------------------------
2019-03-18 22:31:47,211 [INFO] ---------------------------------
2019-03-18 22:31:47,212 [INFO] Summary:
2019-03-18 22:31:47,212 [INFO] Batch 80000, worst loss 0.060876 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 22:31:47,213 [INFO] Regularization: 3075.525879 * 0.0000010000 = 0.0030755259
2019-03-18 22:31:47,214 [INFO] Sum of grad norms: 0.090493
2019-03-18 22:31:47,215 [INFO] ---------------------------------
2019-03-18 22:31:52,132 [INFO] ---------------------------------
2019-03-18 22:31:52,133 [INFO] Evaluation:
2019-03-18 22:31:52,133 [INFO] Batch 80000, worst loss 0.057638 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:31:52,134 [INFO] New best loss 0.057638, saved to file classifier/1552933539/1552944712_2_classifier_80000.pth
2019-03-18 22:31:52,145 [INFO] Target
2019-03-18 22:31:52,146 [INFO] [[0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 ...
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]]
2019-03-18 22:31:52,148 [INFO] Classifier output
2019-03-18 22:31:52,149 [INFO] [[0.990224 0.990218 0.990314 ... 0.987519 0.988341 0.987904]
 [0.990177 0.990173 0.99027  ... 0.987905 0.98849  0.988169]
 [0.99003  0.990068 0.990139 ... 0.989723 0.989789 0.990009]
 ...
 [0.010104 0.010026 0.010192 ... 0.990117 0.988978 0.989857]
 [0.990061 0.99009  0.990167 ... 0.989368 0.989531 0.989651]
 [0.990121 0.990068 0.990113 ... 0.010536 0.007637 0.008546]]
2019-03-18 22:31:52,151 [INFO] ---------------------------------
2019-03-18 22:32:11,105 [INFO] ---------------------------------
2019-03-18 22:32:11,106 [INFO] Summary:
2019-03-18 22:32:11,107 [INFO] Batch 81000, worst loss 0.060630 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 22:32:11,107 [INFO] Regularization: 3071.704346 * 0.0000010000 = 0.0030717044
2019-03-18 22:32:11,108 [INFO] Sum of grad norms: 0.085143
2019-03-18 22:32:11,108 [INFO] ---------------------------------
2019-03-18 22:32:30,144 [INFO] ---------------------------------
2019-03-18 22:32:30,145 [INFO] Summary:
2019-03-18 22:32:30,145 [INFO] Batch 82000, worst loss 0.060853 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 22:32:30,146 [INFO] Regularization: 3069.026855 * 0.0000010000 = 0.0030690269
2019-03-18 22:32:30,146 [INFO] Sum of grad norms: 0.135748
2019-03-18 22:32:30,147 [INFO] ---------------------------------
2019-03-18 22:32:49,070 [INFO] ---------------------------------
2019-03-18 22:32:49,071 [INFO] Summary:
2019-03-18 22:32:49,071 [INFO] Batch 83000, worst loss 0.060788 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 22:32:49,072 [INFO] Regularization: 3067.277100 * 0.0000010000 = 0.0030672771
2019-03-18 22:32:49,073 [INFO] Sum of grad norms: 0.048722
2019-03-18 22:32:49,073 [INFO] ---------------------------------
2019-03-18 22:33:07,731 [INFO] ---------------------------------
2019-03-18 22:33:07,732 [INFO] Summary:
2019-03-18 22:33:07,733 [INFO] Batch 84000, worst loss 0.060788 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 22:33:07,733 [INFO] Regularization: 3065.118408 * 0.0000010000 = 0.0030651183
2019-03-18 22:33:07,734 [INFO] Sum of grad norms: 0.063906
2019-03-18 22:33:07,734 [INFO] ---------------------------------
2019-03-18 22:33:26,843 [INFO] ---------------------------------
2019-03-18 22:33:26,844 [INFO] Summary:
2019-03-18 22:33:26,845 [INFO] Batch 85000, worst loss 0.060824 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 22:33:26,845 [INFO] Regularization: 3062.804199 * 0.0000010000 = 0.0030628042
2019-03-18 22:33:26,846 [INFO] Sum of grad norms: 0.050249
2019-03-18 22:33:26,847 [INFO] ---------------------------------
2019-03-18 22:33:45,510 [INFO] ---------------------------------
2019-03-18 22:33:45,511 [INFO] Summary:
2019-03-18 22:33:45,511 [INFO] Batch 86000, worst loss 0.060604 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 22:33:45,512 [INFO] Regularization: 3060.630859 * 0.0000010000 = 0.0030606308
2019-03-18 22:33:45,512 [INFO] Sum of grad norms: 0.036630
2019-03-18 22:33:45,513 [INFO] ---------------------------------
2019-03-18 22:34:03,998 [INFO] ---------------------------------
2019-03-18 22:34:03,999 [INFO] Summary:
2019-03-18 22:34:03,999 [INFO] Batch 87000, worst loss 0.060803 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 22:34:04,000 [INFO] Regularization: 3058.537354 * 0.0000010000 = 0.0030585374
2019-03-18 22:34:04,001 [INFO] Sum of grad norms: 0.058937
2019-03-18 22:34:04,001 [INFO] ---------------------------------
2019-03-18 22:34:22,929 [INFO] ---------------------------------
2019-03-18 22:34:22,930 [INFO] Summary:
2019-03-18 22:34:22,931 [INFO] Batch 88000, worst loss 0.060790 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 22:34:22,932 [INFO] Regularization: 3056.377930 * 0.0000010000 = 0.0030563779
2019-03-18 22:34:22,932 [INFO] Sum of grad norms: 0.052257
2019-03-18 22:34:22,933 [INFO] ---------------------------------
2019-03-18 22:34:41,630 [INFO] ---------------------------------
2019-03-18 22:34:41,631 [INFO] Summary:
2019-03-18 22:34:41,631 [INFO] Batch 89000, worst loss 0.060712 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 22:34:41,632 [INFO] Regularization: 3054.276367 * 0.0000010000 = 0.0030542763
2019-03-18 22:34:41,632 [INFO] Sum of grad norms: 0.080660
2019-03-18 22:34:41,633 [INFO] ---------------------------------
2019-03-18 22:35:00,420 [INFO] ---------------------------------
2019-03-18 22:35:00,421 [INFO] Summary:
2019-03-18 22:35:00,421 [INFO] Batch 90000, worst loss 0.060703 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 22:35:00,422 [INFO] Regularization: 3052.076416 * 0.0000010000 = 0.0030520763
2019-03-18 22:35:00,422 [INFO] Sum of grad norms: 0.106383
2019-03-18 22:35:00,423 [INFO] ---------------------------------
2019-03-18 22:35:05,381 [INFO] ---------------------------------
2019-03-18 22:35:05,381 [INFO] Evaluation:
2019-03-18 22:35:05,382 [INFO] Batch 90000, worst loss 0.057550 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:35:05,383 [INFO] New best loss 0.057550, saved to file classifier/1552933539/1552944905_2_classifier_90000.pth
2019-03-18 22:35:05,394 [INFO] Target
2019-03-18 22:35:05,394 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 ...
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-18 22:35:05,396 [INFO] Classifier output
2019-03-18 22:35:05,397 [INFO] [[0.990042 0.989953 0.990134 ... 0.010069 0.009656 0.00838 ]
 [0.99002  0.990033 0.990144 ... 0.98832  0.988817 0.988642]
 [0.989908 0.989939 0.990031 ... 0.989474 0.989621 0.989729]
 ...
 [0.010135 0.009935 0.009841 ... 0.993551 0.996452 0.994564]
 [0.989836 0.989857 0.989853 ... 0.009733 0.006288 0.007531]
 [0.98999  0.990008 0.990114 ... 0.988639 0.989037 0.988943]]
2019-03-18 22:35:05,402 [INFO] ---------------------------------
2019-03-18 22:35:24,365 [INFO] ---------------------------------
2019-03-18 22:35:24,366 [INFO] Summary:
2019-03-18 22:35:24,367 [INFO] Batch 91000, worst loss 0.060854 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 22:35:24,367 [INFO] Regularization: 3050.044434 * 0.0000010000 = 0.0030500444
2019-03-18 22:35:24,368 [INFO] Sum of grad norms: 0.068887
2019-03-18 22:35:24,369 [INFO] ---------------------------------
2019-03-18 22:35:43,440 [INFO] ---------------------------------
2019-03-18 22:35:43,441 [INFO] Summary:
2019-03-18 22:35:43,442 [INFO] Batch 92000, worst loss 0.060666 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 22:35:43,442 [INFO] Regularization: 3048.673584 * 0.0000010000 = 0.0030486735
2019-03-18 22:35:43,443 [INFO] Sum of grad norms: 0.041421
2019-03-18 22:35:43,443 [INFO] ---------------------------------
2019-03-18 22:36:02,152 [INFO] ---------------------------------
2019-03-18 22:36:02,153 [INFO] Summary:
2019-03-18 22:36:02,153 [INFO] Batch 93000, worst loss 0.060570 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 22:36:02,154 [INFO] Regularization: 3047.623535 * 0.0000010000 = 0.0030476234
2019-03-18 22:36:02,154 [INFO] Sum of grad norms: 0.121622
2019-03-18 22:36:02,155 [INFO] ---------------------------------
2019-03-18 22:36:20,842 [INFO] ---------------------------------
2019-03-18 22:36:20,843 [INFO] Summary:
2019-03-18 22:36:20,844 [INFO] Batch 94000, worst loss 0.060665 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 22:36:20,844 [INFO] Regularization: 3046.610596 * 0.0000010000 = 0.0030466106
2019-03-18 22:36:20,845 [INFO] Sum of grad norms: 0.065494
2019-03-18 22:36:20,846 [INFO] ---------------------------------
2019-03-18 22:36:39,589 [INFO] ---------------------------------
2019-03-18 22:36:39,590 [INFO] Summary:
2019-03-18 22:36:39,591 [INFO] Batch 95000, worst loss 0.060672 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 22:36:39,591 [INFO] Regularization: 3045.431641 * 0.0000010000 = 0.0030454316
2019-03-18 22:36:39,592 [INFO] Sum of grad norms: 0.100310
2019-03-18 22:36:39,593 [INFO] ---------------------------------
2019-03-18 22:36:58,773 [INFO] ---------------------------------
2019-03-18 22:36:58,774 [INFO] Summary:
2019-03-18 22:36:58,774 [INFO] Batch 96000, worst loss 0.060651 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 22:36:58,775 [INFO] Regularization: 3044.216064 * 0.0000010000 = 0.0030442160
2019-03-18 22:36:58,775 [INFO] Sum of grad norms: 0.041678
2019-03-18 22:36:58,776 [INFO] ---------------------------------
2019-03-18 22:37:17,946 [INFO] ---------------------------------
2019-03-18 22:37:17,947 [INFO] Summary:
2019-03-18 22:37:17,948 [INFO] Batch 97000, worst loss 0.060627 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 22:37:17,949 [INFO] Regularization: 3043.329834 * 0.0000010000 = 0.0030433298
2019-03-18 22:37:17,950 [INFO] Sum of grad norms: 0.035057
2019-03-18 22:37:17,951 [INFO] ---------------------------------
2019-03-18 22:37:36,334 [INFO] ---------------------------------
2019-03-18 22:37:36,335 [INFO] Summary:
2019-03-18 22:37:36,336 [INFO] Batch 98000, worst loss 0.060617 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 22:37:36,336 [INFO] Regularization: 3042.218750 * 0.0000010000 = 0.0030422187
2019-03-18 22:37:36,337 [INFO] Sum of grad norms: 0.026147
2019-03-18 22:37:36,338 [INFO] ---------------------------------
2019-03-18 22:37:54,920 [INFO] ---------------------------------
2019-03-18 22:37:54,921 [INFO] Summary:
2019-03-18 22:37:54,921 [INFO] Batch 99000, worst loss 0.060783 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 22:37:54,922 [INFO] Regularization: 3041.043457 * 0.0000010000 = 0.0030410434
2019-03-18 22:37:54,923 [INFO] Sum of grad norms: 0.026649
2019-03-18 22:37:54,923 [INFO] ---------------------------------
2019-03-18 22:38:13,682 [INFO] ---------------------------------
2019-03-18 22:38:13,682 [INFO] Summary:
2019-03-18 22:38:13,683 [INFO] Batch 100000, worst loss 0.060628 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 22:38:13,684 [INFO] Regularization: 3040.088135 * 0.0000010000 = 0.0030400881
2019-03-18 22:38:13,684 [INFO] Sum of grad norms: 0.033398
2019-03-18 22:38:13,685 [INFO] ---------------------------------
2019-03-18 22:38:18,649 [INFO] ---------------------------------
2019-03-18 22:38:18,649 [INFO] Evaluation:
2019-03-18 22:38:18,650 [INFO] Batch 100000, worst loss 0.057895 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:38:18,651 [INFO] ---------------------------------
2019-03-18 22:38:37,355 [INFO] ---------------------------------
2019-03-18 22:38:37,356 [INFO] Summary:
2019-03-18 22:38:37,357 [INFO] Batch 101000, worst loss 0.060771 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 22:38:37,358 [INFO] Regularization: 3039.059082 * 0.0000010000 = 0.0030390590
2019-03-18 22:38:37,360 [INFO] Sum of grad norms: 0.083066
2019-03-18 22:38:37,361 [INFO] ---------------------------------
2019-03-18 22:38:55,875 [INFO] ---------------------------------
2019-03-18 22:38:55,876 [INFO] Summary:
2019-03-18 22:38:55,877 [INFO] Batch 102000, worst loss 0.060751 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 22:38:55,877 [INFO] Regularization: 3038.313232 * 0.0000010000 = 0.0030383132
2019-03-18 22:38:55,878 [INFO] Sum of grad norms: 0.023249
2019-03-18 22:38:55,879 [INFO] ---------------------------------
2019-03-18 22:39:14,663 [INFO] ---------------------------------
2019-03-18 22:39:14,664 [INFO] Summary:
2019-03-18 22:39:14,664 [INFO] Batch 103000, worst loss 0.060955 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 22:39:14,665 [INFO] Regularization: 3037.665283 * 0.0000010000 = 0.0030376653
2019-03-18 22:39:14,665 [INFO] Sum of grad norms: 0.050749
2019-03-18 22:39:14,666 [INFO] ---------------------------------
2019-03-18 22:39:33,057 [INFO] ---------------------------------
2019-03-18 22:39:33,058 [INFO] Summary:
2019-03-18 22:39:33,059 [INFO] Batch 104000, worst loss 0.060701 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 22:39:33,059 [INFO] Regularization: 3037.068115 * 0.0000010000 = 0.0030370681
2019-03-18 22:39:33,060 [INFO] Sum of grad norms: 0.064340
2019-03-18 22:39:33,061 [INFO] ---------------------------------
2019-03-18 22:39:51,490 [INFO] ---------------------------------
2019-03-18 22:39:51,491 [INFO] Summary:
2019-03-18 22:39:51,492 [INFO] Batch 105000, worst loss 0.060681 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 22:39:51,492 [INFO] Regularization: 3036.490479 * 0.0000010000 = 0.0030364904
2019-03-18 22:39:51,493 [INFO] Sum of grad norms: 0.060004
2019-03-18 22:39:51,493 [INFO] ---------------------------------
2019-03-18 22:40:10,529 [INFO] ---------------------------------
2019-03-18 22:40:10,530 [INFO] Summary:
2019-03-18 22:40:10,531 [INFO] Batch 106000, worst loss 0.060738 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 22:40:10,531 [INFO] Regularization: 3035.997314 * 0.0000010000 = 0.0030359973
2019-03-18 22:40:10,532 [INFO] Sum of grad norms: 0.047624
2019-03-18 22:40:10,533 [INFO] ---------------------------------
2019-03-18 22:40:29,587 [INFO] ---------------------------------
2019-03-18 22:40:29,588 [INFO] Summary:
2019-03-18 22:40:29,589 [INFO] Batch 107000, worst loss 0.060631 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 22:40:29,589 [INFO] Regularization: 3035.378906 * 0.0000010000 = 0.0030353789
2019-03-18 22:40:29,590 [INFO] Sum of grad norms: 0.081960
2019-03-18 22:40:29,590 [INFO] ---------------------------------
2019-03-18 22:40:48,082 [INFO] ---------------------------------
2019-03-18 22:40:48,083 [INFO] Summary:
2019-03-18 22:40:48,083 [INFO] Batch 108000, worst loss 0.060649 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 22:40:48,084 [INFO] Regularization: 3034.895752 * 0.0000010000 = 0.0030348958
2019-03-18 22:40:48,084 [INFO] Sum of grad norms: 0.096773
2019-03-18 22:40:48,085 [INFO] ---------------------------------
2019-03-18 22:41:06,756 [INFO] ---------------------------------
2019-03-18 22:41:06,757 [INFO] Summary:
2019-03-18 22:41:06,757 [INFO] Batch 109000, worst loss 0.060624 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 22:41:06,758 [INFO] Regularization: 3034.236572 * 0.0000010000 = 0.0030342366
2019-03-18 22:41:06,759 [INFO] Sum of grad norms: 0.034953
2019-03-18 22:41:06,759 [INFO] ---------------------------------
2019-03-18 22:41:25,561 [INFO] ---------------------------------
2019-03-18 22:41:25,562 [INFO] Summary:
2019-03-18 22:41:25,563 [INFO] Batch 110000, worst loss 0.060734 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 22:41:25,563 [INFO] Regularization: 3033.766113 * 0.0000010000 = 0.0030337661
2019-03-18 22:41:25,563 [INFO] Sum of grad norms: 0.074462
2019-03-18 22:41:25,564 [INFO] ---------------------------------
2019-03-18 22:41:30,482 [INFO] ---------------------------------
2019-03-18 22:41:30,483 [INFO] Evaluation:
2019-03-18 22:41:30,484 [INFO] Batch 110000, worst loss 0.057727 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:41:30,484 [INFO] ---------------------------------
2019-03-18 22:41:49,113 [INFO] ---------------------------------
2019-03-18 22:41:49,114 [INFO] Summary:
2019-03-18 22:41:49,114 [INFO] Batch 111000, worst loss 0.060733 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 22:41:49,115 [INFO] Regularization: 3033.241943 * 0.0000010000 = 0.0030332420
2019-03-18 22:41:49,115 [INFO] Sum of grad norms: 0.086165
2019-03-18 22:41:49,116 [INFO] ---------------------------------
2019-03-18 22:42:08,186 [INFO] ---------------------------------
2019-03-18 22:42:08,188 [INFO] Summary:
2019-03-18 22:42:08,188 [INFO] Batch 112000, worst loss 0.060701 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 22:42:08,189 [INFO] Regularization: 3032.869141 * 0.0000010000 = 0.0030328692
2019-03-18 22:42:08,189 [INFO] Sum of grad norms: 0.044469
2019-03-18 22:42:08,190 [INFO] ---------------------------------
2019-03-18 22:42:27,207 [INFO] ---------------------------------
2019-03-18 22:42:27,208 [INFO] Summary:
2019-03-18 22:42:27,209 [INFO] Batch 113000, worst loss 0.060802 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 22:42:27,210 [INFO] Regularization: 3032.486328 * 0.0000010000 = 0.0030324864
2019-03-18 22:42:27,210 [INFO] Sum of grad norms: 0.044489
2019-03-18 22:42:27,211 [INFO] ---------------------------------
2019-03-18 22:42:45,918 [INFO] ---------------------------------
2019-03-18 22:42:45,919 [INFO] Summary:
2019-03-18 22:42:45,920 [INFO] Batch 114000, worst loss 0.060794 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 22:42:45,920 [INFO] Regularization: 3032.187988 * 0.0000010000 = 0.0030321879
2019-03-18 22:42:45,921 [INFO] Sum of grad norms: 0.028108
2019-03-18 22:42:45,921 [INFO] ---------------------------------
2019-03-18 22:43:04,572 [INFO] ---------------------------------
2019-03-18 22:43:04,573 [INFO] Summary:
2019-03-18 22:43:04,573 [INFO] Batch 115000, worst loss 0.060709 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 22:43:04,574 [INFO] Regularization: 3031.941406 * 0.0000010000 = 0.0030319414
2019-03-18 22:43:04,574 [INFO] Sum of grad norms: 0.031879
2019-03-18 22:43:04,575 [INFO] ---------------------------------
2019-03-18 22:43:23,508 [INFO] ---------------------------------
2019-03-18 22:43:23,509 [INFO] Summary:
2019-03-18 22:43:23,510 [INFO] Batch 116000, worst loss 0.060582 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 22:43:23,510 [INFO] Regularization: 3031.663330 * 0.0000010000 = 0.0030316634
2019-03-18 22:43:23,511 [INFO] Sum of grad norms: 0.040531
2019-03-18 22:43:23,511 [INFO] ---------------------------------
2019-03-18 22:43:42,432 [INFO] ---------------------------------
2019-03-18 22:43:42,433 [INFO] Summary:
2019-03-18 22:43:42,434 [INFO] Batch 117000, worst loss 0.060674 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 22:43:42,434 [INFO] Regularization: 3031.389648 * 0.0000010000 = 0.0030313896
2019-03-18 22:43:42,435 [INFO] Sum of grad norms: 0.049684
2019-03-18 22:43:42,435 [INFO] ---------------------------------
2019-03-18 22:44:01,107 [INFO] ---------------------------------
2019-03-18 22:44:01,108 [INFO] Summary:
2019-03-18 22:44:01,109 [INFO] Batch 118000, worst loss 0.060753 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 22:44:01,110 [INFO] Regularization: 3031.124023 * 0.0000010000 = 0.0030311241
2019-03-18 22:44:01,110 [INFO] Sum of grad norms: 0.033973
2019-03-18 22:44:01,111 [INFO] ---------------------------------
2019-03-18 22:44:19,903 [INFO] ---------------------------------
2019-03-18 22:44:19,904 [INFO] Summary:
2019-03-18 22:44:19,905 [INFO] Batch 119000, worst loss 0.060633 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 22:44:19,905 [INFO] Regularization: 3030.862305 * 0.0000010000 = 0.0030308622
2019-03-18 22:44:19,906 [INFO] Sum of grad norms: 0.075671
2019-03-18 22:44:19,906 [INFO] ---------------------------------
2019-03-18 22:44:38,550 [INFO] ---------------------------------
2019-03-18 22:44:38,551 [INFO] Summary:
2019-03-18 22:44:38,552 [INFO] Batch 120000, worst loss 0.060680 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-18 22:44:38,552 [INFO] Regularization: 3030.538086 * 0.0000010000 = 0.0030305381
2019-03-18 22:44:38,553 [INFO] Sum of grad norms: 0.028546
2019-03-18 22:44:38,553 [INFO] ---------------------------------
2019-03-18 22:44:43,507 [INFO] ---------------------------------
2019-03-18 22:44:43,507 [INFO] Evaluation:
2019-03-18 22:44:43,508 [INFO] Batch 120000, worst loss 0.057614 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:44:43,509 [INFO] ---------------------------------
2019-03-18 22:45:02,492 [INFO] ---------------------------------
2019-03-18 22:45:02,492 [INFO] Summary:
2019-03-18 22:45:02,493 [INFO] Batch 121000, worst loss 0.060576 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:45:02,493 [INFO] Regularization: 3030.261963 * 0.0000010000 = 0.0030302620
2019-03-18 22:45:02,494 [INFO] Sum of grad norms: 0.026816
2019-03-18 22:45:02,495 [INFO] ---------------------------------
2019-03-18 22:45:21,200 [INFO] ---------------------------------
2019-03-18 22:45:21,201 [INFO] Summary:
2019-03-18 22:45:21,202 [INFO] Batch 122000, worst loss 0.060575 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:45:21,202 [INFO] Regularization: 3030.056396 * 0.0000010000 = 0.0030300564
2019-03-18 22:45:21,203 [INFO] Sum of grad norms: 0.030272
2019-03-18 22:45:21,203 [INFO] ---------------------------------
2019-03-18 22:45:40,241 [INFO] ---------------------------------
2019-03-18 22:45:40,242 [INFO] Summary:
2019-03-18 22:45:40,242 [INFO] Batch 123000, worst loss 0.060680 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:45:40,243 [INFO] Regularization: 3029.900879 * 0.0000010000 = 0.0030299008
2019-03-18 22:45:40,243 [INFO] Sum of grad norms: 0.025895
2019-03-18 22:45:40,244 [INFO] ---------------------------------
2019-03-18 22:45:59,062 [INFO] ---------------------------------
2019-03-18 22:45:59,063 [INFO] Summary:
2019-03-18 22:45:59,063 [INFO] Batch 124000, worst loss 0.060539 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:45:59,064 [INFO] Regularization: 3029.732178 * 0.0000010000 = 0.0030297323
2019-03-18 22:45:59,064 [INFO] Sum of grad norms: 0.036141
2019-03-18 22:45:59,065 [INFO] ---------------------------------
2019-03-18 22:46:17,817 [INFO] ---------------------------------
2019-03-18 22:46:17,818 [INFO] Summary:
2019-03-18 22:46:17,818 [INFO] Batch 125000, worst loss 0.060570 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:46:17,819 [INFO] Regularization: 3029.608887 * 0.0000010000 = 0.0030296089
2019-03-18 22:46:17,819 [INFO] Sum of grad norms: 0.068606
2019-03-18 22:46:17,820 [INFO] ---------------------------------
2019-03-18 22:46:36,363 [INFO] ---------------------------------
2019-03-18 22:46:36,363 [INFO] Summary:
2019-03-18 22:46:36,364 [INFO] Batch 126000, worst loss 0.060630 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:46:36,364 [INFO] Regularization: 3029.467041 * 0.0000010000 = 0.0030294671
2019-03-18 22:46:36,365 [INFO] Sum of grad norms: 0.061502
2019-03-18 22:46:36,365 [INFO] ---------------------------------
2019-03-18 22:46:55,212 [INFO] ---------------------------------
2019-03-18 22:46:55,213 [INFO] Summary:
2019-03-18 22:46:55,213 [INFO] Batch 127000, worst loss 0.060683 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:46:55,214 [INFO] Regularization: 3029.332275 * 0.0000010000 = 0.0030293323
2019-03-18 22:46:55,214 [INFO] Sum of grad norms: 0.029335
2019-03-18 22:46:55,215 [INFO] ---------------------------------
2019-03-18 22:47:13,683 [INFO] ---------------------------------
2019-03-18 22:47:13,684 [INFO] Summary:
2019-03-18 22:47:13,685 [INFO] Batch 128000, worst loss 0.060626 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:47:13,686 [INFO] Regularization: 3029.169922 * 0.0000010000 = 0.0030291700
2019-03-18 22:47:13,686 [INFO] Sum of grad norms: 0.021959
2019-03-18 22:47:13,687 [INFO] ---------------------------------
2019-03-18 22:47:32,944 [INFO] ---------------------------------
2019-03-18 22:47:32,945 [INFO] Summary:
2019-03-18 22:47:32,945 [INFO] Batch 129000, worst loss 0.060725 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:47:32,946 [INFO] Regularization: 3029.027100 * 0.0000010000 = 0.0030290270
2019-03-18 22:47:32,946 [INFO] Sum of grad norms: 0.068429
2019-03-18 22:47:32,947 [INFO] ---------------------------------
2019-03-18 22:47:51,612 [INFO] ---------------------------------
2019-03-18 22:47:51,614 [INFO] Summary:
2019-03-18 22:47:51,614 [INFO] Batch 130000, worst loss 0.060599 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:47:51,615 [INFO] Regularization: 3028.875488 * 0.0000010000 = 0.0030288754
2019-03-18 22:47:51,615 [INFO] Sum of grad norms: 0.027073
2019-03-18 22:47:51,616 [INFO] ---------------------------------
2019-03-18 22:47:56,519 [INFO] ---------------------------------
2019-03-18 22:47:56,520 [INFO] Evaluation:
2019-03-18 22:47:56,521 [INFO] Batch 130000, worst loss 0.057568 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:47:56,522 [INFO] ---------------------------------
2019-03-18 22:48:15,211 [INFO] ---------------------------------
2019-03-18 22:48:15,212 [INFO] Summary:
2019-03-18 22:48:15,212 [INFO] Batch 131000, worst loss 0.060686 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:48:15,213 [INFO] Regularization: 3028.730957 * 0.0000010000 = 0.0030287309
2019-03-18 22:48:15,213 [INFO] Sum of grad norms: 0.041262
2019-03-18 22:48:15,214 [INFO] ---------------------------------
2019-03-18 22:48:33,752 [INFO] ---------------------------------
2019-03-18 22:48:33,753 [INFO] Summary:
2019-03-18 22:48:33,754 [INFO] Batch 132000, worst loss 0.060658 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:48:33,754 [INFO] Regularization: 3028.618164 * 0.0000010000 = 0.0030286182
2019-03-18 22:48:33,755 [INFO] Sum of grad norms: 0.041298
2019-03-18 22:48:33,755 [INFO] ---------------------------------
2019-03-18 22:48:52,513 [INFO] ---------------------------------
2019-03-18 22:48:52,514 [INFO] Summary:
2019-03-18 22:48:52,515 [INFO] Batch 133000, worst loss 0.060804 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:48:52,515 [INFO] Regularization: 3028.543701 * 0.0000010000 = 0.0030285437
2019-03-18 22:48:52,516 [INFO] Sum of grad norms: 0.042687
2019-03-18 22:48:52,516 [INFO] ---------------------------------
2019-03-18 22:49:11,202 [INFO] ---------------------------------
2019-03-18 22:49:11,203 [INFO] Summary:
2019-03-18 22:49:11,203 [INFO] Batch 134000, worst loss 0.060686 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:49:11,204 [INFO] Regularization: 3028.485107 * 0.0000010000 = 0.0030284850
2019-03-18 22:49:11,204 [INFO] Sum of grad norms: 0.042487
2019-03-18 22:49:11,205 [INFO] ---------------------------------
2019-03-18 22:49:30,279 [INFO] ---------------------------------
2019-03-18 22:49:30,280 [INFO] Summary:
2019-03-18 22:49:30,281 [INFO] Batch 135000, worst loss 0.060619 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:49:30,281 [INFO] Regularization: 3028.385986 * 0.0000010000 = 0.0030283860
2019-03-18 22:49:30,282 [INFO] Sum of grad norms: 0.034716
2019-03-18 22:49:30,283 [INFO] ---------------------------------
2019-03-18 22:49:49,081 [INFO] ---------------------------------
2019-03-18 22:49:49,082 [INFO] Summary:
2019-03-18 22:49:49,083 [INFO] Batch 136000, worst loss 0.060675 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:49:49,083 [INFO] Regularization: 3028.296387 * 0.0000010000 = 0.0030282964
2019-03-18 22:49:49,084 [INFO] Sum of grad norms: 0.048481
2019-03-18 22:49:49,084 [INFO] ---------------------------------
2019-03-18 22:50:07,820 [INFO] ---------------------------------
2019-03-18 22:50:07,822 [INFO] Summary:
2019-03-18 22:50:07,823 [INFO] Batch 137000, worst loss 0.060795 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:50:07,824 [INFO] Regularization: 3028.217041 * 0.0000010000 = 0.0030282170
2019-03-18 22:50:07,825 [INFO] Sum of grad norms: 0.036992
2019-03-18 22:50:07,826 [INFO] ---------------------------------
2019-03-18 22:50:26,398 [INFO] ---------------------------------
2019-03-18 22:50:26,399 [INFO] Summary:
2019-03-18 22:50:26,400 [INFO] Batch 138000, worst loss 0.060713 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:50:26,400 [INFO] Regularization: 3028.142578 * 0.0000010000 = 0.0030281425
2019-03-18 22:50:26,401 [INFO] Sum of grad norms: 0.013654
2019-03-18 22:50:26,402 [INFO] ---------------------------------
2019-03-18 22:50:45,123 [INFO] ---------------------------------
2019-03-18 22:50:45,124 [INFO] Summary:
2019-03-18 22:50:45,125 [INFO] Batch 139000, worst loss 0.060536 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:50:45,125 [INFO] Regularization: 3028.089355 * 0.0000010000 = 0.0030280894
2019-03-18 22:50:45,126 [INFO] Sum of grad norms: 0.040844
2019-03-18 22:50:45,126 [INFO] ---------------------------------
2019-03-18 22:51:03,777 [INFO] ---------------------------------
2019-03-18 22:51:03,778 [INFO] Summary:
2019-03-18 22:51:03,779 [INFO] Batch 140000, worst loss 0.060577 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-18 22:51:03,779 [INFO] Regularization: 3028.018555 * 0.0000010000 = 0.0030280186
2019-03-18 22:51:03,780 [INFO] Sum of grad norms: 0.042202
2019-03-18 22:51:03,781 [INFO] ---------------------------------
2019-03-18 22:51:08,694 [INFO] ---------------------------------
2019-03-18 22:51:08,695 [INFO] Evaluation:
2019-03-18 22:51:08,695 [INFO] Batch 140000, worst loss 0.057615 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:51:08,696 [INFO] ---------------------------------
2019-03-18 22:51:27,397 [INFO] ---------------------------------
2019-03-18 22:51:27,398 [INFO] Summary:
2019-03-18 22:51:27,399 [INFO] Batch 141000, worst loss 0.060683 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:51:27,399 [INFO] Regularization: 3027.950684 * 0.0000010000 = 0.0030279506
2019-03-18 22:51:27,400 [INFO] Sum of grad norms: 0.040340
2019-03-18 22:51:27,401 [INFO] ---------------------------------
2019-03-18 22:51:46,299 [INFO] ---------------------------------
2019-03-18 22:51:46,300 [INFO] Summary:
2019-03-18 22:51:46,300 [INFO] Batch 142000, worst loss 0.060683 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:51:46,301 [INFO] Regularization: 3027.905762 * 0.0000010000 = 0.0030279057
2019-03-18 22:51:46,301 [INFO] Sum of grad norms: 0.048077
2019-03-18 22:51:46,302 [INFO] ---------------------------------
2019-03-18 22:52:05,099 [INFO] ---------------------------------
2019-03-18 22:52:05,100 [INFO] Summary:
2019-03-18 22:52:05,101 [INFO] Batch 143000, worst loss 0.060732 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:52:05,101 [INFO] Regularization: 3027.863770 * 0.0000010000 = 0.0030278638
2019-03-18 22:52:05,102 [INFO] Sum of grad norms: 0.044533
2019-03-18 22:52:05,102 [INFO] ---------------------------------
2019-03-18 22:52:24,024 [INFO] ---------------------------------
2019-03-18 22:52:24,025 [INFO] Summary:
2019-03-18 22:52:24,026 [INFO] Batch 144000, worst loss 0.060732 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:52:24,026 [INFO] Regularization: 3027.823486 * 0.0000010000 = 0.0030278235
2019-03-18 22:52:24,027 [INFO] Sum of grad norms: 0.045808
2019-03-18 22:52:24,028 [INFO] ---------------------------------
2019-03-18 22:52:43,182 [INFO] ---------------------------------
2019-03-18 22:52:43,183 [INFO] Summary:
2019-03-18 22:52:43,184 [INFO] Batch 145000, worst loss 0.060689 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:52:43,184 [INFO] Regularization: 3027.776611 * 0.0000010000 = 0.0030277765
2019-03-18 22:52:43,185 [INFO] Sum of grad norms: 0.029861
2019-03-18 22:52:43,185 [INFO] ---------------------------------
2019-03-18 22:53:01,994 [INFO] ---------------------------------
2019-03-18 22:53:01,995 [INFO] Summary:
2019-03-18 22:53:01,996 [INFO] Batch 146000, worst loss 0.060916 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:53:01,996 [INFO] Regularization: 3027.743896 * 0.0000010000 = 0.0030277439
2019-03-18 22:53:01,997 [INFO] Sum of grad norms: 0.045607
2019-03-18 22:53:01,997 [INFO] ---------------------------------
2019-03-18 22:53:20,926 [INFO] ---------------------------------
2019-03-18 22:53:20,927 [INFO] Summary:
2019-03-18 22:53:20,928 [INFO] Batch 147000, worst loss 0.060644 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:53:20,928 [INFO] Regularization: 3027.688232 * 0.0000010000 = 0.0030276882
2019-03-18 22:53:20,929 [INFO] Sum of grad norms: 0.027165
2019-03-18 22:53:20,930 [INFO] ---------------------------------
2019-03-18 22:53:39,697 [INFO] ---------------------------------
2019-03-18 22:53:39,698 [INFO] Summary:
2019-03-18 22:53:39,699 [INFO] Batch 148000, worst loss 0.060626 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:53:39,699 [INFO] Regularization: 3027.655518 * 0.0000010000 = 0.0030276554
2019-03-18 22:53:39,700 [INFO] Sum of grad norms: 0.028952
2019-03-18 22:53:39,700 [INFO] ---------------------------------
2019-03-18 22:53:58,500 [INFO] ---------------------------------
2019-03-18 22:53:58,501 [INFO] Summary:
2019-03-18 22:53:58,502 [INFO] Batch 149000, worst loss 0.060663 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:53:58,503 [INFO] Regularization: 3027.626221 * 0.0000010000 = 0.0030276263
2019-03-18 22:53:58,503 [INFO] Sum of grad norms: 0.034238
2019-03-18 22:53:58,504 [INFO] ---------------------------------
2019-03-18 22:54:17,111 [INFO] ---------------------------------
2019-03-18 22:54:17,112 [INFO] Summary:
2019-03-18 22:54:17,113 [INFO] Batch 150000, worst loss 0.060660 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:54:17,113 [INFO] Regularization: 3027.589844 * 0.0000010000 = 0.0030275898
2019-03-18 22:54:17,114 [INFO] Sum of grad norms: 0.019094
2019-03-18 22:54:17,115 [INFO] ---------------------------------
2019-03-18 22:54:22,091 [INFO] ---------------------------------
2019-03-18 22:54:22,092 [INFO] Evaluation:
2019-03-18 22:54:22,093 [INFO] Batch 150000, worst loss 0.057526 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:54:22,097 [INFO] New best loss 0.057526, saved to file classifier/1552933539/1552946062_2_classifier_150000.pth
2019-03-18 22:54:22,107 [INFO] Target
2019-03-18 22:54:22,108 [INFO] [[0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-18 22:54:22,110 [INFO] Classifier output
2019-03-18 22:54:22,111 [INFO] [[0.990056 0.990058 0.990166 ... 0.987894 0.988549 0.988164]
 [0.990028 0.989977 0.990117 ... 0.010635 0.007968 0.00886 ]
 [0.989873 0.989911 0.989973 ... 0.99021  0.990016 0.990218]
 ...
 [0.990028 0.989977 0.990117 ... 0.010635 0.007968 0.00886 ]
 [0.990478 0.990356 0.990058 ... 0.067929 0.062493 0.063154]
 [0.989939 0.989962 0.990043 ... 0.989377 0.989444 0.989445]]
2019-03-18 22:54:22,112 [INFO] ---------------------------------
2019-03-18 22:54:40,709 [INFO] ---------------------------------
2019-03-18 22:54:40,710 [INFO] Summary:
2019-03-18 22:54:40,711 [INFO] Batch 151000, worst loss 0.060621 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:54:40,711 [INFO] Regularization: 3027.554688 * 0.0000010000 = 0.0030275546
2019-03-18 22:54:40,712 [INFO] Sum of grad norms: 0.046777
2019-03-18 22:54:40,713 [INFO] ---------------------------------
2019-03-18 22:54:59,200 [INFO] ---------------------------------
2019-03-18 22:54:59,201 [INFO] Summary:
2019-03-18 22:54:59,202 [INFO] Batch 152000, worst loss 0.060587 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:54:59,202 [INFO] Regularization: 3027.533691 * 0.0000010000 = 0.0030275336
2019-03-18 22:54:59,203 [INFO] Sum of grad norms: 0.049504
2019-03-18 22:54:59,204 [INFO] ---------------------------------
2019-03-18 22:55:17,796 [INFO] ---------------------------------
2019-03-18 22:55:17,797 [INFO] Summary:
2019-03-18 22:55:17,797 [INFO] Batch 153000, worst loss 0.060679 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:55:17,798 [INFO] Regularization: 3027.512939 * 0.0000010000 = 0.0030275129
2019-03-18 22:55:17,798 [INFO] Sum of grad norms: 0.044944
2019-03-18 22:55:17,799 [INFO] ---------------------------------
2019-03-18 22:55:36,546 [INFO] ---------------------------------
2019-03-18 22:55:36,546 [INFO] Summary:
2019-03-18 22:55:36,547 [INFO] Batch 154000, worst loss 0.060695 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:55:36,548 [INFO] Regularization: 3027.493652 * 0.0000010000 = 0.0030274936
2019-03-18 22:55:36,549 [INFO] Sum of grad norms: 0.038890
2019-03-18 22:55:36,550 [INFO] ---------------------------------
2019-03-18 22:55:55,473 [INFO] ---------------------------------
2019-03-18 22:55:55,474 [INFO] Summary:
2019-03-18 22:55:55,474 [INFO] Batch 155000, worst loss 0.060999 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:55:55,475 [INFO] Regularization: 3027.471924 * 0.0000010000 = 0.0030274719
2019-03-18 22:55:55,475 [INFO] Sum of grad norms: 0.039860
2019-03-18 22:55:55,476 [INFO] ---------------------------------
2019-03-18 22:56:14,303 [INFO] ---------------------------------
2019-03-18 22:56:14,304 [INFO] Summary:
2019-03-18 22:56:14,305 [INFO] Batch 156000, worst loss 0.060999 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:56:14,306 [INFO] Regularization: 3027.458496 * 0.0000010000 = 0.0030274584
2019-03-18 22:56:14,307 [INFO] Sum of grad norms: 0.039716
2019-03-18 22:56:14,308 [INFO] ---------------------------------
2019-03-18 22:56:33,148 [INFO] ---------------------------------
2019-03-18 22:56:33,149 [INFO] Summary:
2019-03-18 22:56:33,149 [INFO] Batch 157000, worst loss 0.060550 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:56:33,150 [INFO] Regularization: 3027.443604 * 0.0000010000 = 0.0030274435
2019-03-18 22:56:33,151 [INFO] Sum of grad norms: 0.021670
2019-03-18 22:56:33,151 [INFO] ---------------------------------
2019-03-18 22:56:51,550 [INFO] ---------------------------------
2019-03-18 22:56:51,551 [INFO] Summary:
2019-03-18 22:56:51,551 [INFO] Batch 158000, worst loss 0.060689 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:56:51,552 [INFO] Regularization: 3027.419922 * 0.0000010000 = 0.0030274200
2019-03-18 22:56:51,552 [INFO] Sum of grad norms: 0.054325
2019-03-18 22:56:51,553 [INFO] ---------------------------------
2019-03-18 22:57:10,124 [INFO] ---------------------------------
2019-03-18 22:57:10,125 [INFO] Summary:
2019-03-18 22:57:10,126 [INFO] Batch 159000, worst loss 0.060689 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:57:10,126 [INFO] Regularization: 3027.400879 * 0.0000010000 = 0.0030274009
2019-03-18 22:57:10,127 [INFO] Sum of grad norms: 0.050890
2019-03-18 22:57:10,127 [INFO] ---------------------------------
2019-03-18 22:57:28,925 [INFO] ---------------------------------
2019-03-18 22:57:28,926 [INFO] Summary:
2019-03-18 22:57:28,927 [INFO] Batch 160000, worst loss 0.060525 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:57:28,928 [INFO] Regularization: 3027.386475 * 0.0000010000 = 0.0030273865
2019-03-18 22:57:28,928 [INFO] Sum of grad norms: 0.028799
2019-03-18 22:57:28,929 [INFO] ---------------------------------
2019-03-18 22:57:33,829 [INFO] ---------------------------------
2019-03-18 22:57:33,831 [INFO] Evaluation:
2019-03-18 22:57:33,832 [INFO] Batch 160000, worst loss 0.057537 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 22:57:33,833 [INFO] ---------------------------------
2019-03-18 22:57:52,352 [INFO] ---------------------------------
2019-03-18 22:57:52,353 [INFO] Summary:
2019-03-18 22:57:52,354 [INFO] Batch 161000, worst loss 0.060564 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:57:52,355 [INFO] Regularization: 3027.367188 * 0.0000010000 = 0.0030273672
2019-03-18 22:57:52,355 [INFO] Sum of grad norms: 0.061648
2019-03-18 22:57:52,356 [INFO] ---------------------------------
2019-03-18 22:58:11,243 [INFO] ---------------------------------
2019-03-18 22:58:11,244 [INFO] Summary:
2019-03-18 22:58:11,244 [INFO] Batch 162000, worst loss 0.060583 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:58:11,245 [INFO] Regularization: 3027.354248 * 0.0000010000 = 0.0030273541
2019-03-18 22:58:11,245 [INFO] Sum of grad norms: 0.022030
2019-03-18 22:58:11,246 [INFO] ---------------------------------
2019-03-18 22:58:29,698 [INFO] ---------------------------------
2019-03-18 22:58:29,699 [INFO] Summary:
2019-03-18 22:58:29,700 [INFO] Batch 163000, worst loss 0.060880 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:58:29,700 [INFO] Regularization: 3027.343506 * 0.0000010000 = 0.0030273434
2019-03-18 22:58:29,701 [INFO] Sum of grad norms: 0.054512
2019-03-18 22:58:29,701 [INFO] ---------------------------------
2019-03-18 22:58:48,273 [INFO] ---------------------------------
2019-03-18 22:58:48,274 [INFO] Summary:
2019-03-18 22:58:48,274 [INFO] Batch 164000, worst loss 0.060724 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:58:48,275 [INFO] Regularization: 3027.335693 * 0.0000010000 = 0.0030273357
2019-03-18 22:58:48,275 [INFO] Sum of grad norms: 0.034171
2019-03-18 22:58:48,276 [INFO] ---------------------------------
2019-03-18 22:59:07,001 [INFO] ---------------------------------
2019-03-18 22:59:07,002 [INFO] Summary:
2019-03-18 22:59:07,002 [INFO] Batch 165000, worst loss 0.060685 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:59:07,003 [INFO] Regularization: 3027.325195 * 0.0000010000 = 0.0030273253
2019-03-18 22:59:07,003 [INFO] Sum of grad norms: 0.061706
2019-03-18 22:59:07,004 [INFO] ---------------------------------
2019-03-18 22:59:25,979 [INFO] ---------------------------------
2019-03-18 22:59:25,980 [INFO] Summary:
2019-03-18 22:59:25,981 [INFO] Batch 166000, worst loss 0.060576 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:59:25,981 [INFO] Regularization: 3027.314209 * 0.0000010000 = 0.0030273141
2019-03-18 22:59:25,982 [INFO] Sum of grad norms: 0.047529
2019-03-18 22:59:25,982 [INFO] ---------------------------------
2019-03-18 22:59:44,636 [INFO] ---------------------------------
2019-03-18 22:59:44,637 [INFO] Summary:
2019-03-18 22:59:44,638 [INFO] Batch 167000, worst loss 0.060576 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 22:59:44,638 [INFO] Regularization: 3027.307129 * 0.0000010000 = 0.0030273071
2019-03-18 22:59:44,639 [INFO] Sum of grad norms: 0.031875
2019-03-18 22:59:44,640 [INFO] ---------------------------------
2019-03-18 23:00:03,518 [INFO] ---------------------------------
2019-03-18 23:00:03,519 [INFO] Summary:
2019-03-18 23:00:03,519 [INFO] Batch 168000, worst loss 0.060551 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:00:03,520 [INFO] Regularization: 3027.297119 * 0.0000010000 = 0.0030272971
2019-03-18 23:00:03,520 [INFO] Sum of grad norms: 0.023688
2019-03-18 23:00:03,521 [INFO] ---------------------------------
2019-03-18 23:00:22,232 [INFO] ---------------------------------
2019-03-18 23:00:22,233 [INFO] Summary:
2019-03-18 23:00:22,234 [INFO] Batch 169000, worst loss 0.060551 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:00:22,234 [INFO] Regularization: 3027.288574 * 0.0000010000 = 0.0030272885
2019-03-18 23:00:22,235 [INFO] Sum of grad norms: 0.023410
2019-03-18 23:00:22,235 [INFO] ---------------------------------
2019-03-18 23:00:40,661 [INFO] ---------------------------------
2019-03-18 23:00:40,662 [INFO] Summary:
2019-03-18 23:00:40,662 [INFO] Batch 170000, worst loss 0.060685 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:00:40,663 [INFO] Regularization: 3027.279053 * 0.0000010000 = 0.0030272789
2019-03-18 23:00:40,663 [INFO] Sum of grad norms: 0.033355
2019-03-18 23:00:40,664 [INFO] ---------------------------------
2019-03-18 23:00:45,552 [INFO] ---------------------------------
2019-03-18 23:00:45,553 [INFO] Evaluation:
2019-03-18 23:00:45,554 [INFO] Batch 170000, worst loss 0.057657 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:00:45,555 [INFO] ---------------------------------
2019-03-18 23:01:04,456 [INFO] ---------------------------------
2019-03-18 23:01:04,456 [INFO] Summary:
2019-03-18 23:01:04,457 [INFO] Batch 171000, worst loss 0.060680 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:01:04,458 [INFO] Regularization: 3027.273926 * 0.0000010000 = 0.0030272738
2019-03-18 23:01:04,458 [INFO] Sum of grad norms: 0.035578
2019-03-18 23:01:04,459 [INFO] ---------------------------------
2019-03-18 23:01:23,035 [INFO] ---------------------------------
2019-03-18 23:01:23,036 [INFO] Summary:
2019-03-18 23:01:23,037 [INFO] Batch 172000, worst loss 0.060680 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:01:23,037 [INFO] Regularization: 3027.268799 * 0.0000010000 = 0.0030272687
2019-03-18 23:01:23,038 [INFO] Sum of grad norms: 0.044061
2019-03-18 23:01:23,038 [INFO] ---------------------------------
2019-03-18 23:01:41,993 [INFO] ---------------------------------
2019-03-18 23:01:41,994 [INFO] Summary:
2019-03-18 23:01:41,994 [INFO] Batch 173000, worst loss 0.060560 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:01:41,995 [INFO] Regularization: 3027.264648 * 0.0000010000 = 0.0030272647
2019-03-18 23:01:41,996 [INFO] Sum of grad norms: 0.026823
2019-03-18 23:01:41,997 [INFO] ---------------------------------
2019-03-18 23:02:00,698 [INFO] ---------------------------------
2019-03-18 23:02:00,699 [INFO] Summary:
2019-03-18 23:02:00,700 [INFO] Batch 174000, worst loss 0.060779 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:02:00,701 [INFO] Regularization: 3027.261719 * 0.0000010000 = 0.0030272617
2019-03-18 23:02:00,701 [INFO] Sum of grad norms: 0.026091
2019-03-18 23:02:00,702 [INFO] ---------------------------------
2019-03-18 23:02:19,333 [INFO] ---------------------------------
2019-03-18 23:02:19,334 [INFO] Summary:
2019-03-18 23:02:19,334 [INFO] Batch 175000, worst loss 0.060779 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:02:19,335 [INFO] Regularization: 3027.258301 * 0.0000010000 = 0.0030272582
2019-03-18 23:02:19,336 [INFO] Sum of grad norms: 0.092906
2019-03-18 23:02:19,336 [INFO] ---------------------------------
2019-03-18 23:02:37,931 [INFO] ---------------------------------
2019-03-18 23:02:37,932 [INFO] Summary:
2019-03-18 23:02:37,933 [INFO] Batch 176000, worst loss 0.060708 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:02:37,933 [INFO] Regularization: 3027.255371 * 0.0000010000 = 0.0030272554
2019-03-18 23:02:37,934 [INFO] Sum of grad norms: 0.052094
2019-03-18 23:02:37,935 [INFO] ---------------------------------
2019-03-18 23:02:56,983 [INFO] ---------------------------------
2019-03-18 23:02:56,984 [INFO] Summary:
2019-03-18 23:02:56,985 [INFO] Batch 177000, worst loss 0.060607 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:02:56,986 [INFO] Regularization: 3027.252930 * 0.0000010000 = 0.0030272529
2019-03-18 23:02:56,986 [INFO] Sum of grad norms: 0.077665
2019-03-18 23:02:56,987 [INFO] ---------------------------------
2019-03-18 23:03:15,894 [INFO] ---------------------------------
2019-03-18 23:03:15,895 [INFO] Summary:
2019-03-18 23:03:15,896 [INFO] Batch 178000, worst loss 0.060685 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:03:15,896 [INFO] Regularization: 3027.248779 * 0.0000010000 = 0.0030272487
2019-03-18 23:03:15,897 [INFO] Sum of grad norms: 0.016721
2019-03-18 23:03:15,898 [INFO] ---------------------------------
2019-03-18 23:03:34,513 [INFO] ---------------------------------
2019-03-18 23:03:34,514 [INFO] Summary:
2019-03-18 23:03:34,515 [INFO] Batch 179000, worst loss 0.060698 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:03:34,515 [INFO] Regularization: 3027.244629 * 0.0000010000 = 0.0030272447
2019-03-18 23:03:34,516 [INFO] Sum of grad norms: 0.028472
2019-03-18 23:03:34,516 [INFO] ---------------------------------
2019-03-18 23:03:53,083 [INFO] ---------------------------------
2019-03-18 23:03:53,084 [INFO] Summary:
2019-03-18 23:03:53,084 [INFO] Batch 180000, worst loss 0.060698 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:03:53,085 [INFO] Regularization: 3027.242188 * 0.0000010000 = 0.0030272421
2019-03-18 23:03:53,085 [INFO] Sum of grad norms: 0.044898
2019-03-18 23:03:53,086 [INFO] ---------------------------------
2019-03-18 23:03:57,994 [INFO] ---------------------------------
2019-03-18 23:03:57,995 [INFO] Evaluation:
2019-03-18 23:03:57,997 [INFO] Batch 180000, worst loss 0.057509 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:03:57,999 [INFO] New best loss 0.057509, saved to file classifier/1552933539/1552946637_2_classifier_180000.pth
2019-03-18 23:03:58,010 [INFO] Target
2019-03-18 23:03:58,010 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 ...
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]]
2019-03-18 23:03:58,012 [INFO] Classifier output
2019-03-18 23:03:58,013 [INFO] [[0.990203 0.99017  0.989598 ... 0.001856 0.000732 0.001184]
 [0.990203 0.99017  0.989598 ... 0.001856 0.000732 0.001184]
 [0.989925 0.98992  0.989916 ... 0.009786 0.008087 0.008614]
 ...
 [0.010434 0.010061 0.010526 ... 0.988179 0.988607 0.989123]
 [0.989946 0.989937 0.989949 ... 0.009773 0.008597 0.00926 ]
 [0.009984 0.01006  0.009624 ... 0.988849 0.987078 0.988527]]
2019-03-18 23:03:58,015 [INFO] ---------------------------------
2019-03-18 23:04:16,572 [INFO] ---------------------------------
2019-03-18 23:04:16,573 [INFO] Summary:
2019-03-18 23:04:16,574 [INFO] Batch 181000, worst loss 0.060603 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:04:16,574 [INFO] Regularization: 3027.238037 * 0.0000010000 = 0.0030272380
2019-03-18 23:04:16,575 [INFO] Sum of grad norms: 0.023858
2019-03-18 23:04:16,575 [INFO] ---------------------------------
2019-03-18 23:04:35,216 [INFO] ---------------------------------
2019-03-18 23:04:35,217 [INFO] Summary:
2019-03-18 23:04:35,218 [INFO] Batch 182000, worst loss 0.060715 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:04:35,218 [INFO] Regularization: 3027.236816 * 0.0000010000 = 0.0030272368
2019-03-18 23:04:35,219 [INFO] Sum of grad norms: 0.032150
2019-03-18 23:04:35,220 [INFO] ---------------------------------
2019-03-18 23:04:53,612 [INFO] ---------------------------------
2019-03-18 23:04:53,613 [INFO] Summary:
2019-03-18 23:04:53,614 [INFO] Batch 183000, worst loss 0.060715 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:04:53,614 [INFO] Regularization: 3027.235352 * 0.0000010000 = 0.0030272354
2019-03-18 23:04:53,615 [INFO] Sum of grad norms: 0.033062
2019-03-18 23:04:53,615 [INFO] ---------------------------------
2019-03-18 23:05:12,250 [INFO] ---------------------------------
2019-03-18 23:05:12,251 [INFO] Summary:
2019-03-18 23:05:12,252 [INFO] Batch 184000, worst loss 0.060636 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:05:12,252 [INFO] Regularization: 3027.233887 * 0.0000010000 = 0.0030272340
2019-03-18 23:05:12,253 [INFO] Sum of grad norms: 0.031682
2019-03-18 23:05:12,253 [INFO] ---------------------------------
2019-03-18 23:05:30,875 [INFO] ---------------------------------
2019-03-18 23:05:30,876 [INFO] Summary:
2019-03-18 23:05:30,877 [INFO] Batch 185000, worst loss 0.060546 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:05:30,878 [INFO] Regularization: 3027.232910 * 0.0000010000 = 0.0030272328
2019-03-18 23:05:30,878 [INFO] Sum of grad norms: 0.020842
2019-03-18 23:05:30,879 [INFO] ---------------------------------
2019-03-18 23:05:49,725 [INFO] ---------------------------------
2019-03-18 23:05:49,726 [INFO] Summary:
2019-03-18 23:05:49,726 [INFO] Batch 186000, worst loss 0.060693 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:05:49,727 [INFO] Regularization: 3027.231934 * 0.0000010000 = 0.0030272319
2019-03-18 23:05:49,727 [INFO] Sum of grad norms: 0.025547
2019-03-18 23:05:49,728 [INFO] ---------------------------------
2019-03-18 23:06:08,339 [INFO] ---------------------------------
2019-03-18 23:06:08,340 [INFO] Summary:
2019-03-18 23:06:08,341 [INFO] Batch 187000, worst loss 0.060667 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:06:08,341 [INFO] Regularization: 3027.231201 * 0.0000010000 = 0.0030272312
2019-03-18 23:06:08,342 [INFO] Sum of grad norms: 0.014532
2019-03-18 23:06:08,342 [INFO] ---------------------------------
2019-03-18 23:06:26,954 [INFO] ---------------------------------
2019-03-18 23:06:26,955 [INFO] Summary:
2019-03-18 23:06:26,955 [INFO] Batch 188000, worst loss 0.060690 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:06:26,956 [INFO] Regularization: 3027.230469 * 0.0000010000 = 0.0030272305
2019-03-18 23:06:26,956 [INFO] Sum of grad norms: 0.077831
2019-03-18 23:06:26,957 [INFO] ---------------------------------
2019-03-18 23:06:45,698 [INFO] ---------------------------------
2019-03-18 23:06:45,699 [INFO] Summary:
2019-03-18 23:06:45,700 [INFO] Batch 189000, worst loss 0.060765 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:06:45,701 [INFO] Regularization: 3027.229492 * 0.0000010000 = 0.0030272296
2019-03-18 23:06:45,701 [INFO] Sum of grad norms: 0.046339
2019-03-18 23:06:45,702 [INFO] ---------------------------------
2019-03-18 23:07:04,270 [INFO] ---------------------------------
2019-03-18 23:07:04,271 [INFO] Summary:
2019-03-18 23:07:04,272 [INFO] Batch 190000, worst loss 0.060710 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:07:04,273 [INFO] Regularization: 3027.228027 * 0.0000010000 = 0.0030272279
2019-03-18 23:07:04,273 [INFO] Sum of grad norms: 0.025009
2019-03-18 23:07:04,274 [INFO] ---------------------------------
2019-03-18 23:07:09,258 [INFO] ---------------------------------
2019-03-18 23:07:09,259 [INFO] Evaluation:
2019-03-18 23:07:09,260 [INFO] Batch 190000, worst loss 0.057683 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:07:09,261 [INFO] ---------------------------------
2019-03-18 23:07:28,014 [INFO] ---------------------------------
2019-03-18 23:07:28,015 [INFO] Summary:
2019-03-18 23:07:28,016 [INFO] Batch 191000, worst loss 0.060679 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:07:28,016 [INFO] Regularization: 3027.227051 * 0.0000010000 = 0.0030272270
2019-03-18 23:07:28,017 [INFO] Sum of grad norms: 0.054047
2019-03-18 23:07:28,017 [INFO] ---------------------------------
2019-03-18 23:07:46,878 [INFO] ---------------------------------
2019-03-18 23:07:46,879 [INFO] Summary:
2019-03-18 23:07:46,880 [INFO] Batch 192000, worst loss 0.060742 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:07:46,880 [INFO] Regularization: 3027.226562 * 0.0000010000 = 0.0030272265
2019-03-18 23:07:46,881 [INFO] Sum of grad norms: 0.021175
2019-03-18 23:07:46,881 [INFO] ---------------------------------
2019-03-18 23:08:05,568 [INFO] ---------------------------------
2019-03-18 23:08:05,569 [INFO] Summary:
2019-03-18 23:08:05,569 [INFO] Batch 193000, worst loss 0.060716 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:08:05,570 [INFO] Regularization: 3027.226074 * 0.0000010000 = 0.0030272261
2019-03-18 23:08:05,571 [INFO] Sum of grad norms: 0.033186
2019-03-18 23:08:05,571 [INFO] ---------------------------------
2019-03-18 23:08:24,471 [INFO] ---------------------------------
2019-03-18 23:08:24,472 [INFO] Summary:
2019-03-18 23:08:24,472 [INFO] Batch 194000, worst loss 0.060716 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:08:24,473 [INFO] Regularization: 3027.225830 * 0.0000010000 = 0.0030272258
2019-03-18 23:08:24,473 [INFO] Sum of grad norms: 0.028554
2019-03-18 23:08:24,474 [INFO] ---------------------------------
2019-03-18 23:08:43,295 [INFO] ---------------------------------
2019-03-18 23:08:43,296 [INFO] Summary:
2019-03-18 23:08:43,297 [INFO] Batch 195000, worst loss 0.060827 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:08:43,297 [INFO] Regularization: 3027.225586 * 0.0000010000 = 0.0030272256
2019-03-18 23:08:43,298 [INFO] Sum of grad norms: 0.099259
2019-03-18 23:08:43,298 [INFO] ---------------------------------
2019-03-18 23:09:02,120 [INFO] ---------------------------------
2019-03-18 23:09:02,121 [INFO] Summary:
2019-03-18 23:09:02,122 [INFO] Batch 196000, worst loss 0.060638 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:09:02,123 [INFO] Regularization: 3027.225098 * 0.0000010000 = 0.0030272251
2019-03-18 23:09:02,123 [INFO] Sum of grad norms: 0.046108
2019-03-18 23:09:02,124 [INFO] ---------------------------------
2019-03-18 23:09:20,677 [INFO] ---------------------------------
2019-03-18 23:09:20,678 [INFO] Summary:
2019-03-18 23:09:20,678 [INFO] Batch 197000, worst loss 0.060638 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:09:20,679 [INFO] Regularization: 3027.224854 * 0.0000010000 = 0.0030272249
2019-03-18 23:09:20,679 [INFO] Sum of grad norms: 0.045778
2019-03-18 23:09:20,680 [INFO] ---------------------------------
2019-03-18 23:09:39,264 [INFO] ---------------------------------
2019-03-18 23:09:39,265 [INFO] Summary:
2019-03-18 23:09:39,266 [INFO] Batch 198000, worst loss 0.060634 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:09:39,266 [INFO] Regularization: 3027.224365 * 0.0000010000 = 0.0030272244
2019-03-18 23:09:39,267 [INFO] Sum of grad norms: 0.068432
2019-03-18 23:09:39,267 [INFO] ---------------------------------
2019-03-18 23:09:57,664 [INFO] ---------------------------------
2019-03-18 23:09:57,665 [INFO] Summary:
2019-03-18 23:09:57,666 [INFO] Batch 199000, worst loss 0.060686 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:09:57,666 [INFO] Regularization: 3027.224121 * 0.0000010000 = 0.0030272242
2019-03-18 23:09:57,667 [INFO] Sum of grad norms: 0.043703
2019-03-18 23:09:57,667 [INFO] ---------------------------------
2019-03-18 23:10:16,259 [INFO] ---------------------------------
2019-03-18 23:10:16,260 [INFO] Summary:
2019-03-18 23:10:16,261 [INFO] Batch 200000, worst loss 0.060690 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:10:16,262 [INFO] Regularization: 3027.223633 * 0.0000010000 = 0.0030272235
2019-03-18 23:10:16,262 [INFO] Sum of grad norms: 0.039194
2019-03-18 23:10:16,263 [INFO] ---------------------------------
2019-03-18 23:10:21,164 [INFO] ---------------------------------
2019-03-18 23:10:21,166 [INFO] Evaluation:
2019-03-18 23:10:21,167 [INFO] Batch 200000, worst loss 0.057663 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:10:21,167 [INFO] ---------------------------------
2019-03-18 23:10:39,891 [INFO] ---------------------------------
2019-03-18 23:10:39,892 [INFO] Summary:
2019-03-18 23:10:39,893 [INFO] Batch 201000, worst loss 0.060676 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:10:39,893 [INFO] Regularization: 3027.223633 * 0.0000010000 = 0.0030272235
2019-03-18 23:10:39,894 [INFO] Sum of grad norms: 0.024706
2019-03-18 23:10:39,894 [INFO] ---------------------------------
2019-03-18 23:10:59,007 [INFO] ---------------------------------
2019-03-18 23:10:59,008 [INFO] Summary:
2019-03-18 23:10:59,009 [INFO] Batch 202000, worst loss 0.060742 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:10:59,009 [INFO] Regularization: 3027.223389 * 0.0000010000 = 0.0030272233
2019-03-18 23:10:59,010 [INFO] Sum of grad norms: 0.034761
2019-03-18 23:10:59,010 [INFO] ---------------------------------
2019-03-18 23:11:17,715 [INFO] ---------------------------------
2019-03-18 23:11:17,716 [INFO] Summary:
2019-03-18 23:11:17,716 [INFO] Batch 203000, worst loss 0.060621 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:11:17,717 [INFO] Regularization: 3027.223145 * 0.0000010000 = 0.0030272231
2019-03-18 23:11:17,717 [INFO] Sum of grad norms: 0.035091
2019-03-18 23:11:17,718 [INFO] ---------------------------------
2019-03-18 23:11:36,391 [INFO] ---------------------------------
2019-03-18 23:11:36,392 [INFO] Summary:
2019-03-18 23:11:36,392 [INFO] Batch 204000, worst loss 0.060713 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:11:36,393 [INFO] Regularization: 3027.223389 * 0.0000010000 = 0.0030272233
2019-03-18 23:11:36,393 [INFO] Sum of grad norms: 0.026811
2019-03-18 23:11:36,394 [INFO] ---------------------------------
2019-03-18 23:11:55,092 [INFO] ---------------------------------
2019-03-18 23:11:55,093 [INFO] Summary:
2019-03-18 23:11:55,094 [INFO] Batch 205000, worst loss 0.060713 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:11:55,095 [INFO] Regularization: 3027.223145 * 0.0000010000 = 0.0030272231
2019-03-18 23:11:55,095 [INFO] Sum of grad norms: 0.058418
2019-03-18 23:11:55,096 [INFO] ---------------------------------
2019-03-18 23:12:13,964 [INFO] ---------------------------------
2019-03-18 23:12:13,965 [INFO] Summary:
2019-03-18 23:12:13,966 [INFO] Batch 206000, worst loss 0.060705 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:12:13,966 [INFO] Regularization: 3027.223145 * 0.0000010000 = 0.0030272231
2019-03-18 23:12:13,967 [INFO] Sum of grad norms: 0.035684
2019-03-18 23:12:13,967 [INFO] ---------------------------------
2019-03-18 23:12:32,881 [INFO] ---------------------------------
2019-03-18 23:12:32,882 [INFO] Summary:
2019-03-18 23:12:32,883 [INFO] Batch 207000, worst loss 0.060736 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:12:32,884 [INFO] Regularization: 3027.223145 * 0.0000010000 = 0.0030272231
2019-03-18 23:12:32,884 [INFO] Sum of grad norms: 0.059550
2019-03-18 23:12:32,885 [INFO] ---------------------------------
2019-03-18 23:12:51,454 [INFO] ---------------------------------
2019-03-18 23:12:51,455 [INFO] Summary:
2019-03-18 23:12:51,456 [INFO] Batch 208000, worst loss 0.060736 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:12:51,456 [INFO] Regularization: 3027.222656 * 0.0000010000 = 0.0030272226
2019-03-18 23:12:51,457 [INFO] Sum of grad norms: 0.080574
2019-03-18 23:12:51,457 [INFO] ---------------------------------
2019-03-18 23:13:10,378 [INFO] ---------------------------------
2019-03-18 23:13:10,379 [INFO] Summary:
2019-03-18 23:13:10,380 [INFO] Batch 209000, worst loss 0.060785 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:13:10,380 [INFO] Regularization: 3027.222656 * 0.0000010000 = 0.0030272226
2019-03-18 23:13:10,381 [INFO] Sum of grad norms: 0.033974
2019-03-18 23:13:10,381 [INFO] ---------------------------------
2019-03-18 23:13:29,119 [INFO] ---------------------------------
2019-03-18 23:13:29,120 [INFO] Summary:
2019-03-18 23:13:29,120 [INFO] Batch 210000, worst loss 0.060785 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:13:29,121 [INFO] Regularization: 3027.222656 * 0.0000010000 = 0.0030272226
2019-03-18 23:13:29,121 [INFO] Sum of grad norms: 0.026918
2019-03-18 23:13:29,122 [INFO] ---------------------------------
2019-03-18 23:13:34,025 [INFO] ---------------------------------
2019-03-18 23:13:34,026 [INFO] Evaluation:
2019-03-18 23:13:34,027 [INFO] Batch 210000, worst loss 0.057556 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:13:34,028 [INFO] ---------------------------------
2019-03-18 23:13:52,642 [INFO] ---------------------------------
2019-03-18 23:13:52,643 [INFO] Summary:
2019-03-18 23:13:52,643 [INFO] Batch 211000, worst loss 0.060707 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:13:52,644 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:13:52,644 [INFO] Sum of grad norms: 0.051247
2019-03-18 23:13:52,645 [INFO] ---------------------------------
2019-03-18 23:14:11,195 [INFO] ---------------------------------
2019-03-18 23:14:11,196 [INFO] Summary:
2019-03-18 23:14:11,197 [INFO] Batch 212000, worst loss 0.060793 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:14:11,197 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:14:11,198 [INFO] Sum of grad norms: 0.046658
2019-03-18 23:14:11,198 [INFO] ---------------------------------
2019-03-18 23:14:30,023 [INFO] ---------------------------------
2019-03-18 23:14:30,024 [INFO] Summary:
2019-03-18 23:14:30,024 [INFO] Batch 213000, worst loss 0.060730 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:14:30,025 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:14:30,025 [INFO] Sum of grad norms: 0.020445
2019-03-18 23:14:30,026 [INFO] ---------------------------------
2019-03-18 23:14:48,686 [INFO] ---------------------------------
2019-03-18 23:14:48,687 [INFO] Summary:
2019-03-18 23:14:48,688 [INFO] Batch 214000, worst loss 0.060741 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:14:48,688 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:14:48,689 [INFO] Sum of grad norms: 0.091220
2019-03-18 23:14:48,689 [INFO] ---------------------------------
2019-03-18 23:15:07,310 [INFO] ---------------------------------
2019-03-18 23:15:07,311 [INFO] Summary:
2019-03-18 23:15:07,312 [INFO] Batch 215000, worst loss 0.060741 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:15:07,312 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:15:07,313 [INFO] Sum of grad norms: 0.029819
2019-03-18 23:15:07,313 [INFO] ---------------------------------
2019-03-18 23:15:25,594 [INFO] ---------------------------------
2019-03-18 23:15:25,595 [INFO] Summary:
2019-03-18 23:15:25,596 [INFO] Batch 216000, worst loss 0.060782 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:15:25,596 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:15:25,597 [INFO] Sum of grad norms: 0.134566
2019-03-18 23:15:25,598 [INFO] ---------------------------------
2019-03-18 23:15:44,485 [INFO] ---------------------------------
2019-03-18 23:15:44,486 [INFO] Summary:
2019-03-18 23:15:44,486 [INFO] Batch 217000, worst loss 0.060656 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:15:44,487 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:15:44,488 [INFO] Sum of grad norms: 0.046241
2019-03-18 23:15:44,489 [INFO] ---------------------------------
2019-03-18 23:16:03,508 [INFO] ---------------------------------
2019-03-18 23:16:03,509 [INFO] Summary:
2019-03-18 23:16:03,509 [INFO] Batch 218000, worst loss 0.060643 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:16:03,510 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:16:03,510 [INFO] Sum of grad norms: 0.071622
2019-03-18 23:16:03,511 [INFO] ---------------------------------
2019-03-18 23:16:22,252 [INFO] ---------------------------------
2019-03-18 23:16:22,253 [INFO] Summary:
2019-03-18 23:16:22,253 [INFO] Batch 219000, worst loss 0.060796 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:16:22,254 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:16:22,254 [INFO] Sum of grad norms: 0.036240
2019-03-18 23:16:22,255 [INFO] ---------------------------------
2019-03-18 23:16:41,055 [INFO] ---------------------------------
2019-03-18 23:16:41,055 [INFO] Summary:
2019-03-18 23:16:41,056 [INFO] Batch 220000, worst loss 0.060981 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:16:41,057 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:16:41,057 [INFO] Sum of grad norms: 0.048042
2019-03-18 23:16:41,058 [INFO] ---------------------------------
2019-03-18 23:16:46,013 [INFO] ---------------------------------
2019-03-18 23:16:46,014 [INFO] Evaluation:
2019-03-18 23:16:46,015 [INFO] Batch 220000, worst loss 0.057660 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:16:46,016 [INFO] ---------------------------------
2019-03-18 23:17:04,822 [INFO] ---------------------------------
2019-03-18 23:17:04,822 [INFO] Summary:
2019-03-18 23:17:04,823 [INFO] Batch 221000, worst loss 0.060664 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:17:04,824 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:17:04,824 [INFO] Sum of grad norms: 0.056253
2019-03-18 23:17:04,825 [INFO] ---------------------------------
2019-03-18 23:17:23,750 [INFO] ---------------------------------
2019-03-18 23:17:23,751 [INFO] Summary:
2019-03-18 23:17:23,751 [INFO] Batch 222000, worst loss 0.060682 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:17:23,752 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:17:23,752 [INFO] Sum of grad norms: 0.026340
2019-03-18 23:17:23,753 [INFO] ---------------------------------
2019-03-18 23:17:42,667 [INFO] ---------------------------------
2019-03-18 23:17:42,668 [INFO] Summary:
2019-03-18 23:17:42,669 [INFO] Batch 223000, worst loss 0.060848 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:17:42,670 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:17:42,670 [INFO] Sum of grad norms: 0.041859
2019-03-18 23:17:42,671 [INFO] ---------------------------------
2019-03-18 23:18:01,429 [INFO] ---------------------------------
2019-03-18 23:18:01,430 [INFO] Summary:
2019-03-18 23:18:01,431 [INFO] Batch 224000, worst loss 0.060852 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:18:01,432 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:18:01,432 [INFO] Sum of grad norms: 0.037191
2019-03-18 23:18:01,433 [INFO] ---------------------------------
2019-03-18 23:18:19,688 [INFO] ---------------------------------
2019-03-18 23:18:19,689 [INFO] Summary:
2019-03-18 23:18:19,690 [INFO] Batch 225000, worst loss 0.060852 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:18:19,690 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:18:19,691 [INFO] Sum of grad norms: 0.064231
2019-03-18 23:18:19,691 [INFO] ---------------------------------
2019-03-18 23:18:38,067 [INFO] ---------------------------------
2019-03-18 23:18:38,068 [INFO] Summary:
2019-03-18 23:18:38,068 [INFO] Batch 226000, worst loss 0.060742 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:18:38,069 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:18:38,069 [INFO] Sum of grad norms: 0.024897
2019-03-18 23:18:38,070 [INFO] ---------------------------------
2019-03-18 23:18:57,013 [INFO] ---------------------------------
2019-03-18 23:18:57,014 [INFO] Summary:
2019-03-18 23:18:57,015 [INFO] Batch 227000, worst loss 0.060639 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:18:57,015 [INFO] Regularization: 3027.222168 * 0.0000010000 = 0.0030272221
2019-03-18 23:18:57,016 [INFO] Sum of grad norms: 0.046223
2019-03-18 23:18:57,017 [INFO] ---------------------------------
2019-03-18 23:19:15,750 [INFO] ---------------------------------
2019-03-18 23:19:15,751 [INFO] Summary:
2019-03-18 23:19:15,752 [INFO] Batch 228000, worst loss 0.060639 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:19:15,752 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:19:15,753 [INFO] Sum of grad norms: 0.024985
2019-03-18 23:19:15,753 [INFO] ---------------------------------
2019-03-18 23:19:34,481 [INFO] ---------------------------------
2019-03-18 23:19:34,482 [INFO] Summary:
2019-03-18 23:19:34,482 [INFO] Batch 229000, worst loss 0.060772 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:19:34,483 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:19:34,484 [INFO] Sum of grad norms: 0.080612
2019-03-18 23:19:34,484 [INFO] ---------------------------------
2019-03-18 23:19:53,425 [INFO] ---------------------------------
2019-03-18 23:19:53,426 [INFO] Summary:
2019-03-18 23:19:53,426 [INFO] Batch 230000, worst loss 0.060725 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:19:53,427 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:19:53,427 [INFO] Sum of grad norms: 0.083713
2019-03-18 23:19:53,428 [INFO] ---------------------------------
2019-03-18 23:19:58,337 [INFO] ---------------------------------
2019-03-18 23:19:58,338 [INFO] Evaluation:
2019-03-18 23:19:58,339 [INFO] Batch 230000, worst loss 0.057772 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:19:58,341 [INFO] ---------------------------------
2019-03-18 23:20:17,128 [INFO] ---------------------------------
2019-03-18 23:20:17,129 [INFO] Summary:
2019-03-18 23:20:17,130 [INFO] Batch 231000, worst loss 0.060618 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:20:17,130 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:20:17,131 [INFO] Sum of grad norms: 0.077825
2019-03-18 23:20:17,131 [INFO] ---------------------------------
2019-03-18 23:20:35,740 [INFO] ---------------------------------
2019-03-18 23:20:35,741 [INFO] Summary:
2019-03-18 23:20:35,742 [INFO] Batch 232000, worst loss 0.060627 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:20:35,743 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:20:35,743 [INFO] Sum of grad norms: 0.074343
2019-03-18 23:20:35,744 [INFO] ---------------------------------
2019-03-18 23:20:54,033 [INFO] ---------------------------------
2019-03-18 23:20:54,034 [INFO] Summary:
2019-03-18 23:20:54,035 [INFO] Batch 233000, worst loss 0.060627 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:20:54,035 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:20:54,036 [INFO] Sum of grad norms: 0.036690
2019-03-18 23:20:54,036 [INFO] ---------------------------------
2019-03-18 23:21:12,789 [INFO] ---------------------------------
2019-03-18 23:21:12,790 [INFO] Summary:
2019-03-18 23:21:12,790 [INFO] Batch 234000, worst loss 0.060590 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:21:12,791 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:21:12,791 [INFO] Sum of grad norms: 0.085709
2019-03-18 23:21:12,792 [INFO] ---------------------------------
2019-03-18 23:21:31,605 [INFO] ---------------------------------
2019-03-18 23:21:31,606 [INFO] Summary:
2019-03-18 23:21:31,607 [INFO] Batch 235000, worst loss 0.060695 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:21:31,607 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:21:31,608 [INFO] Sum of grad norms: 0.025820
2019-03-18 23:21:31,608 [INFO] ---------------------------------
2019-03-18 23:21:50,561 [INFO] ---------------------------------
2019-03-18 23:21:50,562 [INFO] Summary:
2019-03-18 23:21:50,563 [INFO] Batch 236000, worst loss 0.060695 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:21:50,564 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:21:50,564 [INFO] Sum of grad norms: 0.038663
2019-03-18 23:21:50,565 [INFO] ---------------------------------
2019-03-18 23:22:09,219 [INFO] ---------------------------------
2019-03-18 23:22:09,220 [INFO] Summary:
2019-03-18 23:22:09,220 [INFO] Batch 237000, worst loss 0.060662 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:22:09,221 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:22:09,221 [INFO] Sum of grad norms: 0.056078
2019-03-18 23:22:09,222 [INFO] ---------------------------------
2019-03-18 23:22:27,724 [INFO] ---------------------------------
2019-03-18 23:22:27,725 [INFO] Summary:
2019-03-18 23:22:27,726 [INFO] Batch 238000, worst loss 0.060783 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:22:27,726 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:22:27,727 [INFO] Sum of grad norms: 0.046487
2019-03-18 23:22:27,727 [INFO] ---------------------------------
2019-03-18 23:22:46,489 [INFO] ---------------------------------
2019-03-18 23:22:46,490 [INFO] Summary:
2019-03-18 23:22:46,491 [INFO] Batch 239000, worst loss 0.060669 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:22:46,491 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:22:46,492 [INFO] Sum of grad norms: 0.035520
2019-03-18 23:22:46,492 [INFO] ---------------------------------
2019-03-18 23:23:05,150 [INFO] ---------------------------------
2019-03-18 23:23:05,151 [INFO] Summary:
2019-03-18 23:23:05,152 [INFO] Batch 240000, worst loss 0.060576 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:23:05,153 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:23:05,153 [INFO] Sum of grad norms: 0.031449
2019-03-18 23:23:05,154 [INFO] ---------------------------------
2019-03-18 23:23:10,093 [INFO] ---------------------------------
2019-03-18 23:23:10,094 [INFO] Evaluation:
2019-03-18 23:23:10,094 [INFO] Batch 240000, worst loss 0.057685 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:23:10,095 [INFO] ---------------------------------
2019-03-18 23:23:28,775 [INFO] ---------------------------------
2019-03-18 23:23:28,776 [INFO] Summary:
2019-03-18 23:23:28,776 [INFO] Batch 241000, worst loss 0.060712 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:23:28,777 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:23:28,777 [INFO] Sum of grad norms: 0.047069
2019-03-18 23:23:28,778 [INFO] ---------------------------------
2019-03-18 23:23:47,634 [INFO] ---------------------------------
2019-03-18 23:23:47,634 [INFO] Summary:
2019-03-18 23:23:47,635 [INFO] Batch 242000, worst loss 0.060833 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:23:47,635 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:23:47,636 [INFO] Sum of grad norms: 0.104691
2019-03-18 23:23:47,636 [INFO] ---------------------------------
2019-03-18 23:24:06,573 [INFO] ---------------------------------
2019-03-18 23:24:06,575 [INFO] Summary:
2019-03-18 23:24:06,575 [INFO] Batch 243000, worst loss 0.060739 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:24:06,576 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:24:06,576 [INFO] Sum of grad norms: 0.057482
2019-03-18 23:24:06,577 [INFO] ---------------------------------
2019-03-18 23:24:25,730 [INFO] ---------------------------------
2019-03-18 23:24:25,731 [INFO] Summary:
2019-03-18 23:24:25,731 [INFO] Batch 244000, worst loss 0.060739 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:24:25,732 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:24:25,732 [INFO] Sum of grad norms: 0.077156
2019-03-18 23:24:25,733 [INFO] ---------------------------------
2019-03-18 23:24:44,349 [INFO] ---------------------------------
2019-03-18 23:24:44,350 [INFO] Summary:
2019-03-18 23:24:44,351 [INFO] Batch 245000, worst loss 0.060544 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:24:44,351 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:24:44,352 [INFO] Sum of grad norms: 0.033247
2019-03-18 23:24:44,353 [INFO] ---------------------------------
2019-03-18 23:25:03,033 [INFO] ---------------------------------
2019-03-18 23:25:03,034 [INFO] Summary:
2019-03-18 23:25:03,034 [INFO] Batch 246000, worst loss 0.060667 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:25:03,035 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:25:03,035 [INFO] Sum of grad norms: 0.038025
2019-03-18 23:25:03,036 [INFO] ---------------------------------
2019-03-18 23:25:21,827 [INFO] ---------------------------------
2019-03-18 23:25:21,828 [INFO] Summary:
2019-03-18 23:25:21,828 [INFO] Batch 247000, worst loss 0.060832 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:25:21,829 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:25:21,829 [INFO] Sum of grad norms: 0.040112
2019-03-18 23:25:21,830 [INFO] ---------------------------------
2019-03-18 23:25:40,465 [INFO] ---------------------------------
2019-03-18 23:25:40,466 [INFO] Summary:
2019-03-18 23:25:40,467 [INFO] Batch 248000, worst loss 0.060661 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:25:40,467 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:25:40,468 [INFO] Sum of grad norms: 0.043354
2019-03-18 23:25:40,468 [INFO] ---------------------------------
2019-03-18 23:25:59,287 [INFO] ---------------------------------
2019-03-18 23:25:59,288 [INFO] Summary:
2019-03-18 23:25:59,289 [INFO] Batch 249000, worst loss 0.060661 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:25:59,290 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:25:59,291 [INFO] Sum of grad norms: 0.033294
2019-03-18 23:25:59,292 [INFO] ---------------------------------
2019-03-18 23:26:18,319 [INFO] ---------------------------------
2019-03-18 23:26:18,320 [INFO] Summary:
2019-03-18 23:26:18,321 [INFO] Batch 250000, worst loss 0.060569 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-18 23:26:18,321 [INFO] Regularization: 3027.221924 * 0.0000010000 = 0.0030272219
2019-03-18 23:26:18,322 [INFO] Sum of grad norms: 0.031383
2019-03-18 23:26:18,323 [INFO] ---------------------------------
2019-03-18 23:26:23,269 [INFO] ---------------------------------
2019-03-18 23:26:23,272 [INFO] Evaluation:
2019-03-18 23:26:23,273 [INFO] Batch 250000, worst loss 0.057539 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:26:23,274 [INFO] ---------------------------------
2019-03-18 23:26:23,275 [INFO] Finished training, saved to file classifier/1552933539/1552947983_2_classifier_final.pth
2019-03-18 23:26:23,446 [INFO] ---------------------------------
2019-03-18 23:26:23,448 [INFO] Training model #3: (1, 64, 201) @ 1
2019-03-18 23:26:39,849 [INFO] ---------------------------------
2019-03-18 23:26:39,850 [INFO] Summary:
2019-03-18 23:26:39,850 [INFO] Batch 1000, worst loss 23.780668 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-18 23:26:39,851 [INFO] Regularization: 9540.705078 * 0.0000010000 = 0.0095407050
2019-03-18 23:26:39,852 [INFO] Sum of grad norms: 1.508381
2019-03-18 23:26:39,852 [INFO] ---------------------------------
2019-03-18 23:26:56,210 [INFO] ---------------------------------
2019-03-18 23:26:56,211 [INFO] Summary:
2019-03-18 23:26:56,212 [INFO] Batch 2000, worst loss 0.200775 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-18 23:26:56,212 [INFO] Regularization: 8310.504883 * 0.0000010000 = 0.0083105052
2019-03-18 23:26:56,213 [INFO] Sum of grad norms: 4.553490
2019-03-18 23:26:56,214 [INFO] ---------------------------------
2019-03-18 23:27:15,000 [INFO] ---------------------------------
2019-03-18 23:27:15,001 [INFO] Summary:
2019-03-18 23:27:15,002 [INFO] Batch 3000, worst loss 0.089411 (incl. reg.) of 1000 batches, learning rate 0.002008 @cl.-depth 1
2019-03-18 23:27:15,002 [INFO] Regularization: 7493.996094 * 0.0000010000 = 0.0074939961
2019-03-18 23:27:15,003 [INFO] Sum of grad norms: 2.329612
2019-03-18 23:27:15,003 [INFO] ---------------------------------
2019-03-18 23:27:34,082 [INFO] ---------------------------------
2019-03-18 23:27:34,083 [INFO] Summary:
2019-03-18 23:27:34,084 [INFO] Batch 4000, worst loss 0.077878 (incl. reg.) of 1000 batches, learning rate 0.000894 @cl.-depth 1
2019-03-18 23:27:34,084 [INFO] Regularization: 6978.020508 * 0.0000010000 = 0.0069780205
2019-03-18 23:27:34,085 [INFO] Sum of grad norms: 0.727045
2019-03-18 23:27:34,085 [INFO] ---------------------------------
2019-03-18 23:27:52,810 [INFO] ---------------------------------
2019-03-18 23:27:52,811 [INFO] Summary:
2019-03-18 23:27:52,811 [INFO] Batch 5000, worst loss 0.073986 (incl. reg.) of 1000 batches, learning rate 0.000779 @cl.-depth 1
2019-03-18 23:27:52,812 [INFO] Regularization: 6549.078125 * 0.0000010000 = 0.0065490780
2019-03-18 23:27:52,812 [INFO] Sum of grad norms: 0.390104
2019-03-18 23:27:52,813 [INFO] ---------------------------------
2019-03-18 23:28:12,099 [INFO] ---------------------------------
2019-03-18 23:28:12,100 [INFO] Summary:
2019-03-18 23:28:12,101 [INFO] Batch 6000, worst loss 0.073460 (incl. reg.) of 1000 batches, learning rate 0.000740 @cl.-depth 1
2019-03-18 23:28:12,101 [INFO] Regularization: 6217.808105 * 0.0000010000 = 0.0062178080
2019-03-18 23:28:12,102 [INFO] Sum of grad norms: 0.141837
2019-03-18 23:28:12,102 [INFO] ---------------------------------
2019-03-18 23:28:30,881 [INFO] ---------------------------------
2019-03-18 23:28:30,882 [INFO] Summary:
2019-03-18 23:28:30,883 [INFO] Batch 7000, worst loss 0.072460 (incl. reg.) of 1000 batches, learning rate 0.000735 @cl.-depth 1
2019-03-18 23:28:30,883 [INFO] Regularization: 5936.084961 * 0.0000010000 = 0.0059360848
2019-03-18 23:28:30,884 [INFO] Sum of grad norms: 0.482290
2019-03-18 23:28:30,885 [INFO] ---------------------------------
2019-03-18 23:28:49,893 [INFO] ---------------------------------
2019-03-18 23:28:49,894 [INFO] Summary:
2019-03-18 23:28:49,894 [INFO] Batch 8000, worst loss 0.071702 (incl. reg.) of 1000 batches, learning rate 0.000725 @cl.-depth 1
2019-03-18 23:28:49,895 [INFO] Regularization: 5700.311523 * 0.0000010000 = 0.0057003116
2019-03-18 23:28:49,896 [INFO] Sum of grad norms: 0.291927
2019-03-18 23:28:49,896 [INFO] ---------------------------------
2019-03-18 23:29:08,697 [INFO] ---------------------------------
2019-03-18 23:29:08,698 [INFO] Summary:
2019-03-18 23:29:08,699 [INFO] Batch 9000, worst loss 0.071012 (incl. reg.) of 1000 batches, learning rate 0.000717 @cl.-depth 1
2019-03-18 23:29:08,700 [INFO] Regularization: 5506.894531 * 0.0000010000 = 0.0055068946
2019-03-18 23:29:08,702 [INFO] Sum of grad norms: 0.321844
2019-03-18 23:29:08,703 [INFO] ---------------------------------
2019-03-18 23:29:27,628 [INFO] ---------------------------------
2019-03-18 23:29:27,629 [INFO] Summary:
2019-03-18 23:29:27,630 [INFO] Batch 10000, worst loss 0.070571 (incl. reg.) of 1000 batches, learning rate 0.000710 @cl.-depth 1
2019-03-18 23:29:27,631 [INFO] Regularization: 5351.425781 * 0.0000010000 = 0.0053514256
2019-03-18 23:29:27,631 [INFO] Sum of grad norms: 0.577301
2019-03-18 23:29:27,632 [INFO] ---------------------------------
2019-03-18 23:29:32,516 [INFO] ---------------------------------
2019-03-18 23:29:32,517 [INFO] Evaluation:
2019-03-18 23:29:32,517 [INFO] Batch 10000, worst loss 0.063935 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:29:32,519 [INFO] ---------------------------------
2019-03-18 23:29:51,537 [INFO] ---------------------------------
2019-03-18 23:29:51,538 [INFO] Summary:
2019-03-18 23:29:51,539 [INFO] Batch 11000, worst loss 0.069661 (incl. reg.) of 1000 batches, learning rate 0.000706 @cl.-depth 1
2019-03-18 23:29:51,539 [INFO] Regularization: 5229.377930 * 0.0000010000 = 0.0052293781
2019-03-18 23:29:51,540 [INFO] Sum of grad norms: 0.095881
2019-03-18 23:29:51,541 [INFO] ---------------------------------
2019-03-18 23:30:11,058 [INFO] ---------------------------------
2019-03-18 23:30:11,059 [INFO] Summary:
2019-03-18 23:30:11,060 [INFO] Batch 12000, worst loss 0.071002 (incl. reg.) of 1000 batches, learning rate 0.000697 @cl.-depth 1
2019-03-18 23:30:11,060 [INFO] Regularization: 5122.776367 * 0.0000010000 = 0.0051227761
2019-03-18 23:30:11,061 [INFO] Sum of grad norms: 0.631274
2019-03-18 23:30:11,061 [INFO] ---------------------------------
2019-03-18 23:30:29,821 [INFO] ---------------------------------
2019-03-18 23:30:29,822 [INFO] Summary:
2019-03-18 23:30:29,823 [INFO] Batch 13000, worst loss 0.068190 (incl. reg.) of 1000 batches, learning rate 0.000697 @cl.-depth 1
2019-03-18 23:30:29,824 [INFO] Regularization: 5033.494629 * 0.0000010000 = 0.0050334944
2019-03-18 23:30:29,824 [INFO] Sum of grad norms: 0.147983
2019-03-18 23:30:29,825 [INFO] ---------------------------------
2019-03-18 23:30:48,506 [INFO] ---------------------------------
2019-03-18 23:30:48,507 [INFO] Summary:
2019-03-18 23:30:48,507 [INFO] Batch 14000, worst loss 0.068067 (incl. reg.) of 1000 batches, learning rate 0.000682 @cl.-depth 1
2019-03-18 23:30:48,508 [INFO] Regularization: 4951.086426 * 0.0000010000 = 0.0049510864
2019-03-18 23:30:48,508 [INFO] Sum of grad norms: 0.264337
2019-03-18 23:30:48,509 [INFO] ---------------------------------
2019-03-18 23:31:07,533 [INFO] ---------------------------------
2019-03-18 23:31:07,534 [INFO] Summary:
2019-03-18 23:31:07,534 [INFO] Batch 15000, worst loss 0.067996 (incl. reg.) of 1000 batches, learning rate 0.000681 @cl.-depth 1
2019-03-18 23:31:07,535 [INFO] Regularization: 4885.708984 * 0.0000010000 = 0.0048857089
2019-03-18 23:31:07,535 [INFO] Sum of grad norms: 0.440534
2019-03-18 23:31:07,536 [INFO] ---------------------------------
2019-03-18 23:31:26,730 [INFO] ---------------------------------
2019-03-18 23:31:26,731 [INFO] Summary:
2019-03-18 23:31:26,732 [INFO] Batch 16000, worst loss 0.067746 (incl. reg.) of 1000 batches, learning rate 0.000680 @cl.-depth 1
2019-03-18 23:31:26,732 [INFO] Regularization: 4820.976562 * 0.0000010000 = 0.0048209764
2019-03-18 23:31:26,733 [INFO] Sum of grad norms: 0.332250
2019-03-18 23:31:26,734 [INFO] ---------------------------------
2019-03-18 23:31:45,870 [INFO] ---------------------------------
2019-03-18 23:31:45,871 [INFO] Summary:
2019-03-18 23:31:45,872 [INFO] Batch 17000, worst loss 0.067778 (incl. reg.) of 1000 batches, learning rate 0.000677 @cl.-depth 1
2019-03-18 23:31:45,872 [INFO] Regularization: 4750.607910 * 0.0000010000 = 0.0047506080
2019-03-18 23:31:45,873 [INFO] Sum of grad norms: 0.426527
2019-03-18 23:31:45,874 [INFO] ---------------------------------
2019-03-18 23:32:04,794 [INFO] ---------------------------------
2019-03-18 23:32:04,795 [INFO] Summary:
2019-03-18 23:32:04,795 [INFO] Batch 18000, worst loss 0.068481 (incl. reg.) of 1000 batches, learning rate 0.000677 @cl.-depth 1
2019-03-18 23:32:04,796 [INFO] Regularization: 4707.982910 * 0.0000010000 = 0.0047079828
2019-03-18 23:32:04,796 [INFO] Sum of grad norms: 1.378441
2019-03-18 23:32:04,797 [INFO] ---------------------------------
2019-03-18 23:32:23,493 [INFO] ---------------------------------
2019-03-18 23:32:23,494 [INFO] Summary:
2019-03-18 23:32:23,496 [INFO] Batch 19000, worst loss 0.066748 (incl. reg.) of 1000 batches, learning rate 0.000677 @cl.-depth 1
2019-03-18 23:32:23,496 [INFO] Regularization: 4645.926270 * 0.0000010000 = 0.0046459264
2019-03-18 23:32:23,497 [INFO] Sum of grad norms: 0.998688
2019-03-18 23:32:23,498 [INFO] ---------------------------------
2019-03-18 23:32:42,896 [INFO] ---------------------------------
2019-03-18 23:32:42,897 [INFO] Summary:
2019-03-18 23:32:42,898 [INFO] Batch 20000, worst loss 0.068477 (incl. reg.) of 1000 batches, learning rate 0.000667 @cl.-depth 1
2019-03-18 23:32:42,898 [INFO] Regularization: 4593.487793 * 0.0000010000 = 0.0045934878
2019-03-18 23:32:42,899 [INFO] Sum of grad norms: 1.600039
2019-03-18 23:32:42,899 [INFO] ---------------------------------
2019-03-18 23:32:47,806 [INFO] ---------------------------------
2019-03-18 23:32:47,807 [INFO] Evaluation:
2019-03-18 23:32:47,808 [INFO] Batch 20000, worst loss 0.063029 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:32:47,808 [INFO] ---------------------------------
2019-03-18 23:33:07,039 [INFO] ---------------------------------
2019-03-18 23:33:07,040 [INFO] Summary:
2019-03-18 23:33:07,040 [INFO] Batch 21000, worst loss 0.065895 (incl. reg.) of 1000 batches, learning rate 0.000667 @cl.-depth 1
2019-03-18 23:33:07,041 [INFO] Regularization: 4531.901855 * 0.0000010000 = 0.0045319018
2019-03-18 23:33:07,041 [INFO] Sum of grad norms: 0.090383
2019-03-18 23:33:07,042 [INFO] ---------------------------------
2019-03-18 23:33:25,628 [INFO] ---------------------------------
2019-03-18 23:33:25,629 [INFO] Summary:
2019-03-18 23:33:25,629 [INFO] Batch 22000, worst loss 0.067823 (incl. reg.) of 1000 batches, learning rate 0.000659 @cl.-depth 1
2019-03-18 23:33:25,630 [INFO] Regularization: 4472.495605 * 0.0000010000 = 0.0044724955
2019-03-18 23:33:25,630 [INFO] Sum of grad norms: 0.913249
2019-03-18 23:33:25,631 [INFO] ---------------------------------
2019-03-18 23:33:44,613 [INFO] ---------------------------------
2019-03-18 23:33:44,614 [INFO] Summary:
2019-03-18 23:33:44,614 [INFO] Batch 23000, worst loss 0.066091 (incl. reg.) of 1000 batches, learning rate 0.000659 @cl.-depth 1
2019-03-18 23:33:44,615 [INFO] Regularization: 4408.656738 * 0.0000010000 = 0.0044086566
2019-03-18 23:33:44,615 [INFO] Sum of grad norms: 0.321448
2019-03-18 23:33:44,616 [INFO] ---------------------------------
2019-03-18 23:34:03,477 [INFO] ---------------------------------
2019-03-18 23:34:03,478 [INFO] Summary:
2019-03-18 23:34:03,479 [INFO] Batch 24000, worst loss 0.066006 (incl. reg.) of 1000 batches, learning rate 0.000659 @cl.-depth 1
2019-03-18 23:34:03,479 [INFO] Regularization: 4359.908203 * 0.0000010000 = 0.0043599084
2019-03-18 23:34:03,480 [INFO] Sum of grad norms: 0.206814
2019-03-18 23:34:03,480 [INFO] ---------------------------------
2019-03-18 23:34:22,753 [INFO] ---------------------------------
2019-03-18 23:34:22,754 [INFO] Summary:
2019-03-18 23:34:22,755 [INFO] Batch 25000, worst loss 0.066862 (incl. reg.) of 1000 batches, learning rate 0.000659 @cl.-depth 1
2019-03-18 23:34:22,755 [INFO] Regularization: 4303.060547 * 0.0000010000 = 0.0043030605
2019-03-18 23:34:22,756 [INFO] Sum of grad norms: 0.766943
2019-03-18 23:34:22,757 [INFO] ---------------------------------
2019-03-18 23:34:41,671 [INFO] ---------------------------------
2019-03-18 23:34:41,672 [INFO] Summary:
2019-03-18 23:34:41,673 [INFO] Batch 26000, worst loss 0.066720 (incl. reg.) of 1000 batches, learning rate 0.000659 @cl.-depth 1
2019-03-18 23:34:41,673 [INFO] Regularization: 4247.794434 * 0.0000010000 = 0.0042477944
2019-03-18 23:34:41,674 [INFO] Sum of grad norms: 0.702604
2019-03-18 23:34:41,675 [INFO] ---------------------------------
2019-03-18 23:35:00,515 [INFO] ---------------------------------
2019-03-18 23:35:00,516 [INFO] Summary:
2019-03-18 23:35:00,516 [INFO] Batch 27000, worst loss 0.065987 (incl. reg.) of 1000 batches, learning rate 0.000659 @cl.-depth 1
2019-03-18 23:35:00,517 [INFO] Regularization: 4193.131836 * 0.0000010000 = 0.0041931318
2019-03-18 23:35:00,517 [INFO] Sum of grad norms: 0.233911
2019-03-18 23:35:00,518 [INFO] ---------------------------------
2019-03-18 23:35:19,491 [INFO] ---------------------------------
2019-03-18 23:35:19,491 [INFO] Summary:
2019-03-18 23:35:19,492 [INFO] Batch 28000, worst loss 0.064936 (incl. reg.) of 1000 batches, learning rate 0.000659 @cl.-depth 1
2019-03-18 23:35:19,493 [INFO] Regularization: 4145.941406 * 0.0000010000 = 0.0041459412
2019-03-18 23:35:19,493 [INFO] Sum of grad norms: 0.383693
2019-03-18 23:35:19,494 [INFO] ---------------------------------
2019-03-18 23:35:38,193 [INFO] ---------------------------------
2019-03-18 23:35:38,194 [INFO] Summary:
2019-03-18 23:35:38,195 [INFO] Batch 29000, worst loss 0.064525 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-18 23:35:38,196 [INFO] Regularization: 4084.263184 * 0.0000010000 = 0.0040842630
2019-03-18 23:35:38,196 [INFO] Sum of grad norms: 0.817834
2019-03-18 23:35:38,197 [INFO] ---------------------------------
2019-03-18 23:35:57,300 [INFO] ---------------------------------
2019-03-18 23:35:57,301 [INFO] Summary:
2019-03-18 23:35:57,302 [INFO] Batch 30000, worst loss 0.064419 (incl. reg.) of 1000 batches, learning rate 0.000645 @cl.-depth 1
2019-03-18 23:35:57,303 [INFO] Regularization: 4032.703369 * 0.0000010000 = 0.0040327036
2019-03-18 23:35:57,303 [INFO] Sum of grad norms: 0.147718
2019-03-18 23:35:57,304 [INFO] ---------------------------------
2019-03-18 23:36:02,232 [INFO] ---------------------------------
2019-03-18 23:36:02,234 [INFO] Evaluation:
2019-03-18 23:36:02,235 [INFO] Batch 30000, worst loss 0.059653 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:36:02,235 [INFO] ---------------------------------
2019-03-18 23:36:21,499 [INFO] ---------------------------------
2019-03-18 23:36:21,500 [INFO] Summary:
2019-03-18 23:36:21,501 [INFO] Batch 31000, worst loss 0.063811 (incl. reg.) of 1000 batches, learning rate 0.000644 @cl.-depth 1
2019-03-18 23:36:21,501 [INFO] Regularization: 3987.601562 * 0.0000010000 = 0.0039876015
2019-03-18 23:36:21,502 [INFO] Sum of grad norms: 0.304133
2019-03-18 23:36:21,502 [INFO] ---------------------------------
2019-03-18 23:36:40,115 [INFO] ---------------------------------
2019-03-18 23:36:40,116 [INFO] Summary:
2019-03-18 23:36:40,116 [INFO] Batch 32000, worst loss 0.064770 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-18 23:36:40,117 [INFO] Regularization: 3930.332275 * 0.0000010000 = 0.0039303321
2019-03-18 23:36:40,117 [INFO] Sum of grad norms: 0.072138
2019-03-18 23:36:40,118 [INFO] ---------------------------------
2019-03-18 23:36:58,810 [INFO] ---------------------------------
2019-03-18 23:36:58,811 [INFO] Summary:
2019-03-18 23:36:58,812 [INFO] Batch 33000, worst loss 0.064612 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-18 23:36:58,812 [INFO] Regularization: 3884.378662 * 0.0000010000 = 0.0038843786
2019-03-18 23:36:58,813 [INFO] Sum of grad norms: 0.849047
2019-03-18 23:36:58,813 [INFO] ---------------------------------
2019-03-18 23:37:17,592 [INFO] ---------------------------------
2019-03-18 23:37:17,593 [INFO] Summary:
2019-03-18 23:37:17,593 [INFO] Batch 34000, worst loss 0.064912 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-18 23:37:17,594 [INFO] Regularization: 3826.962646 * 0.0000010000 = 0.0038269626
2019-03-18 23:37:17,594 [INFO] Sum of grad norms: 0.123960
2019-03-18 23:37:17,595 [INFO] ---------------------------------
2019-03-18 23:37:36,445 [INFO] ---------------------------------
2019-03-18 23:37:36,446 [INFO] Summary:
2019-03-18 23:37:36,447 [INFO] Batch 35000, worst loss 0.063700 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-18 23:37:36,447 [INFO] Regularization: 3781.922363 * 0.0000010000 = 0.0037819224
2019-03-18 23:37:36,448 [INFO] Sum of grad norms: 0.644728
2019-03-18 23:37:36,449 [INFO] ---------------------------------
2019-03-18 23:37:55,455 [INFO] ---------------------------------
2019-03-18 23:37:55,456 [INFO] Summary:
2019-03-18 23:37:55,456 [INFO] Batch 36000, worst loss 0.064072 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-18 23:37:55,457 [INFO] Regularization: 3735.398682 * 0.0000010000 = 0.0037353986
2019-03-18 23:37:55,457 [INFO] Sum of grad norms: 0.279743
2019-03-18 23:37:55,458 [INFO] ---------------------------------
2019-03-18 23:38:14,144 [INFO] ---------------------------------
2019-03-18 23:38:14,145 [INFO] Summary:
2019-03-18 23:38:14,146 [INFO] Batch 37000, worst loss 0.063181 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-18 23:38:14,146 [INFO] Regularization: 3683.934326 * 0.0000010000 = 0.0036839342
2019-03-18 23:38:14,147 [INFO] Sum of grad norms: 0.389218
2019-03-18 23:38:14,147 [INFO] ---------------------------------
2019-03-18 23:38:32,645 [INFO] ---------------------------------
2019-03-18 23:38:32,646 [INFO] Summary:
2019-03-18 23:38:32,647 [INFO] Batch 38000, worst loss 0.063242 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-18 23:38:32,647 [INFO] Regularization: 3640.206299 * 0.0000010000 = 0.0036402063
2019-03-18 23:38:32,648 [INFO] Sum of grad norms: 0.063910
2019-03-18 23:38:32,649 [INFO] ---------------------------------
2019-03-18 23:38:51,600 [INFO] ---------------------------------
2019-03-18 23:38:51,601 [INFO] Summary:
2019-03-18 23:38:51,601 [INFO] Batch 39000, worst loss 0.063672 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-18 23:38:51,602 [INFO] Regularization: 3595.895508 * 0.0000010000 = 0.0035958956
2019-03-18 23:38:51,602 [INFO] Sum of grad norms: 0.203790
2019-03-18 23:38:51,603 [INFO] ---------------------------------
2019-03-18 23:39:10,396 [INFO] ---------------------------------
2019-03-18 23:39:10,397 [INFO] Summary:
2019-03-18 23:39:10,398 [INFO] Batch 40000, worst loss 0.063220 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-18 23:39:10,399 [INFO] Regularization: 3543.402588 * 0.0000010000 = 0.0035434025
2019-03-18 23:39:10,400 [INFO] Sum of grad norms: 0.065336
2019-03-18 23:39:10,400 [INFO] ---------------------------------
2019-03-18 23:39:15,300 [INFO] ---------------------------------
2019-03-18 23:39:15,301 [INFO] Evaluation:
2019-03-18 23:39:15,302 [INFO] Batch 40000, worst loss 0.059404 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:39:15,303 [INFO] ---------------------------------
2019-03-18 23:39:34,167 [INFO] ---------------------------------
2019-03-18 23:39:34,168 [INFO] Summary:
2019-03-18 23:39:34,169 [INFO] Batch 41000, worst loss 0.063865 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-18 23:39:34,169 [INFO] Regularization: 3497.465576 * 0.0000010000 = 0.0034974655
2019-03-18 23:39:34,170 [INFO] Sum of grad norms: 0.079099
2019-03-18 23:39:34,170 [INFO] ---------------------------------
2019-03-18 23:39:52,953 [INFO] ---------------------------------
2019-03-18 23:39:52,954 [INFO] Summary:
2019-03-18 23:39:52,954 [INFO] Batch 42000, worst loss 0.062315 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-18 23:39:52,955 [INFO] Regularization: 3466.132568 * 0.0000010000 = 0.0034661326
2019-03-18 23:39:52,955 [INFO] Sum of grad norms: 0.151252
2019-03-18 23:39:52,956 [INFO] ---------------------------------
2019-03-18 23:40:12,134 [INFO] ---------------------------------
2019-03-18 23:40:12,135 [INFO] Summary:
2019-03-18 23:40:12,135 [INFO] Batch 43000, worst loss 0.062199 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-18 23:40:12,136 [INFO] Regularization: 3439.024902 * 0.0000010000 = 0.0034390248
2019-03-18 23:40:12,136 [INFO] Sum of grad norms: 0.236681
2019-03-18 23:40:12,137 [INFO] ---------------------------------
2019-03-18 23:40:31,061 [INFO] ---------------------------------
2019-03-18 23:40:31,062 [INFO] Summary:
2019-03-18 23:40:31,062 [INFO] Batch 44000, worst loss 0.062042 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-18 23:40:31,063 [INFO] Regularization: 3407.656494 * 0.0000010000 = 0.0034076564
2019-03-18 23:40:31,063 [INFO] Sum of grad norms: 0.381655
2019-03-18 23:40:31,064 [INFO] ---------------------------------
2019-03-18 23:40:49,799 [INFO] ---------------------------------
2019-03-18 23:40:49,800 [INFO] Summary:
2019-03-18 23:40:49,800 [INFO] Batch 45000, worst loss 0.062341 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-18 23:40:49,801 [INFO] Regularization: 3381.281250 * 0.0000010000 = 0.0033812812
2019-03-18 23:40:49,801 [INFO] Sum of grad norms: 0.093879
2019-03-18 23:40:49,802 [INFO] ---------------------------------
2019-03-18 23:41:08,759 [INFO] ---------------------------------
2019-03-18 23:41:08,760 [INFO] Summary:
2019-03-18 23:41:08,760 [INFO] Batch 46000, worst loss 0.062416 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-18 23:41:08,761 [INFO] Regularization: 3352.922119 * 0.0000010000 = 0.0033529222
2019-03-18 23:41:08,761 [INFO] Sum of grad norms: 0.068787
2019-03-18 23:41:08,762 [INFO] ---------------------------------
2019-03-18 23:41:27,811 [INFO] ---------------------------------
2019-03-18 23:41:27,812 [INFO] Summary:
2019-03-18 23:41:27,813 [INFO] Batch 47000, worst loss 0.062054 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-18 23:41:27,813 [INFO] Regularization: 3325.388428 * 0.0000010000 = 0.0033253885
2019-03-18 23:41:27,814 [INFO] Sum of grad norms: 0.253232
2019-03-18 23:41:27,815 [INFO] ---------------------------------
2019-03-18 23:41:46,911 [INFO] ---------------------------------
2019-03-18 23:41:46,912 [INFO] Summary:
2019-03-18 23:41:46,913 [INFO] Batch 48000, worst loss 0.061894 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-18 23:41:46,913 [INFO] Regularization: 3299.647217 * 0.0000010000 = 0.0032996472
2019-03-18 23:41:46,913 [INFO] Sum of grad norms: 0.090161
2019-03-18 23:41:46,914 [INFO] ---------------------------------
2019-03-18 23:42:05,595 [INFO] ---------------------------------
2019-03-18 23:42:05,596 [INFO] Summary:
2019-03-18 23:42:05,596 [INFO] Batch 49000, worst loss 0.061816 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-18 23:42:05,597 [INFO] Regularization: 3275.756348 * 0.0000010000 = 0.0032757563
2019-03-18 23:42:05,597 [INFO] Sum of grad norms: 0.208948
2019-03-18 23:42:05,598 [INFO] ---------------------------------
2019-03-18 23:42:24,245 [INFO] ---------------------------------
2019-03-18 23:42:24,246 [INFO] Summary:
2019-03-18 23:42:24,246 [INFO] Batch 50000, worst loss 0.062175 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-18 23:42:24,247 [INFO] Regularization: 3250.448242 * 0.0000010000 = 0.0032504483
2019-03-18 23:42:24,247 [INFO] Sum of grad norms: 0.030155
2019-03-18 23:42:24,248 [INFO] ---------------------------------
2019-03-18 23:42:29,200 [INFO] ---------------------------------
2019-03-18 23:42:29,201 [INFO] Evaluation:
2019-03-18 23:42:29,201 [INFO] Batch 50000, worst loss 0.058533 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:42:29,204 [INFO] ---------------------------------
2019-03-18 23:42:48,079 [INFO] ---------------------------------
2019-03-18 23:42:48,080 [INFO] Summary:
2019-03-18 23:42:48,081 [INFO] Batch 51000, worst loss 0.061781 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-18 23:42:48,082 [INFO] Regularization: 3226.520752 * 0.0000010000 = 0.0032265207
2019-03-18 23:42:48,082 [INFO] Sum of grad norms: 0.023363
2019-03-18 23:42:48,083 [INFO] ---------------------------------
2019-03-18 23:43:06,878 [INFO] ---------------------------------
2019-03-18 23:43:06,879 [INFO] Summary:
2019-03-18 23:43:06,879 [INFO] Batch 52000, worst loss 0.061729 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-18 23:43:06,880 [INFO] Regularization: 3209.421875 * 0.0000010000 = 0.0032094219
2019-03-18 23:43:06,880 [INFO] Sum of grad norms: 0.101181
2019-03-18 23:43:06,881 [INFO] ---------------------------------
2019-03-18 23:43:25,284 [INFO] ---------------------------------
2019-03-18 23:43:25,285 [INFO] Summary:
2019-03-18 23:43:25,285 [INFO] Batch 53000, worst loss 0.061565 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-18 23:43:25,286 [INFO] Regularization: 3196.361084 * 0.0000010000 = 0.0031963610
2019-03-18 23:43:25,286 [INFO] Sum of grad norms: 0.111947
2019-03-18 23:43:25,287 [INFO] ---------------------------------
2019-03-18 23:43:43,947 [INFO] ---------------------------------
2019-03-18 23:43:43,947 [INFO] Summary:
2019-03-18 23:43:43,948 [INFO] Batch 54000, worst loss 0.061489 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-18 23:43:43,949 [INFO] Regularization: 3181.672119 * 0.0000010000 = 0.0031816722
2019-03-18 23:43:43,950 [INFO] Sum of grad norms: 0.067668
2019-03-18 23:43:43,950 [INFO] ---------------------------------
2019-03-18 23:44:02,959 [INFO] ---------------------------------
2019-03-18 23:44:02,960 [INFO] Summary:
2019-03-18 23:44:02,961 [INFO] Batch 55000, worst loss 0.061317 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-18 23:44:02,961 [INFO] Regularization: 3167.969971 * 0.0000010000 = 0.0031679699
2019-03-18 23:44:02,962 [INFO] Sum of grad norms: 0.110121
2019-03-18 23:44:02,963 [INFO] ---------------------------------
2019-03-18 23:44:21,745 [INFO] ---------------------------------
2019-03-18 23:44:21,746 [INFO] Summary:
2019-03-18 23:44:21,746 [INFO] Batch 56000, worst loss 0.061574 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-18 23:44:21,747 [INFO] Regularization: 3154.968018 * 0.0000010000 = 0.0031549679
2019-03-18 23:44:21,747 [INFO] Sum of grad norms: 0.032336
2019-03-18 23:44:21,748 [INFO] ---------------------------------
2019-03-18 23:44:40,582 [INFO] ---------------------------------
2019-03-18 23:44:40,583 [INFO] Summary:
2019-03-18 23:44:40,583 [INFO] Batch 57000, worst loss 0.061575 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-18 23:44:40,584 [INFO] Regularization: 3140.764404 * 0.0000010000 = 0.0031407643
2019-03-18 23:44:40,584 [INFO] Sum of grad norms: 0.177551
2019-03-18 23:44:40,585 [INFO] ---------------------------------
2019-03-18 23:44:59,077 [INFO] ---------------------------------
2019-03-18 23:44:59,078 [INFO] Summary:
2019-03-18 23:44:59,079 [INFO] Batch 58000, worst loss 0.061478 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-18 23:44:59,079 [INFO] Regularization: 3128.306152 * 0.0000010000 = 0.0031283062
2019-03-18 23:44:59,080 [INFO] Sum of grad norms: 0.198472
2019-03-18 23:44:59,080 [INFO] ---------------------------------
2019-03-18 23:45:18,220 [INFO] ---------------------------------
2019-03-18 23:45:18,221 [INFO] Summary:
2019-03-18 23:45:18,222 [INFO] Batch 59000, worst loss 0.061490 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-18 23:45:18,222 [INFO] Regularization: 3114.701660 * 0.0000010000 = 0.0031147017
2019-03-18 23:45:18,223 [INFO] Sum of grad norms: 0.120132
2019-03-18 23:45:18,223 [INFO] ---------------------------------
2019-03-18 23:45:36,822 [INFO] ---------------------------------
2019-03-18 23:45:36,823 [INFO] Summary:
2019-03-18 23:45:36,824 [INFO] Batch 60000, worst loss 0.061943 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-18 23:45:36,824 [INFO] Regularization: 3101.375977 * 0.0000010000 = 0.0031013759
2019-03-18 23:45:36,825 [INFO] Sum of grad norms: 0.109269
2019-03-18 23:45:36,825 [INFO] ---------------------------------
2019-03-18 23:45:41,727 [INFO] ---------------------------------
2019-03-18 23:45:41,728 [INFO] Evaluation:
2019-03-18 23:45:41,729 [INFO] Batch 60000, worst loss 0.058298 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:45:41,731 [INFO] ---------------------------------
2019-03-18 23:46:00,673 [INFO] ---------------------------------
2019-03-18 23:46:00,674 [INFO] Summary:
2019-03-18 23:46:00,674 [INFO] Batch 61000, worst loss 0.061273 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-18 23:46:00,675 [INFO] Regularization: 3087.918701 * 0.0000010000 = 0.0030879187
2019-03-18 23:46:00,675 [INFO] Sum of grad norms: 0.093614
2019-03-18 23:46:00,676 [INFO] ---------------------------------
2019-03-18 23:46:19,225 [INFO] ---------------------------------
2019-03-18 23:46:19,226 [INFO] Summary:
2019-03-18 23:46:19,226 [INFO] Batch 62000, worst loss 0.061283 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-18 23:46:19,227 [INFO] Regularization: 3079.959961 * 0.0000010000 = 0.0030799599
2019-03-18 23:46:19,228 [INFO] Sum of grad norms: 0.067240
2019-03-18 23:46:19,228 [INFO] ---------------------------------
2019-03-18 23:46:38,136 [INFO] ---------------------------------
2019-03-18 23:46:38,137 [INFO] Summary:
2019-03-18 23:46:38,137 [INFO] Batch 63000, worst loss 0.061354 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-18 23:46:38,138 [INFO] Regularization: 3072.155762 * 0.0000010000 = 0.0030721556
2019-03-18 23:46:38,138 [INFO] Sum of grad norms: 0.052961
2019-03-18 23:46:38,139 [INFO] ---------------------------------
2019-03-18 23:46:56,791 [INFO] ---------------------------------
2019-03-18 23:46:56,792 [INFO] Summary:
2019-03-18 23:46:56,793 [INFO] Batch 64000, worst loss 0.061067 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-18 23:46:56,793 [INFO] Regularization: 3065.500000 * 0.0000010000 = 0.0030654999
2019-03-18 23:46:56,794 [INFO] Sum of grad norms: 0.141352
2019-03-18 23:46:56,795 [INFO] ---------------------------------
2019-03-18 23:47:15,422 [INFO] ---------------------------------
2019-03-18 23:47:15,423 [INFO] Summary:
2019-03-18 23:47:15,424 [INFO] Batch 65000, worst loss 0.061384 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-18 23:47:15,424 [INFO] Regularization: 3058.002686 * 0.0000010000 = 0.0030580026
2019-03-18 23:47:15,425 [INFO] Sum of grad norms: 0.058092
2019-03-18 23:47:15,426 [INFO] ---------------------------------
2019-03-18 23:47:34,107 [INFO] ---------------------------------
2019-03-18 23:47:34,109 [INFO] Summary:
2019-03-18 23:47:34,109 [INFO] Batch 66000, worst loss 0.061418 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-18 23:47:34,110 [INFO] Regularization: 3051.224609 * 0.0000010000 = 0.0030512246
2019-03-18 23:47:34,110 [INFO] Sum of grad norms: 0.105333
2019-03-18 23:47:34,111 [INFO] ---------------------------------
2019-03-18 23:47:52,614 [INFO] ---------------------------------
2019-03-18 23:47:52,615 [INFO] Summary:
2019-03-18 23:47:52,615 [INFO] Batch 67000, worst loss 0.061120 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-18 23:47:52,616 [INFO] Regularization: 3044.861816 * 0.0000010000 = 0.0030448618
2019-03-18 23:47:52,616 [INFO] Sum of grad norms: 0.124317
2019-03-18 23:47:52,617 [INFO] ---------------------------------
2019-03-18 23:48:11,469 [INFO] ---------------------------------
2019-03-18 23:48:11,470 [INFO] Summary:
2019-03-18 23:48:11,471 [INFO] Batch 68000, worst loss 0.061597 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-18 23:48:11,471 [INFO] Regularization: 3037.919678 * 0.0000010000 = 0.0030379198
2019-03-18 23:48:11,472 [INFO] Sum of grad norms: 0.053837
2019-03-18 23:48:11,472 [INFO] ---------------------------------
2019-03-18 23:48:30,069 [INFO] ---------------------------------
2019-03-18 23:48:30,070 [INFO] Summary:
2019-03-18 23:48:30,071 [INFO] Batch 69000, worst loss 0.061172 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-18 23:48:30,072 [INFO] Regularization: 3030.960938 * 0.0000010000 = 0.0030309609
2019-03-18 23:48:30,072 [INFO] Sum of grad norms: 0.103098
2019-03-18 23:48:30,073 [INFO] ---------------------------------
2019-03-18 23:48:48,601 [INFO] ---------------------------------
2019-03-18 23:48:48,602 [INFO] Summary:
2019-03-18 23:48:48,603 [INFO] Batch 70000, worst loss 0.061233 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-18 23:48:48,603 [INFO] Regularization: 3024.549561 * 0.0000010000 = 0.0030245495
2019-03-18 23:48:48,604 [INFO] Sum of grad norms: 0.035379
2019-03-18 23:48:48,605 [INFO] ---------------------------------
2019-03-18 23:48:53,516 [INFO] ---------------------------------
2019-03-18 23:48:53,517 [INFO] Evaluation:
2019-03-18 23:48:53,519 [INFO] Batch 70000, worst loss 0.058183 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:48:53,520 [INFO] ---------------------------------
2019-03-18 23:49:12,338 [INFO] ---------------------------------
2019-03-18 23:49:12,339 [INFO] Summary:
2019-03-18 23:49:12,340 [INFO] Batch 71000, worst loss 0.061107 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 23:49:12,341 [INFO] Regularization: 3017.296631 * 0.0000010000 = 0.0030172965
2019-03-18 23:49:12,341 [INFO] Sum of grad norms: 0.031223
2019-03-18 23:49:12,342 [INFO] ---------------------------------
2019-03-18 23:49:31,068 [INFO] ---------------------------------
2019-03-18 23:49:31,068 [INFO] Summary:
2019-03-18 23:49:31,069 [INFO] Batch 72000, worst loss 0.061158 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 23:49:31,069 [INFO] Regularization: 3012.532471 * 0.0000010000 = 0.0030125324
2019-03-18 23:49:31,070 [INFO] Sum of grad norms: 0.084062
2019-03-18 23:49:31,071 [INFO] ---------------------------------
2019-03-18 23:49:49,837 [INFO] ---------------------------------
2019-03-18 23:49:49,838 [INFO] Summary:
2019-03-18 23:49:49,839 [INFO] Batch 73000, worst loss 0.061072 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 23:49:49,840 [INFO] Regularization: 3009.138428 * 0.0000010000 = 0.0030091384
2019-03-18 23:49:49,840 [INFO] Sum of grad norms: 0.040385
2019-03-18 23:49:49,841 [INFO] ---------------------------------
2019-03-18 23:50:08,457 [INFO] ---------------------------------
2019-03-18 23:50:08,458 [INFO] Summary:
2019-03-18 23:50:08,459 [INFO] Batch 74000, worst loss 0.061458 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 23:50:08,459 [INFO] Regularization: 3005.386475 * 0.0000010000 = 0.0030053866
2019-03-18 23:50:08,460 [INFO] Sum of grad norms: 0.068610
2019-03-18 23:50:08,460 [INFO] ---------------------------------
2019-03-18 23:50:27,595 [INFO] ---------------------------------
2019-03-18 23:50:27,596 [INFO] Summary:
2019-03-18 23:50:27,596 [INFO] Batch 75000, worst loss 0.061451 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 23:50:27,597 [INFO] Regularization: 3001.756836 * 0.0000010000 = 0.0030017567
2019-03-18 23:50:27,597 [INFO] Sum of grad norms: 0.072921
2019-03-18 23:50:27,598 [INFO] ---------------------------------
2019-03-18 23:50:46,343 [INFO] ---------------------------------
2019-03-18 23:50:46,344 [INFO] Summary:
2019-03-18 23:50:46,344 [INFO] Batch 76000, worst loss 0.061050 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 23:50:46,345 [INFO] Regularization: 2998.021973 * 0.0000010000 = 0.0029980219
2019-03-18 23:50:46,345 [INFO] Sum of grad norms: 0.032617
2019-03-18 23:50:46,346 [INFO] ---------------------------------
2019-03-18 23:51:04,989 [INFO] ---------------------------------
2019-03-18 23:51:04,990 [INFO] Summary:
2019-03-18 23:51:04,990 [INFO] Batch 77000, worst loss 0.061184 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 23:51:04,991 [INFO] Regularization: 2994.126221 * 0.0000010000 = 0.0029941262
2019-03-18 23:51:04,991 [INFO] Sum of grad norms: 0.066841
2019-03-18 23:51:04,992 [INFO] ---------------------------------
2019-03-18 23:51:23,553 [INFO] ---------------------------------
2019-03-18 23:51:23,554 [INFO] Summary:
2019-03-18 23:51:23,555 [INFO] Batch 78000, worst loss 0.061246 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 23:51:23,556 [INFO] Regularization: 2990.801758 * 0.0000010000 = 0.0029908018
2019-03-18 23:51:23,556 [INFO] Sum of grad norms: 0.040385
2019-03-18 23:51:23,557 [INFO] ---------------------------------
2019-03-18 23:51:42,415 [INFO] ---------------------------------
2019-03-18 23:51:42,416 [INFO] Summary:
2019-03-18 23:51:42,417 [INFO] Batch 79000, worst loss 0.061131 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 23:51:42,418 [INFO] Regularization: 2986.851562 * 0.0000010000 = 0.0029868516
2019-03-18 23:51:42,418 [INFO] Sum of grad norms: 0.063033
2019-03-18 23:51:42,419 [INFO] ---------------------------------
2019-03-18 23:52:01,033 [INFO] ---------------------------------
2019-03-18 23:52:01,034 [INFO] Summary:
2019-03-18 23:52:01,035 [INFO] Batch 80000, worst loss 0.061251 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-18 23:52:01,036 [INFO] Regularization: 2983.427734 * 0.0000010000 = 0.0029834278
2019-03-18 23:52:01,036 [INFO] Sum of grad norms: 0.241776
2019-03-18 23:52:01,037 [INFO] ---------------------------------
2019-03-18 23:52:05,978 [INFO] ---------------------------------
2019-03-18 23:52:05,982 [INFO] Evaluation:
2019-03-18 23:52:05,987 [INFO] Batch 80000, worst loss 0.058224 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:52:05,988 [INFO] ---------------------------------
2019-03-18 23:52:24,765 [INFO] ---------------------------------
2019-03-18 23:52:24,766 [INFO] Summary:
2019-03-18 23:52:24,766 [INFO] Batch 81000, worst loss 0.060897 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 23:52:24,767 [INFO] Regularization: 2979.337646 * 0.0000010000 = 0.0029793377
2019-03-18 23:52:24,767 [INFO] Sum of grad norms: 0.031264
2019-03-18 23:52:24,768 [INFO] ---------------------------------
2019-03-18 23:52:43,393 [INFO] ---------------------------------
2019-03-18 23:52:43,394 [INFO] Summary:
2019-03-18 23:52:43,395 [INFO] Batch 82000, worst loss 0.061194 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 23:52:43,396 [INFO] Regularization: 2976.987549 * 0.0000010000 = 0.0029769875
2019-03-18 23:52:43,396 [INFO] Sum of grad norms: 0.095705
2019-03-18 23:52:43,397 [INFO] ---------------------------------
2019-03-18 23:53:02,396 [INFO] ---------------------------------
2019-03-18 23:53:02,398 [INFO] Summary:
2019-03-18 23:53:02,398 [INFO] Batch 83000, worst loss 0.061110 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 23:53:02,399 [INFO] Regularization: 2975.400391 * 0.0000010000 = 0.0029754003
2019-03-18 23:53:02,400 [INFO] Sum of grad norms: 0.026266
2019-03-18 23:53:02,400 [INFO] ---------------------------------
2019-03-18 23:53:21,216 [INFO] ---------------------------------
2019-03-18 23:53:21,217 [INFO] Summary:
2019-03-18 23:53:21,218 [INFO] Batch 84000, worst loss 0.061296 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 23:53:21,218 [INFO] Regularization: 2973.293945 * 0.0000010000 = 0.0029732939
2019-03-18 23:53:21,219 [INFO] Sum of grad norms: 0.031624
2019-03-18 23:53:21,220 [INFO] ---------------------------------
2019-03-18 23:53:40,148 [INFO] ---------------------------------
2019-03-18 23:53:40,149 [INFO] Summary:
2019-03-18 23:53:40,150 [INFO] Batch 85000, worst loss 0.061043 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 23:53:40,151 [INFO] Regularization: 2971.447266 * 0.0000010000 = 0.0029714473
2019-03-18 23:53:40,152 [INFO] Sum of grad norms: 0.052814
2019-03-18 23:53:40,152 [INFO] ---------------------------------
2019-03-18 23:53:59,001 [INFO] ---------------------------------
2019-03-18 23:53:59,002 [INFO] Summary:
2019-03-18 23:53:59,002 [INFO] Batch 86000, worst loss 0.061008 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 23:53:59,003 [INFO] Regularization: 2969.617188 * 0.0000010000 = 0.0029696173
2019-03-18 23:53:59,003 [INFO] Sum of grad norms: 0.068437
2019-03-18 23:53:59,004 [INFO] ---------------------------------
2019-03-18 23:54:17,570 [INFO] ---------------------------------
2019-03-18 23:54:17,571 [INFO] Summary:
2019-03-18 23:54:17,572 [INFO] Batch 87000, worst loss 0.061115 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 23:54:17,572 [INFO] Regularization: 2967.840332 * 0.0000010000 = 0.0029678403
2019-03-18 23:54:17,573 [INFO] Sum of grad norms: 0.051417
2019-03-18 23:54:17,573 [INFO] ---------------------------------
2019-03-18 23:54:36,535 [INFO] ---------------------------------
2019-03-18 23:54:36,536 [INFO] Summary:
2019-03-18 23:54:36,536 [INFO] Batch 88000, worst loss 0.061092 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 23:54:36,537 [INFO] Regularization: 2965.670898 * 0.0000010000 = 0.0029656708
2019-03-18 23:54:36,537 [INFO] Sum of grad norms: 0.114995
2019-03-18 23:54:36,538 [INFO] ---------------------------------
2019-03-18 23:54:55,022 [INFO] ---------------------------------
2019-03-18 23:54:55,024 [INFO] Summary:
2019-03-18 23:54:55,024 [INFO] Batch 89000, worst loss 0.061100 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 23:54:55,025 [INFO] Regularization: 2963.688477 * 0.0000010000 = 0.0029636885
2019-03-18 23:54:55,025 [INFO] Sum of grad norms: 0.088537
2019-03-18 23:54:55,026 [INFO] ---------------------------------
2019-03-18 23:55:13,813 [INFO] ---------------------------------
2019-03-18 23:55:13,814 [INFO] Summary:
2019-03-18 23:55:13,814 [INFO] Batch 90000, worst loss 0.060990 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-18 23:55:13,815 [INFO] Regularization: 2962.092041 * 0.0000010000 = 0.0029620919
2019-03-18 23:55:13,815 [INFO] Sum of grad norms: 0.052353
2019-03-18 23:55:13,816 [INFO] ---------------------------------
2019-03-18 23:55:18,743 [INFO] ---------------------------------
2019-03-18 23:55:18,744 [INFO] Evaluation:
2019-03-18 23:55:18,746 [INFO] Batch 90000, worst loss 0.058050 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:55:18,747 [INFO] ---------------------------------
2019-03-18 23:55:37,409 [INFO] ---------------------------------
2019-03-18 23:55:37,410 [INFO] Summary:
2019-03-18 23:55:37,411 [INFO] Batch 91000, worst loss 0.061142 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 23:55:37,411 [INFO] Regularization: 2960.283936 * 0.0000010000 = 0.0029602840
2019-03-18 23:55:37,412 [INFO] Sum of grad norms: 0.048102
2019-03-18 23:55:37,412 [INFO] ---------------------------------
2019-03-18 23:55:56,371 [INFO] ---------------------------------
2019-03-18 23:55:56,371 [INFO] Summary:
2019-03-18 23:55:56,372 [INFO] Batch 92000, worst loss 0.061129 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 23:55:56,373 [INFO] Regularization: 2958.928711 * 0.0000010000 = 0.0029589287
2019-03-18 23:55:56,373 [INFO] Sum of grad norms: 0.026149
2019-03-18 23:55:56,374 [INFO] ---------------------------------
2019-03-18 23:56:15,121 [INFO] ---------------------------------
2019-03-18 23:56:15,122 [INFO] Summary:
2019-03-18 23:56:15,122 [INFO] Batch 93000, worst loss 0.061256 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 23:56:15,123 [INFO] Regularization: 2957.790771 * 0.0000010000 = 0.0029577909
2019-03-18 23:56:15,123 [INFO] Sum of grad norms: 0.095413
2019-03-18 23:56:15,124 [INFO] ---------------------------------
2019-03-18 23:56:34,057 [INFO] ---------------------------------
2019-03-18 23:56:34,058 [INFO] Summary:
2019-03-18 23:56:34,058 [INFO] Batch 94000, worst loss 0.061136 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 23:56:34,059 [INFO] Regularization: 2956.733887 * 0.0000010000 = 0.0029567338
2019-03-18 23:56:34,059 [INFO] Sum of grad norms: 0.074523
2019-03-18 23:56:34,060 [INFO] ---------------------------------
2019-03-18 23:56:52,682 [INFO] ---------------------------------
2019-03-18 23:56:52,683 [INFO] Summary:
2019-03-18 23:56:52,684 [INFO] Batch 95000, worst loss 0.061071 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 23:56:52,685 [INFO] Regularization: 2955.774902 * 0.0000010000 = 0.0029557750
2019-03-18 23:56:52,685 [INFO] Sum of grad norms: 0.084297
2019-03-18 23:56:52,686 [INFO] ---------------------------------
2019-03-18 23:57:11,184 [INFO] ---------------------------------
2019-03-18 23:57:11,186 [INFO] Summary:
2019-03-18 23:57:11,186 [INFO] Batch 96000, worst loss 0.061072 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 23:57:11,187 [INFO] Regularization: 2954.654785 * 0.0000010000 = 0.0029546549
2019-03-18 23:57:11,187 [INFO] Sum of grad norms: 0.062412
2019-03-18 23:57:11,188 [INFO] ---------------------------------
2019-03-18 23:57:29,868 [INFO] ---------------------------------
2019-03-18 23:57:29,869 [INFO] Summary:
2019-03-18 23:57:29,870 [INFO] Batch 97000, worst loss 0.060860 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 23:57:29,871 [INFO] Regularization: 2953.802734 * 0.0000010000 = 0.0029538027
2019-03-18 23:57:29,871 [INFO] Sum of grad norms: 0.032078
2019-03-18 23:57:29,872 [INFO] ---------------------------------
2019-03-18 23:57:48,607 [INFO] ---------------------------------
2019-03-18 23:57:48,608 [INFO] Summary:
2019-03-18 23:57:48,609 [INFO] Batch 98000, worst loss 0.061048 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 23:57:48,609 [INFO] Regularization: 2952.828369 * 0.0000010000 = 0.0029528283
2019-03-18 23:57:48,610 [INFO] Sum of grad norms: 0.051385
2019-03-18 23:57:48,611 [INFO] ---------------------------------
2019-03-18 23:58:07,357 [INFO] ---------------------------------
2019-03-18 23:58:07,358 [INFO] Summary:
2019-03-18 23:58:07,358 [INFO] Batch 99000, worst loss 0.061133 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 23:58:07,359 [INFO] Regularization: 2952.040039 * 0.0000010000 = 0.0029520399
2019-03-18 23:58:07,359 [INFO] Sum of grad norms: 0.035593
2019-03-18 23:58:07,360 [INFO] ---------------------------------
2019-03-18 23:58:25,923 [INFO] ---------------------------------
2019-03-18 23:58:25,924 [INFO] Summary:
2019-03-18 23:58:25,925 [INFO] Batch 100000, worst loss 0.061190 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-18 23:58:25,925 [INFO] Regularization: 2951.117188 * 0.0000010000 = 0.0029511172
2019-03-18 23:58:25,926 [INFO] Sum of grad norms: 0.029353
2019-03-18 23:58:25,926 [INFO] ---------------------------------
2019-03-18 23:58:30,887 [INFO] ---------------------------------
2019-03-18 23:58:30,888 [INFO] Evaluation:
2019-03-18 23:58:30,889 [INFO] Batch 100000, worst loss 0.058360 (without reg.) of 1000 batches @cl.-depth 1
2019-03-18 23:58:30,890 [INFO] ---------------------------------
2019-03-18 23:58:50,077 [INFO] ---------------------------------
2019-03-18 23:58:50,078 [INFO] Summary:
2019-03-18 23:58:50,078 [INFO] Batch 101000, worst loss 0.061370 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 23:58:50,079 [INFO] Regularization: 2950.212891 * 0.0000010000 = 0.0029502129
2019-03-18 23:58:50,079 [INFO] Sum of grad norms: 0.047036
2019-03-18 23:58:50,080 [INFO] ---------------------------------
2019-03-18 23:59:08,921 [INFO] ---------------------------------
2019-03-18 23:59:08,922 [INFO] Summary:
2019-03-18 23:59:08,923 [INFO] Batch 102000, worst loss 0.061284 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 23:59:08,924 [INFO] Regularization: 2949.449463 * 0.0000010000 = 0.0029494495
2019-03-18 23:59:08,924 [INFO] Sum of grad norms: 0.020206
2019-03-18 23:59:08,925 [INFO] ---------------------------------
2019-03-18 23:59:27,697 [INFO] ---------------------------------
2019-03-18 23:59:27,698 [INFO] Summary:
2019-03-18 23:59:27,698 [INFO] Batch 103000, worst loss 0.061312 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 23:59:27,699 [INFO] Regularization: 2948.865479 * 0.0000010000 = 0.0029488655
2019-03-18 23:59:27,700 [INFO] Sum of grad norms: 0.051928
2019-03-18 23:59:27,701 [INFO] ---------------------------------
2019-03-18 23:59:46,652 [INFO] ---------------------------------
2019-03-18 23:59:46,652 [INFO] Summary:
2019-03-18 23:59:46,653 [INFO] Batch 104000, worst loss 0.061232 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-18 23:59:46,653 [INFO] Regularization: 2948.372070 * 0.0000010000 = 0.0029483722
2019-03-18 23:59:46,654 [INFO] Sum of grad norms: 0.047318
2019-03-18 23:59:46,655 [INFO] ---------------------------------
2019-03-19 00:00:05,139 [INFO] ---------------------------------
2019-03-19 00:00:05,140 [INFO] Summary:
2019-03-19 00:00:05,141 [INFO] Batch 105000, worst loss 0.061162 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 00:00:05,141 [INFO] Regularization: 2947.969238 * 0.0000010000 = 0.0029479691
2019-03-19 00:00:05,142 [INFO] Sum of grad norms: 0.037819
2019-03-19 00:00:05,142 [INFO] ---------------------------------
2019-03-19 00:00:23,736 [INFO] ---------------------------------
2019-03-19 00:00:23,737 [INFO] Summary:
2019-03-19 00:00:23,737 [INFO] Batch 106000, worst loss 0.061061 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 00:00:23,738 [INFO] Regularization: 2947.587646 * 0.0000010000 = 0.0029475878
2019-03-19 00:00:23,738 [INFO] Sum of grad norms: 0.045401
2019-03-19 00:00:23,739 [INFO] ---------------------------------
2019-03-19 00:00:42,322 [INFO] ---------------------------------
2019-03-19 00:00:42,323 [INFO] Summary:
2019-03-19 00:00:42,323 [INFO] Batch 107000, worst loss 0.061105 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 00:00:42,324 [INFO] Regularization: 2947.071533 * 0.0000010000 = 0.0029470716
2019-03-19 00:00:42,324 [INFO] Sum of grad norms: 0.069715
2019-03-19 00:00:42,325 [INFO] ---------------------------------
2019-03-19 00:01:00,866 [INFO] ---------------------------------
2019-03-19 00:01:00,867 [INFO] Summary:
2019-03-19 00:01:00,867 [INFO] Batch 108000, worst loss 0.060869 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 00:01:00,868 [INFO] Regularization: 2946.533203 * 0.0000010000 = 0.0029465333
2019-03-19 00:01:00,868 [INFO] Sum of grad norms: 0.072927
2019-03-19 00:01:00,869 [INFO] ---------------------------------
2019-03-19 00:01:19,870 [INFO] ---------------------------------
2019-03-19 00:01:19,871 [INFO] Summary:
2019-03-19 00:01:19,871 [INFO] Batch 109000, worst loss 0.060963 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 00:01:19,872 [INFO] Regularization: 2946.026855 * 0.0000010000 = 0.0029460269
2019-03-19 00:01:19,872 [INFO] Sum of grad norms: 0.090079
2019-03-19 00:01:19,873 [INFO] ---------------------------------
2019-03-19 00:01:38,594 [INFO] ---------------------------------
2019-03-19 00:01:38,595 [INFO] Summary:
2019-03-19 00:01:38,596 [INFO] Batch 110000, worst loss 0.061065 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 00:01:38,596 [INFO] Regularization: 2945.642334 * 0.0000010000 = 0.0029456422
2019-03-19 00:01:38,597 [INFO] Sum of grad norms: 0.113095
2019-03-19 00:01:38,598 [INFO] ---------------------------------
2019-03-19 00:01:43,461 [INFO] ---------------------------------
2019-03-19 00:01:43,461 [INFO] Evaluation:
2019-03-19 00:01:43,463 [INFO] Batch 110000, worst loss 0.058275 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:01:43,464 [INFO] ---------------------------------
2019-03-19 00:02:01,870 [INFO] ---------------------------------
2019-03-19 00:02:01,872 [INFO] Summary:
2019-03-19 00:02:01,872 [INFO] Batch 111000, worst loss 0.060987 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 00:02:01,873 [INFO] Regularization: 2945.118164 * 0.0000010000 = 0.0029451181
2019-03-19 00:02:01,874 [INFO] Sum of grad norms: 0.086501
2019-03-19 00:02:01,874 [INFO] ---------------------------------
2019-03-19 00:02:20,732 [INFO] ---------------------------------
2019-03-19 00:02:20,733 [INFO] Summary:
2019-03-19 00:02:20,734 [INFO] Batch 112000, worst loss 0.061166 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 00:02:20,735 [INFO] Regularization: 2944.801758 * 0.0000010000 = 0.0029448017
2019-03-19 00:02:20,736 [INFO] Sum of grad norms: 0.035370
2019-03-19 00:02:20,736 [INFO] ---------------------------------
2019-03-19 00:02:39,269 [INFO] ---------------------------------
2019-03-19 00:02:39,270 [INFO] Summary:
2019-03-19 00:02:39,271 [INFO] Batch 113000, worst loss 0.061464 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 00:02:39,271 [INFO] Regularization: 2944.500000 * 0.0000010000 = 0.0029445000
2019-03-19 00:02:39,272 [INFO] Sum of grad norms: 0.029146
2019-03-19 00:02:39,272 [INFO] ---------------------------------
2019-03-19 00:02:57,788 [INFO] ---------------------------------
2019-03-19 00:02:57,789 [INFO] Summary:
2019-03-19 00:02:57,790 [INFO] Batch 114000, worst loss 0.061459 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 00:02:57,791 [INFO] Regularization: 2944.318359 * 0.0000010000 = 0.0029443183
2019-03-19 00:02:57,792 [INFO] Sum of grad norms: 0.031756
2019-03-19 00:02:57,792 [INFO] ---------------------------------
2019-03-19 00:03:16,557 [INFO] ---------------------------------
2019-03-19 00:03:16,558 [INFO] Summary:
2019-03-19 00:03:16,558 [INFO] Batch 115000, worst loss 0.061182 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 00:03:16,559 [INFO] Regularization: 2944.039062 * 0.0000010000 = 0.0029440389
2019-03-19 00:03:16,559 [INFO] Sum of grad norms: 0.050447
2019-03-19 00:03:16,560 [INFO] ---------------------------------
2019-03-19 00:03:35,188 [INFO] ---------------------------------
2019-03-19 00:03:35,189 [INFO] Summary:
2019-03-19 00:03:35,189 [INFO] Batch 116000, worst loss 0.060950 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 00:03:35,190 [INFO] Regularization: 2943.790283 * 0.0000010000 = 0.0029437903
2019-03-19 00:03:35,190 [INFO] Sum of grad norms: 0.024672
2019-03-19 00:03:35,191 [INFO] ---------------------------------
2019-03-19 00:03:53,891 [INFO] ---------------------------------
2019-03-19 00:03:53,892 [INFO] Summary:
2019-03-19 00:03:53,893 [INFO] Batch 117000, worst loss 0.061126 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 00:03:53,893 [INFO] Regularization: 2943.560547 * 0.0000010000 = 0.0029435605
2019-03-19 00:03:53,894 [INFO] Sum of grad norms: 0.082682
2019-03-19 00:03:53,894 [INFO] ---------------------------------
2019-03-19 00:04:12,275 [INFO] ---------------------------------
2019-03-19 00:04:12,276 [INFO] Summary:
2019-03-19 00:04:12,277 [INFO] Batch 118000, worst loss 0.061100 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 00:04:12,277 [INFO] Regularization: 2943.279785 * 0.0000010000 = 0.0029432797
2019-03-19 00:04:12,278 [INFO] Sum of grad norms: 0.094390
2019-03-19 00:04:12,279 [INFO] ---------------------------------
2019-03-19 00:04:31,076 [INFO] ---------------------------------
2019-03-19 00:04:31,077 [INFO] Summary:
2019-03-19 00:04:31,077 [INFO] Batch 119000, worst loss 0.061070 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 00:04:31,078 [INFO] Regularization: 2943.105957 * 0.0000010000 = 0.0029431060
2019-03-19 00:04:31,078 [INFO] Sum of grad norms: 0.062175
2019-03-19 00:04:31,079 [INFO] ---------------------------------
2019-03-19 00:04:49,711 [INFO] ---------------------------------
2019-03-19 00:04:49,713 [INFO] Summary:
2019-03-19 00:04:49,714 [INFO] Batch 120000, worst loss 0.060856 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 00:04:49,715 [INFO] Regularization: 2942.870117 * 0.0000010000 = 0.0029428701
2019-03-19 00:04:49,715 [INFO] Sum of grad norms: 0.027584
2019-03-19 00:04:49,716 [INFO] ---------------------------------
2019-03-19 00:04:54,712 [INFO] ---------------------------------
2019-03-19 00:04:54,713 [INFO] Evaluation:
2019-03-19 00:04:54,714 [INFO] Batch 120000, worst loss 0.057972 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:04:54,718 [INFO] ---------------------------------
2019-03-19 00:05:13,631 [INFO] ---------------------------------
2019-03-19 00:05:13,633 [INFO] Summary:
2019-03-19 00:05:13,633 [INFO] Batch 121000, worst loss 0.060948 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:05:13,634 [INFO] Regularization: 2942.566650 * 0.0000010000 = 0.0029425665
2019-03-19 00:05:13,634 [INFO] Sum of grad norms: 0.058135
2019-03-19 00:05:13,635 [INFO] ---------------------------------
2019-03-19 00:05:32,113 [INFO] ---------------------------------
2019-03-19 00:05:32,114 [INFO] Summary:
2019-03-19 00:05:32,115 [INFO] Batch 122000, worst loss 0.060933 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:05:32,115 [INFO] Regularization: 2942.382568 * 0.0000010000 = 0.0029423826
2019-03-19 00:05:32,116 [INFO] Sum of grad norms: 0.121590
2019-03-19 00:05:32,117 [INFO] ---------------------------------
2019-03-19 00:05:50,514 [INFO] ---------------------------------
2019-03-19 00:05:50,515 [INFO] Summary:
2019-03-19 00:05:50,515 [INFO] Batch 123000, worst loss 0.061347 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:05:50,516 [INFO] Regularization: 2942.223877 * 0.0000010000 = 0.0029422238
2019-03-19 00:05:50,516 [INFO] Sum of grad norms: 0.025013
2019-03-19 00:05:50,517 [INFO] ---------------------------------
2019-03-19 00:06:09,270 [INFO] ---------------------------------
2019-03-19 00:06:09,271 [INFO] Summary:
2019-03-19 00:06:09,271 [INFO] Batch 124000, worst loss 0.060907 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:06:09,272 [INFO] Regularization: 2942.091309 * 0.0000010000 = 0.0029420913
2019-03-19 00:06:09,272 [INFO] Sum of grad norms: 0.042377
2019-03-19 00:06:09,273 [INFO] ---------------------------------
2019-03-19 00:06:27,725 [INFO] ---------------------------------
2019-03-19 00:06:27,726 [INFO] Summary:
2019-03-19 00:06:27,727 [INFO] Batch 125000, worst loss 0.060894 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:06:27,727 [INFO] Regularization: 2941.990234 * 0.0000010000 = 0.0029419903
2019-03-19 00:06:27,728 [INFO] Sum of grad norms: 0.038463
2019-03-19 00:06:27,729 [INFO] ---------------------------------
2019-03-19 00:06:46,425 [INFO] ---------------------------------
2019-03-19 00:06:46,425 [INFO] Summary:
2019-03-19 00:06:46,426 [INFO] Batch 126000, worst loss 0.061116 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:06:46,427 [INFO] Regularization: 2941.858398 * 0.0000010000 = 0.0029418585
2019-03-19 00:06:46,427 [INFO] Sum of grad norms: 0.104014
2019-03-19 00:06:46,428 [INFO] ---------------------------------
2019-03-19 00:07:05,182 [INFO] ---------------------------------
2019-03-19 00:07:05,183 [INFO] Summary:
2019-03-19 00:07:05,183 [INFO] Batch 127000, worst loss 0.060994 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:07:05,184 [INFO] Regularization: 2941.730957 * 0.0000010000 = 0.0029417309
2019-03-19 00:07:05,184 [INFO] Sum of grad norms: 0.027019
2019-03-19 00:07:05,185 [INFO] ---------------------------------
2019-03-19 00:07:24,078 [INFO] ---------------------------------
2019-03-19 00:07:24,079 [INFO] Summary:
2019-03-19 00:07:24,080 [INFO] Batch 128000, worst loss 0.060996 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:07:24,080 [INFO] Regularization: 2941.558838 * 0.0000010000 = 0.0029415588
2019-03-19 00:07:24,081 [INFO] Sum of grad norms: 0.048468
2019-03-19 00:07:24,081 [INFO] ---------------------------------
2019-03-19 00:07:42,947 [INFO] ---------------------------------
2019-03-19 00:07:42,948 [INFO] Summary:
2019-03-19 00:07:42,948 [INFO] Batch 129000, worst loss 0.060961 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:07:42,949 [INFO] Regularization: 2941.433594 * 0.0000010000 = 0.0029414336
2019-03-19 00:07:42,949 [INFO] Sum of grad norms: 0.056740
2019-03-19 00:07:42,950 [INFO] ---------------------------------
2019-03-19 00:08:01,625 [INFO] ---------------------------------
2019-03-19 00:08:01,626 [INFO] Summary:
2019-03-19 00:08:01,626 [INFO] Batch 130000, worst loss 0.061046 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:08:01,627 [INFO] Regularization: 2941.354004 * 0.0000010000 = 0.0029413539
2019-03-19 00:08:01,627 [INFO] Sum of grad norms: 0.065891
2019-03-19 00:08:01,628 [INFO] ---------------------------------
2019-03-19 00:08:06,551 [INFO] ---------------------------------
2019-03-19 00:08:06,552 [INFO] Evaluation:
2019-03-19 00:08:06,552 [INFO] Batch 130000, worst loss 0.058231 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:08:06,553 [INFO] ---------------------------------
2019-03-19 00:08:25,394 [INFO] ---------------------------------
2019-03-19 00:08:25,395 [INFO] Summary:
2019-03-19 00:08:25,395 [INFO] Batch 131000, worst loss 0.061383 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:08:25,396 [INFO] Regularization: 2941.253418 * 0.0000010000 = 0.0029412534
2019-03-19 00:08:25,396 [INFO] Sum of grad norms: 0.039011
2019-03-19 00:08:25,397 [INFO] ---------------------------------
2019-03-19 00:08:44,121 [INFO] ---------------------------------
2019-03-19 00:08:44,122 [INFO] Summary:
2019-03-19 00:08:44,123 [INFO] Batch 132000, worst loss 0.061114 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:08:44,124 [INFO] Regularization: 2941.133789 * 0.0000010000 = 0.0029411337
2019-03-19 00:08:44,125 [INFO] Sum of grad norms: 0.084966
2019-03-19 00:08:44,125 [INFO] ---------------------------------
2019-03-19 00:09:02,910 [INFO] ---------------------------------
2019-03-19 00:09:02,911 [INFO] Summary:
2019-03-19 00:09:02,912 [INFO] Batch 133000, worst loss 0.061089 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:09:02,912 [INFO] Regularization: 2941.068115 * 0.0000010000 = 0.0029410680
2019-03-19 00:09:02,913 [INFO] Sum of grad norms: 0.048098
2019-03-19 00:09:02,913 [INFO] ---------------------------------
2019-03-19 00:09:21,660 [INFO] ---------------------------------
2019-03-19 00:09:21,661 [INFO] Summary:
2019-03-19 00:09:21,661 [INFO] Batch 134000, worst loss 0.060900 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:09:21,662 [INFO] Regularization: 2941.031738 * 0.0000010000 = 0.0029410317
2019-03-19 00:09:21,662 [INFO] Sum of grad norms: 0.082972
2019-03-19 00:09:21,663 [INFO] ---------------------------------
2019-03-19 00:09:40,191 [INFO] ---------------------------------
2019-03-19 00:09:40,192 [INFO] Summary:
2019-03-19 00:09:40,192 [INFO] Batch 135000, worst loss 0.060901 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:09:40,193 [INFO] Regularization: 2940.944824 * 0.0000010000 = 0.0029409449
2019-03-19 00:09:40,193 [INFO] Sum of grad norms: 0.063526
2019-03-19 00:09:40,194 [INFO] ---------------------------------
2019-03-19 00:09:58,963 [INFO] ---------------------------------
2019-03-19 00:09:58,965 [INFO] Summary:
2019-03-19 00:09:58,965 [INFO] Batch 136000, worst loss 0.061015 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:09:58,966 [INFO] Regularization: 2940.878174 * 0.0000010000 = 0.0029408783
2019-03-19 00:09:58,966 [INFO] Sum of grad norms: 0.023139
2019-03-19 00:09:58,967 [INFO] ---------------------------------
2019-03-19 00:10:17,499 [INFO] ---------------------------------
2019-03-19 00:10:17,500 [INFO] Summary:
2019-03-19 00:10:17,500 [INFO] Batch 137000, worst loss 0.061115 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:10:17,501 [INFO] Regularization: 2940.829834 * 0.0000010000 = 0.0029408298
2019-03-19 00:10:17,501 [INFO] Sum of grad norms: 0.033180
2019-03-19 00:10:17,502 [INFO] ---------------------------------
2019-03-19 00:10:35,895 [INFO] ---------------------------------
2019-03-19 00:10:35,896 [INFO] Summary:
2019-03-19 00:10:35,896 [INFO] Batch 138000, worst loss 0.061027 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:10:35,897 [INFO] Regularization: 2940.781250 * 0.0000010000 = 0.0029407812
2019-03-19 00:10:35,898 [INFO] Sum of grad norms: 0.080020
2019-03-19 00:10:35,899 [INFO] ---------------------------------
2019-03-19 00:10:54,988 [INFO] ---------------------------------
2019-03-19 00:10:54,989 [INFO] Summary:
2019-03-19 00:10:54,990 [INFO] Batch 139000, worst loss 0.060814 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:10:54,990 [INFO] Regularization: 2940.722656 * 0.0000010000 = 0.0029407227
2019-03-19 00:10:54,991 [INFO] Sum of grad norms: 0.026287
2019-03-19 00:10:54,991 [INFO] ---------------------------------
2019-03-19 00:11:13,666 [INFO] ---------------------------------
2019-03-19 00:11:13,667 [INFO] Summary:
2019-03-19 00:11:13,668 [INFO] Batch 140000, worst loss 0.061064 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 00:11:13,668 [INFO] Regularization: 2940.665039 * 0.0000010000 = 0.0029406650
2019-03-19 00:11:13,669 [INFO] Sum of grad norms: 0.114115
2019-03-19 00:11:13,669 [INFO] ---------------------------------
2019-03-19 00:11:18,631 [INFO] ---------------------------------
2019-03-19 00:11:18,632 [INFO] Evaluation:
2019-03-19 00:11:18,633 [INFO] Batch 140000, worst loss 0.058136 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:11:18,635 [INFO] ---------------------------------
2019-03-19 00:11:37,086 [INFO] ---------------------------------
2019-03-19 00:11:37,087 [INFO] Summary:
2019-03-19 00:11:37,087 [INFO] Batch 141000, worst loss 0.061007 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:11:37,088 [INFO] Regularization: 2940.608154 * 0.0000010000 = 0.0029406082
2019-03-19 00:11:37,088 [INFO] Sum of grad norms: 0.036472
2019-03-19 00:11:37,089 [INFO] ---------------------------------
2019-03-19 00:11:55,931 [INFO] ---------------------------------
2019-03-19 00:11:55,932 [INFO] Summary:
2019-03-19 00:11:55,933 [INFO] Batch 142000, worst loss 0.060902 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:11:55,933 [INFO] Regularization: 2940.562988 * 0.0000010000 = 0.0029405630
2019-03-19 00:11:55,934 [INFO] Sum of grad norms: 0.039477
2019-03-19 00:11:55,934 [INFO] ---------------------------------
2019-03-19 00:12:14,588 [INFO] ---------------------------------
2019-03-19 00:12:14,589 [INFO] Summary:
2019-03-19 00:12:14,590 [INFO] Batch 143000, worst loss 0.060975 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:12:14,590 [INFO] Regularization: 2940.533203 * 0.0000010000 = 0.0029405332
2019-03-19 00:12:14,591 [INFO] Sum of grad norms: 0.044342
2019-03-19 00:12:14,592 [INFO] ---------------------------------
2019-03-19 00:12:33,809 [INFO] ---------------------------------
2019-03-19 00:12:33,810 [INFO] Summary:
2019-03-19 00:12:33,811 [INFO] Batch 144000, worst loss 0.060891 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:12:33,811 [INFO] Regularization: 2940.498047 * 0.0000010000 = 0.0029404981
2019-03-19 00:12:33,812 [INFO] Sum of grad norms: 0.107477
2019-03-19 00:12:33,812 [INFO] ---------------------------------
2019-03-19 00:12:52,352 [INFO] ---------------------------------
2019-03-19 00:12:52,353 [INFO] Summary:
2019-03-19 00:12:52,353 [INFO] Batch 145000, worst loss 0.060942 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:12:52,354 [INFO] Regularization: 2940.468750 * 0.0000010000 = 0.0029404687
2019-03-19 00:12:52,354 [INFO] Sum of grad norms: 0.068733
2019-03-19 00:12:52,355 [INFO] ---------------------------------
2019-03-19 00:13:10,818 [INFO] ---------------------------------
2019-03-19 00:13:10,819 [INFO] Summary:
2019-03-19 00:13:10,820 [INFO] Batch 146000, worst loss 0.061076 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:13:10,821 [INFO] Regularization: 2940.432129 * 0.0000010000 = 0.0029404322
2019-03-19 00:13:10,821 [INFO] Sum of grad norms: 0.037027
2019-03-19 00:13:10,822 [INFO] ---------------------------------
2019-03-19 00:13:29,644 [INFO] ---------------------------------
2019-03-19 00:13:29,645 [INFO] Summary:
2019-03-19 00:13:29,645 [INFO] Batch 147000, worst loss 0.061089 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:13:29,646 [INFO] Regularization: 2940.397705 * 0.0000010000 = 0.0029403977
2019-03-19 00:13:29,646 [INFO] Sum of grad norms: 0.046525
2019-03-19 00:13:29,647 [INFO] ---------------------------------
2019-03-19 00:13:48,497 [INFO] ---------------------------------
2019-03-19 00:13:48,498 [INFO] Summary:
2019-03-19 00:13:48,499 [INFO] Batch 148000, worst loss 0.061089 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:13:48,499 [INFO] Regularization: 2940.361816 * 0.0000010000 = 0.0029403619
2019-03-19 00:13:48,500 [INFO] Sum of grad norms: 0.064660
2019-03-19 00:13:48,500 [INFO] ---------------------------------
2019-03-19 00:14:07,290 [INFO] ---------------------------------
2019-03-19 00:14:07,291 [INFO] Summary:
2019-03-19 00:14:07,291 [INFO] Batch 149000, worst loss 0.060972 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:14:07,292 [INFO] Regularization: 2940.331055 * 0.0000010000 = 0.0029403311
2019-03-19 00:14:07,292 [INFO] Sum of grad norms: 0.042737
2019-03-19 00:14:07,293 [INFO] ---------------------------------
2019-03-19 00:14:26,220 [INFO] ---------------------------------
2019-03-19 00:14:26,221 [INFO] Summary:
2019-03-19 00:14:26,222 [INFO] Batch 150000, worst loss 0.061225 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:14:26,222 [INFO] Regularization: 2940.303467 * 0.0000010000 = 0.0029403034
2019-03-19 00:14:26,223 [INFO] Sum of grad norms: 0.122499
2019-03-19 00:14:26,223 [INFO] ---------------------------------
2019-03-19 00:14:31,299 [INFO] ---------------------------------
2019-03-19 00:14:31,300 [INFO] Evaluation:
2019-03-19 00:14:31,303 [INFO] Batch 150000, worst loss 0.058124 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:14:31,304 [INFO] ---------------------------------
2019-03-19 00:14:49,840 [INFO] ---------------------------------
2019-03-19 00:14:49,841 [INFO] Summary:
2019-03-19 00:14:49,842 [INFO] Batch 151000, worst loss 0.060898 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:14:49,842 [INFO] Regularization: 2940.271729 * 0.0000010000 = 0.0029402717
2019-03-19 00:14:49,843 [INFO] Sum of grad norms: 0.080773
2019-03-19 00:14:49,843 [INFO] ---------------------------------
2019-03-19 00:15:08,747 [INFO] ---------------------------------
2019-03-19 00:15:08,748 [INFO] Summary:
2019-03-19 00:15:08,749 [INFO] Batch 152000, worst loss 0.061206 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:15:08,749 [INFO] Regularization: 2940.248047 * 0.0000010000 = 0.0029402480
2019-03-19 00:15:08,750 [INFO] Sum of grad norms: 0.020663
2019-03-19 00:15:08,750 [INFO] ---------------------------------
2019-03-19 00:15:27,529 [INFO] ---------------------------------
2019-03-19 00:15:27,530 [INFO] Summary:
2019-03-19 00:15:27,531 [INFO] Batch 153000, worst loss 0.060999 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:15:27,531 [INFO] Regularization: 2940.233398 * 0.0000010000 = 0.0029402333
2019-03-19 00:15:27,532 [INFO] Sum of grad norms: 0.041282
2019-03-19 00:15:27,533 [INFO] ---------------------------------
2019-03-19 00:15:46,169 [INFO] ---------------------------------
2019-03-19 00:15:46,170 [INFO] Summary:
2019-03-19 00:15:46,170 [INFO] Batch 154000, worst loss 0.061208 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:15:46,171 [INFO] Regularization: 2940.216309 * 0.0000010000 = 0.0029402163
2019-03-19 00:15:46,171 [INFO] Sum of grad norms: 0.033200
2019-03-19 00:15:46,172 [INFO] ---------------------------------
2019-03-19 00:16:04,934 [INFO] ---------------------------------
2019-03-19 00:16:04,934 [INFO] Summary:
2019-03-19 00:16:04,935 [INFO] Batch 155000, worst loss 0.060938 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:16:04,936 [INFO] Regularization: 2940.199951 * 0.0000010000 = 0.0029402000
2019-03-19 00:16:04,937 [INFO] Sum of grad norms: 0.043505
2019-03-19 00:16:04,938 [INFO] ---------------------------------
2019-03-19 00:16:23,763 [INFO] ---------------------------------
2019-03-19 00:16:23,765 [INFO] Summary:
2019-03-19 00:16:23,765 [INFO] Batch 156000, worst loss 0.061383 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:16:23,766 [INFO] Regularization: 2940.184570 * 0.0000010000 = 0.0029401847
2019-03-19 00:16:23,766 [INFO] Sum of grad norms: 0.026638
2019-03-19 00:16:23,767 [INFO] ---------------------------------
2019-03-19 00:16:42,569 [INFO] ---------------------------------
2019-03-19 00:16:42,570 [INFO] Summary:
2019-03-19 00:16:42,571 [INFO] Batch 157000, worst loss 0.060940 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:16:42,571 [INFO] Regularization: 2940.171631 * 0.0000010000 = 0.0029401716
2019-03-19 00:16:42,572 [INFO] Sum of grad norms: 0.040716
2019-03-19 00:16:42,572 [INFO] ---------------------------------
2019-03-19 00:17:01,590 [INFO] ---------------------------------
2019-03-19 00:17:01,591 [INFO] Summary:
2019-03-19 00:17:01,591 [INFO] Batch 158000, worst loss 0.061161 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:17:01,592 [INFO] Regularization: 2940.148682 * 0.0000010000 = 0.0029401486
2019-03-19 00:17:01,592 [INFO] Sum of grad norms: 0.063880
2019-03-19 00:17:01,593 [INFO] ---------------------------------
2019-03-19 00:17:20,223 [INFO] ---------------------------------
2019-03-19 00:17:20,224 [INFO] Summary:
2019-03-19 00:17:20,225 [INFO] Batch 159000, worst loss 0.061160 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:17:20,225 [INFO] Regularization: 2940.140381 * 0.0000010000 = 0.0029401404
2019-03-19 00:17:20,226 [INFO] Sum of grad norms: 0.074870
2019-03-19 00:17:20,227 [INFO] ---------------------------------
2019-03-19 00:17:39,275 [INFO] ---------------------------------
2019-03-19 00:17:39,276 [INFO] Summary:
2019-03-19 00:17:39,277 [INFO] Batch 160000, worst loss 0.061044 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:17:39,277 [INFO] Regularization: 2940.122314 * 0.0000010000 = 0.0029401223
2019-03-19 00:17:39,278 [INFO] Sum of grad norms: 0.046128
2019-03-19 00:17:39,279 [INFO] ---------------------------------
2019-03-19 00:17:44,271 [INFO] ---------------------------------
2019-03-19 00:17:44,272 [INFO] Evaluation:
2019-03-19 00:17:44,273 [INFO] Batch 160000, worst loss 0.057966 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:17:44,274 [INFO] ---------------------------------
2019-03-19 00:18:03,158 [INFO] ---------------------------------
2019-03-19 00:18:03,159 [INFO] Summary:
2019-03-19 00:18:03,159 [INFO] Batch 161000, worst loss 0.061050 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:18:03,160 [INFO] Regularization: 2940.108643 * 0.0000010000 = 0.0029401085
2019-03-19 00:18:03,160 [INFO] Sum of grad norms: 0.108684
2019-03-19 00:18:03,161 [INFO] ---------------------------------
2019-03-19 00:18:21,688 [INFO] ---------------------------------
2019-03-19 00:18:21,689 [INFO] Summary:
2019-03-19 00:18:21,690 [INFO] Batch 162000, worst loss 0.060968 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:18:21,690 [INFO] Regularization: 2940.089844 * 0.0000010000 = 0.0029400899
2019-03-19 00:18:21,691 [INFO] Sum of grad norms: 0.041923
2019-03-19 00:18:21,692 [INFO] ---------------------------------
2019-03-19 00:18:40,709 [INFO] ---------------------------------
2019-03-19 00:18:40,710 [INFO] Summary:
2019-03-19 00:18:40,711 [INFO] Batch 163000, worst loss 0.061278 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:18:40,711 [INFO] Regularization: 2940.082764 * 0.0000010000 = 0.0029400827
2019-03-19 00:18:40,712 [INFO] Sum of grad norms: 0.080624
2019-03-19 00:18:40,712 [INFO] ---------------------------------
2019-03-19 00:18:59,619 [INFO] ---------------------------------
2019-03-19 00:18:59,620 [INFO] Summary:
2019-03-19 00:18:59,621 [INFO] Batch 164000, worst loss 0.061344 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:18:59,621 [INFO] Regularization: 2940.077881 * 0.0000010000 = 0.0029400778
2019-03-19 00:18:59,622 [INFO] Sum of grad norms: 0.050722
2019-03-19 00:18:59,623 [INFO] ---------------------------------
2019-03-19 00:19:18,456 [INFO] ---------------------------------
2019-03-19 00:19:18,457 [INFO] Summary:
2019-03-19 00:19:18,458 [INFO] Batch 165000, worst loss 0.061048 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:19:18,458 [INFO] Regularization: 2940.068848 * 0.0000010000 = 0.0029400690
2019-03-19 00:19:18,459 [INFO] Sum of grad norms: 0.034510
2019-03-19 00:19:18,459 [INFO] ---------------------------------
2019-03-19 00:19:37,400 [INFO] ---------------------------------
2019-03-19 00:19:37,401 [INFO] Summary:
2019-03-19 00:19:37,401 [INFO] Batch 166000, worst loss 0.061048 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:19:37,402 [INFO] Regularization: 2940.061523 * 0.0000010000 = 0.0029400615
2019-03-19 00:19:37,402 [INFO] Sum of grad norms: 0.034886
2019-03-19 00:19:37,403 [INFO] ---------------------------------
2019-03-19 00:19:56,085 [INFO] ---------------------------------
2019-03-19 00:19:56,086 [INFO] Summary:
2019-03-19 00:19:56,086 [INFO] Batch 167000, worst loss 0.060996 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:19:56,087 [INFO] Regularization: 2940.057861 * 0.0000010000 = 0.0029400578
2019-03-19 00:19:56,088 [INFO] Sum of grad norms: 0.055008
2019-03-19 00:19:56,088 [INFO] ---------------------------------
2019-03-19 00:20:14,726 [INFO] ---------------------------------
2019-03-19 00:20:14,727 [INFO] Summary:
2019-03-19 00:20:14,728 [INFO] Batch 168000, worst loss 0.061088 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:20:14,729 [INFO] Regularization: 2940.049072 * 0.0000010000 = 0.0029400492
2019-03-19 00:20:14,729 [INFO] Sum of grad norms: 0.113550
2019-03-19 00:20:14,730 [INFO] ---------------------------------
2019-03-19 00:20:33,336 [INFO] ---------------------------------
2019-03-19 00:20:33,337 [INFO] Summary:
2019-03-19 00:20:33,338 [INFO] Batch 169000, worst loss 0.060998 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:20:33,339 [INFO] Regularization: 2940.039551 * 0.0000010000 = 0.0029400396
2019-03-19 00:20:33,339 [INFO] Sum of grad norms: 0.028189
2019-03-19 00:20:33,340 [INFO] ---------------------------------
2019-03-19 00:20:52,259 [INFO] ---------------------------------
2019-03-19 00:20:52,260 [INFO] Summary:
2019-03-19 00:20:52,260 [INFO] Batch 170000, worst loss 0.061038 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:20:52,261 [INFO] Regularization: 2940.031006 * 0.0000010000 = 0.0029400310
2019-03-19 00:20:52,261 [INFO] Sum of grad norms: 0.077482
2019-03-19 00:20:52,262 [INFO] ---------------------------------
2019-03-19 00:20:57,149 [INFO] ---------------------------------
2019-03-19 00:20:57,149 [INFO] Evaluation:
2019-03-19 00:20:57,150 [INFO] Batch 170000, worst loss 0.058255 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:20:57,151 [INFO] ---------------------------------
2019-03-19 00:21:15,894 [INFO] ---------------------------------
2019-03-19 00:21:15,895 [INFO] Summary:
2019-03-19 00:21:15,896 [INFO] Batch 171000, worst loss 0.060963 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:21:15,896 [INFO] Regularization: 2940.022217 * 0.0000010000 = 0.0029400222
2019-03-19 00:21:15,897 [INFO] Sum of grad norms: 0.033624
2019-03-19 00:21:15,897 [INFO] ---------------------------------
2019-03-19 00:21:34,936 [INFO] ---------------------------------
2019-03-19 00:21:34,937 [INFO] Summary:
2019-03-19 00:21:34,938 [INFO] Batch 172000, worst loss 0.060898 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:21:34,938 [INFO] Regularization: 2940.017822 * 0.0000010000 = 0.0029400177
2019-03-19 00:21:34,939 [INFO] Sum of grad norms: 0.060131
2019-03-19 00:21:34,939 [INFO] ---------------------------------
2019-03-19 00:21:53,446 [INFO] ---------------------------------
2019-03-19 00:21:53,447 [INFO] Summary:
2019-03-19 00:21:53,447 [INFO] Batch 173000, worst loss 0.061217 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:21:53,448 [INFO] Regularization: 2940.015137 * 0.0000010000 = 0.0029400152
2019-03-19 00:21:53,448 [INFO] Sum of grad norms: 0.051694
2019-03-19 00:21:53,449 [INFO] ---------------------------------
2019-03-19 00:22:12,109 [INFO] ---------------------------------
2019-03-19 00:22:12,110 [INFO] Summary:
2019-03-19 00:22:12,111 [INFO] Batch 174000, worst loss 0.061217 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:22:12,111 [INFO] Regularization: 2940.010986 * 0.0000010000 = 0.0029400110
2019-03-19 00:22:12,112 [INFO] Sum of grad norms: 0.038188
2019-03-19 00:22:12,112 [INFO] ---------------------------------
2019-03-19 00:22:30,872 [INFO] ---------------------------------
2019-03-19 00:22:30,873 [INFO] Summary:
2019-03-19 00:22:30,874 [INFO] Batch 175000, worst loss 0.061046 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:22:30,875 [INFO] Regularization: 2940.007812 * 0.0000010000 = 0.0029400077
2019-03-19 00:22:30,876 [INFO] Sum of grad norms: 0.057506
2019-03-19 00:22:30,877 [INFO] ---------------------------------
2019-03-19 00:22:49,585 [INFO] ---------------------------------
2019-03-19 00:22:49,586 [INFO] Summary:
2019-03-19 00:22:49,587 [INFO] Batch 176000, worst loss 0.061063 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:22:49,587 [INFO] Regularization: 2940.005127 * 0.0000010000 = 0.0029400052
2019-03-19 00:22:49,588 [INFO] Sum of grad norms: 0.059717
2019-03-19 00:22:49,588 [INFO] ---------------------------------
2019-03-19 00:23:08,410 [INFO] ---------------------------------
2019-03-19 00:23:08,411 [INFO] Summary:
2019-03-19 00:23:08,411 [INFO] Batch 177000, worst loss 0.061063 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:23:08,412 [INFO] Regularization: 2940.001953 * 0.0000010000 = 0.0029400019
2019-03-19 00:23:08,413 [INFO] Sum of grad norms: 0.070275
2019-03-19 00:23:08,414 [INFO] ---------------------------------
2019-03-19 00:23:26,996 [INFO] ---------------------------------
2019-03-19 00:23:26,997 [INFO] Summary:
2019-03-19 00:23:26,997 [INFO] Batch 178000, worst loss 0.061284 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:23:26,998 [INFO] Regularization: 2939.996582 * 0.0000010000 = 0.0029399965
2019-03-19 00:23:26,998 [INFO] Sum of grad norms: 0.040309
2019-03-19 00:23:26,999 [INFO] ---------------------------------
2019-03-19 00:23:45,242 [INFO] ---------------------------------
2019-03-19 00:23:45,242 [INFO] Summary:
2019-03-19 00:23:45,243 [INFO] Batch 179000, worst loss 0.061326 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:23:45,243 [INFO] Regularization: 2939.993896 * 0.0000010000 = 0.0029399940
2019-03-19 00:23:45,244 [INFO] Sum of grad norms: 0.055301
2019-03-19 00:23:45,245 [INFO] ---------------------------------
2019-03-19 00:24:04,231 [INFO] ---------------------------------
2019-03-19 00:24:04,231 [INFO] Summary:
2019-03-19 00:24:04,232 [INFO] Batch 180000, worst loss 0.061039 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:24:04,232 [INFO] Regularization: 2939.991211 * 0.0000010000 = 0.0029399912
2019-03-19 00:24:04,233 [INFO] Sum of grad norms: 0.030545
2019-03-19 00:24:04,234 [INFO] ---------------------------------
2019-03-19 00:24:09,103 [INFO] ---------------------------------
2019-03-19 00:24:09,104 [INFO] Evaluation:
2019-03-19 00:24:09,105 [INFO] Batch 180000, worst loss 0.057920 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:24:09,105 [INFO] ---------------------------------
2019-03-19 00:24:27,712 [INFO] ---------------------------------
2019-03-19 00:24:27,713 [INFO] Summary:
2019-03-19 00:24:27,714 [INFO] Batch 181000, worst loss 0.060986 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:24:27,714 [INFO] Regularization: 2939.987549 * 0.0000010000 = 0.0029399875
2019-03-19 00:24:27,715 [INFO] Sum of grad norms: 0.059154
2019-03-19 00:24:27,715 [INFO] ---------------------------------
2019-03-19 00:24:46,076 [INFO] ---------------------------------
2019-03-19 00:24:46,077 [INFO] Summary:
2019-03-19 00:24:46,078 [INFO] Batch 182000, worst loss 0.061268 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:24:46,078 [INFO] Regularization: 2939.986816 * 0.0000010000 = 0.0029399868
2019-03-19 00:24:46,079 [INFO] Sum of grad norms: 0.027181
2019-03-19 00:24:46,080 [INFO] ---------------------------------
2019-03-19 00:25:04,665 [INFO] ---------------------------------
2019-03-19 00:25:04,666 [INFO] Summary:
2019-03-19 00:25:04,667 [INFO] Batch 183000, worst loss 0.060897 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:25:04,668 [INFO] Regularization: 2939.985840 * 0.0000010000 = 0.0029399858
2019-03-19 00:25:04,668 [INFO] Sum of grad norms: 0.053128
2019-03-19 00:25:04,669 [INFO] ---------------------------------
2019-03-19 00:25:23,346 [INFO] ---------------------------------
2019-03-19 00:25:23,347 [INFO] Summary:
2019-03-19 00:25:23,348 [INFO] Batch 184000, worst loss 0.061072 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:25:23,348 [INFO] Regularization: 2939.984863 * 0.0000010000 = 0.0029399849
2019-03-19 00:25:23,348 [INFO] Sum of grad norms: 0.077511
2019-03-19 00:25:23,349 [INFO] ---------------------------------
2019-03-19 00:25:42,101 [INFO] ---------------------------------
2019-03-19 00:25:42,102 [INFO] Summary:
2019-03-19 00:25:42,103 [INFO] Batch 185000, worst loss 0.061072 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:25:42,104 [INFO] Regularization: 2939.984131 * 0.0000010000 = 0.0029399842
2019-03-19 00:25:42,104 [INFO] Sum of grad norms: 0.026002
2019-03-19 00:25:42,105 [INFO] ---------------------------------
2019-03-19 00:26:00,901 [INFO] ---------------------------------
2019-03-19 00:26:00,902 [INFO] Summary:
2019-03-19 00:26:00,903 [INFO] Batch 186000, worst loss 0.061131 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:26:00,903 [INFO] Regularization: 2939.982910 * 0.0000010000 = 0.0029399828
2019-03-19 00:26:00,904 [INFO] Sum of grad norms: 0.059488
2019-03-19 00:26:00,904 [INFO] ---------------------------------
2019-03-19 00:26:19,627 [INFO] ---------------------------------
2019-03-19 00:26:19,628 [INFO] Summary:
2019-03-19 00:26:19,629 [INFO] Batch 187000, worst loss 0.060858 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:26:19,629 [INFO] Regularization: 2939.982910 * 0.0000010000 = 0.0029399828
2019-03-19 00:26:19,630 [INFO] Sum of grad norms: 0.026240
2019-03-19 00:26:19,630 [INFO] ---------------------------------
2019-03-19 00:26:37,899 [INFO] ---------------------------------
2019-03-19 00:26:37,900 [INFO] Summary:
2019-03-19 00:26:37,901 [INFO] Batch 188000, worst loss 0.061027 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:26:37,902 [INFO] Regularization: 2939.982666 * 0.0000010000 = 0.0029399826
2019-03-19 00:26:37,903 [INFO] Sum of grad norms: 0.101596
2019-03-19 00:26:37,904 [INFO] ---------------------------------
2019-03-19 00:26:56,452 [INFO] ---------------------------------
2019-03-19 00:26:56,453 [INFO] Summary:
2019-03-19 00:26:56,454 [INFO] Batch 189000, worst loss 0.061124 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:26:56,454 [INFO] Regularization: 2939.980957 * 0.0000010000 = 0.0029399809
2019-03-19 00:26:56,455 [INFO] Sum of grad norms: 0.039009
2019-03-19 00:26:56,455 [INFO] ---------------------------------
2019-03-19 00:27:15,040 [INFO] ---------------------------------
2019-03-19 00:27:15,041 [INFO] Summary:
2019-03-19 00:27:15,041 [INFO] Batch 190000, worst loss 0.061092 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:27:15,042 [INFO] Regularization: 2939.979736 * 0.0000010000 = 0.0029399798
2019-03-19 00:27:15,043 [INFO] Sum of grad norms: 0.038733
2019-03-19 00:27:15,043 [INFO] ---------------------------------
2019-03-19 00:27:19,974 [INFO] ---------------------------------
2019-03-19 00:27:19,975 [INFO] Evaluation:
2019-03-19 00:27:19,978 [INFO] Batch 190000, worst loss 0.058234 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:27:19,979 [INFO] ---------------------------------
2019-03-19 00:27:38,481 [INFO] ---------------------------------
2019-03-19 00:27:38,482 [INFO] Summary:
2019-03-19 00:27:38,483 [INFO] Batch 191000, worst loss 0.061367 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:27:38,483 [INFO] Regularization: 2939.978271 * 0.0000010000 = 0.0029399781
2019-03-19 00:27:38,484 [INFO] Sum of grad norms: 0.065983
2019-03-19 00:27:38,484 [INFO] ---------------------------------
2019-03-19 00:27:57,604 [INFO] ---------------------------------
2019-03-19 00:27:57,605 [INFO] Summary:
2019-03-19 00:27:57,606 [INFO] Batch 192000, worst loss 0.061367 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:27:57,606 [INFO] Regularization: 2939.978027 * 0.0000010000 = 0.0029399779
2019-03-19 00:27:57,607 [INFO] Sum of grad norms: 0.021446
2019-03-19 00:27:57,607 [INFO] ---------------------------------
2019-03-19 00:28:16,190 [INFO] ---------------------------------
2019-03-19 00:28:16,191 [INFO] Summary:
2019-03-19 00:28:16,192 [INFO] Batch 193000, worst loss 0.061023 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:28:16,192 [INFO] Regularization: 2939.977051 * 0.0000010000 = 0.0029399770
2019-03-19 00:28:16,193 [INFO] Sum of grad norms: 0.069573
2019-03-19 00:28:16,193 [INFO] ---------------------------------
2019-03-19 00:28:34,798 [INFO] ---------------------------------
2019-03-19 00:28:34,799 [INFO] Summary:
2019-03-19 00:28:34,799 [INFO] Batch 194000, worst loss 0.060952 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:28:34,800 [INFO] Regularization: 2939.976562 * 0.0000010000 = 0.0029399765
2019-03-19 00:28:34,800 [INFO] Sum of grad norms: 0.025165
2019-03-19 00:28:34,801 [INFO] ---------------------------------
2019-03-19 00:28:53,500 [INFO] ---------------------------------
2019-03-19 00:28:53,501 [INFO] Summary:
2019-03-19 00:28:53,502 [INFO] Batch 195000, worst loss 0.061272 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:28:53,502 [INFO] Regularization: 2939.976074 * 0.0000010000 = 0.0029399761
2019-03-19 00:28:53,503 [INFO] Sum of grad norms: 0.027224
2019-03-19 00:28:53,504 [INFO] ---------------------------------
2019-03-19 00:29:12,062 [INFO] ---------------------------------
2019-03-19 00:29:12,063 [INFO] Summary:
2019-03-19 00:29:12,063 [INFO] Batch 196000, worst loss 0.061108 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:29:12,064 [INFO] Regularization: 2939.975586 * 0.0000010000 = 0.0029399756
2019-03-19 00:29:12,064 [INFO] Sum of grad norms: 0.074653
2019-03-19 00:29:12,065 [INFO] ---------------------------------
2019-03-19 00:29:30,537 [INFO] ---------------------------------
2019-03-19 00:29:30,538 [INFO] Summary:
2019-03-19 00:29:30,538 [INFO] Batch 197000, worst loss 0.060910 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:29:30,539 [INFO] Regularization: 2939.975098 * 0.0000010000 = 0.0029399751
2019-03-19 00:29:30,539 [INFO] Sum of grad norms: 0.044225
2019-03-19 00:29:30,540 [INFO] ---------------------------------
2019-03-19 00:29:49,711 [INFO] ---------------------------------
2019-03-19 00:29:49,712 [INFO] Summary:
2019-03-19 00:29:49,713 [INFO] Batch 198000, worst loss 0.061119 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:29:49,713 [INFO] Regularization: 2939.974609 * 0.0000010000 = 0.0029399747
2019-03-19 00:29:49,714 [INFO] Sum of grad norms: 0.053330
2019-03-19 00:29:49,714 [INFO] ---------------------------------
2019-03-19 00:30:08,436 [INFO] ---------------------------------
2019-03-19 00:30:08,437 [INFO] Summary:
2019-03-19 00:30:08,438 [INFO] Batch 199000, worst loss 0.061210 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:30:08,438 [INFO] Regularization: 2939.974121 * 0.0000010000 = 0.0029399742
2019-03-19 00:30:08,439 [INFO] Sum of grad norms: 0.034610
2019-03-19 00:30:08,439 [INFO] ---------------------------------
2019-03-19 00:30:26,919 [INFO] ---------------------------------
2019-03-19 00:30:26,920 [INFO] Summary:
2019-03-19 00:30:26,920 [INFO] Batch 200000, worst loss 0.061244 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:30:26,921 [INFO] Regularization: 2939.974609 * 0.0000010000 = 0.0029399747
2019-03-19 00:30:26,921 [INFO] Sum of grad norms: 0.116335
2019-03-19 00:30:26,922 [INFO] ---------------------------------
2019-03-19 00:30:31,906 [INFO] ---------------------------------
2019-03-19 00:30:31,907 [INFO] Evaluation:
2019-03-19 00:30:31,907 [INFO] Batch 200000, worst loss 0.058093 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:30:31,908 [INFO] ---------------------------------
2019-03-19 00:30:50,785 [INFO] ---------------------------------
2019-03-19 00:30:50,785 [INFO] Summary:
2019-03-19 00:30:50,786 [INFO] Batch 201000, worst loss 0.061148 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:30:50,786 [INFO] Regularization: 2939.974121 * 0.0000010000 = 0.0029399742
2019-03-19 00:30:50,787 [INFO] Sum of grad norms: 0.043098
2019-03-19 00:30:50,787 [INFO] ---------------------------------
2019-03-19 00:31:09,283 [INFO] ---------------------------------
2019-03-19 00:31:09,284 [INFO] Summary:
2019-03-19 00:31:09,284 [INFO] Batch 202000, worst loss 0.061057 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:31:09,285 [INFO] Regularization: 2939.974121 * 0.0000010000 = 0.0029399742
2019-03-19 00:31:09,285 [INFO] Sum of grad norms: 0.042050
2019-03-19 00:31:09,286 [INFO] ---------------------------------
2019-03-19 00:31:28,283 [INFO] ---------------------------------
2019-03-19 00:31:28,284 [INFO] Summary:
2019-03-19 00:31:28,284 [INFO] Batch 203000, worst loss 0.060970 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:31:28,285 [INFO] Regularization: 2939.973877 * 0.0000010000 = 0.0029399740
2019-03-19 00:31:28,285 [INFO] Sum of grad norms: 0.083193
2019-03-19 00:31:28,286 [INFO] ---------------------------------
2019-03-19 00:31:47,518 [INFO] ---------------------------------
2019-03-19 00:31:47,519 [INFO] Summary:
2019-03-19 00:31:47,520 [INFO] Batch 204000, worst loss 0.061185 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:31:47,520 [INFO] Regularization: 2939.973633 * 0.0000010000 = 0.0029399737
2019-03-19 00:31:47,521 [INFO] Sum of grad norms: 0.033719
2019-03-19 00:31:47,522 [INFO] ---------------------------------
2019-03-19 00:32:06,393 [INFO] ---------------------------------
2019-03-19 00:32:06,394 [INFO] Summary:
2019-03-19 00:32:06,395 [INFO] Batch 205000, worst loss 0.061185 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:32:06,395 [INFO] Regularization: 2939.973389 * 0.0000010000 = 0.0029399735
2019-03-19 00:32:06,396 [INFO] Sum of grad norms: 0.061860
2019-03-19 00:32:06,397 [INFO] ---------------------------------
2019-03-19 00:32:25,007 [INFO] ---------------------------------
2019-03-19 00:32:25,008 [INFO] Summary:
2019-03-19 00:32:25,009 [INFO] Batch 206000, worst loss 0.061009 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:32:25,009 [INFO] Regularization: 2939.973633 * 0.0000010000 = 0.0029399737
2019-03-19 00:32:25,010 [INFO] Sum of grad norms: 0.031321
2019-03-19 00:32:25,010 [INFO] ---------------------------------
2019-03-19 00:32:43,860 [INFO] ---------------------------------
2019-03-19 00:32:43,861 [INFO] Summary:
2019-03-19 00:32:43,861 [INFO] Batch 207000, worst loss 0.060973 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:32:43,862 [INFO] Regularization: 2939.973633 * 0.0000010000 = 0.0029399737
2019-03-19 00:32:43,862 [INFO] Sum of grad norms: 0.036677
2019-03-19 00:32:43,863 [INFO] ---------------------------------
2019-03-19 00:33:02,386 [INFO] ---------------------------------
2019-03-19 00:33:02,387 [INFO] Summary:
2019-03-19 00:33:02,387 [INFO] Batch 208000, worst loss 0.061137 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:33:02,388 [INFO] Regularization: 2939.973633 * 0.0000010000 = 0.0029399737
2019-03-19 00:33:02,388 [INFO] Sum of grad norms: 0.046858
2019-03-19 00:33:02,389 [INFO] ---------------------------------
2019-03-19 00:33:21,368 [INFO] ---------------------------------
2019-03-19 00:33:21,369 [INFO] Summary:
2019-03-19 00:33:21,370 [INFO] Batch 209000, worst loss 0.061015 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:33:21,370 [INFO] Regularization: 2939.973633 * 0.0000010000 = 0.0029399737
2019-03-19 00:33:21,371 [INFO] Sum of grad norms: 0.047350
2019-03-19 00:33:21,371 [INFO] ---------------------------------
2019-03-19 00:33:40,472 [INFO] ---------------------------------
2019-03-19 00:33:40,473 [INFO] Summary:
2019-03-19 00:33:40,474 [INFO] Batch 210000, worst loss 0.060887 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:33:40,474 [INFO] Regularization: 2939.973389 * 0.0000010000 = 0.0029399735
2019-03-19 00:33:40,475 [INFO] Sum of grad norms: 0.123473
2019-03-19 00:33:40,476 [INFO] ---------------------------------
2019-03-19 00:33:45,327 [INFO] ---------------------------------
2019-03-19 00:33:45,328 [INFO] Evaluation:
2019-03-19 00:33:45,330 [INFO] Batch 210000, worst loss 0.057778 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:33:45,331 [INFO] ---------------------------------
2019-03-19 00:34:03,923 [INFO] ---------------------------------
2019-03-19 00:34:03,924 [INFO] Summary:
2019-03-19 00:34:03,924 [INFO] Batch 211000, worst loss 0.060971 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:34:03,925 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:34:03,925 [INFO] Sum of grad norms: 0.029474
2019-03-19 00:34:03,926 [INFO] ---------------------------------
2019-03-19 00:34:22,634 [INFO] ---------------------------------
2019-03-19 00:34:22,635 [INFO] Summary:
2019-03-19 00:34:22,636 [INFO] Batch 212000, worst loss 0.061280 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:34:22,636 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:34:22,637 [INFO] Sum of grad norms: 0.024930
2019-03-19 00:34:22,637 [INFO] ---------------------------------
2019-03-19 00:34:41,265 [INFO] ---------------------------------
2019-03-19 00:34:41,266 [INFO] Summary:
2019-03-19 00:34:41,266 [INFO] Batch 213000, worst loss 0.061251 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:34:41,267 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:34:41,267 [INFO] Sum of grad norms: 0.033418
2019-03-19 00:34:41,268 [INFO] ---------------------------------
2019-03-19 00:35:00,119 [INFO] ---------------------------------
2019-03-19 00:35:00,120 [INFO] Summary:
2019-03-19 00:35:00,121 [INFO] Batch 214000, worst loss 0.061251 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:35:00,121 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:35:00,122 [INFO] Sum of grad norms: 0.064169
2019-03-19 00:35:00,123 [INFO] ---------------------------------
2019-03-19 00:35:18,780 [INFO] ---------------------------------
2019-03-19 00:35:18,781 [INFO] Summary:
2019-03-19 00:35:18,781 [INFO] Batch 215000, worst loss 0.061057 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:35:18,782 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:35:18,782 [INFO] Sum of grad norms: 0.023317
2019-03-19 00:35:18,783 [INFO] ---------------------------------
2019-03-19 00:35:37,557 [INFO] ---------------------------------
2019-03-19 00:35:37,558 [INFO] Summary:
2019-03-19 00:35:37,559 [INFO] Batch 216000, worst loss 0.061115 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:35:37,560 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:35:37,560 [INFO] Sum of grad norms: 0.098381
2019-03-19 00:35:37,561 [INFO] ---------------------------------
2019-03-19 00:35:55,915 [INFO] ---------------------------------
2019-03-19 00:35:55,915 [INFO] Summary:
2019-03-19 00:35:55,916 [INFO] Batch 217000, worst loss 0.061209 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:35:55,916 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:35:55,917 [INFO] Sum of grad norms: 0.029135
2019-03-19 00:35:55,917 [INFO] ---------------------------------
2019-03-19 00:36:14,852 [INFO] ---------------------------------
2019-03-19 00:36:14,853 [INFO] Summary:
2019-03-19 00:36:14,853 [INFO] Batch 218000, worst loss 0.061209 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:36:14,854 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:36:14,855 [INFO] Sum of grad norms: 0.074954
2019-03-19 00:36:14,855 [INFO] ---------------------------------
2019-03-19 00:36:33,650 [INFO] ---------------------------------
2019-03-19 00:36:33,651 [INFO] Summary:
2019-03-19 00:36:33,652 [INFO] Batch 219000, worst loss 0.061099 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:36:33,652 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:36:33,653 [INFO] Sum of grad norms: 0.042703
2019-03-19 00:36:33,654 [INFO] ---------------------------------
2019-03-19 00:36:52,407 [INFO] ---------------------------------
2019-03-19 00:36:52,408 [INFO] Summary:
2019-03-19 00:36:52,408 [INFO] Batch 220000, worst loss 0.061559 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:36:52,409 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:36:52,410 [INFO] Sum of grad norms: 0.083003
2019-03-19 00:36:52,410 [INFO] ---------------------------------
2019-03-19 00:36:57,342 [INFO] ---------------------------------
2019-03-19 00:36:57,343 [INFO] Evaluation:
2019-03-19 00:36:57,344 [INFO] Batch 220000, worst loss 0.058096 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:36:57,345 [INFO] ---------------------------------
2019-03-19 00:37:15,962 [INFO] ---------------------------------
2019-03-19 00:37:15,962 [INFO] Summary:
2019-03-19 00:37:15,963 [INFO] Batch 221000, worst loss 0.061032 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:37:15,963 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:37:15,964 [INFO] Sum of grad norms: 0.055468
2019-03-19 00:37:15,965 [INFO] ---------------------------------
2019-03-19 00:37:34,793 [INFO] ---------------------------------
2019-03-19 00:37:34,794 [INFO] Summary:
2019-03-19 00:37:34,795 [INFO] Batch 222000, worst loss 0.061088 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:37:34,796 [INFO] Regularization: 2939.973145 * 0.0000010000 = 0.0029399730
2019-03-19 00:37:34,796 [INFO] Sum of grad norms: 0.056659
2019-03-19 00:37:34,797 [INFO] ---------------------------------
2019-03-19 00:37:53,728 [INFO] ---------------------------------
2019-03-19 00:37:53,729 [INFO] Summary:
2019-03-19 00:37:53,729 [INFO] Batch 223000, worst loss 0.061120 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:37:53,730 [INFO] Regularization: 2939.972900 * 0.0000010000 = 0.0029399728
2019-03-19 00:37:53,730 [INFO] Sum of grad norms: 0.037319
2019-03-19 00:37:53,731 [INFO] ---------------------------------
2019-03-19 00:38:12,601 [INFO] ---------------------------------
2019-03-19 00:38:12,602 [INFO] Summary:
2019-03-19 00:38:12,603 [INFO] Batch 224000, worst loss 0.061034 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:38:12,604 [INFO] Regularization: 2939.972900 * 0.0000010000 = 0.0029399728
2019-03-19 00:38:12,604 [INFO] Sum of grad norms: 0.037778
2019-03-19 00:38:12,605 [INFO] ---------------------------------
2019-03-19 00:38:31,257 [INFO] ---------------------------------
2019-03-19 00:38:31,258 [INFO] Summary:
2019-03-19 00:38:31,258 [INFO] Batch 225000, worst loss 0.061184 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:38:31,259 [INFO] Regularization: 2939.972900 * 0.0000010000 = 0.0029399728
2019-03-19 00:38:31,259 [INFO] Sum of grad norms: 0.126764
2019-03-19 00:38:31,260 [INFO] ---------------------------------
2019-03-19 00:38:50,263 [INFO] ---------------------------------
2019-03-19 00:38:50,264 [INFO] Summary:
2019-03-19 00:38:50,264 [INFO] Batch 226000, worst loss 0.061305 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:38:50,265 [INFO] Regularization: 2939.972900 * 0.0000010000 = 0.0029399728
2019-03-19 00:38:50,265 [INFO] Sum of grad norms: 0.042445
2019-03-19 00:38:50,266 [INFO] ---------------------------------
2019-03-19 00:39:09,319 [INFO] ---------------------------------
2019-03-19 00:39:09,320 [INFO] Summary:
2019-03-19 00:39:09,320 [INFO] Batch 227000, worst loss 0.061305 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:39:09,321 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:39:09,321 [INFO] Sum of grad norms: 0.056972
2019-03-19 00:39:09,322 [INFO] ---------------------------------
2019-03-19 00:39:28,168 [INFO] ---------------------------------
2019-03-19 00:39:28,169 [INFO] Summary:
2019-03-19 00:39:28,170 [INFO] Batch 228000, worst loss 0.061381 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:39:28,171 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:39:28,172 [INFO] Sum of grad norms: 0.074389
2019-03-19 00:39:28,172 [INFO] ---------------------------------
2019-03-19 00:39:46,662 [INFO] ---------------------------------
2019-03-19 00:39:46,663 [INFO] Summary:
2019-03-19 00:39:46,663 [INFO] Batch 229000, worst loss 0.061174 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:39:46,664 [INFO] Regularization: 2939.972900 * 0.0000010000 = 0.0029399728
2019-03-19 00:39:46,664 [INFO] Sum of grad norms: 0.044549
2019-03-19 00:39:46,665 [INFO] ---------------------------------
2019-03-19 00:40:05,342 [INFO] ---------------------------------
2019-03-19 00:40:05,343 [INFO] Summary:
2019-03-19 00:40:05,344 [INFO] Batch 230000, worst loss 0.061176 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:40:05,345 [INFO] Regularization: 2939.972900 * 0.0000010000 = 0.0029399728
2019-03-19 00:40:05,345 [INFO] Sum of grad norms: 0.035123
2019-03-19 00:40:05,346 [INFO] ---------------------------------
2019-03-19 00:40:10,263 [INFO] ---------------------------------
2019-03-19 00:40:10,263 [INFO] Evaluation:
2019-03-19 00:40:10,266 [INFO] Batch 230000, worst loss 0.058129 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:40:10,268 [INFO] ---------------------------------
2019-03-19 00:40:29,320 [INFO] ---------------------------------
2019-03-19 00:40:29,321 [INFO] Summary:
2019-03-19 00:40:29,322 [INFO] Batch 231000, worst loss 0.061069 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:40:29,322 [INFO] Regularization: 2939.972900 * 0.0000010000 = 0.0029399728
2019-03-19 00:40:29,323 [INFO] Sum of grad norms: 0.027102
2019-03-19 00:40:29,323 [INFO] ---------------------------------
2019-03-19 00:40:47,996 [INFO] ---------------------------------
2019-03-19 00:40:47,997 [INFO] Summary:
2019-03-19 00:40:47,998 [INFO] Batch 232000, worst loss 0.060904 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:40:47,999 [INFO] Regularization: 2939.972900 * 0.0000010000 = 0.0029399728
2019-03-19 00:40:47,999 [INFO] Sum of grad norms: 0.034736
2019-03-19 00:40:48,000 [INFO] ---------------------------------
2019-03-19 00:41:06,387 [INFO] ---------------------------------
2019-03-19 00:41:06,389 [INFO] Summary:
2019-03-19 00:41:06,389 [INFO] Batch 233000, worst loss 0.060981 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:41:06,390 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:41:06,390 [INFO] Sum of grad norms: 0.036004
2019-03-19 00:41:06,391 [INFO] ---------------------------------
2019-03-19 00:41:25,384 [INFO] ---------------------------------
2019-03-19 00:41:25,385 [INFO] Summary:
2019-03-19 00:41:25,385 [INFO] Batch 234000, worst loss 0.060926 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:41:25,386 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:41:25,387 [INFO] Sum of grad norms: 0.030064
2019-03-19 00:41:25,387 [INFO] ---------------------------------
2019-03-19 00:41:43,815 [INFO] ---------------------------------
2019-03-19 00:41:43,815 [INFO] Summary:
2019-03-19 00:41:43,816 [INFO] Batch 235000, worst loss 0.061237 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:41:43,817 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:41:43,817 [INFO] Sum of grad norms: 0.060264
2019-03-19 00:41:43,818 [INFO] ---------------------------------
2019-03-19 00:42:02,392 [INFO] ---------------------------------
2019-03-19 00:42:02,393 [INFO] Summary:
2019-03-19 00:42:02,393 [INFO] Batch 236000, worst loss 0.061237 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:42:02,394 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:42:02,394 [INFO] Sum of grad norms: 0.024738
2019-03-19 00:42:02,395 [INFO] ---------------------------------
2019-03-19 00:42:20,780 [INFO] ---------------------------------
2019-03-19 00:42:20,781 [INFO] Summary:
2019-03-19 00:42:20,782 [INFO] Batch 237000, worst loss 0.061075 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:42:20,782 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:42:20,783 [INFO] Sum of grad norms: 0.045696
2019-03-19 00:42:20,783 [INFO] ---------------------------------
2019-03-19 00:42:40,120 [INFO] ---------------------------------
2019-03-19 00:42:40,121 [INFO] Summary:
2019-03-19 00:42:40,122 [INFO] Batch 238000, worst loss 0.061075 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:42:40,122 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:42:40,123 [INFO] Sum of grad norms: 0.027487
2019-03-19 00:42:40,123 [INFO] ---------------------------------
2019-03-19 00:42:58,658 [INFO] ---------------------------------
2019-03-19 00:42:58,659 [INFO] Summary:
2019-03-19 00:42:58,659 [INFO] Batch 239000, worst loss 0.061091 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:42:58,660 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:42:58,661 [INFO] Sum of grad norms: 0.052490
2019-03-19 00:42:58,661 [INFO] ---------------------------------
2019-03-19 00:43:17,309 [INFO] ---------------------------------
2019-03-19 00:43:17,310 [INFO] Summary:
2019-03-19 00:43:17,311 [INFO] Batch 240000, worst loss 0.060947 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:43:17,311 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:43:17,312 [INFO] Sum of grad norms: 0.033771
2019-03-19 00:43:17,312 [INFO] ---------------------------------
2019-03-19 00:43:22,244 [INFO] ---------------------------------
2019-03-19 00:43:22,245 [INFO] Evaluation:
2019-03-19 00:43:22,245 [INFO] Batch 240000, worst loss 0.058303 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:43:22,246 [INFO] ---------------------------------
2019-03-19 00:43:41,139 [INFO] ---------------------------------
2019-03-19 00:43:41,140 [INFO] Summary:
2019-03-19 00:43:41,141 [INFO] Batch 241000, worst loss 0.061243 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:43:41,142 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:43:41,142 [INFO] Sum of grad norms: 0.024039
2019-03-19 00:43:41,143 [INFO] ---------------------------------
2019-03-19 00:43:59,976 [INFO] ---------------------------------
2019-03-19 00:43:59,977 [INFO] Summary:
2019-03-19 00:43:59,977 [INFO] Batch 242000, worst loss 0.061271 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:43:59,978 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:43:59,978 [INFO] Sum of grad norms: 0.068229
2019-03-19 00:43:59,979 [INFO] ---------------------------------
2019-03-19 00:44:18,833 [INFO] ---------------------------------
2019-03-19 00:44:18,834 [INFO] Summary:
2019-03-19 00:44:18,835 [INFO] Batch 243000, worst loss 0.061117 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:44:18,836 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:44:18,836 [INFO] Sum of grad norms: 0.029047
2019-03-19 00:44:18,837 [INFO] ---------------------------------
2019-03-19 00:44:37,724 [INFO] ---------------------------------
2019-03-19 00:44:37,725 [INFO] Summary:
2019-03-19 00:44:37,726 [INFO] Batch 244000, worst loss 0.061007 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:44:37,726 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:44:37,727 [INFO] Sum of grad norms: 0.109345
2019-03-19 00:44:37,728 [INFO] ---------------------------------
2019-03-19 00:44:56,395 [INFO] ---------------------------------
2019-03-19 00:44:56,396 [INFO] Summary:
2019-03-19 00:44:56,396 [INFO] Batch 245000, worst loss 0.061045 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:44:56,397 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:44:56,397 [INFO] Sum of grad norms: 0.050286
2019-03-19 00:44:56,398 [INFO] ---------------------------------
2019-03-19 00:45:15,079 [INFO] ---------------------------------
2019-03-19 00:45:15,080 [INFO] Summary:
2019-03-19 00:45:15,080 [INFO] Batch 246000, worst loss 0.061045 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:45:15,081 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:45:15,081 [INFO] Sum of grad norms: 0.033125
2019-03-19 00:45:15,082 [INFO] ---------------------------------
2019-03-19 00:45:33,807 [INFO] ---------------------------------
2019-03-19 00:45:33,808 [INFO] Summary:
2019-03-19 00:45:33,809 [INFO] Batch 247000, worst loss 0.061145 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:45:33,810 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:45:33,810 [INFO] Sum of grad norms: 0.034496
2019-03-19 00:45:33,811 [INFO] ---------------------------------
2019-03-19 00:45:52,227 [INFO] ---------------------------------
2019-03-19 00:45:52,228 [INFO] Summary:
2019-03-19 00:45:52,228 [INFO] Batch 248000, worst loss 0.061032 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:45:52,229 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:45:52,229 [INFO] Sum of grad norms: 0.076974
2019-03-19 00:45:52,230 [INFO] ---------------------------------
2019-03-19 00:46:11,278 [INFO] ---------------------------------
2019-03-19 00:46:11,279 [INFO] Summary:
2019-03-19 00:46:11,280 [INFO] Batch 249000, worst loss 0.061061 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:46:11,280 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:46:11,281 [INFO] Sum of grad norms: 0.035783
2019-03-19 00:46:11,282 [INFO] ---------------------------------
2019-03-19 00:46:29,964 [INFO] ---------------------------------
2019-03-19 00:46:29,965 [INFO] Summary:
2019-03-19 00:46:29,966 [INFO] Batch 250000, worst loss 0.060953 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 00:46:29,966 [INFO] Regularization: 2939.972656 * 0.0000010000 = 0.0029399726
2019-03-19 00:46:29,967 [INFO] Sum of grad norms: 0.092446
2019-03-19 00:46:29,967 [INFO] ---------------------------------
2019-03-19 00:46:34,887 [INFO] ---------------------------------
2019-03-19 00:46:34,888 [INFO] Evaluation:
2019-03-19 00:46:34,889 [INFO] Batch 250000, worst loss 0.058009 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:46:34,889 [INFO] ---------------------------------
2019-03-19 00:46:34,890 [INFO] Finished training, saved to file classifier/1552933539/1552952794_3_classifier_final.pth
2019-03-19 00:46:35,070 [INFO] ---------------------------------
2019-03-19 00:46:35,071 [INFO] Training model #4: (1, 64, 201) @ 1
2019-03-19 00:46:51,028 [INFO] ---------------------------------
2019-03-19 00:46:51,029 [INFO] Summary:
2019-03-19 00:46:51,030 [INFO] Batch 1000, worst loss 14.183844 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 00:46:51,031 [INFO] Regularization: 9574.404297 * 0.0000010000 = 0.0095744040
2019-03-19 00:46:51,031 [INFO] Sum of grad norms: 0.831333
2019-03-19 00:46:51,032 [INFO] ---------------------------------
2019-03-19 00:47:07,206 [INFO] ---------------------------------
2019-03-19 00:47:07,207 [INFO] Summary:
2019-03-19 00:47:07,208 [INFO] Batch 2000, worst loss 0.153626 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 00:47:07,209 [INFO] Regularization: 8398.823242 * 0.0000010000 = 0.0083988234
2019-03-19 00:47:07,209 [INFO] Sum of grad norms: 4.326875
2019-03-19 00:47:07,210 [INFO] ---------------------------------
2019-03-19 00:47:25,999 [INFO] ---------------------------------
2019-03-19 00:47:26,000 [INFO] Summary:
2019-03-19 00:47:26,001 [INFO] Batch 3000, worst loss 0.120004 (incl. reg.) of 1000 batches, learning rate 0.001536 @cl.-depth 1
2019-03-19 00:47:26,001 [INFO] Regularization: 7423.346191 * 0.0000010000 = 0.0074233464
2019-03-19 00:47:26,002 [INFO] Sum of grad norms: 0.318114
2019-03-19 00:47:26,003 [INFO] ---------------------------------
2019-03-19 00:47:45,248 [INFO] ---------------------------------
2019-03-19 00:47:45,249 [INFO] Summary:
2019-03-19 00:47:45,249 [INFO] Batch 4000, worst loss 0.080285 (incl. reg.) of 1000 batches, learning rate 0.001200 @cl.-depth 1
2019-03-19 00:47:45,250 [INFO] Regularization: 6945.511719 * 0.0000010000 = 0.0069455118
2019-03-19 00:47:45,250 [INFO] Sum of grad norms: 0.593147
2019-03-19 00:47:45,251 [INFO] ---------------------------------
2019-03-19 00:48:04,198 [INFO] ---------------------------------
2019-03-19 00:48:04,199 [INFO] Summary:
2019-03-19 00:48:04,200 [INFO] Batch 5000, worst loss 0.077111 (incl. reg.) of 1000 batches, learning rate 0.000803 @cl.-depth 1
2019-03-19 00:48:04,201 [INFO] Regularization: 6661.212402 * 0.0000010000 = 0.0066612125
2019-03-19 00:48:04,201 [INFO] Sum of grad norms: 0.266766
2019-03-19 00:48:04,202 [INFO] ---------------------------------
2019-03-19 00:48:22,907 [INFO] ---------------------------------
2019-03-19 00:48:22,908 [INFO] Summary:
2019-03-19 00:48:22,908 [INFO] Batch 6000, worst loss 0.075849 (incl. reg.) of 1000 batches, learning rate 0.000771 @cl.-depth 1
2019-03-19 00:48:22,909 [INFO] Regularization: 6428.608398 * 0.0000010000 = 0.0064286082
2019-03-19 00:48:22,909 [INFO] Sum of grad norms: 0.281387
2019-03-19 00:48:22,910 [INFO] ---------------------------------
2019-03-19 00:48:42,067 [INFO] ---------------------------------
2019-03-19 00:48:42,068 [INFO] Summary:
2019-03-19 00:48:42,069 [INFO] Batch 7000, worst loss 0.076366 (incl. reg.) of 1000 batches, learning rate 0.000758 @cl.-depth 1
2019-03-19 00:48:42,069 [INFO] Regularization: 6249.105957 * 0.0000010000 = 0.0062491060
2019-03-19 00:48:42,070 [INFO] Sum of grad norms: 0.359078
2019-03-19 00:48:42,070 [INFO] ---------------------------------
2019-03-19 00:49:01,097 [INFO] ---------------------------------
2019-03-19 00:49:01,097 [INFO] Summary:
2019-03-19 00:49:01,098 [INFO] Batch 8000, worst loss 0.075982 (incl. reg.) of 1000 batches, learning rate 0.000758 @cl.-depth 1
2019-03-19 00:49:01,099 [INFO] Regularization: 6097.371582 * 0.0000010000 = 0.0060973717
2019-03-19 00:49:01,100 [INFO] Sum of grad norms: 0.518807
2019-03-19 00:49:01,101 [INFO] ---------------------------------
2019-03-19 00:49:19,858 [INFO] ---------------------------------
2019-03-19 00:49:19,859 [INFO] Summary:
2019-03-19 00:49:19,859 [INFO] Batch 9000, worst loss 0.075610 (incl. reg.) of 1000 batches, learning rate 0.000758 @cl.-depth 1
2019-03-19 00:49:19,860 [INFO] Regularization: 5943.581543 * 0.0000010000 = 0.0059435815
2019-03-19 00:49:19,860 [INFO] Sum of grad norms: 0.931094
2019-03-19 00:49:19,861 [INFO] ---------------------------------
2019-03-19 00:49:38,620 [INFO] ---------------------------------
2019-03-19 00:49:38,621 [INFO] Summary:
2019-03-19 00:49:38,622 [INFO] Batch 10000, worst loss 0.074642 (incl. reg.) of 1000 batches, learning rate 0.000756 @cl.-depth 1
2019-03-19 00:49:38,623 [INFO] Regularization: 5828.454102 * 0.0000010000 = 0.0058284542
2019-03-19 00:49:38,623 [INFO] Sum of grad norms: 0.673310
2019-03-19 00:49:38,624 [INFO] ---------------------------------
2019-03-19 00:49:43,505 [INFO] ---------------------------------
2019-03-19 00:49:43,506 [INFO] Evaluation:
2019-03-19 00:49:43,507 [INFO] Batch 10000, worst loss 0.064895 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:49:43,508 [INFO] ---------------------------------
2019-03-19 00:50:02,620 [INFO] ---------------------------------
2019-03-19 00:50:02,621 [INFO] Summary:
2019-03-19 00:50:02,622 [INFO] Batch 11000, worst loss 0.072004 (incl. reg.) of 1000 batches, learning rate 0.000746 @cl.-depth 1
2019-03-19 00:50:02,622 [INFO] Regularization: 5737.883789 * 0.0000010000 = 0.0057378840
2019-03-19 00:50:02,623 [INFO] Sum of grad norms: 0.522228
2019-03-19 00:50:02,623 [INFO] ---------------------------------
2019-03-19 00:50:21,771 [INFO] ---------------------------------
2019-03-19 00:50:21,772 [INFO] Summary:
2019-03-19 00:50:21,772 [INFO] Batch 12000, worst loss 0.073210 (incl. reg.) of 1000 batches, learning rate 0.000720 @cl.-depth 1
2019-03-19 00:50:21,773 [INFO] Regularization: 5654.540527 * 0.0000010000 = 0.0056545404
2019-03-19 00:50:21,773 [INFO] Sum of grad norms: 0.773165
2019-03-19 00:50:21,774 [INFO] ---------------------------------
2019-03-19 00:50:40,738 [INFO] ---------------------------------
2019-03-19 00:50:40,739 [INFO] Summary:
2019-03-19 00:50:40,740 [INFO] Batch 13000, worst loss 0.070264 (incl. reg.) of 1000 batches, learning rate 0.000720 @cl.-depth 1
2019-03-19 00:50:40,740 [INFO] Regularization: 5598.583496 * 0.0000010000 = 0.0055985833
2019-03-19 00:50:40,741 [INFO] Sum of grad norms: 0.551834
2019-03-19 00:50:40,741 [INFO] ---------------------------------
2019-03-19 00:50:59,883 [INFO] ---------------------------------
2019-03-19 00:50:59,884 [INFO] Summary:
2019-03-19 00:50:59,885 [INFO] Batch 14000, worst loss 0.069337 (incl. reg.) of 1000 batches, learning rate 0.000703 @cl.-depth 1
2019-03-19 00:50:59,885 [INFO] Regularization: 5528.957520 * 0.0000010000 = 0.0055289576
2019-03-19 00:50:59,886 [INFO] Sum of grad norms: 0.930097
2019-03-19 00:50:59,887 [INFO] ---------------------------------
2019-03-19 00:51:18,434 [INFO] ---------------------------------
2019-03-19 00:51:18,435 [INFO] Summary:
2019-03-19 00:51:18,436 [INFO] Batch 15000, worst loss 0.071841 (incl. reg.) of 1000 batches, learning rate 0.000693 @cl.-depth 1
2019-03-19 00:51:18,436 [INFO] Regularization: 5458.533691 * 0.0000010000 = 0.0054585338
2019-03-19 00:51:18,437 [INFO] Sum of grad norms: 0.613463
2019-03-19 00:51:18,437 [INFO] ---------------------------------
2019-03-19 00:51:37,638 [INFO] ---------------------------------
2019-03-19 00:51:37,639 [INFO] Summary:
2019-03-19 00:51:37,640 [INFO] Batch 16000, worst loss 0.070960 (incl. reg.) of 1000 batches, learning rate 0.000693 @cl.-depth 1
2019-03-19 00:51:37,640 [INFO] Regularization: 5404.055176 * 0.0000010000 = 0.0054040551
2019-03-19 00:51:37,641 [INFO] Sum of grad norms: 0.641173
2019-03-19 00:51:37,642 [INFO] ---------------------------------
2019-03-19 00:51:56,661 [INFO] ---------------------------------
2019-03-19 00:51:56,662 [INFO] Summary:
2019-03-19 00:51:56,662 [INFO] Batch 17000, worst loss 0.071598 (incl. reg.) of 1000 batches, learning rate 0.000693 @cl.-depth 1
2019-03-19 00:51:56,663 [INFO] Regularization: 5329.464844 * 0.0000010000 = 0.0053294650
2019-03-19 00:51:56,663 [INFO] Sum of grad norms: 0.297278
2019-03-19 00:51:56,664 [INFO] ---------------------------------
2019-03-19 00:52:15,611 [INFO] ---------------------------------
2019-03-19 00:52:15,612 [INFO] Summary:
2019-03-19 00:52:15,612 [INFO] Batch 18000, worst loss 0.068359 (incl. reg.) of 1000 batches, learning rate 0.000693 @cl.-depth 1
2019-03-19 00:52:15,613 [INFO] Regularization: 5264.053711 * 0.0000010000 = 0.0052640536
2019-03-19 00:52:15,613 [INFO] Sum of grad norms: 1.124512
2019-03-19 00:52:15,614 [INFO] ---------------------------------
2019-03-19 00:52:34,851 [INFO] ---------------------------------
2019-03-19 00:52:34,852 [INFO] Summary:
2019-03-19 00:52:34,852 [INFO] Batch 19000, worst loss 0.067892 (incl. reg.) of 1000 batches, learning rate 0.000684 @cl.-depth 1
2019-03-19 00:52:34,853 [INFO] Regularization: 5205.894043 * 0.0000010000 = 0.0052058939
2019-03-19 00:52:34,853 [INFO] Sum of grad norms: 0.369917
2019-03-19 00:52:34,854 [INFO] ---------------------------------
2019-03-19 00:52:53,806 [INFO] ---------------------------------
2019-03-19 00:52:53,807 [INFO] Summary:
2019-03-19 00:52:53,807 [INFO] Batch 20000, worst loss 0.069669 (incl. reg.) of 1000 batches, learning rate 0.000679 @cl.-depth 1
2019-03-19 00:52:53,808 [INFO] Regularization: 5152.796875 * 0.0000010000 = 0.0051527969
2019-03-19 00:52:53,808 [INFO] Sum of grad norms: 1.121209
2019-03-19 00:52:53,809 [INFO] ---------------------------------
2019-03-19 00:52:58,703 [INFO] ---------------------------------
2019-03-19 00:52:58,704 [INFO] Evaluation:
2019-03-19 00:52:58,707 [INFO] Batch 20000, worst loss 0.060924 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:52:58,708 [INFO] ---------------------------------
2019-03-19 00:53:17,739 [INFO] ---------------------------------
2019-03-19 00:53:17,740 [INFO] Summary:
2019-03-19 00:53:17,740 [INFO] Batch 21000, worst loss 0.070412 (incl. reg.) of 1000 batches, learning rate 0.000679 @cl.-depth 1
2019-03-19 00:53:17,741 [INFO] Regularization: 5082.137695 * 0.0000010000 = 0.0050821379
2019-03-19 00:53:17,741 [INFO] Sum of grad norms: 0.273487
2019-03-19 00:53:17,742 [INFO] ---------------------------------
2019-03-19 00:53:36,927 [INFO] ---------------------------------
2019-03-19 00:53:36,928 [INFO] Summary:
2019-03-19 00:53:36,928 [INFO] Batch 22000, worst loss 0.069653 (incl. reg.) of 1000 batches, learning rate 0.000679 @cl.-depth 1
2019-03-19 00:53:36,929 [INFO] Regularization: 5028.078613 * 0.0000010000 = 0.0050280788
2019-03-19 00:53:36,929 [INFO] Sum of grad norms: 0.975394
2019-03-19 00:53:36,930 [INFO] ---------------------------------
2019-03-19 00:53:56,006 [INFO] ---------------------------------
2019-03-19 00:53:56,007 [INFO] Summary:
2019-03-19 00:53:56,008 [INFO] Batch 23000, worst loss 0.066359 (incl. reg.) of 1000 batches, learning rate 0.000679 @cl.-depth 1
2019-03-19 00:53:56,008 [INFO] Regularization: 4955.567871 * 0.0000010000 = 0.0049555679
2019-03-19 00:53:56,009 [INFO] Sum of grad norms: 1.384529
2019-03-19 00:53:56,009 [INFO] ---------------------------------
2019-03-19 00:54:15,213 [INFO] ---------------------------------
2019-03-19 00:54:15,214 [INFO] Summary:
2019-03-19 00:54:15,215 [INFO] Batch 24000, worst loss 0.066275 (incl. reg.) of 1000 batches, learning rate 0.000664 @cl.-depth 1
2019-03-19 00:54:15,216 [INFO] Regularization: 4892.148926 * 0.0000010000 = 0.0048921490
2019-03-19 00:54:15,216 [INFO] Sum of grad norms: 0.245530
2019-03-19 00:54:15,217 [INFO] ---------------------------------
2019-03-19 00:54:33,873 [INFO] ---------------------------------
2019-03-19 00:54:33,874 [INFO] Summary:
2019-03-19 00:54:33,874 [INFO] Batch 25000, worst loss 0.066059 (incl. reg.) of 1000 batches, learning rate 0.000663 @cl.-depth 1
2019-03-19 00:54:33,875 [INFO] Regularization: 4831.226562 * 0.0000010000 = 0.0048312265
2019-03-19 00:54:33,875 [INFO] Sum of grad norms: 0.127490
2019-03-19 00:54:33,876 [INFO] ---------------------------------
2019-03-19 00:54:52,532 [INFO] ---------------------------------
2019-03-19 00:54:52,533 [INFO] Summary:
2019-03-19 00:54:52,533 [INFO] Batch 26000, worst loss 0.070048 (incl. reg.) of 1000 batches, learning rate 0.000661 @cl.-depth 1
2019-03-19 00:54:52,534 [INFO] Regularization: 4773.590332 * 0.0000010000 = 0.0047735902
2019-03-19 00:54:52,534 [INFO] Sum of grad norms: 0.605789
2019-03-19 00:54:52,535 [INFO] ---------------------------------
2019-03-19 00:55:11,097 [INFO] ---------------------------------
2019-03-19 00:55:11,098 [INFO] Summary:
2019-03-19 00:55:11,099 [INFO] Batch 27000, worst loss 0.066241 (incl. reg.) of 1000 batches, learning rate 0.000661 @cl.-depth 1
2019-03-19 00:55:11,099 [INFO] Regularization: 4736.354004 * 0.0000010000 = 0.0047363541
2019-03-19 00:55:11,100 [INFO] Sum of grad norms: 1.308314
2019-03-19 00:55:11,100 [INFO] ---------------------------------
2019-03-19 00:55:29,627 [INFO] ---------------------------------
2019-03-19 00:55:29,628 [INFO] Summary:
2019-03-19 00:55:29,629 [INFO] Batch 28000, worst loss 0.066058 (incl. reg.) of 1000 batches, learning rate 0.000661 @cl.-depth 1
2019-03-19 00:55:29,629 [INFO] Regularization: 4646.338867 * 0.0000010000 = 0.0046463390
2019-03-19 00:55:29,630 [INFO] Sum of grad norms: 0.338512
2019-03-19 00:55:29,630 [INFO] ---------------------------------
2019-03-19 00:55:48,364 [INFO] ---------------------------------
2019-03-19 00:55:48,365 [INFO] Summary:
2019-03-19 00:55:48,366 [INFO] Batch 29000, worst loss 0.067084 (incl. reg.) of 1000 batches, learning rate 0.000661 @cl.-depth 1
2019-03-19 00:55:48,366 [INFO] Regularization: 4587.405762 * 0.0000010000 = 0.0045874058
2019-03-19 00:55:48,367 [INFO] Sum of grad norms: 0.548102
2019-03-19 00:55:48,367 [INFO] ---------------------------------
2019-03-19 00:56:07,091 [INFO] ---------------------------------
2019-03-19 00:56:07,092 [INFO] Summary:
2019-03-19 00:56:07,093 [INFO] Batch 30000, worst loss 0.067329 (incl. reg.) of 1000 batches, learning rate 0.000661 @cl.-depth 1
2019-03-19 00:56:07,094 [INFO] Regularization: 4540.899902 * 0.0000010000 = 0.0045408998
2019-03-19 00:56:07,095 [INFO] Sum of grad norms: 0.601263
2019-03-19 00:56:07,096 [INFO] ---------------------------------
2019-03-19 00:56:12,021 [INFO] ---------------------------------
2019-03-19 00:56:12,022 [INFO] Evaluation:
2019-03-19 00:56:12,023 [INFO] Batch 30000, worst loss 0.060289 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:56:12,024 [INFO] ---------------------------------
2019-03-19 00:56:30,906 [INFO] ---------------------------------
2019-03-19 00:56:30,907 [INFO] Summary:
2019-03-19 00:56:30,908 [INFO] Batch 31000, worst loss 0.065187 (incl. reg.) of 1000 batches, learning rate 0.000661 @cl.-depth 1
2019-03-19 00:56:30,908 [INFO] Regularization: 4473.913574 * 0.0000010000 = 0.0044739135
2019-03-19 00:56:30,909 [INFO] Sum of grad norms: 0.111913
2019-03-19 00:56:30,910 [INFO] ---------------------------------
2019-03-19 00:56:49,621 [INFO] ---------------------------------
2019-03-19 00:56:49,622 [INFO] Summary:
2019-03-19 00:56:49,622 [INFO] Batch 32000, worst loss 0.065141 (incl. reg.) of 1000 batches, learning rate 0.000652 @cl.-depth 1
2019-03-19 00:56:49,623 [INFO] Regularization: 4412.703613 * 0.0000010000 = 0.0044127037
2019-03-19 00:56:49,624 [INFO] Sum of grad norms: 0.340879
2019-03-19 00:56:49,624 [INFO] ---------------------------------
2019-03-19 00:57:08,865 [INFO] ---------------------------------
2019-03-19 00:57:08,867 [INFO] Summary:
2019-03-19 00:57:08,867 [INFO] Batch 33000, worst loss 0.067553 (incl. reg.) of 1000 batches, learning rate 0.000651 @cl.-depth 1
2019-03-19 00:57:08,868 [INFO] Regularization: 4361.049805 * 0.0000010000 = 0.0043610497
2019-03-19 00:57:08,869 [INFO] Sum of grad norms: 1.113762
2019-03-19 00:57:08,870 [INFO] ---------------------------------
2019-03-19 00:57:28,279 [INFO] ---------------------------------
2019-03-19 00:57:28,280 [INFO] Summary:
2019-03-19 00:57:28,281 [INFO] Batch 34000, worst loss 0.064841 (incl. reg.) of 1000 batches, learning rate 0.000651 @cl.-depth 1
2019-03-19 00:57:28,281 [INFO] Regularization: 4309.391602 * 0.0000010000 = 0.0043093916
2019-03-19 00:57:28,282 [INFO] Sum of grad norms: 0.159696
2019-03-19 00:57:28,282 [INFO] ---------------------------------
2019-03-19 00:57:47,152 [INFO] ---------------------------------
2019-03-19 00:57:47,153 [INFO] Summary:
2019-03-19 00:57:47,154 [INFO] Batch 35000, worst loss 0.064579 (incl. reg.) of 1000 batches, learning rate 0.000648 @cl.-depth 1
2019-03-19 00:57:47,154 [INFO] Regularization: 4261.158203 * 0.0000010000 = 0.0042611584
2019-03-19 00:57:47,155 [INFO] Sum of grad norms: 0.077250
2019-03-19 00:57:47,156 [INFO] ---------------------------------
2019-03-19 00:58:05,787 [INFO] ---------------------------------
2019-03-19 00:58:05,788 [INFO] Summary:
2019-03-19 00:58:05,788 [INFO] Batch 36000, worst loss 0.068354 (incl. reg.) of 1000 batches, learning rate 0.000646 @cl.-depth 1
2019-03-19 00:58:05,789 [INFO] Regularization: 4216.203125 * 0.0000010000 = 0.0042162030
2019-03-19 00:58:05,790 [INFO] Sum of grad norms: 0.099051
2019-03-19 00:58:05,790 [INFO] ---------------------------------
2019-03-19 00:58:24,427 [INFO] ---------------------------------
2019-03-19 00:58:24,427 [INFO] Summary:
2019-03-19 00:58:24,428 [INFO] Batch 37000, worst loss 0.064313 (incl. reg.) of 1000 batches, learning rate 0.000646 @cl.-depth 1
2019-03-19 00:58:24,429 [INFO] Regularization: 4158.602539 * 0.0000010000 = 0.0041586026
2019-03-19 00:58:24,429 [INFO] Sum of grad norms: 1.098134
2019-03-19 00:58:24,430 [INFO] ---------------------------------
2019-03-19 00:58:43,371 [INFO] ---------------------------------
2019-03-19 00:58:43,372 [INFO] Summary:
2019-03-19 00:58:43,372 [INFO] Batch 38000, worst loss 0.065919 (incl. reg.) of 1000 batches, learning rate 0.000643 @cl.-depth 1
2019-03-19 00:58:43,373 [INFO] Regularization: 4107.708984 * 0.0000010000 = 0.0041077090
2019-03-19 00:58:43,373 [INFO] Sum of grad norms: 0.088166
2019-03-19 00:58:43,374 [INFO] ---------------------------------
2019-03-19 00:59:02,177 [INFO] ---------------------------------
2019-03-19 00:59:02,177 [INFO] Summary:
2019-03-19 00:59:02,178 [INFO] Batch 39000, worst loss 0.064462 (incl. reg.) of 1000 batches, learning rate 0.000643 @cl.-depth 1
2019-03-19 00:59:02,178 [INFO] Regularization: 4059.842529 * 0.0000010000 = 0.0040598423
2019-03-19 00:59:02,179 [INFO] Sum of grad norms: 0.171958
2019-03-19 00:59:02,180 [INFO] ---------------------------------
2019-03-19 00:59:21,199 [INFO] ---------------------------------
2019-03-19 00:59:21,200 [INFO] Summary:
2019-03-19 00:59:21,200 [INFO] Batch 40000, worst loss 0.070164 (incl. reg.) of 1000 batches, learning rate 0.000643 @cl.-depth 1
2019-03-19 00:59:21,201 [INFO] Regularization: 4019.385498 * 0.0000010000 = 0.0040193857
2019-03-19 00:59:21,202 [INFO] Sum of grad norms: 0.128732
2019-03-19 00:59:21,203 [INFO] ---------------------------------
2019-03-19 00:59:26,064 [INFO] ---------------------------------
2019-03-19 00:59:26,065 [INFO] Evaluation:
2019-03-19 00:59:26,066 [INFO] Batch 40000, worst loss 0.058985 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 00:59:26,066 [INFO] ---------------------------------
2019-03-19 00:59:44,737 [INFO] ---------------------------------
2019-03-19 00:59:44,738 [INFO] Summary:
2019-03-19 00:59:44,739 [INFO] Batch 41000, worst loss 0.065005 (incl. reg.) of 1000 batches, learning rate 0.000322 @cl.-depth 1
2019-03-19 00:59:44,740 [INFO] Regularization: 3972.101807 * 0.0000010000 = 0.0039721020
2019-03-19 00:59:44,740 [INFO] Sum of grad norms: 0.117772
2019-03-19 00:59:44,741 [INFO] ---------------------------------
2019-03-19 01:00:03,617 [INFO] ---------------------------------
2019-03-19 01:00:03,617 [INFO] Summary:
2019-03-19 01:00:03,618 [INFO] Batch 42000, worst loss 0.063056 (incl. reg.) of 1000 batches, learning rate 0.000322 @cl.-depth 1
2019-03-19 01:00:03,619 [INFO] Regularization: 3931.703613 * 0.0000010000 = 0.0039317035
2019-03-19 01:00:03,619 [INFO] Sum of grad norms: 0.223489
2019-03-19 01:00:03,620 [INFO] ---------------------------------
2019-03-19 01:00:22,534 [INFO] ---------------------------------
2019-03-19 01:00:22,535 [INFO] Summary:
2019-03-19 01:00:22,535 [INFO] Batch 43000, worst loss 0.062922 (incl. reg.) of 1000 batches, learning rate 0.000322 @cl.-depth 1
2019-03-19 01:00:22,536 [INFO] Regularization: 3901.713867 * 0.0000010000 = 0.0039017138
2019-03-19 01:00:22,536 [INFO] Sum of grad norms: 0.081340
2019-03-19 01:00:22,537 [INFO] ---------------------------------
2019-03-19 01:00:41,083 [INFO] ---------------------------------
2019-03-19 01:00:41,084 [INFO] Summary:
2019-03-19 01:00:41,084 [INFO] Batch 44000, worst loss 0.063076 (incl. reg.) of 1000 batches, learning rate 0.000322 @cl.-depth 1
2019-03-19 01:00:41,085 [INFO] Regularization: 3872.482910 * 0.0000010000 = 0.0038724828
2019-03-19 01:00:41,085 [INFO] Sum of grad norms: 0.042771
2019-03-19 01:00:41,086 [INFO] ---------------------------------
2019-03-19 01:00:59,869 [INFO] ---------------------------------
2019-03-19 01:00:59,870 [INFO] Summary:
2019-03-19 01:00:59,871 [INFO] Batch 45000, worst loss 0.062695 (incl. reg.) of 1000 batches, learning rate 0.000322 @cl.-depth 1
2019-03-19 01:00:59,871 [INFO] Regularization: 3843.631348 * 0.0000010000 = 0.0038436314
2019-03-19 01:00:59,872 [INFO] Sum of grad norms: 0.341265
2019-03-19 01:00:59,872 [INFO] ---------------------------------
2019-03-19 01:01:19,086 [INFO] ---------------------------------
2019-03-19 01:01:19,088 [INFO] Summary:
2019-03-19 01:01:19,088 [INFO] Batch 46000, worst loss 0.062674 (incl. reg.) of 1000 batches, learning rate 0.000322 @cl.-depth 1
2019-03-19 01:01:19,089 [INFO] Regularization: 3815.940918 * 0.0000010000 = 0.0038159408
2019-03-19 01:01:19,090 [INFO] Sum of grad norms: 0.091740
2019-03-19 01:01:19,090 [INFO] ---------------------------------
2019-03-19 01:01:37,728 [INFO] ---------------------------------
2019-03-19 01:01:37,729 [INFO] Summary:
2019-03-19 01:01:37,730 [INFO] Batch 47000, worst loss 0.062313 (incl. reg.) of 1000 batches, learning rate 0.000322 @cl.-depth 1
2019-03-19 01:01:37,730 [INFO] Regularization: 3788.087402 * 0.0000010000 = 0.0037880873
2019-03-19 01:01:37,731 [INFO] Sum of grad norms: 0.109558
2019-03-19 01:01:37,731 [INFO] ---------------------------------
2019-03-19 01:01:56,385 [INFO] ---------------------------------
2019-03-19 01:01:56,386 [INFO] Summary:
2019-03-19 01:01:56,387 [INFO] Batch 48000, worst loss 0.062071 (incl. reg.) of 1000 batches, learning rate 0.000322 @cl.-depth 1
2019-03-19 01:01:56,387 [INFO] Regularization: 3759.872070 * 0.0000010000 = 0.0037598722
2019-03-19 01:01:56,388 [INFO] Sum of grad norms: 0.076502
2019-03-19 01:01:56,389 [INFO] ---------------------------------
2019-03-19 01:02:15,650 [INFO] ---------------------------------
2019-03-19 01:02:15,651 [INFO] Summary:
2019-03-19 01:02:15,652 [INFO] Batch 49000, worst loss 0.062298 (incl. reg.) of 1000 batches, learning rate 0.000322 @cl.-depth 1
2019-03-19 01:02:15,653 [INFO] Regularization: 3732.792969 * 0.0000010000 = 0.0037327930
2019-03-19 01:02:15,653 [INFO] Sum of grad norms: 0.090378
2019-03-19 01:02:15,654 [INFO] ---------------------------------
2019-03-19 01:02:35,150 [INFO] ---------------------------------
2019-03-19 01:02:35,151 [INFO] Summary:
2019-03-19 01:02:35,152 [INFO] Batch 50000, worst loss 0.062407 (incl. reg.) of 1000 batches, learning rate 0.000322 @cl.-depth 1
2019-03-19 01:02:35,152 [INFO] Regularization: 3705.499512 * 0.0000010000 = 0.0037054995
2019-03-19 01:02:35,153 [INFO] Sum of grad norms: 0.808972
2019-03-19 01:02:35,154 [INFO] ---------------------------------
2019-03-19 01:02:40,040 [INFO] ---------------------------------
2019-03-19 01:02:40,041 [INFO] Evaluation:
2019-03-19 01:02:40,043 [INFO] Batch 50000, worst loss 0.058276 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:02:40,044 [INFO] ---------------------------------
2019-03-19 01:02:58,911 [INFO] ---------------------------------
2019-03-19 01:02:58,912 [INFO] Summary:
2019-03-19 01:02:58,913 [INFO] Batch 51000, worst loss 0.062286 (incl. reg.) of 1000 batches, learning rate 0.000161 @cl.-depth 1
2019-03-19 01:02:58,913 [INFO] Regularization: 3678.935791 * 0.0000010000 = 0.0036789358
2019-03-19 01:02:58,914 [INFO] Sum of grad norms: 0.066388
2019-03-19 01:02:58,914 [INFO] ---------------------------------
2019-03-19 01:03:17,413 [INFO] ---------------------------------
2019-03-19 01:03:17,414 [INFO] Summary:
2019-03-19 01:03:17,415 [INFO] Batch 52000, worst loss 0.061810 (incl. reg.) of 1000 batches, learning rate 0.000161 @cl.-depth 1
2019-03-19 01:03:17,416 [INFO] Regularization: 3660.440430 * 0.0000010000 = 0.0036604404
2019-03-19 01:03:17,417 [INFO] Sum of grad norms: 0.105343
2019-03-19 01:03:17,417 [INFO] ---------------------------------
2019-03-19 01:03:36,202 [INFO] ---------------------------------
2019-03-19 01:03:36,203 [INFO] Summary:
2019-03-19 01:03:36,203 [INFO] Batch 53000, worst loss 0.061910 (incl. reg.) of 1000 batches, learning rate 0.000161 @cl.-depth 1
2019-03-19 01:03:36,204 [INFO] Regularization: 3644.431885 * 0.0000010000 = 0.0036444319
2019-03-19 01:03:36,204 [INFO] Sum of grad norms: 0.136046
2019-03-19 01:03:36,205 [INFO] ---------------------------------
2019-03-19 01:03:55,226 [INFO] ---------------------------------
2019-03-19 01:03:55,227 [INFO] Summary:
2019-03-19 01:03:55,227 [INFO] Batch 54000, worst loss 0.061844 (incl. reg.) of 1000 batches, learning rate 0.000161 @cl.-depth 1
2019-03-19 01:03:55,228 [INFO] Regularization: 3628.974121 * 0.0000010000 = 0.0036289741
2019-03-19 01:03:55,228 [INFO] Sum of grad norms: 0.046524
2019-03-19 01:03:55,229 [INFO] ---------------------------------
2019-03-19 01:04:14,017 [INFO] ---------------------------------
2019-03-19 01:04:14,018 [INFO] Summary:
2019-03-19 01:04:14,019 [INFO] Batch 55000, worst loss 0.061778 (incl. reg.) of 1000 batches, learning rate 0.000161 @cl.-depth 1
2019-03-19 01:04:14,019 [INFO] Regularization: 3613.493652 * 0.0000010000 = 0.0036134936
2019-03-19 01:04:14,020 [INFO] Sum of grad norms: 0.090498
2019-03-19 01:04:14,021 [INFO] ---------------------------------
2019-03-19 01:04:32,771 [INFO] ---------------------------------
2019-03-19 01:04:32,773 [INFO] Summary:
2019-03-19 01:04:32,773 [INFO] Batch 56000, worst loss 0.061585 (incl. reg.) of 1000 batches, learning rate 0.000161 @cl.-depth 1
2019-03-19 01:04:32,774 [INFO] Regularization: 3598.957031 * 0.0000010000 = 0.0035989571
2019-03-19 01:04:32,774 [INFO] Sum of grad norms: 0.068064
2019-03-19 01:04:32,775 [INFO] ---------------------------------
2019-03-19 01:04:51,217 [INFO] ---------------------------------
2019-03-19 01:04:51,218 [INFO] Summary:
2019-03-19 01:04:51,219 [INFO] Batch 57000, worst loss 0.061778 (incl. reg.) of 1000 batches, learning rate 0.000161 @cl.-depth 1
2019-03-19 01:04:51,219 [INFO] Regularization: 3583.258789 * 0.0000010000 = 0.0035832587
2019-03-19 01:04:51,220 [INFO] Sum of grad norms: 0.139544
2019-03-19 01:04:51,220 [INFO] ---------------------------------
2019-03-19 01:05:09,800 [INFO] ---------------------------------
2019-03-19 01:05:09,801 [INFO] Summary:
2019-03-19 01:05:09,801 [INFO] Batch 58000, worst loss 0.061706 (incl. reg.) of 1000 batches, learning rate 0.000161 @cl.-depth 1
2019-03-19 01:05:09,802 [INFO] Regularization: 3568.775146 * 0.0000010000 = 0.0035687753
2019-03-19 01:05:09,802 [INFO] Sum of grad norms: 0.080480
2019-03-19 01:05:09,803 [INFO] ---------------------------------
2019-03-19 01:05:28,781 [INFO] ---------------------------------
2019-03-19 01:05:28,782 [INFO] Summary:
2019-03-19 01:05:28,783 [INFO] Batch 59000, worst loss 0.061493 (incl. reg.) of 1000 batches, learning rate 0.000161 @cl.-depth 1
2019-03-19 01:05:28,783 [INFO] Regularization: 3553.188965 * 0.0000010000 = 0.0035531889
2019-03-19 01:05:28,784 [INFO] Sum of grad norms: 0.122882
2019-03-19 01:05:28,784 [INFO] ---------------------------------
2019-03-19 01:05:47,633 [INFO] ---------------------------------
2019-03-19 01:05:47,634 [INFO] Summary:
2019-03-19 01:05:47,634 [INFO] Batch 60000, worst loss 0.061999 (incl. reg.) of 1000 batches, learning rate 0.000161 @cl.-depth 1
2019-03-19 01:05:47,635 [INFO] Regularization: 3539.210938 * 0.0000010000 = 0.0035392109
2019-03-19 01:05:47,636 [INFO] Sum of grad norms: 0.077901
2019-03-19 01:05:47,636 [INFO] ---------------------------------
2019-03-19 01:05:52,550 [INFO] ---------------------------------
2019-03-19 01:05:52,551 [INFO] Evaluation:
2019-03-19 01:05:52,552 [INFO] Batch 60000, worst loss 0.058020 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:05:52,553 [INFO] ---------------------------------
2019-03-19 01:06:11,266 [INFO] ---------------------------------
2019-03-19 01:06:11,267 [INFO] Summary:
2019-03-19 01:06:11,268 [INFO] Batch 61000, worst loss 0.061641 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 01:06:11,268 [INFO] Regularization: 3524.191406 * 0.0000010000 = 0.0035241914
2019-03-19 01:06:11,269 [INFO] Sum of grad norms: 0.089808
2019-03-19 01:06:11,269 [INFO] ---------------------------------
2019-03-19 01:06:30,350 [INFO] ---------------------------------
2019-03-19 01:06:30,351 [INFO] Summary:
2019-03-19 01:06:30,351 [INFO] Batch 62000, worst loss 0.061695 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 01:06:30,352 [INFO] Regularization: 3514.346191 * 0.0000010000 = 0.0035143462
2019-03-19 01:06:30,352 [INFO] Sum of grad norms: 0.034746
2019-03-19 01:06:30,353 [INFO] ---------------------------------
2019-03-19 01:06:49,037 [INFO] ---------------------------------
2019-03-19 01:06:49,038 [INFO] Summary:
2019-03-19 01:06:49,038 [INFO] Batch 63000, worst loss 0.061623 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 01:06:49,039 [INFO] Regularization: 3506.710449 * 0.0000010000 = 0.0035067105
2019-03-19 01:06:49,039 [INFO] Sum of grad norms: 0.063081
2019-03-19 01:06:49,040 [INFO] ---------------------------------
2019-03-19 01:07:07,852 [INFO] ---------------------------------
2019-03-19 01:07:07,853 [INFO] Summary:
2019-03-19 01:07:07,853 [INFO] Batch 64000, worst loss 0.061614 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 01:07:07,854 [INFO] Regularization: 3498.849854 * 0.0000010000 = 0.0034988499
2019-03-19 01:07:07,855 [INFO] Sum of grad norms: 0.052181
2019-03-19 01:07:07,855 [INFO] ---------------------------------
2019-03-19 01:07:26,381 [INFO] ---------------------------------
2019-03-19 01:07:26,382 [INFO] Summary:
2019-03-19 01:07:26,383 [INFO] Batch 65000, worst loss 0.061538 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 01:07:26,383 [INFO] Regularization: 3490.638428 * 0.0000010000 = 0.0034906385
2019-03-19 01:07:26,384 [INFO] Sum of grad norms: 0.065415
2019-03-19 01:07:26,385 [INFO] ---------------------------------
2019-03-19 01:07:45,358 [INFO] ---------------------------------
2019-03-19 01:07:45,359 [INFO] Summary:
2019-03-19 01:07:45,359 [INFO] Batch 66000, worst loss 0.061473 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 01:07:45,360 [INFO] Regularization: 3483.420166 * 0.0000010000 = 0.0034834202
2019-03-19 01:07:45,360 [INFO] Sum of grad norms: 0.038672
2019-03-19 01:07:45,361 [INFO] ---------------------------------
2019-03-19 01:08:03,826 [INFO] ---------------------------------
2019-03-19 01:08:03,827 [INFO] Summary:
2019-03-19 01:08:03,828 [INFO] Batch 67000, worst loss 0.061330 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 01:08:03,828 [INFO] Regularization: 3475.138916 * 0.0000010000 = 0.0034751389
2019-03-19 01:08:03,829 [INFO] Sum of grad norms: 0.054774
2019-03-19 01:08:03,829 [INFO] ---------------------------------
2019-03-19 01:08:23,103 [INFO] ---------------------------------
2019-03-19 01:08:23,104 [INFO] Summary:
2019-03-19 01:08:23,105 [INFO] Batch 68000, worst loss 0.061428 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 01:08:23,105 [INFO] Regularization: 3467.260742 * 0.0000010000 = 0.0034672606
2019-03-19 01:08:23,106 [INFO] Sum of grad norms: 0.067968
2019-03-19 01:08:23,106 [INFO] ---------------------------------
2019-03-19 01:08:41,892 [INFO] ---------------------------------
2019-03-19 01:08:41,893 [INFO] Summary:
2019-03-19 01:08:41,894 [INFO] Batch 69000, worst loss 0.061539 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 01:08:41,895 [INFO] Regularization: 3458.112061 * 0.0000010000 = 0.0034581120
2019-03-19 01:08:41,895 [INFO] Sum of grad norms: 0.068747
2019-03-19 01:08:41,896 [INFO] ---------------------------------
2019-03-19 01:09:00,941 [INFO] ---------------------------------
2019-03-19 01:09:00,941 [INFO] Summary:
2019-03-19 01:09:00,942 [INFO] Batch 70000, worst loss 0.061469 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 01:09:00,943 [INFO] Regularization: 3450.946777 * 0.0000010000 = 0.0034509469
2019-03-19 01:09:00,943 [INFO] Sum of grad norms: 0.078015
2019-03-19 01:09:00,944 [INFO] ---------------------------------
2019-03-19 01:09:05,813 [INFO] ---------------------------------
2019-03-19 01:09:05,814 [INFO] Evaluation:
2019-03-19 01:09:05,815 [INFO] Batch 70000, worst loss 0.057909 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:09:05,815 [INFO] ---------------------------------
2019-03-19 01:09:24,576 [INFO] ---------------------------------
2019-03-19 01:09:24,576 [INFO] Summary:
2019-03-19 01:09:24,577 [INFO] Batch 71000, worst loss 0.061251 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 01:09:24,578 [INFO] Regularization: 3442.983887 * 0.0000010000 = 0.0034429838
2019-03-19 01:09:24,578 [INFO] Sum of grad norms: 0.066168
2019-03-19 01:09:24,579 [INFO] ---------------------------------
2019-03-19 01:09:43,192 [INFO] ---------------------------------
2019-03-19 01:09:43,193 [INFO] Summary:
2019-03-19 01:09:43,193 [INFO] Batch 72000, worst loss 0.061396 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 01:09:43,194 [INFO] Regularization: 3438.072510 * 0.0000010000 = 0.0034380725
2019-03-19 01:09:43,194 [INFO] Sum of grad norms: 0.025489
2019-03-19 01:09:43,195 [INFO] ---------------------------------
2019-03-19 01:10:02,096 [INFO] ---------------------------------
2019-03-19 01:10:02,097 [INFO] Summary:
2019-03-19 01:10:02,098 [INFO] Batch 73000, worst loss 0.061219 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 01:10:02,098 [INFO] Regularization: 3434.144531 * 0.0000010000 = 0.0034341444
2019-03-19 01:10:02,099 [INFO] Sum of grad norms: 0.053017
2019-03-19 01:10:02,099 [INFO] ---------------------------------
2019-03-19 01:10:20,692 [INFO] ---------------------------------
2019-03-19 01:10:20,693 [INFO] Summary:
2019-03-19 01:10:20,694 [INFO] Batch 74000, worst loss 0.061271 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 01:10:20,694 [INFO] Regularization: 3429.970703 * 0.0000010000 = 0.0034299707
2019-03-19 01:10:20,695 [INFO] Sum of grad norms: 0.043435
2019-03-19 01:10:20,695 [INFO] ---------------------------------
2019-03-19 01:10:39,236 [INFO] ---------------------------------
2019-03-19 01:10:39,237 [INFO] Summary:
2019-03-19 01:10:39,238 [INFO] Batch 75000, worst loss 0.061351 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 01:10:39,238 [INFO] Regularization: 3425.394043 * 0.0000010000 = 0.0034253940
2019-03-19 01:10:39,239 [INFO] Sum of grad norms: 0.056145
2019-03-19 01:10:39,239 [INFO] ---------------------------------
2019-03-19 01:10:58,216 [INFO] ---------------------------------
2019-03-19 01:10:58,217 [INFO] Summary:
2019-03-19 01:10:58,218 [INFO] Batch 76000, worst loss 0.061205 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 01:10:58,218 [INFO] Regularization: 3421.208008 * 0.0000010000 = 0.0034212079
2019-03-19 01:10:58,219 [INFO] Sum of grad norms: 0.025291
2019-03-19 01:10:58,219 [INFO] ---------------------------------
2019-03-19 01:11:16,813 [INFO] ---------------------------------
2019-03-19 01:11:16,814 [INFO] Summary:
2019-03-19 01:11:16,815 [INFO] Batch 77000, worst loss 0.061271 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 01:11:16,815 [INFO] Regularization: 3417.014404 * 0.0000010000 = 0.0034170144
2019-03-19 01:11:16,816 [INFO] Sum of grad norms: 0.082125
2019-03-19 01:11:16,817 [INFO] ---------------------------------
2019-03-19 01:11:35,654 [INFO] ---------------------------------
2019-03-19 01:11:35,655 [INFO] Summary:
2019-03-19 01:11:35,656 [INFO] Batch 78000, worst loss 0.061372 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 01:11:35,656 [INFO] Regularization: 3413.012939 * 0.0000010000 = 0.0034130129
2019-03-19 01:11:35,657 [INFO] Sum of grad norms: 0.120938
2019-03-19 01:11:35,657 [INFO] ---------------------------------
2019-03-19 01:11:54,558 [INFO] ---------------------------------
2019-03-19 01:11:54,559 [INFO] Summary:
2019-03-19 01:11:54,560 [INFO] Batch 79000, worst loss 0.061260 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 01:11:54,560 [INFO] Regularization: 3408.221436 * 0.0000010000 = 0.0034082215
2019-03-19 01:11:54,561 [INFO] Sum of grad norms: 0.037870
2019-03-19 01:11:54,561 [INFO] ---------------------------------
2019-03-19 01:12:13,403 [INFO] ---------------------------------
2019-03-19 01:12:13,404 [INFO] Summary:
2019-03-19 01:12:13,405 [INFO] Batch 80000, worst loss 0.061324 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 01:12:13,405 [INFO] Regularization: 3404.106689 * 0.0000010000 = 0.0034041067
2019-03-19 01:12:13,406 [INFO] Sum of grad norms: 0.042621
2019-03-19 01:12:13,406 [INFO] ---------------------------------
2019-03-19 01:12:18,337 [INFO] ---------------------------------
2019-03-19 01:12:18,338 [INFO] Evaluation:
2019-03-19 01:12:18,339 [INFO] Batch 80000, worst loss 0.057883 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:12:18,339 [INFO] ---------------------------------
2019-03-19 01:12:37,180 [INFO] ---------------------------------
2019-03-19 01:12:37,181 [INFO] Summary:
2019-03-19 01:12:37,181 [INFO] Batch 81000, worst loss 0.061223 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 01:12:37,182 [INFO] Regularization: 3399.897705 * 0.0000010000 = 0.0033998976
2019-03-19 01:12:37,183 [INFO] Sum of grad norms: 0.047990
2019-03-19 01:12:37,183 [INFO] ---------------------------------
2019-03-19 01:12:55,848 [INFO] ---------------------------------
2019-03-19 01:12:55,849 [INFO] Summary:
2019-03-19 01:12:55,850 [INFO] Batch 82000, worst loss 0.061215 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 01:12:55,850 [INFO] Regularization: 3396.855713 * 0.0000010000 = 0.0033968557
2019-03-19 01:12:55,851 [INFO] Sum of grad norms: 0.048735
2019-03-19 01:12:55,851 [INFO] ---------------------------------
2019-03-19 01:13:14,655 [INFO] ---------------------------------
2019-03-19 01:13:14,656 [INFO] Summary:
2019-03-19 01:13:14,656 [INFO] Batch 83000, worst loss 0.061257 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 01:13:14,657 [INFO] Regularization: 3394.850830 * 0.0000010000 = 0.0033948508
2019-03-19 01:13:14,658 [INFO] Sum of grad norms: 0.020543
2019-03-19 01:13:14,658 [INFO] ---------------------------------
2019-03-19 01:13:33,475 [INFO] ---------------------------------
2019-03-19 01:13:33,476 [INFO] Summary:
2019-03-19 01:13:33,476 [INFO] Batch 84000, worst loss 0.061257 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 01:13:33,477 [INFO] Regularization: 3392.401855 * 0.0000010000 = 0.0033924018
2019-03-19 01:13:33,477 [INFO] Sum of grad norms: 0.037496
2019-03-19 01:13:33,478 [INFO] ---------------------------------
2019-03-19 01:13:52,256 [INFO] ---------------------------------
2019-03-19 01:13:52,258 [INFO] Summary:
2019-03-19 01:13:52,258 [INFO] Batch 85000, worst loss 0.061329 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 01:13:52,259 [INFO] Regularization: 3390.239746 * 0.0000010000 = 0.0033902398
2019-03-19 01:13:52,261 [INFO] Sum of grad norms: 0.068583
2019-03-19 01:13:52,261 [INFO] ---------------------------------
2019-03-19 01:14:11,304 [INFO] ---------------------------------
2019-03-19 01:14:11,304 [INFO] Summary:
2019-03-19 01:14:11,305 [INFO] Batch 86000, worst loss 0.061328 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 01:14:11,305 [INFO] Regularization: 3387.814453 * 0.0000010000 = 0.0033878144
2019-03-19 01:14:11,306 [INFO] Sum of grad norms: 0.029368
2019-03-19 01:14:11,307 [INFO] ---------------------------------
2019-03-19 01:14:29,771 [INFO] ---------------------------------
2019-03-19 01:14:29,772 [INFO] Summary:
2019-03-19 01:14:29,773 [INFO] Batch 87000, worst loss 0.061284 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 01:14:29,773 [INFO] Regularization: 3385.634277 * 0.0000010000 = 0.0033856342
2019-03-19 01:14:29,774 [INFO] Sum of grad norms: 0.031505
2019-03-19 01:14:29,775 [INFO] ---------------------------------
2019-03-19 01:14:48,656 [INFO] ---------------------------------
2019-03-19 01:14:48,657 [INFO] Summary:
2019-03-19 01:14:48,658 [INFO] Batch 88000, worst loss 0.061087 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 01:14:48,659 [INFO] Regularization: 3383.439941 * 0.0000010000 = 0.0033834400
2019-03-19 01:14:48,661 [INFO] Sum of grad norms: 0.028375
2019-03-19 01:14:48,663 [INFO] ---------------------------------
2019-03-19 01:15:07,612 [INFO] ---------------------------------
2019-03-19 01:15:07,613 [INFO] Summary:
2019-03-19 01:15:07,613 [INFO] Batch 89000, worst loss 0.061143 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 01:15:07,614 [INFO] Regularization: 3381.088379 * 0.0000010000 = 0.0033810884
2019-03-19 01:15:07,615 [INFO] Sum of grad norms: 0.025700
2019-03-19 01:15:07,615 [INFO] ---------------------------------
2019-03-19 01:15:26,353 [INFO] ---------------------------------
2019-03-19 01:15:26,354 [INFO] Summary:
2019-03-19 01:15:26,354 [INFO] Batch 90000, worst loss 0.061153 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 01:15:26,355 [INFO] Regularization: 3379.024414 * 0.0000010000 = 0.0033790243
2019-03-19 01:15:26,355 [INFO] Sum of grad norms: 0.035930
2019-03-19 01:15:26,356 [INFO] ---------------------------------
2019-03-19 01:15:31,207 [INFO] ---------------------------------
2019-03-19 01:15:31,208 [INFO] Evaluation:
2019-03-19 01:15:31,209 [INFO] Batch 90000, worst loss 0.057771 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:15:31,209 [INFO] ---------------------------------
2019-03-19 01:15:49,614 [INFO] ---------------------------------
2019-03-19 01:15:49,615 [INFO] Summary:
2019-03-19 01:15:49,615 [INFO] Batch 91000, worst loss 0.061319 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 01:15:49,616 [INFO] Regularization: 3376.667236 * 0.0000010000 = 0.0033766672
2019-03-19 01:15:49,616 [INFO] Sum of grad norms: 0.051573
2019-03-19 01:15:49,617 [INFO] ---------------------------------
2019-03-19 01:16:08,696 [INFO] ---------------------------------
2019-03-19 01:16:08,697 [INFO] Summary:
2019-03-19 01:16:08,698 [INFO] Batch 92000, worst loss 0.061154 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 01:16:08,699 [INFO] Regularization: 3375.371094 * 0.0000010000 = 0.0033753710
2019-03-19 01:16:08,699 [INFO] Sum of grad norms: 0.024058
2019-03-19 01:16:08,700 [INFO] ---------------------------------
2019-03-19 01:16:27,639 [INFO] ---------------------------------
2019-03-19 01:16:27,640 [INFO] Summary:
2019-03-19 01:16:27,641 [INFO] Batch 93000, worst loss 0.061145 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 01:16:27,641 [INFO] Regularization: 3374.230957 * 0.0000010000 = 0.0033742310
2019-03-19 01:16:27,642 [INFO] Sum of grad norms: 0.053106
2019-03-19 01:16:27,642 [INFO] ---------------------------------
2019-03-19 01:16:46,409 [INFO] ---------------------------------
2019-03-19 01:16:46,409 [INFO] Summary:
2019-03-19 01:16:46,410 [INFO] Batch 94000, worst loss 0.061141 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 01:16:46,411 [INFO] Regularization: 3373.192871 * 0.0000010000 = 0.0033731929
2019-03-19 01:16:46,411 [INFO] Sum of grad norms: 0.026603
2019-03-19 01:16:46,412 [INFO] ---------------------------------
2019-03-19 01:17:04,850 [INFO] ---------------------------------
2019-03-19 01:17:04,851 [INFO] Summary:
2019-03-19 01:17:04,852 [INFO] Batch 95000, worst loss 0.061236 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 01:17:04,852 [INFO] Regularization: 3371.736816 * 0.0000010000 = 0.0033717367
2019-03-19 01:17:04,853 [INFO] Sum of grad norms: 0.061300
2019-03-19 01:17:04,853 [INFO] ---------------------------------
2019-03-19 01:17:23,242 [INFO] ---------------------------------
2019-03-19 01:17:23,243 [INFO] Summary:
2019-03-19 01:17:23,244 [INFO] Batch 96000, worst loss 0.061244 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 01:17:23,245 [INFO] Regularization: 3370.561523 * 0.0000010000 = 0.0033705614
2019-03-19 01:17:23,245 [INFO] Sum of grad norms: 0.066044
2019-03-19 01:17:23,246 [INFO] ---------------------------------
2019-03-19 01:17:42,354 [INFO] ---------------------------------
2019-03-19 01:17:42,355 [INFO] Summary:
2019-03-19 01:17:42,356 [INFO] Batch 97000, worst loss 0.061104 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 01:17:42,357 [INFO] Regularization: 3369.429443 * 0.0000010000 = 0.0033694294
2019-03-19 01:17:42,358 [INFO] Sum of grad norms: 0.091573
2019-03-19 01:17:42,359 [INFO] ---------------------------------
2019-03-19 01:18:01,434 [INFO] ---------------------------------
2019-03-19 01:18:01,435 [INFO] Summary:
2019-03-19 01:18:01,436 [INFO] Batch 98000, worst loss 0.061101 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 01:18:01,436 [INFO] Regularization: 3368.359619 * 0.0000010000 = 0.0033683595
2019-03-19 01:18:01,437 [INFO] Sum of grad norms: 0.067856
2019-03-19 01:18:01,438 [INFO] ---------------------------------
2019-03-19 01:18:20,323 [INFO] ---------------------------------
2019-03-19 01:18:20,324 [INFO] Summary:
2019-03-19 01:18:20,324 [INFO] Batch 99000, worst loss 0.061087 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 01:18:20,325 [INFO] Regularization: 3367.220459 * 0.0000010000 = 0.0033672205
2019-03-19 01:18:20,325 [INFO] Sum of grad norms: 0.022181
2019-03-19 01:18:20,326 [INFO] ---------------------------------
2019-03-19 01:18:39,217 [INFO] ---------------------------------
2019-03-19 01:18:39,218 [INFO] Summary:
2019-03-19 01:18:39,219 [INFO] Batch 100000, worst loss 0.061124 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 01:18:39,219 [INFO] Regularization: 3366.294434 * 0.0000010000 = 0.0033662943
2019-03-19 01:18:39,220 [INFO] Sum of grad norms: 0.079861
2019-03-19 01:18:39,221 [INFO] ---------------------------------
2019-03-19 01:18:44,113 [INFO] ---------------------------------
2019-03-19 01:18:44,114 [INFO] Evaluation:
2019-03-19 01:18:44,115 [INFO] Batch 100000, worst loss 0.057875 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:18:44,115 [INFO] ---------------------------------
2019-03-19 01:19:02,877 [INFO] ---------------------------------
2019-03-19 01:19:02,879 [INFO] Summary:
2019-03-19 01:19:02,879 [INFO] Batch 101000, worst loss 0.061282 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 01:19:02,880 [INFO] Regularization: 3365.258301 * 0.0000010000 = 0.0033652582
2019-03-19 01:19:02,880 [INFO] Sum of grad norms: 0.033136
2019-03-19 01:19:02,881 [INFO] ---------------------------------
2019-03-19 01:19:21,803 [INFO] ---------------------------------
2019-03-19 01:19:21,804 [INFO] Summary:
2019-03-19 01:19:21,805 [INFO] Batch 102000, worst loss 0.061374 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 01:19:21,805 [INFO] Regularization: 3364.440674 * 0.0000010000 = 0.0033644408
2019-03-19 01:19:21,806 [INFO] Sum of grad norms: 0.078700
2019-03-19 01:19:21,807 [INFO] ---------------------------------
2019-03-19 01:19:40,879 [INFO] ---------------------------------
2019-03-19 01:19:40,881 [INFO] Summary:
2019-03-19 01:19:40,881 [INFO] Batch 103000, worst loss 0.061445 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 01:19:40,882 [INFO] Regularization: 3363.797852 * 0.0000010000 = 0.0033637979
2019-03-19 01:19:40,882 [INFO] Sum of grad norms: 0.040071
2019-03-19 01:19:40,883 [INFO] ---------------------------------
2019-03-19 01:19:59,658 [INFO] ---------------------------------
2019-03-19 01:19:59,659 [INFO] Summary:
2019-03-19 01:19:59,659 [INFO] Batch 104000, worst loss 0.061144 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 01:19:59,660 [INFO] Regularization: 3363.196533 * 0.0000010000 = 0.0033631965
2019-03-19 01:19:59,660 [INFO] Sum of grad norms: 0.065104
2019-03-19 01:19:59,661 [INFO] ---------------------------------
2019-03-19 01:20:18,241 [INFO] ---------------------------------
2019-03-19 01:20:18,242 [INFO] Summary:
2019-03-19 01:20:18,242 [INFO] Batch 105000, worst loss 0.061249 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 01:20:18,243 [INFO] Regularization: 3362.597412 * 0.0000010000 = 0.0033625974
2019-03-19 01:20:18,244 [INFO] Sum of grad norms: 0.047928
2019-03-19 01:20:18,244 [INFO] ---------------------------------
2019-03-19 01:20:36,627 [INFO] ---------------------------------
2019-03-19 01:20:36,628 [INFO] Summary:
2019-03-19 01:20:36,629 [INFO] Batch 106000, worst loss 0.061104 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 01:20:36,629 [INFO] Regularization: 3362.007324 * 0.0000010000 = 0.0033620072
2019-03-19 01:20:36,630 [INFO] Sum of grad norms: 0.071290
2019-03-19 01:20:36,630 [INFO] ---------------------------------
2019-03-19 01:20:55,310 [INFO] ---------------------------------
2019-03-19 01:20:55,311 [INFO] Summary:
2019-03-19 01:20:55,312 [INFO] Batch 107000, worst loss 0.061084 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 01:20:55,312 [INFO] Regularization: 3361.468750 * 0.0000010000 = 0.0033614687
2019-03-19 01:20:55,313 [INFO] Sum of grad norms: 0.048246
2019-03-19 01:20:55,313 [INFO] ---------------------------------
2019-03-19 01:21:13,957 [INFO] ---------------------------------
2019-03-19 01:21:13,958 [INFO] Summary:
2019-03-19 01:21:13,959 [INFO] Batch 108000, worst loss 0.060977 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 01:21:13,960 [INFO] Regularization: 3360.941895 * 0.0000010000 = 0.0033609418
2019-03-19 01:21:13,960 [INFO] Sum of grad norms: 0.038493
2019-03-19 01:21:13,961 [INFO] ---------------------------------
2019-03-19 01:21:32,435 [INFO] ---------------------------------
2019-03-19 01:21:32,436 [INFO] Summary:
2019-03-19 01:21:32,437 [INFO] Batch 109000, worst loss 0.061130 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 01:21:32,438 [INFO] Regularization: 3360.309814 * 0.0000010000 = 0.0033603099
2019-03-19 01:21:32,438 [INFO] Sum of grad norms: 0.028845
2019-03-19 01:21:32,439 [INFO] ---------------------------------
2019-03-19 01:21:51,188 [INFO] ---------------------------------
2019-03-19 01:21:51,189 [INFO] Summary:
2019-03-19 01:21:51,189 [INFO] Batch 110000, worst loss 0.061133 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 01:21:51,190 [INFO] Regularization: 3359.906006 * 0.0000010000 = 0.0033599059
2019-03-19 01:21:51,190 [INFO] Sum of grad norms: 0.034973
2019-03-19 01:21:51,191 [INFO] ---------------------------------
2019-03-19 01:21:56,162 [INFO] ---------------------------------
2019-03-19 01:21:56,164 [INFO] Evaluation:
2019-03-19 01:21:56,165 [INFO] Batch 110000, worst loss 0.057838 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:21:56,165 [INFO] ---------------------------------
2019-03-19 01:22:14,849 [INFO] ---------------------------------
2019-03-19 01:22:14,850 [INFO] Summary:
2019-03-19 01:22:14,850 [INFO] Batch 111000, worst loss 0.061290 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-19 01:22:14,851 [INFO] Regularization: 3359.333252 * 0.0000010000 = 0.0033593331
2019-03-19 01:22:14,851 [INFO] Sum of grad norms: 0.020650
2019-03-19 01:22:14,852 [INFO] ---------------------------------
2019-03-19 01:22:33,674 [INFO] ---------------------------------
2019-03-19 01:22:33,675 [INFO] Summary:
2019-03-19 01:22:33,675 [INFO] Batch 112000, worst loss 0.061132 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-19 01:22:33,676 [INFO] Regularization: 3358.953125 * 0.0000010000 = 0.0033589532
2019-03-19 01:22:33,677 [INFO] Sum of grad norms: 0.028214
2019-03-19 01:22:33,677 [INFO] ---------------------------------
2019-03-19 01:22:52,983 [INFO] ---------------------------------
2019-03-19 01:22:52,984 [INFO] Summary:
2019-03-19 01:22:52,984 [INFO] Batch 113000, worst loss 0.061318 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-19 01:22:52,985 [INFO] Regularization: 3358.647705 * 0.0000010000 = 0.0033586477
2019-03-19 01:22:52,985 [INFO] Sum of grad norms: 0.056180
2019-03-19 01:22:52,986 [INFO] ---------------------------------
2019-03-19 01:23:11,533 [INFO] ---------------------------------
2019-03-19 01:23:11,534 [INFO] Summary:
2019-03-19 01:23:11,534 [INFO] Batch 114000, worst loss 0.061311 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-19 01:23:11,535 [INFO] Regularization: 3358.394287 * 0.0000010000 = 0.0033583944
2019-03-19 01:23:11,535 [INFO] Sum of grad norms: 0.033035
2019-03-19 01:23:11,536 [INFO] ---------------------------------
2019-03-19 01:23:30,283 [INFO] ---------------------------------
2019-03-19 01:23:30,284 [INFO] Summary:
2019-03-19 01:23:30,285 [INFO] Batch 115000, worst loss 0.061240 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-19 01:23:30,285 [INFO] Regularization: 3358.119873 * 0.0000010000 = 0.0033581199
2019-03-19 01:23:30,286 [INFO] Sum of grad norms: 0.030875
2019-03-19 01:23:30,287 [INFO] ---------------------------------
2019-03-19 01:23:48,973 [INFO] ---------------------------------
2019-03-19 01:23:48,974 [INFO] Summary:
2019-03-19 01:23:48,974 [INFO] Batch 116000, worst loss 0.061124 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-19 01:23:48,975 [INFO] Regularization: 3357.851807 * 0.0000010000 = 0.0033578519
2019-03-19 01:23:48,975 [INFO] Sum of grad norms: 0.070028
2019-03-19 01:23:48,976 [INFO] ---------------------------------
2019-03-19 01:24:08,027 [INFO] ---------------------------------
2019-03-19 01:24:08,028 [INFO] Summary:
2019-03-19 01:24:08,029 [INFO] Batch 117000, worst loss 0.061269 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-19 01:24:08,030 [INFO] Regularization: 3357.551758 * 0.0000010000 = 0.0033575518
2019-03-19 01:24:08,030 [INFO] Sum of grad norms: 0.019668
2019-03-19 01:24:08,031 [INFO] ---------------------------------
2019-03-19 01:24:26,570 [INFO] ---------------------------------
2019-03-19 01:24:26,571 [INFO] Summary:
2019-03-19 01:24:26,571 [INFO] Batch 118000, worst loss 0.061267 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-19 01:24:26,572 [INFO] Regularization: 3357.221436 * 0.0000010000 = 0.0033572214
2019-03-19 01:24:26,572 [INFO] Sum of grad norms: 0.061454
2019-03-19 01:24:26,573 [INFO] ---------------------------------
2019-03-19 01:24:44,779 [INFO] ---------------------------------
2019-03-19 01:24:44,780 [INFO] Summary:
2019-03-19 01:24:44,780 [INFO] Batch 119000, worst loss 0.061073 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-19 01:24:44,781 [INFO] Regularization: 3356.925537 * 0.0000010000 = 0.0033569254
2019-03-19 01:24:44,782 [INFO] Sum of grad norms: 0.073611
2019-03-19 01:24:44,782 [INFO] ---------------------------------
2019-03-19 01:25:03,444 [INFO] ---------------------------------
2019-03-19 01:25:03,445 [INFO] Summary:
2019-03-19 01:25:03,446 [INFO] Batch 120000, worst loss 0.061027 (incl. reg.) of 1000 batches, learning rate 0.000003 @cl.-depth 1
2019-03-19 01:25:03,446 [INFO] Regularization: 3356.608643 * 0.0000010000 = 0.0033566086
2019-03-19 01:25:03,447 [INFO] Sum of grad norms: 0.062808
2019-03-19 01:25:03,448 [INFO] ---------------------------------
2019-03-19 01:25:08,311 [INFO] ---------------------------------
2019-03-19 01:25:08,312 [INFO] Evaluation:
2019-03-19 01:25:08,313 [INFO] Batch 120000, worst loss 0.057765 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:25:08,317 [INFO] ---------------------------------
2019-03-19 01:25:26,896 [INFO] ---------------------------------
2019-03-19 01:25:26,897 [INFO] Summary:
2019-03-19 01:25:26,898 [INFO] Batch 121000, worst loss 0.061006 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:25:26,898 [INFO] Regularization: 3356.345215 * 0.0000010000 = 0.0033563452
2019-03-19 01:25:26,899 [INFO] Sum of grad norms: 0.076214
2019-03-19 01:25:26,899 [INFO] ---------------------------------
2019-03-19 01:25:45,249 [INFO] ---------------------------------
2019-03-19 01:25:45,250 [INFO] Summary:
2019-03-19 01:25:45,251 [INFO] Batch 122000, worst loss 0.060949 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:25:45,251 [INFO] Regularization: 3356.133545 * 0.0000010000 = 0.0033561336
2019-03-19 01:25:45,252 [INFO] Sum of grad norms: 0.049874
2019-03-19 01:25:45,252 [INFO] ---------------------------------
2019-03-19 01:26:03,864 [INFO] ---------------------------------
2019-03-19 01:26:03,865 [INFO] Summary:
2019-03-19 01:26:03,866 [INFO] Batch 123000, worst loss 0.061074 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:26:03,867 [INFO] Regularization: 3355.963135 * 0.0000010000 = 0.0033559632
2019-03-19 01:26:03,868 [INFO] Sum of grad norms: 0.033113
2019-03-19 01:26:03,868 [INFO] ---------------------------------
2019-03-19 01:26:22,726 [INFO] ---------------------------------
2019-03-19 01:26:22,727 [INFO] Summary:
2019-03-19 01:26:22,727 [INFO] Batch 124000, worst loss 0.061135 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:26:22,728 [INFO] Regularization: 3355.806152 * 0.0000010000 = 0.0033558062
2019-03-19 01:26:22,728 [INFO] Sum of grad norms: 0.052245
2019-03-19 01:26:22,729 [INFO] ---------------------------------
2019-03-19 01:26:41,047 [INFO] ---------------------------------
2019-03-19 01:26:41,048 [INFO] Summary:
2019-03-19 01:26:41,049 [INFO] Batch 125000, worst loss 0.061154 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:26:41,049 [INFO] Regularization: 3355.669434 * 0.0000010000 = 0.0033556693
2019-03-19 01:26:41,050 [INFO] Sum of grad norms: 0.043687
2019-03-19 01:26:41,050 [INFO] ---------------------------------
2019-03-19 01:26:59,849 [INFO] ---------------------------------
2019-03-19 01:26:59,849 [INFO] Summary:
2019-03-19 01:26:59,850 [INFO] Batch 126000, worst loss 0.061110 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:26:59,850 [INFO] Regularization: 3355.531738 * 0.0000010000 = 0.0033555317
2019-03-19 01:26:59,851 [INFO] Sum of grad norms: 0.039628
2019-03-19 01:26:59,852 [INFO] ---------------------------------
2019-03-19 01:27:18,520 [INFO] ---------------------------------
2019-03-19 01:27:18,521 [INFO] Summary:
2019-03-19 01:27:18,521 [INFO] Batch 127000, worst loss 0.061102 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:27:18,522 [INFO] Regularization: 3355.375000 * 0.0000010000 = 0.0033553750
2019-03-19 01:27:18,523 [INFO] Sum of grad norms: 0.033065
2019-03-19 01:27:18,523 [INFO] ---------------------------------
2019-03-19 01:27:37,206 [INFO] ---------------------------------
2019-03-19 01:27:37,207 [INFO] Summary:
2019-03-19 01:27:37,208 [INFO] Batch 128000, worst loss 0.061131 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:27:37,208 [INFO] Regularization: 3355.228027 * 0.0000010000 = 0.0033552281
2019-03-19 01:27:37,209 [INFO] Sum of grad norms: 0.121383
2019-03-19 01:27:37,209 [INFO] ---------------------------------
2019-03-19 01:27:56,064 [INFO] ---------------------------------
2019-03-19 01:27:56,065 [INFO] Summary:
2019-03-19 01:27:56,066 [INFO] Batch 129000, worst loss 0.061076 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:27:56,067 [INFO] Regularization: 3355.090332 * 0.0000010000 = 0.0033550903
2019-03-19 01:27:56,068 [INFO] Sum of grad norms: 0.052839
2019-03-19 01:27:56,069 [INFO] ---------------------------------
2019-03-19 01:28:14,496 [INFO] ---------------------------------
2019-03-19 01:28:14,497 [INFO] Summary:
2019-03-19 01:28:14,498 [INFO] Batch 130000, worst loss 0.061164 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:28:14,498 [INFO] Regularization: 3354.955322 * 0.0000010000 = 0.0033549552
2019-03-19 01:28:14,499 [INFO] Sum of grad norms: 0.029966
2019-03-19 01:28:14,499 [INFO] ---------------------------------
2019-03-19 01:28:19,366 [INFO] ---------------------------------
2019-03-19 01:28:19,367 [INFO] Evaluation:
2019-03-19 01:28:19,370 [INFO] Batch 130000, worst loss 0.057749 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:28:19,371 [INFO] ---------------------------------
2019-03-19 01:28:37,689 [INFO] ---------------------------------
2019-03-19 01:28:37,690 [INFO] Summary:
2019-03-19 01:28:37,691 [INFO] Batch 131000, worst loss 0.061320 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:28:37,691 [INFO] Regularization: 3354.815430 * 0.0000010000 = 0.0033548155
2019-03-19 01:28:37,692 [INFO] Sum of grad norms: 0.039777
2019-03-19 01:28:37,692 [INFO] ---------------------------------
2019-03-19 01:28:55,971 [INFO] ---------------------------------
2019-03-19 01:28:55,972 [INFO] Summary:
2019-03-19 01:28:55,973 [INFO] Batch 132000, worst loss 0.061101 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:28:55,973 [INFO] Regularization: 3354.703857 * 0.0000010000 = 0.0033547038
2019-03-19 01:28:55,974 [INFO] Sum of grad norms: 0.022279
2019-03-19 01:28:55,974 [INFO] ---------------------------------
2019-03-19 01:29:14,583 [INFO] ---------------------------------
2019-03-19 01:29:14,584 [INFO] Summary:
2019-03-19 01:29:14,585 [INFO] Batch 133000, worst loss 0.061302 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:29:14,585 [INFO] Regularization: 3354.617432 * 0.0000010000 = 0.0033546174
2019-03-19 01:29:14,586 [INFO] Sum of grad norms: 0.050457
2019-03-19 01:29:14,586 [INFO] ---------------------------------
2019-03-19 01:29:33,410 [INFO] ---------------------------------
2019-03-19 01:29:33,411 [INFO] Summary:
2019-03-19 01:29:33,412 [INFO] Batch 134000, worst loss 0.061000 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:29:33,412 [INFO] Regularization: 3354.552979 * 0.0000010000 = 0.0033545529
2019-03-19 01:29:33,413 [INFO] Sum of grad norms: 0.026924
2019-03-19 01:29:33,413 [INFO] ---------------------------------
2019-03-19 01:29:52,145 [INFO] ---------------------------------
2019-03-19 01:29:52,145 [INFO] Summary:
2019-03-19 01:29:52,146 [INFO] Batch 135000, worst loss 0.061186 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:29:52,146 [INFO] Regularization: 3354.458740 * 0.0000010000 = 0.0033544588
2019-03-19 01:29:52,147 [INFO] Sum of grad norms: 0.026932
2019-03-19 01:29:52,148 [INFO] ---------------------------------
2019-03-19 01:30:11,151 [INFO] ---------------------------------
2019-03-19 01:30:11,152 [INFO] Summary:
2019-03-19 01:30:11,153 [INFO] Batch 136000, worst loss 0.061187 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:30:11,153 [INFO] Regularization: 3354.383789 * 0.0000010000 = 0.0033543839
2019-03-19 01:30:11,154 [INFO] Sum of grad norms: 0.077408
2019-03-19 01:30:11,154 [INFO] ---------------------------------
2019-03-19 01:30:30,003 [INFO] ---------------------------------
2019-03-19 01:30:30,004 [INFO] Summary:
2019-03-19 01:30:30,005 [INFO] Batch 137000, worst loss 0.061354 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:30:30,005 [INFO] Regularization: 3354.320801 * 0.0000010000 = 0.0033543208
2019-03-19 01:30:30,006 [INFO] Sum of grad norms: 0.030492
2019-03-19 01:30:30,007 [INFO] ---------------------------------
2019-03-19 01:30:48,459 [INFO] ---------------------------------
2019-03-19 01:30:48,460 [INFO] Summary:
2019-03-19 01:30:48,460 [INFO] Batch 138000, worst loss 0.061354 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:30:48,461 [INFO] Regularization: 3354.242676 * 0.0000010000 = 0.0033542428
2019-03-19 01:30:48,461 [INFO] Sum of grad norms: 0.028645
2019-03-19 01:30:48,462 [INFO] ---------------------------------
2019-03-19 01:31:07,122 [INFO] ---------------------------------
2019-03-19 01:31:07,123 [INFO] Summary:
2019-03-19 01:31:07,123 [INFO] Batch 139000, worst loss 0.060992 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:31:07,124 [INFO] Regularization: 3354.182617 * 0.0000010000 = 0.0033541827
2019-03-19 01:31:07,124 [INFO] Sum of grad norms: 0.075509
2019-03-19 01:31:07,125 [INFO] ---------------------------------
2019-03-19 01:31:25,868 [INFO] ---------------------------------
2019-03-19 01:31:25,870 [INFO] Summary:
2019-03-19 01:31:25,871 [INFO] Batch 140000, worst loss 0.061049 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 01:31:25,872 [INFO] Regularization: 3354.098633 * 0.0000010000 = 0.0033540986
2019-03-19 01:31:25,873 [INFO] Sum of grad norms: 0.026125
2019-03-19 01:31:25,875 [INFO] ---------------------------------
2019-03-19 01:31:30,819 [INFO] ---------------------------------
2019-03-19 01:31:30,820 [INFO] Evaluation:
2019-03-19 01:31:30,820 [INFO] Batch 140000, worst loss 0.057782 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:31:30,821 [INFO] ---------------------------------
2019-03-19 01:31:49,213 [INFO] ---------------------------------
2019-03-19 01:31:49,214 [INFO] Summary:
2019-03-19 01:31:49,214 [INFO] Batch 141000, worst loss 0.061146 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:31:49,215 [INFO] Regularization: 3354.019287 * 0.0000010000 = 0.0033540193
2019-03-19 01:31:49,215 [INFO] Sum of grad norms: 0.041529
2019-03-19 01:31:49,216 [INFO] ---------------------------------
2019-03-19 01:32:07,825 [INFO] ---------------------------------
2019-03-19 01:32:07,826 [INFO] Summary:
2019-03-19 01:32:07,826 [INFO] Batch 142000, worst loss 0.061053 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:32:07,827 [INFO] Regularization: 3353.972656 * 0.0000010000 = 0.0033539727
2019-03-19 01:32:07,827 [INFO] Sum of grad norms: 0.033881
2019-03-19 01:32:07,828 [INFO] ---------------------------------
2019-03-19 01:32:26,707 [INFO] ---------------------------------
2019-03-19 01:32:26,708 [INFO] Summary:
2019-03-19 01:32:26,708 [INFO] Batch 143000, worst loss 0.061030 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:32:26,709 [INFO] Regularization: 3353.921387 * 0.0000010000 = 0.0033539215
2019-03-19 01:32:26,709 [INFO] Sum of grad norms: 0.076228
2019-03-19 01:32:26,710 [INFO] ---------------------------------
2019-03-19 01:32:45,464 [INFO] ---------------------------------
2019-03-19 01:32:45,465 [INFO] Summary:
2019-03-19 01:32:45,466 [INFO] Batch 144000, worst loss 0.061058 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:32:45,466 [INFO] Regularization: 3353.878662 * 0.0000010000 = 0.0033538786
2019-03-19 01:32:45,467 [INFO] Sum of grad norms: 0.031611
2019-03-19 01:32:45,467 [INFO] ---------------------------------
2019-03-19 01:33:04,134 [INFO] ---------------------------------
2019-03-19 01:33:04,135 [INFO] Summary:
2019-03-19 01:33:04,136 [INFO] Batch 145000, worst loss 0.061057 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:33:04,136 [INFO] Regularization: 3353.839355 * 0.0000010000 = 0.0033538393
2019-03-19 01:33:04,137 [INFO] Sum of grad norms: 0.040373
2019-03-19 01:33:04,137 [INFO] ---------------------------------
2019-03-19 01:33:22,852 [INFO] ---------------------------------
2019-03-19 01:33:22,853 [INFO] Summary:
2019-03-19 01:33:22,854 [INFO] Batch 146000, worst loss 0.061190 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:33:22,854 [INFO] Regularization: 3353.794678 * 0.0000010000 = 0.0033537946
2019-03-19 01:33:22,855 [INFO] Sum of grad norms: 0.028952
2019-03-19 01:33:22,855 [INFO] ---------------------------------
2019-03-19 01:33:41,687 [INFO] ---------------------------------
2019-03-19 01:33:41,688 [INFO] Summary:
2019-03-19 01:33:41,689 [INFO] Batch 147000, worst loss 0.061093 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:33:41,689 [INFO] Regularization: 3353.754395 * 0.0000010000 = 0.0033537543
2019-03-19 01:33:41,690 [INFO] Sum of grad norms: 0.033736
2019-03-19 01:33:41,690 [INFO] ---------------------------------
2019-03-19 01:34:00,203 [INFO] ---------------------------------
2019-03-19 01:34:00,204 [INFO] Summary:
2019-03-19 01:34:00,205 [INFO] Batch 148000, worst loss 0.061088 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:34:00,205 [INFO] Regularization: 3353.721680 * 0.0000010000 = 0.0033537217
2019-03-19 01:34:00,206 [INFO] Sum of grad norms: 0.080640
2019-03-19 01:34:00,207 [INFO] ---------------------------------
2019-03-19 01:34:19,164 [INFO] ---------------------------------
2019-03-19 01:34:19,165 [INFO] Summary:
2019-03-19 01:34:19,165 [INFO] Batch 149000, worst loss 0.061133 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:34:19,166 [INFO] Regularization: 3353.676025 * 0.0000010000 = 0.0033536761
2019-03-19 01:34:19,166 [INFO] Sum of grad norms: 0.024863
2019-03-19 01:34:19,167 [INFO] ---------------------------------
2019-03-19 01:34:37,645 [INFO] ---------------------------------
2019-03-19 01:34:37,645 [INFO] Summary:
2019-03-19 01:34:37,646 [INFO] Batch 150000, worst loss 0.061133 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:34:37,647 [INFO] Regularization: 3353.640137 * 0.0000010000 = 0.0033536402
2019-03-19 01:34:37,647 [INFO] Sum of grad norms: 0.073113
2019-03-19 01:34:37,648 [INFO] ---------------------------------
2019-03-19 01:34:42,575 [INFO] ---------------------------------
2019-03-19 01:34:42,576 [INFO] Evaluation:
2019-03-19 01:34:42,577 [INFO] Batch 150000, worst loss 0.057742 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:34:42,579 [INFO] ---------------------------------
2019-03-19 01:35:00,924 [INFO] ---------------------------------
2019-03-19 01:35:00,925 [INFO] Summary:
2019-03-19 01:35:00,926 [INFO] Batch 151000, worst loss 0.061031 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:35:00,926 [INFO] Regularization: 3353.611328 * 0.0000010000 = 0.0033536113
2019-03-19 01:35:00,927 [INFO] Sum of grad norms: 0.075349
2019-03-19 01:35:00,928 [INFO] ---------------------------------
2019-03-19 01:35:20,026 [INFO] ---------------------------------
2019-03-19 01:35:20,027 [INFO] Summary:
2019-03-19 01:35:20,027 [INFO] Batch 152000, worst loss 0.061202 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:35:20,028 [INFO] Regularization: 3353.585449 * 0.0000010000 = 0.0033535855
2019-03-19 01:35:20,028 [INFO] Sum of grad norms: 0.064962
2019-03-19 01:35:20,029 [INFO] ---------------------------------
2019-03-19 01:35:38,552 [INFO] ---------------------------------
2019-03-19 01:35:38,553 [INFO] Summary:
2019-03-19 01:35:38,553 [INFO] Batch 153000, worst loss 0.061236 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:35:38,554 [INFO] Regularization: 3353.564941 * 0.0000010000 = 0.0033535650
2019-03-19 01:35:38,554 [INFO] Sum of grad norms: 0.035302
2019-03-19 01:35:38,555 [INFO] ---------------------------------
2019-03-19 01:35:57,473 [INFO] ---------------------------------
2019-03-19 01:35:57,473 [INFO] Summary:
2019-03-19 01:35:57,474 [INFO] Batch 154000, worst loss 0.061236 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:35:57,475 [INFO] Regularization: 3353.544678 * 0.0000010000 = 0.0033535447
2019-03-19 01:35:57,475 [INFO] Sum of grad norms: 0.063084
2019-03-19 01:35:57,476 [INFO] ---------------------------------
2019-03-19 01:36:16,072 [INFO] ---------------------------------
2019-03-19 01:36:16,073 [INFO] Summary:
2019-03-19 01:36:16,073 [INFO] Batch 155000, worst loss 0.061076 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:36:16,074 [INFO] Regularization: 3353.523682 * 0.0000010000 = 0.0033535238
2019-03-19 01:36:16,074 [INFO] Sum of grad norms: 0.030871
2019-03-19 01:36:16,075 [INFO] ---------------------------------
2019-03-19 01:36:34,779 [INFO] ---------------------------------
2019-03-19 01:36:34,780 [INFO] Summary:
2019-03-19 01:36:34,781 [INFO] Batch 156000, worst loss 0.061258 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:36:34,781 [INFO] Regularization: 3353.504883 * 0.0000010000 = 0.0033535049
2019-03-19 01:36:34,782 [INFO] Sum of grad norms: 0.037818
2019-03-19 01:36:34,783 [INFO] ---------------------------------
2019-03-19 01:36:53,507 [INFO] ---------------------------------
2019-03-19 01:36:53,508 [INFO] Summary:
2019-03-19 01:36:53,508 [INFO] Batch 157000, worst loss 0.061097 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:36:53,509 [INFO] Regularization: 3353.487549 * 0.0000010000 = 0.0033534875
2019-03-19 01:36:53,509 [INFO] Sum of grad norms: 0.046278
2019-03-19 01:36:53,510 [INFO] ---------------------------------
2019-03-19 01:37:12,426 [INFO] ---------------------------------
2019-03-19 01:37:12,428 [INFO] Summary:
2019-03-19 01:37:12,428 [INFO] Batch 158000, worst loss 0.061096 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:37:12,429 [INFO] Regularization: 3353.464111 * 0.0000010000 = 0.0033534642
2019-03-19 01:37:12,430 [INFO] Sum of grad norms: 0.028094
2019-03-19 01:37:12,431 [INFO] ---------------------------------
2019-03-19 01:37:31,282 [INFO] ---------------------------------
2019-03-19 01:37:31,283 [INFO] Summary:
2019-03-19 01:37:31,283 [INFO] Batch 159000, worst loss 0.061165 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:37:31,284 [INFO] Regularization: 3353.444092 * 0.0000010000 = 0.0033534442
2019-03-19 01:37:31,284 [INFO] Sum of grad norms: 0.027582
2019-03-19 01:37:31,285 [INFO] ---------------------------------
2019-03-19 01:37:50,206 [INFO] ---------------------------------
2019-03-19 01:37:50,207 [INFO] Summary:
2019-03-19 01:37:50,207 [INFO] Batch 160000, worst loss 0.061166 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:37:50,208 [INFO] Regularization: 3353.427979 * 0.0000010000 = 0.0033534279
2019-03-19 01:37:50,208 [INFO] Sum of grad norms: 0.027282
2019-03-19 01:37:50,209 [INFO] ---------------------------------
2019-03-19 01:37:55,189 [INFO] ---------------------------------
2019-03-19 01:37:55,189 [INFO] Evaluation:
2019-03-19 01:37:55,190 [INFO] Batch 160000, worst loss 0.057624 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:37:55,191 [INFO] ---------------------------------
2019-03-19 01:38:14,119 [INFO] ---------------------------------
2019-03-19 01:38:14,120 [INFO] Summary:
2019-03-19 01:38:14,121 [INFO] Batch 161000, worst loss 0.061162 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:38:14,121 [INFO] Regularization: 3353.408691 * 0.0000010000 = 0.0033534088
2019-03-19 01:38:14,122 [INFO] Sum of grad norms: 0.045664
2019-03-19 01:38:14,122 [INFO] ---------------------------------
2019-03-19 01:38:33,112 [INFO] ---------------------------------
2019-03-19 01:38:33,113 [INFO] Summary:
2019-03-19 01:38:33,114 [INFO] Batch 162000, worst loss 0.061162 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:38:33,115 [INFO] Regularization: 3353.391846 * 0.0000010000 = 0.0033533918
2019-03-19 01:38:33,115 [INFO] Sum of grad norms: 0.039901
2019-03-19 01:38:33,116 [INFO] ---------------------------------
2019-03-19 01:38:51,736 [INFO] ---------------------------------
2019-03-19 01:38:51,737 [INFO] Summary:
2019-03-19 01:38:51,738 [INFO] Batch 163000, worst loss 0.061361 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:38:51,738 [INFO] Regularization: 3353.379883 * 0.0000010000 = 0.0033533799
2019-03-19 01:38:51,739 [INFO] Sum of grad norms: 0.047154
2019-03-19 01:38:51,740 [INFO] ---------------------------------
2019-03-19 01:39:10,248 [INFO] ---------------------------------
2019-03-19 01:39:10,249 [INFO] Summary:
2019-03-19 01:39:10,250 [INFO] Batch 164000, worst loss 0.061051 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:39:10,250 [INFO] Regularization: 3353.371338 * 0.0000010000 = 0.0033533713
2019-03-19 01:39:10,251 [INFO] Sum of grad norms: 0.063339
2019-03-19 01:39:10,251 [INFO] ---------------------------------
2019-03-19 01:39:28,785 [INFO] ---------------------------------
2019-03-19 01:39:28,786 [INFO] Summary:
2019-03-19 01:39:28,787 [INFO] Batch 165000, worst loss 0.061105 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:39:28,788 [INFO] Regularization: 3353.359863 * 0.0000010000 = 0.0033533599
2019-03-19 01:39:28,789 [INFO] Sum of grad norms: 0.035954
2019-03-19 01:39:28,790 [INFO] ---------------------------------
2019-03-19 01:39:47,417 [INFO] ---------------------------------
2019-03-19 01:39:47,418 [INFO] Summary:
2019-03-19 01:39:47,418 [INFO] Batch 166000, worst loss 0.061076 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:39:47,419 [INFO] Regularization: 3353.350586 * 0.0000010000 = 0.0033533506
2019-03-19 01:39:47,419 [INFO] Sum of grad norms: 0.091661
2019-03-19 01:39:47,420 [INFO] ---------------------------------
2019-03-19 01:40:06,206 [INFO] ---------------------------------
2019-03-19 01:40:06,207 [INFO] Summary:
2019-03-19 01:40:06,208 [INFO] Batch 167000, worst loss 0.061109 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:40:06,209 [INFO] Regularization: 3353.343994 * 0.0000010000 = 0.0033533440
2019-03-19 01:40:06,209 [INFO] Sum of grad norms: 0.044957
2019-03-19 01:40:06,210 [INFO] ---------------------------------
2019-03-19 01:40:25,052 [INFO] ---------------------------------
2019-03-19 01:40:25,053 [INFO] Summary:
2019-03-19 01:40:25,054 [INFO] Batch 168000, worst loss 0.061204 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:40:25,054 [INFO] Regularization: 3353.335693 * 0.0000010000 = 0.0033533357
2019-03-19 01:40:25,055 [INFO] Sum of grad norms: 0.035202
2019-03-19 01:40:25,055 [INFO] ---------------------------------
2019-03-19 01:40:43,728 [INFO] ---------------------------------
2019-03-19 01:40:43,729 [INFO] Summary:
2019-03-19 01:40:43,730 [INFO] Batch 169000, worst loss 0.060985 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:40:43,730 [INFO] Regularization: 3353.326660 * 0.0000010000 = 0.0033533266
2019-03-19 01:40:43,731 [INFO] Sum of grad norms: 0.054121
2019-03-19 01:40:43,732 [INFO] ---------------------------------
2019-03-19 01:41:02,199 [INFO] ---------------------------------
2019-03-19 01:41:02,199 [INFO] Summary:
2019-03-19 01:41:02,200 [INFO] Batch 170000, worst loss 0.061093 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:41:02,201 [INFO] Regularization: 3353.318359 * 0.0000010000 = 0.0033533184
2019-03-19 01:41:02,201 [INFO] Sum of grad norms: 0.076588
2019-03-19 01:41:02,202 [INFO] ---------------------------------
2019-03-19 01:41:07,055 [INFO] ---------------------------------
2019-03-19 01:41:07,056 [INFO] Evaluation:
2019-03-19 01:41:07,057 [INFO] Batch 170000, worst loss 0.057706 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:41:07,057 [INFO] ---------------------------------
2019-03-19 01:41:26,013 [INFO] ---------------------------------
2019-03-19 01:41:26,014 [INFO] Summary:
2019-03-19 01:41:26,015 [INFO] Batch 171000, worst loss 0.061180 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:41:26,016 [INFO] Regularization: 3353.310059 * 0.0000010000 = 0.0033533101
2019-03-19 01:41:26,016 [INFO] Sum of grad norms: 0.062248
2019-03-19 01:41:26,017 [INFO] ---------------------------------
2019-03-19 01:41:44,959 [INFO] ---------------------------------
2019-03-19 01:41:44,960 [INFO] Summary:
2019-03-19 01:41:44,961 [INFO] Batch 172000, worst loss 0.061034 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:41:44,961 [INFO] Regularization: 3353.305664 * 0.0000010000 = 0.0033533056
2019-03-19 01:41:44,962 [INFO] Sum of grad norms: 0.031203
2019-03-19 01:41:44,962 [INFO] ---------------------------------
2019-03-19 01:42:03,863 [INFO] ---------------------------------
2019-03-19 01:42:03,864 [INFO] Summary:
2019-03-19 01:42:03,865 [INFO] Batch 173000, worst loss 0.061037 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:42:03,865 [INFO] Regularization: 3353.301025 * 0.0000010000 = 0.0033533010
2019-03-19 01:42:03,866 [INFO] Sum of grad norms: 0.027927
2019-03-19 01:42:03,867 [INFO] ---------------------------------
2019-03-19 01:42:22,741 [INFO] ---------------------------------
2019-03-19 01:42:22,742 [INFO] Summary:
2019-03-19 01:42:22,743 [INFO] Batch 174000, worst loss 0.061234 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:42:22,743 [INFO] Regularization: 3353.298584 * 0.0000010000 = 0.0033532986
2019-03-19 01:42:22,744 [INFO] Sum of grad norms: 0.053917
2019-03-19 01:42:22,745 [INFO] ---------------------------------
2019-03-19 01:42:41,323 [INFO] ---------------------------------
2019-03-19 01:42:41,325 [INFO] Summary:
2019-03-19 01:42:41,325 [INFO] Batch 175000, worst loss 0.061151 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:42:41,325 [INFO] Regularization: 3353.294678 * 0.0000010000 = 0.0033532947
2019-03-19 01:42:41,326 [INFO] Sum of grad norms: 0.027159
2019-03-19 01:42:41,327 [INFO] ---------------------------------
2019-03-19 01:43:00,559 [INFO] ---------------------------------
2019-03-19 01:43:00,560 [INFO] Summary:
2019-03-19 01:43:00,561 [INFO] Batch 176000, worst loss 0.061093 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:43:00,561 [INFO] Regularization: 3353.291260 * 0.0000010000 = 0.0033532912
2019-03-19 01:43:00,562 [INFO] Sum of grad norms: 0.034415
2019-03-19 01:43:00,563 [INFO] ---------------------------------
2019-03-19 01:43:19,590 [INFO] ---------------------------------
2019-03-19 01:43:19,591 [INFO] Summary:
2019-03-19 01:43:19,592 [INFO] Batch 177000, worst loss 0.061044 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:43:19,592 [INFO] Regularization: 3353.288574 * 0.0000010000 = 0.0033532886
2019-03-19 01:43:19,593 [INFO] Sum of grad norms: 0.018008
2019-03-19 01:43:19,593 [INFO] ---------------------------------
2019-03-19 01:43:37,984 [INFO] ---------------------------------
2019-03-19 01:43:37,985 [INFO] Summary:
2019-03-19 01:43:37,986 [INFO] Batch 178000, worst loss 0.060967 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:43:37,986 [INFO] Regularization: 3353.284180 * 0.0000010000 = 0.0033532842
2019-03-19 01:43:37,987 [INFO] Sum of grad norms: 0.019674
2019-03-19 01:43:37,987 [INFO] ---------------------------------
2019-03-19 01:43:56,840 [INFO] ---------------------------------
2019-03-19 01:43:56,841 [INFO] Summary:
2019-03-19 01:43:56,841 [INFO] Batch 179000, worst loss 0.061161 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:43:56,842 [INFO] Regularization: 3353.281250 * 0.0000010000 = 0.0033532812
2019-03-19 01:43:56,842 [INFO] Sum of grad norms: 0.043794
2019-03-19 01:43:56,843 [INFO] ---------------------------------
2019-03-19 01:44:16,100 [INFO] ---------------------------------
2019-03-19 01:44:16,101 [INFO] Summary:
2019-03-19 01:44:16,101 [INFO] Batch 180000, worst loss 0.061097 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:44:16,102 [INFO] Regularization: 3353.279053 * 0.0000010000 = 0.0033532791
2019-03-19 01:44:16,102 [INFO] Sum of grad norms: 0.027180
2019-03-19 01:44:16,103 [INFO] ---------------------------------
2019-03-19 01:44:21,143 [INFO] ---------------------------------
2019-03-19 01:44:21,145 [INFO] Evaluation:
2019-03-19 01:44:21,146 [INFO] Batch 180000, worst loss 0.057717 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:44:21,147 [INFO] ---------------------------------
2019-03-19 01:44:39,544 [INFO] ---------------------------------
2019-03-19 01:44:39,545 [INFO] Summary:
2019-03-19 01:44:39,545 [INFO] Batch 181000, worst loss 0.061015 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:44:39,546 [INFO] Regularization: 3353.274902 * 0.0000010000 = 0.0033532749
2019-03-19 01:44:39,546 [INFO] Sum of grad norms: 0.040635
2019-03-19 01:44:39,547 [INFO] ---------------------------------
2019-03-19 01:44:58,486 [INFO] ---------------------------------
2019-03-19 01:44:58,487 [INFO] Summary:
2019-03-19 01:44:58,488 [INFO] Batch 182000, worst loss 0.061093 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:44:58,488 [INFO] Regularization: 3353.273926 * 0.0000010000 = 0.0033532740
2019-03-19 01:44:58,489 [INFO] Sum of grad norms: 0.023913
2019-03-19 01:44:58,489 [INFO] ---------------------------------
2019-03-19 01:45:17,421 [INFO] ---------------------------------
2019-03-19 01:45:17,422 [INFO] Summary:
2019-03-19 01:45:17,424 [INFO] Batch 183000, worst loss 0.061060 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:45:17,425 [INFO] Regularization: 3353.271729 * 0.0000010000 = 0.0033532716
2019-03-19 01:45:17,426 [INFO] Sum of grad norms: 0.022241
2019-03-19 01:45:17,427 [INFO] ---------------------------------
2019-03-19 01:45:36,735 [INFO] ---------------------------------
2019-03-19 01:45:36,736 [INFO] Summary:
2019-03-19 01:45:36,737 [INFO] Batch 184000, worst loss 0.061060 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:45:36,737 [INFO] Regularization: 3353.270020 * 0.0000010000 = 0.0033532700
2019-03-19 01:45:36,738 [INFO] Sum of grad norms: 0.026306
2019-03-19 01:45:36,739 [INFO] ---------------------------------
2019-03-19 01:45:55,914 [INFO] ---------------------------------
2019-03-19 01:45:55,915 [INFO] Summary:
2019-03-19 01:45:55,916 [INFO] Batch 185000, worst loss 0.061114 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:45:55,917 [INFO] Regularization: 3353.269287 * 0.0000010000 = 0.0033532693
2019-03-19 01:45:55,917 [INFO] Sum of grad norms: 0.025986
2019-03-19 01:45:55,918 [INFO] ---------------------------------
2019-03-19 01:46:14,330 [INFO] ---------------------------------
2019-03-19 01:46:14,331 [INFO] Summary:
2019-03-19 01:46:14,331 [INFO] Batch 186000, worst loss 0.061094 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:46:14,332 [INFO] Regularization: 3353.268066 * 0.0000010000 = 0.0033532681
2019-03-19 01:46:14,332 [INFO] Sum of grad norms: 0.047340
2019-03-19 01:46:14,333 [INFO] ---------------------------------
2019-03-19 01:46:32,572 [INFO] ---------------------------------
2019-03-19 01:46:32,573 [INFO] Summary:
2019-03-19 01:46:32,573 [INFO] Batch 187000, worst loss 0.061199 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:46:32,574 [INFO] Regularization: 3353.267578 * 0.0000010000 = 0.0033532677
2019-03-19 01:46:32,574 [INFO] Sum of grad norms: 0.049845
2019-03-19 01:46:32,575 [INFO] ---------------------------------
2019-03-19 01:46:51,345 [INFO] ---------------------------------
2019-03-19 01:46:51,346 [INFO] Summary:
2019-03-19 01:46:51,347 [INFO] Batch 188000, worst loss 0.061256 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:46:51,347 [INFO] Regularization: 3353.266113 * 0.0000010000 = 0.0033532660
2019-03-19 01:46:51,348 [INFO] Sum of grad norms: 0.118259
2019-03-19 01:46:51,349 [INFO] ---------------------------------
2019-03-19 01:47:10,222 [INFO] ---------------------------------
2019-03-19 01:47:10,223 [INFO] Summary:
2019-03-19 01:47:10,224 [INFO] Batch 189000, worst loss 0.061256 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:47:10,224 [INFO] Regularization: 3353.265869 * 0.0000010000 = 0.0033532658
2019-03-19 01:47:10,225 [INFO] Sum of grad norms: 0.034855
2019-03-19 01:47:10,225 [INFO] ---------------------------------
2019-03-19 01:47:28,960 [INFO] ---------------------------------
2019-03-19 01:47:28,961 [INFO] Summary:
2019-03-19 01:47:28,962 [INFO] Batch 190000, worst loss 0.061071 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:47:28,963 [INFO] Regularization: 3353.264160 * 0.0000010000 = 0.0033532642
2019-03-19 01:47:28,963 [INFO] Sum of grad norms: 0.024919
2019-03-19 01:47:28,964 [INFO] ---------------------------------
2019-03-19 01:47:33,903 [INFO] ---------------------------------
2019-03-19 01:47:33,904 [INFO] Evaluation:
2019-03-19 01:47:33,905 [INFO] Batch 190000, worst loss 0.057725 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:47:33,905 [INFO] ---------------------------------
2019-03-19 01:47:52,787 [INFO] ---------------------------------
2019-03-19 01:47:52,787 [INFO] Summary:
2019-03-19 01:47:52,788 [INFO] Batch 191000, worst loss 0.061280 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:47:52,788 [INFO] Regularization: 3353.262695 * 0.0000010000 = 0.0033532628
2019-03-19 01:47:52,789 [INFO] Sum of grad norms: 0.027956
2019-03-19 01:47:52,789 [INFO] ---------------------------------
2019-03-19 01:48:11,262 [INFO] ---------------------------------
2019-03-19 01:48:11,263 [INFO] Summary:
2019-03-19 01:48:11,264 [INFO] Batch 192000, worst loss 0.061284 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:48:11,264 [INFO] Regularization: 3353.262207 * 0.0000010000 = 0.0033532621
2019-03-19 01:48:11,265 [INFO] Sum of grad norms: 0.061592
2019-03-19 01:48:11,266 [INFO] ---------------------------------
2019-03-19 01:48:29,756 [INFO] ---------------------------------
2019-03-19 01:48:29,757 [INFO] Summary:
2019-03-19 01:48:29,758 [INFO] Batch 193000, worst loss 0.061208 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:48:29,758 [INFO] Regularization: 3353.261719 * 0.0000010000 = 0.0033532616
2019-03-19 01:48:29,759 [INFO] Sum of grad norms: 0.048461
2019-03-19 01:48:29,760 [INFO] ---------------------------------
2019-03-19 01:48:48,368 [INFO] ---------------------------------
2019-03-19 01:48:48,369 [INFO] Summary:
2019-03-19 01:48:48,369 [INFO] Batch 194000, worst loss 0.061185 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:48:48,370 [INFO] Regularization: 3353.261475 * 0.0000010000 = 0.0033532614
2019-03-19 01:48:48,370 [INFO] Sum of grad norms: 0.039333
2019-03-19 01:48:48,371 [INFO] ---------------------------------
2019-03-19 01:49:07,034 [INFO] ---------------------------------
2019-03-19 01:49:07,035 [INFO] Summary:
2019-03-19 01:49:07,036 [INFO] Batch 195000, worst loss 0.061150 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:49:07,036 [INFO] Regularization: 3353.260742 * 0.0000010000 = 0.0033532607
2019-03-19 01:49:07,037 [INFO] Sum of grad norms: 0.030458
2019-03-19 01:49:07,037 [INFO] ---------------------------------
2019-03-19 01:49:25,591 [INFO] ---------------------------------
2019-03-19 01:49:25,592 [INFO] Summary:
2019-03-19 01:49:25,593 [INFO] Batch 196000, worst loss 0.061140 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:49:25,593 [INFO] Regularization: 3353.260742 * 0.0000010000 = 0.0033532607
2019-03-19 01:49:25,594 [INFO] Sum of grad norms: 0.063596
2019-03-19 01:49:25,594 [INFO] ---------------------------------
2019-03-19 01:49:44,331 [INFO] ---------------------------------
2019-03-19 01:49:44,332 [INFO] Summary:
2019-03-19 01:49:44,333 [INFO] Batch 197000, worst loss 0.060962 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:49:44,334 [INFO] Regularization: 3353.260254 * 0.0000010000 = 0.0033532602
2019-03-19 01:49:44,334 [INFO] Sum of grad norms: 0.022380
2019-03-19 01:49:44,335 [INFO] ---------------------------------
2019-03-19 01:50:02,793 [INFO] ---------------------------------
2019-03-19 01:50:02,794 [INFO] Summary:
2019-03-19 01:50:02,795 [INFO] Batch 198000, worst loss 0.061156 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:50:02,795 [INFO] Regularization: 3353.259766 * 0.0000010000 = 0.0033532598
2019-03-19 01:50:02,796 [INFO] Sum of grad norms: 0.027053
2019-03-19 01:50:02,796 [INFO] ---------------------------------
2019-03-19 01:50:21,709 [INFO] ---------------------------------
2019-03-19 01:50:21,710 [INFO] Summary:
2019-03-19 01:50:21,711 [INFO] Batch 199000, worst loss 0.061181 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:50:21,711 [INFO] Regularization: 3353.259521 * 0.0000010000 = 0.0033532595
2019-03-19 01:50:21,712 [INFO] Sum of grad norms: 0.073904
2019-03-19 01:50:21,712 [INFO] ---------------------------------
2019-03-19 01:50:40,372 [INFO] ---------------------------------
2019-03-19 01:50:40,373 [INFO] Summary:
2019-03-19 01:50:40,374 [INFO] Batch 200000, worst loss 0.061128 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:50:40,374 [INFO] Regularization: 3353.259277 * 0.0000010000 = 0.0033532593
2019-03-19 01:50:40,375 [INFO] Sum of grad norms: 0.031129
2019-03-19 01:50:40,375 [INFO] ---------------------------------
2019-03-19 01:50:45,260 [INFO] ---------------------------------
2019-03-19 01:50:45,261 [INFO] Evaluation:
2019-03-19 01:50:45,262 [INFO] Batch 200000, worst loss 0.057663 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:50:45,262 [INFO] ---------------------------------
2019-03-19 01:51:04,039 [INFO] ---------------------------------
2019-03-19 01:51:04,039 [INFO] Summary:
2019-03-19 01:51:04,040 [INFO] Batch 201000, worst loss 0.061332 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:51:04,040 [INFO] Regularization: 3353.259277 * 0.0000010000 = 0.0033532593
2019-03-19 01:51:04,041 [INFO] Sum of grad norms: 0.028559
2019-03-19 01:51:04,041 [INFO] ---------------------------------
2019-03-19 01:51:22,861 [INFO] ---------------------------------
2019-03-19 01:51:22,862 [INFO] Summary:
2019-03-19 01:51:22,863 [INFO] Batch 202000, worst loss 0.061087 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:51:22,864 [INFO] Regularization: 3353.259033 * 0.0000010000 = 0.0033532591
2019-03-19 01:51:22,864 [INFO] Sum of grad norms: 0.060779
2019-03-19 01:51:22,865 [INFO] ---------------------------------
2019-03-19 01:51:41,616 [INFO] ---------------------------------
2019-03-19 01:51:41,618 [INFO] Summary:
2019-03-19 01:51:41,618 [INFO] Batch 203000, worst loss 0.061076 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:51:41,619 [INFO] Regularization: 3353.259033 * 0.0000010000 = 0.0033532591
2019-03-19 01:51:41,620 [INFO] Sum of grad norms: 0.069665
2019-03-19 01:51:41,620 [INFO] ---------------------------------
2019-03-19 01:52:00,220 [INFO] ---------------------------------
2019-03-19 01:52:00,221 [INFO] Summary:
2019-03-19 01:52:00,222 [INFO] Batch 204000, worst loss 0.061189 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:52:00,223 [INFO] Regularization: 3353.258789 * 0.0000010000 = 0.0033532588
2019-03-19 01:52:00,223 [INFO] Sum of grad norms: 0.053575
2019-03-19 01:52:00,224 [INFO] ---------------------------------
2019-03-19 01:52:19,051 [INFO] ---------------------------------
2019-03-19 01:52:19,052 [INFO] Summary:
2019-03-19 01:52:19,053 [INFO] Batch 205000, worst loss 0.061189 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:52:19,053 [INFO] Regularization: 3353.258545 * 0.0000010000 = 0.0033532586
2019-03-19 01:52:19,054 [INFO] Sum of grad norms: 0.052266
2019-03-19 01:52:19,055 [INFO] ---------------------------------
2019-03-19 01:52:37,741 [INFO] ---------------------------------
2019-03-19 01:52:37,742 [INFO] Summary:
2019-03-19 01:52:37,742 [INFO] Batch 206000, worst loss 0.061120 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:52:37,743 [INFO] Regularization: 3353.258789 * 0.0000010000 = 0.0033532588
2019-03-19 01:52:37,743 [INFO] Sum of grad norms: 0.017642
2019-03-19 01:52:37,744 [INFO] ---------------------------------
2019-03-19 01:52:56,797 [INFO] ---------------------------------
2019-03-19 01:52:56,798 [INFO] Summary:
2019-03-19 01:52:56,798 [INFO] Batch 207000, worst loss 0.061090 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:52:56,799 [INFO] Regularization: 3353.258545 * 0.0000010000 = 0.0033532586
2019-03-19 01:52:56,799 [INFO] Sum of grad norms: 0.038145
2019-03-19 01:52:56,800 [INFO] ---------------------------------
2019-03-19 01:53:16,015 [INFO] ---------------------------------
2019-03-19 01:53:16,016 [INFO] Summary:
2019-03-19 01:53:16,017 [INFO] Batch 208000, worst loss 0.061198 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:53:16,018 [INFO] Regularization: 3353.258545 * 0.0000010000 = 0.0033532586
2019-03-19 01:53:16,019 [INFO] Sum of grad norms: 0.040646
2019-03-19 01:53:16,020 [INFO] ---------------------------------
2019-03-19 01:53:34,897 [INFO] ---------------------------------
2019-03-19 01:53:34,898 [INFO] Summary:
2019-03-19 01:53:34,899 [INFO] Batch 209000, worst loss 0.061198 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:53:34,899 [INFO] Regularization: 3353.258301 * 0.0000010000 = 0.0033532584
2019-03-19 01:53:34,900 [INFO] Sum of grad norms: 0.022372
2019-03-19 01:53:34,900 [INFO] ---------------------------------
2019-03-19 01:53:54,021 [INFO] ---------------------------------
2019-03-19 01:53:54,022 [INFO] Summary:
2019-03-19 01:53:54,022 [INFO] Batch 210000, worst loss 0.061083 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:53:54,023 [INFO] Regularization: 3353.258301 * 0.0000010000 = 0.0033532584
2019-03-19 01:53:54,023 [INFO] Sum of grad norms: 0.091908
2019-03-19 01:53:54,024 [INFO] ---------------------------------
2019-03-19 01:53:58,970 [INFO] ---------------------------------
2019-03-19 01:53:58,971 [INFO] Evaluation:
2019-03-19 01:53:58,972 [INFO] Batch 210000, worst loss 0.057790 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:53:58,974 [INFO] ---------------------------------
2019-03-19 01:54:17,722 [INFO] ---------------------------------
2019-03-19 01:54:17,723 [INFO] Summary:
2019-03-19 01:54:17,723 [INFO] Batch 211000, worst loss 0.061195 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:54:17,724 [INFO] Regularization: 3353.258057 * 0.0000010000 = 0.0033532581
2019-03-19 01:54:17,724 [INFO] Sum of grad norms: 0.037558
2019-03-19 01:54:17,725 [INFO] ---------------------------------
2019-03-19 01:54:36,220 [INFO] ---------------------------------
2019-03-19 01:54:36,221 [INFO] Summary:
2019-03-19 01:54:36,221 [INFO] Batch 212000, worst loss 0.061466 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:54:36,222 [INFO] Regularization: 3353.258057 * 0.0000010000 = 0.0033532581
2019-03-19 01:54:36,223 [INFO] Sum of grad norms: 0.043222
2019-03-19 01:54:36,223 [INFO] ---------------------------------
2019-03-19 01:54:55,028 [INFO] ---------------------------------
2019-03-19 01:54:55,029 [INFO] Summary:
2019-03-19 01:54:55,029 [INFO] Batch 213000, worst loss 0.061281 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:54:55,030 [INFO] Regularization: 3353.258057 * 0.0000010000 = 0.0033532581
2019-03-19 01:54:55,031 [INFO] Sum of grad norms: 0.041436
2019-03-19 01:54:55,031 [INFO] ---------------------------------
2019-03-19 01:55:14,164 [INFO] ---------------------------------
2019-03-19 01:55:14,165 [INFO] Summary:
2019-03-19 01:55:14,165 [INFO] Batch 214000, worst loss 0.061281 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:55:14,166 [INFO] Regularization: 3353.257812 * 0.0000010000 = 0.0033532579
2019-03-19 01:55:14,167 [INFO] Sum of grad norms: 0.050878
2019-03-19 01:55:14,167 [INFO] ---------------------------------
2019-03-19 01:55:32,857 [INFO] ---------------------------------
2019-03-19 01:55:32,858 [INFO] Summary:
2019-03-19 01:55:32,859 [INFO] Batch 215000, worst loss 0.061197 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:55:32,859 [INFO] Regularization: 3353.257812 * 0.0000010000 = 0.0033532579
2019-03-19 01:55:32,860 [INFO] Sum of grad norms: 0.029467
2019-03-19 01:55:32,860 [INFO] ---------------------------------
2019-03-19 01:55:51,580 [INFO] ---------------------------------
2019-03-19 01:55:51,581 [INFO] Summary:
2019-03-19 01:55:51,581 [INFO] Batch 216000, worst loss 0.061311 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:55:51,582 [INFO] Regularization: 3353.257812 * 0.0000010000 = 0.0033532579
2019-03-19 01:55:51,582 [INFO] Sum of grad norms: 0.034153
2019-03-19 01:55:51,583 [INFO] ---------------------------------
2019-03-19 01:56:10,176 [INFO] ---------------------------------
2019-03-19 01:56:10,177 [INFO] Summary:
2019-03-19 01:56:10,177 [INFO] Batch 217000, worst loss 0.061135 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:56:10,178 [INFO] Regularization: 3353.257812 * 0.0000010000 = 0.0033532579
2019-03-19 01:56:10,178 [INFO] Sum of grad norms: 0.060768
2019-03-19 01:56:10,179 [INFO] ---------------------------------
2019-03-19 01:56:28,699 [INFO] ---------------------------------
2019-03-19 01:56:28,700 [INFO] Summary:
2019-03-19 01:56:28,701 [INFO] Batch 218000, worst loss 0.061083 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:56:28,702 [INFO] Regularization: 3353.257568 * 0.0000010000 = 0.0033532577
2019-03-19 01:56:28,703 [INFO] Sum of grad norms: 0.031874
2019-03-19 01:56:28,704 [INFO] ---------------------------------
2019-03-19 01:56:47,333 [INFO] ---------------------------------
2019-03-19 01:56:47,334 [INFO] Summary:
2019-03-19 01:56:47,334 [INFO] Batch 219000, worst loss 0.061104 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:56:47,335 [INFO] Regularization: 3353.257324 * 0.0000010000 = 0.0033532574
2019-03-19 01:56:47,336 [INFO] Sum of grad norms: 0.031251
2019-03-19 01:56:47,336 [INFO] ---------------------------------
2019-03-19 01:57:06,330 [INFO] ---------------------------------
2019-03-19 01:57:06,331 [INFO] Summary:
2019-03-19 01:57:06,332 [INFO] Batch 220000, worst loss 0.061721 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:57:06,332 [INFO] Regularization: 3353.257324 * 0.0000010000 = 0.0033532574
2019-03-19 01:57:06,333 [INFO] Sum of grad norms: 0.045863
2019-03-19 01:57:06,333 [INFO] ---------------------------------
2019-03-19 01:57:11,234 [INFO] ---------------------------------
2019-03-19 01:57:11,235 [INFO] Evaluation:
2019-03-19 01:57:11,236 [INFO] Batch 220000, worst loss 0.057696 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 01:57:11,237 [INFO] ---------------------------------
2019-03-19 01:57:29,624 [INFO] ---------------------------------
2019-03-19 01:57:29,625 [INFO] Summary:
2019-03-19 01:57:29,626 [INFO] Batch 221000, worst loss 0.061059 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:57:29,627 [INFO] Regularization: 3353.257324 * 0.0000010000 = 0.0033532574
2019-03-19 01:57:29,627 [INFO] Sum of grad norms: 0.053648
2019-03-19 01:57:29,628 [INFO] ---------------------------------
2019-03-19 01:57:48,306 [INFO] ---------------------------------
2019-03-19 01:57:48,307 [INFO] Summary:
2019-03-19 01:57:48,308 [INFO] Batch 222000, worst loss 0.061205 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:57:48,308 [INFO] Regularization: 3353.257324 * 0.0000010000 = 0.0033532574
2019-03-19 01:57:48,309 [INFO] Sum of grad norms: 0.039938
2019-03-19 01:57:48,309 [INFO] ---------------------------------
2019-03-19 01:58:06,966 [INFO] ---------------------------------
2019-03-19 01:58:06,967 [INFO] Summary:
2019-03-19 01:58:06,968 [INFO] Batch 223000, worst loss 0.061232 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:58:06,969 [INFO] Regularization: 3353.257080 * 0.0000010000 = 0.0033532570
2019-03-19 01:58:06,969 [INFO] Sum of grad norms: 0.027304
2019-03-19 01:58:06,970 [INFO] ---------------------------------
2019-03-19 01:58:25,853 [INFO] ---------------------------------
2019-03-19 01:58:25,853 [INFO] Summary:
2019-03-19 01:58:25,854 [INFO] Batch 224000, worst loss 0.061154 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:58:25,855 [INFO] Regularization: 3353.257080 * 0.0000010000 = 0.0033532570
2019-03-19 01:58:25,855 [INFO] Sum of grad norms: 0.032961
2019-03-19 01:58:25,856 [INFO] ---------------------------------
2019-03-19 01:58:44,706 [INFO] ---------------------------------
2019-03-19 01:58:44,707 [INFO] Summary:
2019-03-19 01:58:44,708 [INFO] Batch 225000, worst loss 0.061093 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:58:44,708 [INFO] Regularization: 3353.257080 * 0.0000010000 = 0.0033532570
2019-03-19 01:58:44,709 [INFO] Sum of grad norms: 0.075145
2019-03-19 01:58:44,709 [INFO] ---------------------------------
2019-03-19 01:59:03,812 [INFO] ---------------------------------
2019-03-19 01:59:03,813 [INFO] Summary:
2019-03-19 01:59:03,813 [INFO] Batch 226000, worst loss 0.061234 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:59:03,814 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 01:59:03,815 [INFO] Sum of grad norms: 0.052736
2019-03-19 01:59:03,815 [INFO] ---------------------------------
2019-03-19 01:59:22,528 [INFO] ---------------------------------
2019-03-19 01:59:22,529 [INFO] Summary:
2019-03-19 01:59:22,530 [INFO] Batch 227000, worst loss 0.060999 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:59:22,530 [INFO] Regularization: 3353.257324 * 0.0000010000 = 0.0033532574
2019-03-19 01:59:22,531 [INFO] Sum of grad norms: 0.042915
2019-03-19 01:59:22,531 [INFO] ---------------------------------
2019-03-19 01:59:41,079 [INFO] ---------------------------------
2019-03-19 01:59:41,080 [INFO] Summary:
2019-03-19 01:59:41,080 [INFO] Batch 228000, worst loss 0.061264 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 01:59:41,081 [INFO] Regularization: 3353.257324 * 0.0000010000 = 0.0033532574
2019-03-19 01:59:41,081 [INFO] Sum of grad norms: 0.028665
2019-03-19 01:59:41,082 [INFO] ---------------------------------
2019-03-19 02:00:00,087 [INFO] ---------------------------------
2019-03-19 02:00:00,088 [INFO] Summary:
2019-03-19 02:00:00,089 [INFO] Batch 229000, worst loss 0.061379 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:00:00,090 [INFO] Regularization: 3353.257080 * 0.0000010000 = 0.0033532570
2019-03-19 02:00:00,091 [INFO] Sum of grad norms: 0.052154
2019-03-19 02:00:00,091 [INFO] ---------------------------------
2019-03-19 02:00:18,793 [INFO] ---------------------------------
2019-03-19 02:00:18,794 [INFO] Summary:
2019-03-19 02:00:18,794 [INFO] Batch 230000, worst loss 0.061042 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:00:18,795 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:00:18,795 [INFO] Sum of grad norms: 0.068718
2019-03-19 02:00:18,796 [INFO] ---------------------------------
2019-03-19 02:00:23,693 [INFO] ---------------------------------
2019-03-19 02:00:23,694 [INFO] Evaluation:
2019-03-19 02:00:23,694 [INFO] Batch 230000, worst loss 0.057809 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:00:23,695 [INFO] ---------------------------------
2019-03-19 02:00:42,644 [INFO] ---------------------------------
2019-03-19 02:00:42,645 [INFO] Summary:
2019-03-19 02:00:42,646 [INFO] Batch 231000, worst loss 0.061193 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:00:42,646 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:00:42,647 [INFO] Sum of grad norms: 0.096283
2019-03-19 02:00:42,648 [INFO] ---------------------------------
2019-03-19 02:01:01,567 [INFO] ---------------------------------
2019-03-19 02:01:01,568 [INFO] Summary:
2019-03-19 02:01:01,568 [INFO] Batch 232000, worst loss 0.061019 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:01:01,569 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:01:01,569 [INFO] Sum of grad norms: 0.027048
2019-03-19 02:01:01,570 [INFO] ---------------------------------
2019-03-19 02:01:20,265 [INFO] ---------------------------------
2019-03-19 02:01:20,265 [INFO] Summary:
2019-03-19 02:01:20,266 [INFO] Batch 233000, worst loss 0.061183 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:01:20,267 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:01:20,267 [INFO] Sum of grad norms: 0.030536
2019-03-19 02:01:20,268 [INFO] ---------------------------------
2019-03-19 02:01:39,049 [INFO] ---------------------------------
2019-03-19 02:01:39,050 [INFO] Summary:
2019-03-19 02:01:39,051 [INFO] Batch 234000, worst loss 0.061059 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:01:39,052 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:01:39,052 [INFO] Sum of grad norms: 0.050144
2019-03-19 02:01:39,053 [INFO] ---------------------------------
2019-03-19 02:01:57,727 [INFO] ---------------------------------
2019-03-19 02:01:57,728 [INFO] Summary:
2019-03-19 02:01:57,728 [INFO] Batch 235000, worst loss 0.061059 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:01:57,729 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:01:57,730 [INFO] Sum of grad norms: 0.038732
2019-03-19 02:01:57,730 [INFO] ---------------------------------
2019-03-19 02:02:16,393 [INFO] ---------------------------------
2019-03-19 02:02:16,394 [INFO] Summary:
2019-03-19 02:02:16,394 [INFO] Batch 236000, worst loss 0.061195 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:02:16,395 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:02:16,395 [INFO] Sum of grad norms: 0.041839
2019-03-19 02:02:16,396 [INFO] ---------------------------------
2019-03-19 02:02:35,111 [INFO] ---------------------------------
2019-03-19 02:02:35,112 [INFO] Summary:
2019-03-19 02:02:35,113 [INFO] Batch 237000, worst loss 0.061117 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:02:35,114 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:02:35,114 [INFO] Sum of grad norms: 0.040729
2019-03-19 02:02:35,115 [INFO] ---------------------------------
2019-03-19 02:02:54,439 [INFO] ---------------------------------
2019-03-19 02:02:54,440 [INFO] Summary:
2019-03-19 02:02:54,441 [INFO] Batch 238000, worst loss 0.061179 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:02:54,441 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:02:54,442 [INFO] Sum of grad norms: 0.036454
2019-03-19 02:02:54,442 [INFO] ---------------------------------
2019-03-19 02:03:12,935 [INFO] ---------------------------------
2019-03-19 02:03:12,936 [INFO] Summary:
2019-03-19 02:03:12,937 [INFO] Batch 239000, worst loss 0.061134 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:03:12,937 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:03:12,938 [INFO] Sum of grad norms: 0.019095
2019-03-19 02:03:12,938 [INFO] ---------------------------------
2019-03-19 02:03:31,570 [INFO] ---------------------------------
2019-03-19 02:03:31,571 [INFO] Summary:
2019-03-19 02:03:31,571 [INFO] Batch 240000, worst loss 0.061091 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:03:31,572 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:03:31,572 [INFO] Sum of grad norms: 0.047317
2019-03-19 02:03:31,573 [INFO] ---------------------------------
2019-03-19 02:03:36,510 [INFO] ---------------------------------
2019-03-19 02:03:36,511 [INFO] Evaluation:
2019-03-19 02:03:36,514 [INFO] Batch 240000, worst loss 0.057789 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:03:36,515 [INFO] ---------------------------------
2019-03-19 02:03:55,275 [INFO] ---------------------------------
2019-03-19 02:03:55,276 [INFO] Summary:
2019-03-19 02:03:55,277 [INFO] Batch 241000, worst loss 0.060953 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:03:55,278 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:03:55,278 [INFO] Sum of grad norms: 0.054608
2019-03-19 02:03:55,279 [INFO] ---------------------------------
2019-03-19 02:04:14,068 [INFO] ---------------------------------
2019-03-19 02:04:14,069 [INFO] Summary:
2019-03-19 02:04:14,070 [INFO] Batch 242000, worst loss 0.061458 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:04:14,071 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:04:14,071 [INFO] Sum of grad norms: 0.032445
2019-03-19 02:04:14,072 [INFO] ---------------------------------
2019-03-19 02:04:32,281 [INFO] ---------------------------------
2019-03-19 02:04:32,282 [INFO] Summary:
2019-03-19 02:04:32,282 [INFO] Batch 243000, worst loss 0.061119 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:04:32,283 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:04:32,283 [INFO] Sum of grad norms: 0.029571
2019-03-19 02:04:32,284 [INFO] ---------------------------------
2019-03-19 02:04:50,880 [INFO] ---------------------------------
2019-03-19 02:04:50,881 [INFO] Summary:
2019-03-19 02:04:50,882 [INFO] Batch 244000, worst loss 0.061073 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:04:50,882 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:04:50,883 [INFO] Sum of grad norms: 0.070688
2019-03-19 02:04:50,884 [INFO] ---------------------------------
2019-03-19 02:05:09,692 [INFO] ---------------------------------
2019-03-19 02:05:09,693 [INFO] Summary:
2019-03-19 02:05:09,693 [INFO] Batch 245000, worst loss 0.060986 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:05:09,694 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:05:09,694 [INFO] Sum of grad norms: 0.026898
2019-03-19 02:05:09,695 [INFO] ---------------------------------
2019-03-19 02:05:28,377 [INFO] ---------------------------------
2019-03-19 02:05:28,378 [INFO] Summary:
2019-03-19 02:05:28,378 [INFO] Batch 246000, worst loss 0.061150 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:05:28,379 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:05:28,379 [INFO] Sum of grad norms: 0.031426
2019-03-19 02:05:28,380 [INFO] ---------------------------------
2019-03-19 02:05:47,127 [INFO] ---------------------------------
2019-03-19 02:05:47,128 [INFO] Summary:
2019-03-19 02:05:47,128 [INFO] Batch 247000, worst loss 0.061209 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:05:47,129 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:05:47,129 [INFO] Sum of grad norms: 0.063349
2019-03-19 02:05:47,130 [INFO] ---------------------------------
2019-03-19 02:06:05,720 [INFO] ---------------------------------
2019-03-19 02:06:05,721 [INFO] Summary:
2019-03-19 02:06:05,721 [INFO] Batch 248000, worst loss 0.061044 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:06:05,722 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:06:05,722 [INFO] Sum of grad norms: 0.047611
2019-03-19 02:06:05,723 [INFO] ---------------------------------
2019-03-19 02:06:24,620 [INFO] ---------------------------------
2019-03-19 02:06:24,620 [INFO] Summary:
2019-03-19 02:06:24,621 [INFO] Batch 249000, worst loss 0.061316 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:06:24,622 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:06:24,622 [INFO] Sum of grad norms: 0.043726
2019-03-19 02:06:24,623 [INFO] ---------------------------------
2019-03-19 02:06:43,153 [INFO] ---------------------------------
2019-03-19 02:06:43,154 [INFO] Summary:
2019-03-19 02:06:43,155 [INFO] Batch 250000, worst loss 0.061087 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:06:43,156 [INFO] Regularization: 3353.256836 * 0.0000010000 = 0.0033532567
2019-03-19 02:06:43,156 [INFO] Sum of grad norms: 0.065504
2019-03-19 02:06:43,157 [INFO] ---------------------------------
2019-03-19 02:06:47,984 [INFO] ---------------------------------
2019-03-19 02:06:47,984 [INFO] Evaluation:
2019-03-19 02:06:47,986 [INFO] Batch 250000, worst loss 0.057786 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:06:47,987 [INFO] ---------------------------------
2019-03-19 02:06:47,987 [INFO] Finished training, saved to file classifier/1552933539/1552957607_4_classifier_final.pth
2019-03-19 02:06:48,160 [INFO] ---------------------------------
2019-03-19 02:06:48,163 [INFO] Training model #5: (1, 64, 201) @ 1
2019-03-19 02:07:04,241 [INFO] ---------------------------------
2019-03-19 02:07:04,242 [INFO] Summary:
2019-03-19 02:07:04,243 [INFO] Batch 1000, worst loss 27.349569 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 02:07:04,243 [INFO] Regularization: 10744.013672 * 0.0000010000 = 0.0107440138
2019-03-19 02:07:04,244 [INFO] Sum of grad norms: 1.523356
2019-03-19 02:07:04,245 [INFO] ---------------------------------
2019-03-19 02:07:20,474 [INFO] ---------------------------------
2019-03-19 02:07:20,475 [INFO] Summary:
2019-03-19 02:07:20,476 [INFO] Batch 2000, worst loss 0.183882 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 02:07:20,477 [INFO] Regularization: 9233.056641 * 0.0000010000 = 0.0092330566
2019-03-19 02:07:20,478 [INFO] Sum of grad norms: 0.626337
2019-03-19 02:07:20,478 [INFO] ---------------------------------
2019-03-19 02:07:39,553 [INFO] ---------------------------------
2019-03-19 02:07:39,554 [INFO] Summary:
2019-03-19 02:07:39,555 [INFO] Batch 3000, worst loss 0.097326 (incl. reg.) of 1000 batches, learning rate 0.001839 @cl.-depth 1
2019-03-19 02:07:39,556 [INFO] Regularization: 8413.251953 * 0.0000010000 = 0.0084132515
2019-03-19 02:07:39,556 [INFO] Sum of grad norms: 1.569564
2019-03-19 02:07:39,557 [INFO] ---------------------------------
2019-03-19 02:07:58,875 [INFO] ---------------------------------
2019-03-19 02:07:58,875 [INFO] Summary:
2019-03-19 02:07:58,876 [INFO] Batch 4000, worst loss 0.082058 (incl. reg.) of 1000 batches, learning rate 0.000973 @cl.-depth 1
2019-03-19 02:07:58,877 [INFO] Regularization: 7939.701660 * 0.0000010000 = 0.0079397019
2019-03-19 02:07:58,877 [INFO] Sum of grad norms: 1.470041
2019-03-19 02:07:58,878 [INFO] ---------------------------------
2019-03-19 02:08:17,606 [INFO] ---------------------------------
2019-03-19 02:08:17,607 [INFO] Summary:
2019-03-19 02:08:17,607 [INFO] Batch 5000, worst loss 0.077703 (incl. reg.) of 1000 batches, learning rate 0.000821 @cl.-depth 1
2019-03-19 02:08:17,608 [INFO] Regularization: 7486.210938 * 0.0000010000 = 0.0074862111
2019-03-19 02:08:17,608 [INFO] Sum of grad norms: 0.171311
2019-03-19 02:08:17,609 [INFO] ---------------------------------
2019-03-19 02:08:36,986 [INFO] ---------------------------------
2019-03-19 02:08:36,987 [INFO] Summary:
2019-03-19 02:08:36,987 [INFO] Batch 6000, worst loss 0.076193 (incl. reg.) of 1000 batches, learning rate 0.000777 @cl.-depth 1
2019-03-19 02:08:36,988 [INFO] Regularization: 7085.144043 * 0.0000010000 = 0.0070851441
2019-03-19 02:08:36,988 [INFO] Sum of grad norms: 0.453430
2019-03-19 02:08:36,989 [INFO] ---------------------------------
2019-03-19 02:08:55,774 [INFO] ---------------------------------
2019-03-19 02:08:55,774 [INFO] Summary:
2019-03-19 02:08:55,775 [INFO] Batch 7000, worst loss 0.078542 (incl. reg.) of 1000 batches, learning rate 0.000762 @cl.-depth 1
2019-03-19 02:08:55,775 [INFO] Regularization: 6752.139160 * 0.0000010000 = 0.0067521390
2019-03-19 02:08:55,776 [INFO] Sum of grad norms: 0.811197
2019-03-19 02:08:55,776 [INFO] ---------------------------------
2019-03-19 02:09:14,727 [INFO] ---------------------------------
2019-03-19 02:09:14,728 [INFO] Summary:
2019-03-19 02:09:14,729 [INFO] Batch 8000, worst loss 0.074691 (incl. reg.) of 1000 batches, learning rate 0.000762 @cl.-depth 1
2019-03-19 02:09:14,730 [INFO] Regularization: 6464.310547 * 0.0000010000 = 0.0064643105
2019-03-19 02:09:14,731 [INFO] Sum of grad norms: 0.814634
2019-03-19 02:09:14,732 [INFO] ---------------------------------
2019-03-19 02:09:33,983 [INFO] ---------------------------------
2019-03-19 02:09:33,984 [INFO] Summary:
2019-03-19 02:09:33,984 [INFO] Batch 9000, worst loss 0.074305 (incl. reg.) of 1000 batches, learning rate 0.000747 @cl.-depth 1
2019-03-19 02:09:33,985 [INFO] Regularization: 6193.948242 * 0.0000010000 = 0.0061939484
2019-03-19 02:09:33,985 [INFO] Sum of grad norms: 0.647968
2019-03-19 02:09:33,986 [INFO] ---------------------------------
2019-03-19 02:09:52,624 [INFO] ---------------------------------
2019-03-19 02:09:52,625 [INFO] Summary:
2019-03-19 02:09:52,626 [INFO] Batch 10000, worst loss 0.074299 (incl. reg.) of 1000 batches, learning rate 0.000743 @cl.-depth 1
2019-03-19 02:09:52,627 [INFO] Regularization: 5987.424805 * 0.0000010000 = 0.0059874249
2019-03-19 02:09:52,628 [INFO] Sum of grad norms: 0.302198
2019-03-19 02:09:52,629 [INFO] ---------------------------------
2019-03-19 02:09:57,608 [INFO] ---------------------------------
2019-03-19 02:09:57,609 [INFO] Evaluation:
2019-03-19 02:09:57,614 [INFO] Batch 10000, worst loss 0.063460 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:09:57,614 [INFO] ---------------------------------
2019-03-19 02:10:16,635 [INFO] ---------------------------------
2019-03-19 02:10:16,636 [INFO] Summary:
2019-03-19 02:10:16,636 [INFO] Batch 11000, worst loss 0.075182 (incl. reg.) of 1000 batches, learning rate 0.000743 @cl.-depth 1
2019-03-19 02:10:16,637 [INFO] Regularization: 5829.630859 * 0.0000010000 = 0.0058296309
2019-03-19 02:10:16,637 [INFO] Sum of grad norms: 0.484206
2019-03-19 02:10:16,638 [INFO] ---------------------------------
2019-03-19 02:10:35,308 [INFO] ---------------------------------
2019-03-19 02:10:35,309 [INFO] Summary:
2019-03-19 02:10:35,309 [INFO] Batch 12000, worst loss 0.071954 (incl. reg.) of 1000 batches, learning rate 0.000743 @cl.-depth 1
2019-03-19 02:10:35,310 [INFO] Regularization: 5687.365234 * 0.0000010000 = 0.0056873653
2019-03-19 02:10:35,310 [INFO] Sum of grad norms: 0.607873
2019-03-19 02:10:35,311 [INFO] ---------------------------------
2019-03-19 02:10:54,410 [INFO] ---------------------------------
2019-03-19 02:10:54,411 [INFO] Summary:
2019-03-19 02:10:54,412 [INFO] Batch 13000, worst loss 0.070156 (incl. reg.) of 1000 batches, learning rate 0.000720 @cl.-depth 1
2019-03-19 02:10:54,412 [INFO] Regularization: 5574.400879 * 0.0000010000 = 0.0055744010
2019-03-19 02:10:54,413 [INFO] Sum of grad norms: 1.435793
2019-03-19 02:10:54,413 [INFO] ---------------------------------
2019-03-19 02:11:13,326 [INFO] ---------------------------------
2019-03-19 02:11:13,327 [INFO] Summary:
2019-03-19 02:11:13,328 [INFO] Batch 14000, worst loss 0.071041 (incl. reg.) of 1000 batches, learning rate 0.000702 @cl.-depth 1
2019-03-19 02:11:13,328 [INFO] Regularization: 5454.735840 * 0.0000010000 = 0.0054547358
2019-03-19 02:11:13,329 [INFO] Sum of grad norms: 1.302686
2019-03-19 02:11:13,329 [INFO] ---------------------------------
2019-03-19 02:11:32,429 [INFO] ---------------------------------
2019-03-19 02:11:32,430 [INFO] Summary:
2019-03-19 02:11:32,431 [INFO] Batch 15000, worst loss 0.068189 (incl. reg.) of 1000 batches, learning rate 0.000702 @cl.-depth 1
2019-03-19 02:11:32,431 [INFO] Regularization: 5378.170410 * 0.0000010000 = 0.0053781704
2019-03-19 02:11:32,432 [INFO] Sum of grad norms: 0.126611
2019-03-19 02:11:32,432 [INFO] ---------------------------------
2019-03-19 02:11:51,205 [INFO] ---------------------------------
2019-03-19 02:11:51,206 [INFO] Summary:
2019-03-19 02:11:51,207 [INFO] Batch 16000, worst loss 0.068282 (incl. reg.) of 1000 batches, learning rate 0.000682 @cl.-depth 1
2019-03-19 02:11:51,208 [INFO] Regularization: 5304.632812 * 0.0000010000 = 0.0053046327
2019-03-19 02:11:51,208 [INFO] Sum of grad norms: 0.146004
2019-03-19 02:11:51,209 [INFO] ---------------------------------
2019-03-19 02:12:09,883 [INFO] ---------------------------------
2019-03-19 02:12:09,884 [INFO] Summary:
2019-03-19 02:12:09,885 [INFO] Batch 17000, worst loss 0.068964 (incl. reg.) of 1000 batches, learning rate 0.000682 @cl.-depth 1
2019-03-19 02:12:09,885 [INFO] Regularization: 5228.143066 * 0.0000010000 = 0.0052281432
2019-03-19 02:12:09,886 [INFO] Sum of grad norms: 0.401349
2019-03-19 02:12:09,887 [INFO] ---------------------------------
2019-03-19 02:12:28,886 [INFO] ---------------------------------
2019-03-19 02:12:28,887 [INFO] Summary:
2019-03-19 02:12:28,888 [INFO] Batch 18000, worst loss 0.069292 (incl. reg.) of 1000 batches, learning rate 0.000682 @cl.-depth 1
2019-03-19 02:12:28,888 [INFO] Regularization: 5160.459961 * 0.0000010000 = 0.0051604598
2019-03-19 02:12:28,889 [INFO] Sum of grad norms: 0.055393
2019-03-19 02:12:28,889 [INFO] ---------------------------------
2019-03-19 02:12:47,677 [INFO] ---------------------------------
2019-03-19 02:12:47,678 [INFO] Summary:
2019-03-19 02:12:47,679 [INFO] Batch 19000, worst loss 0.067779 (incl. reg.) of 1000 batches, learning rate 0.000682 @cl.-depth 1
2019-03-19 02:12:47,679 [INFO] Regularization: 5093.125000 * 0.0000010000 = 0.0050931252
2019-03-19 02:12:47,680 [INFO] Sum of grad norms: 0.267503
2019-03-19 02:12:47,681 [INFO] ---------------------------------
2019-03-19 02:13:06,650 [INFO] ---------------------------------
2019-03-19 02:13:06,651 [INFO] Summary:
2019-03-19 02:13:06,652 [INFO] Batch 20000, worst loss 0.068499 (incl. reg.) of 1000 batches, learning rate 0.000678 @cl.-depth 1
2019-03-19 02:13:06,652 [INFO] Regularization: 5036.754395 * 0.0000010000 = 0.0050367545
2019-03-19 02:13:06,653 [INFO] Sum of grad norms: 0.627232
2019-03-19 02:13:06,654 [INFO] ---------------------------------
2019-03-19 02:13:11,616 [INFO] ---------------------------------
2019-03-19 02:13:11,617 [INFO] Evaluation:
2019-03-19 02:13:11,618 [INFO] Batch 20000, worst loss 0.060475 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:13:11,619 [INFO] ---------------------------------
2019-03-19 02:13:30,725 [INFO] ---------------------------------
2019-03-19 02:13:30,726 [INFO] Summary:
2019-03-19 02:13:30,726 [INFO] Batch 21000, worst loss 0.070984 (incl. reg.) of 1000 batches, learning rate 0.000678 @cl.-depth 1
2019-03-19 02:13:30,727 [INFO] Regularization: 4980.614746 * 0.0000010000 = 0.0049806149
2019-03-19 02:13:30,727 [INFO] Sum of grad norms: 1.466494
2019-03-19 02:13:30,728 [INFO] ---------------------------------
2019-03-19 02:13:49,814 [INFO] ---------------------------------
2019-03-19 02:13:49,815 [INFO] Summary:
2019-03-19 02:13:49,815 [INFO] Batch 22000, worst loss 0.069665 (incl. reg.) of 1000 batches, learning rate 0.000678 @cl.-depth 1
2019-03-19 02:13:49,816 [INFO] Regularization: 4920.728516 * 0.0000010000 = 0.0049207285
2019-03-19 02:13:49,816 [INFO] Sum of grad norms: 0.725471
2019-03-19 02:13:49,817 [INFO] ---------------------------------
2019-03-19 02:14:08,851 [INFO] ---------------------------------
2019-03-19 02:14:08,852 [INFO] Summary:
2019-03-19 02:14:08,852 [INFO] Batch 23000, worst loss 0.066606 (incl. reg.) of 1000 batches, learning rate 0.000678 @cl.-depth 1
2019-03-19 02:14:08,853 [INFO] Regularization: 4847.447754 * 0.0000010000 = 0.0048474479
2019-03-19 02:14:08,854 [INFO] Sum of grad norms: 0.327465
2019-03-19 02:14:08,854 [INFO] ---------------------------------
2019-03-19 02:14:27,786 [INFO] ---------------------------------
2019-03-19 02:14:27,787 [INFO] Summary:
2019-03-19 02:14:27,788 [INFO] Batch 24000, worst loss 0.069296 (incl. reg.) of 1000 batches, learning rate 0.000666 @cl.-depth 1
2019-03-19 02:14:27,788 [INFO] Regularization: 4788.811035 * 0.0000010000 = 0.0047888109
2019-03-19 02:14:27,789 [INFO] Sum of grad norms: 0.287457
2019-03-19 02:14:27,790 [INFO] ---------------------------------
2019-03-19 02:14:46,446 [INFO] ---------------------------------
2019-03-19 02:14:46,447 [INFO] Summary:
2019-03-19 02:14:46,448 [INFO] Batch 25000, worst loss 0.066207 (incl. reg.) of 1000 batches, learning rate 0.000666 @cl.-depth 1
2019-03-19 02:14:46,448 [INFO] Regularization: 4743.802246 * 0.0000010000 = 0.0047438024
2019-03-19 02:14:46,449 [INFO] Sum of grad norms: 0.218444
2019-03-19 02:14:46,449 [INFO] ---------------------------------
2019-03-19 02:15:05,464 [INFO] ---------------------------------
2019-03-19 02:15:05,465 [INFO] Summary:
2019-03-19 02:15:05,466 [INFO] Batch 26000, worst loss 0.065631 (incl. reg.) of 1000 batches, learning rate 0.000662 @cl.-depth 1
2019-03-19 02:15:05,466 [INFO] Regularization: 4677.795898 * 0.0000010000 = 0.0046777958
2019-03-19 02:15:05,467 [INFO] Sum of grad norms: 0.179786
2019-03-19 02:15:05,467 [INFO] ---------------------------------
2019-03-19 02:15:24,178 [INFO] ---------------------------------
2019-03-19 02:15:24,179 [INFO] Summary:
2019-03-19 02:15:24,179 [INFO] Batch 27000, worst loss 0.065677 (incl. reg.) of 1000 batches, learning rate 0.000656 @cl.-depth 1
2019-03-19 02:15:24,180 [INFO] Regularization: 4629.140137 * 0.0000010000 = 0.0046291403
2019-03-19 02:15:24,181 [INFO] Sum of grad norms: 0.635592
2019-03-19 02:15:24,181 [INFO] ---------------------------------
2019-03-19 02:15:42,964 [INFO] ---------------------------------
2019-03-19 02:15:42,965 [INFO] Summary:
2019-03-19 02:15:42,966 [INFO] Batch 28000, worst loss 0.067032 (incl. reg.) of 1000 batches, learning rate 0.000656 @cl.-depth 1
2019-03-19 02:15:42,967 [INFO] Regularization: 4573.501465 * 0.0000010000 = 0.0045735016
2019-03-19 02:15:42,967 [INFO] Sum of grad norms: 0.076832
2019-03-19 02:15:42,968 [INFO] ---------------------------------
2019-03-19 02:16:01,598 [INFO] ---------------------------------
2019-03-19 02:16:01,599 [INFO] Summary:
2019-03-19 02:16:01,600 [INFO] Batch 29000, worst loss 0.065182 (incl. reg.) of 1000 batches, learning rate 0.000656 @cl.-depth 1
2019-03-19 02:16:01,600 [INFO] Regularization: 4518.532227 * 0.0000010000 = 0.0045185322
2019-03-19 02:16:01,601 [INFO] Sum of grad norms: 0.369847
2019-03-19 02:16:01,601 [INFO] ---------------------------------
2019-03-19 02:16:20,550 [INFO] ---------------------------------
2019-03-19 02:16:20,551 [INFO] Summary:
2019-03-19 02:16:20,551 [INFO] Batch 30000, worst loss 0.068533 (incl. reg.) of 1000 batches, learning rate 0.000652 @cl.-depth 1
2019-03-19 02:16:20,552 [INFO] Regularization: 4473.173828 * 0.0000010000 = 0.0044731740
2019-03-19 02:16:20,552 [INFO] Sum of grad norms: 0.177540
2019-03-19 02:16:20,553 [INFO] ---------------------------------
2019-03-19 02:16:25,554 [INFO] ---------------------------------
2019-03-19 02:16:25,555 [INFO] Evaluation:
2019-03-19 02:16:25,555 [INFO] Batch 30000, worst loss 0.058858 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:16:25,557 [INFO] ---------------------------------
2019-03-19 02:16:44,132 [INFO] ---------------------------------
2019-03-19 02:16:44,133 [INFO] Summary:
2019-03-19 02:16:44,134 [INFO] Batch 31000, worst loss 0.064927 (incl. reg.) of 1000 batches, learning rate 0.000652 @cl.-depth 1
2019-03-19 02:16:44,134 [INFO] Regularization: 4428.920410 * 0.0000010000 = 0.0044289203
2019-03-19 02:16:44,135 [INFO] Sum of grad norms: 0.760058
2019-03-19 02:16:44,136 [INFO] ---------------------------------
2019-03-19 02:17:03,316 [INFO] ---------------------------------
2019-03-19 02:17:03,317 [INFO] Summary:
2019-03-19 02:17:03,318 [INFO] Batch 32000, worst loss 0.066802 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-19 02:17:03,318 [INFO] Regularization: 4371.255371 * 0.0000010000 = 0.0043712552
2019-03-19 02:17:03,319 [INFO] Sum of grad norms: 0.065741
2019-03-19 02:17:03,320 [INFO] ---------------------------------
2019-03-19 02:17:21,952 [INFO] ---------------------------------
2019-03-19 02:17:21,953 [INFO] Summary:
2019-03-19 02:17:21,953 [INFO] Batch 33000, worst loss 0.065318 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-19 02:17:21,954 [INFO] Regularization: 4313.275391 * 0.0000010000 = 0.0043132752
2019-03-19 02:17:21,955 [INFO] Sum of grad norms: 0.439420
2019-03-19 02:17:21,955 [INFO] ---------------------------------
2019-03-19 02:17:40,498 [INFO] ---------------------------------
2019-03-19 02:17:40,499 [INFO] Summary:
2019-03-19 02:17:40,499 [INFO] Batch 34000, worst loss 0.067999 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-19 02:17:40,500 [INFO] Regularization: 4284.915039 * 0.0000010000 = 0.0042849150
2019-03-19 02:17:40,501 [INFO] Sum of grad norms: 0.096756
2019-03-19 02:17:40,501 [INFO] ---------------------------------
2019-03-19 02:17:59,324 [INFO] ---------------------------------
2019-03-19 02:17:59,325 [INFO] Summary:
2019-03-19 02:17:59,325 [INFO] Batch 35000, worst loss 0.063742 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-19 02:17:59,326 [INFO] Regularization: 4218.766602 * 0.0000010000 = 0.0042187665
2019-03-19 02:17:59,326 [INFO] Sum of grad norms: 0.671624
2019-03-19 02:17:59,327 [INFO] ---------------------------------
2019-03-19 02:18:18,168 [INFO] ---------------------------------
2019-03-19 02:18:18,169 [INFO] Summary:
2019-03-19 02:18:18,170 [INFO] Batch 36000, worst loss 0.064973 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-19 02:18:18,171 [INFO] Regularization: 4180.584961 * 0.0000010000 = 0.0041805850
2019-03-19 02:18:18,171 [INFO] Sum of grad norms: 0.204674
2019-03-19 02:18:18,172 [INFO] ---------------------------------
2019-03-19 02:18:36,999 [INFO] ---------------------------------
2019-03-19 02:18:37,000 [INFO] Summary:
2019-03-19 02:18:37,001 [INFO] Batch 37000, worst loss 0.064092 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-19 02:18:37,002 [INFO] Regularization: 4125.939941 * 0.0000010000 = 0.0041259401
2019-03-19 02:18:37,002 [INFO] Sum of grad norms: 0.417312
2019-03-19 02:18:37,003 [INFO] ---------------------------------
2019-03-19 02:18:55,807 [INFO] ---------------------------------
2019-03-19 02:18:55,808 [INFO] Summary:
2019-03-19 02:18:55,809 [INFO] Batch 38000, worst loss 0.064146 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-19 02:18:55,810 [INFO] Regularization: 4080.455566 * 0.0000010000 = 0.0040804558
2019-03-19 02:18:55,811 [INFO] Sum of grad norms: 0.108900
2019-03-19 02:18:55,812 [INFO] ---------------------------------
2019-03-19 02:19:15,136 [INFO] ---------------------------------
2019-03-19 02:19:15,137 [INFO] Summary:
2019-03-19 02:19:15,138 [INFO] Batch 39000, worst loss 0.062883 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-19 02:19:15,138 [INFO] Regularization: 4033.699707 * 0.0000010000 = 0.0040336996
2019-03-19 02:19:15,139 [INFO] Sum of grad norms: 0.294196
2019-03-19 02:19:15,139 [INFO] ---------------------------------
2019-03-19 02:19:33,924 [INFO] ---------------------------------
2019-03-19 02:19:33,924 [INFO] Summary:
2019-03-19 02:19:33,925 [INFO] Batch 40000, worst loss 0.063049 (incl. reg.) of 1000 batches, learning rate 0.000629 @cl.-depth 1
2019-03-19 02:19:33,925 [INFO] Regularization: 3985.899170 * 0.0000010000 = 0.0039858990
2019-03-19 02:19:33,926 [INFO] Sum of grad norms: 0.328137
2019-03-19 02:19:33,926 [INFO] ---------------------------------
2019-03-19 02:19:38,861 [INFO] ---------------------------------
2019-03-19 02:19:38,862 [INFO] Evaluation:
2019-03-19 02:19:38,863 [INFO] Batch 40000, worst loss 0.058478 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:19:38,863 [INFO] ---------------------------------
2019-03-19 02:19:57,963 [INFO] ---------------------------------
2019-03-19 02:19:57,964 [INFO] Summary:
2019-03-19 02:19:57,965 [INFO] Batch 41000, worst loss 0.063399 (incl. reg.) of 1000 batches, learning rate 0.000314 @cl.-depth 1
2019-03-19 02:19:57,966 [INFO] Regularization: 3941.620361 * 0.0000010000 = 0.0039416202
2019-03-19 02:19:57,966 [INFO] Sum of grad norms: 0.269460
2019-03-19 02:19:57,967 [INFO] ---------------------------------
2019-03-19 02:20:16,641 [INFO] ---------------------------------
2019-03-19 02:20:16,642 [INFO] Summary:
2019-03-19 02:20:16,642 [INFO] Batch 42000, worst loss 0.062403 (incl. reg.) of 1000 batches, learning rate 0.000314 @cl.-depth 1
2019-03-19 02:20:16,643 [INFO] Regularization: 3901.091309 * 0.0000010000 = 0.0039010914
2019-03-19 02:20:16,643 [INFO] Sum of grad norms: 0.042455
2019-03-19 02:20:16,644 [INFO] ---------------------------------
2019-03-19 02:20:35,471 [INFO] ---------------------------------
2019-03-19 02:20:35,472 [INFO] Summary:
2019-03-19 02:20:35,473 [INFO] Batch 43000, worst loss 0.062096 (incl. reg.) of 1000 batches, learning rate 0.000314 @cl.-depth 1
2019-03-19 02:20:35,474 [INFO] Regularization: 3870.787354 * 0.0000010000 = 0.0038707873
2019-03-19 02:20:35,475 [INFO] Sum of grad norms: 0.187542
2019-03-19 02:20:35,476 [INFO] ---------------------------------
2019-03-19 02:20:54,483 [INFO] ---------------------------------
2019-03-19 02:20:54,483 [INFO] Summary:
2019-03-19 02:20:54,484 [INFO] Batch 44000, worst loss 0.062245 (incl. reg.) of 1000 batches, learning rate 0.000314 @cl.-depth 1
2019-03-19 02:20:54,484 [INFO] Regularization: 3840.997070 * 0.0000010000 = 0.0038409971
2019-03-19 02:20:54,485 [INFO] Sum of grad norms: 0.049869
2019-03-19 02:20:54,485 [INFO] ---------------------------------
2019-03-19 02:21:13,099 [INFO] ---------------------------------
2019-03-19 02:21:13,100 [INFO] Summary:
2019-03-19 02:21:13,101 [INFO] Batch 45000, worst loss 0.062051 (incl. reg.) of 1000 batches, learning rate 0.000314 @cl.-depth 1
2019-03-19 02:21:13,102 [INFO] Regularization: 3811.949951 * 0.0000010000 = 0.0038119499
2019-03-19 02:21:13,102 [INFO] Sum of grad norms: 0.106925
2019-03-19 02:21:13,103 [INFO] ---------------------------------
2019-03-19 02:21:32,208 [INFO] ---------------------------------
2019-03-19 02:21:32,209 [INFO] Summary:
2019-03-19 02:21:32,210 [INFO] Batch 46000, worst loss 0.062042 (incl. reg.) of 1000 batches, learning rate 0.000314 @cl.-depth 1
2019-03-19 02:21:32,212 [INFO] Regularization: 3785.943848 * 0.0000010000 = 0.0037859438
2019-03-19 02:21:32,213 [INFO] Sum of grad norms: 0.062464
2019-03-19 02:21:32,214 [INFO] ---------------------------------
2019-03-19 02:21:50,803 [INFO] ---------------------------------
2019-03-19 02:21:50,804 [INFO] Summary:
2019-03-19 02:21:50,804 [INFO] Batch 47000, worst loss 0.061817 (incl. reg.) of 1000 batches, learning rate 0.000314 @cl.-depth 1
2019-03-19 02:21:50,805 [INFO] Regularization: 3759.453125 * 0.0000010000 = 0.0037594531
2019-03-19 02:21:50,805 [INFO] Sum of grad norms: 0.063769
2019-03-19 02:21:50,806 [INFO] ---------------------------------
2019-03-19 02:22:09,627 [INFO] ---------------------------------
2019-03-19 02:22:09,628 [INFO] Summary:
2019-03-19 02:22:09,628 [INFO] Batch 48000, worst loss 0.061949 (incl. reg.) of 1000 batches, learning rate 0.000314 @cl.-depth 1
2019-03-19 02:22:09,629 [INFO] Regularization: 3731.508789 * 0.0000010000 = 0.0037315087
2019-03-19 02:22:09,629 [INFO] Sum of grad norms: 0.224566
2019-03-19 02:22:09,630 [INFO] ---------------------------------
2019-03-19 02:22:28,332 [INFO] ---------------------------------
2019-03-19 02:22:28,333 [INFO] Summary:
2019-03-19 02:22:28,334 [INFO] Batch 49000, worst loss 0.061649 (incl. reg.) of 1000 batches, learning rate 0.000314 @cl.-depth 1
2019-03-19 02:22:28,334 [INFO] Regularization: 3703.482422 * 0.0000010000 = 0.0037034824
2019-03-19 02:22:28,335 [INFO] Sum of grad norms: 0.384942
2019-03-19 02:22:28,335 [INFO] ---------------------------------
2019-03-19 02:22:47,145 [INFO] ---------------------------------
2019-03-19 02:22:47,146 [INFO] Summary:
2019-03-19 02:22:47,147 [INFO] Batch 50000, worst loss 0.061934 (incl. reg.) of 1000 batches, learning rate 0.000314 @cl.-depth 1
2019-03-19 02:22:47,147 [INFO] Regularization: 3677.845703 * 0.0000010000 = 0.0036778457
2019-03-19 02:22:47,148 [INFO] Sum of grad norms: 0.408744
2019-03-19 02:22:47,148 [INFO] ---------------------------------
2019-03-19 02:22:52,063 [INFO] ---------------------------------
2019-03-19 02:22:52,064 [INFO] Evaluation:
2019-03-19 02:22:52,065 [INFO] Batch 50000, worst loss 0.057992 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:22:52,066 [INFO] ---------------------------------
2019-03-19 02:23:10,822 [INFO] ---------------------------------
2019-03-19 02:23:10,823 [INFO] Summary:
2019-03-19 02:23:10,823 [INFO] Batch 51000, worst loss 0.061780 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-19 02:23:10,824 [INFO] Regularization: 3651.817383 * 0.0000010000 = 0.0036518173
2019-03-19 02:23:10,824 [INFO] Sum of grad norms: 0.382623
2019-03-19 02:23:10,825 [INFO] ---------------------------------
2019-03-19 02:23:29,826 [INFO] ---------------------------------
2019-03-19 02:23:29,826 [INFO] Summary:
2019-03-19 02:23:29,827 [INFO] Batch 52000, worst loss 0.061660 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-19 02:23:29,828 [INFO] Regularization: 3632.541504 * 0.0000010000 = 0.0036325415
2019-03-19 02:23:29,828 [INFO] Sum of grad norms: 0.074195
2019-03-19 02:23:29,829 [INFO] ---------------------------------
2019-03-19 02:23:48,721 [INFO] ---------------------------------
2019-03-19 02:23:48,722 [INFO] Summary:
2019-03-19 02:23:48,723 [INFO] Batch 53000, worst loss 0.061489 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-19 02:23:48,724 [INFO] Regularization: 3617.480713 * 0.0000010000 = 0.0036174806
2019-03-19 02:23:48,724 [INFO] Sum of grad norms: 0.030733
2019-03-19 02:23:48,725 [INFO] ---------------------------------
2019-03-19 02:24:07,156 [INFO] ---------------------------------
2019-03-19 02:24:07,157 [INFO] Summary:
2019-03-19 02:24:07,158 [INFO] Batch 54000, worst loss 0.061471 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-19 02:24:07,158 [INFO] Regularization: 3603.375732 * 0.0000010000 = 0.0036033757
2019-03-19 02:24:07,159 [INFO] Sum of grad norms: 0.138436
2019-03-19 02:24:07,159 [INFO] ---------------------------------
2019-03-19 02:24:26,090 [INFO] ---------------------------------
2019-03-19 02:24:26,091 [INFO] Summary:
2019-03-19 02:24:26,091 [INFO] Batch 55000, worst loss 0.061394 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-19 02:24:26,092 [INFO] Regularization: 3588.946289 * 0.0000010000 = 0.0035889463
2019-03-19 02:24:26,092 [INFO] Sum of grad norms: 0.048759
2019-03-19 02:24:26,093 [INFO] ---------------------------------
2019-03-19 02:24:44,932 [INFO] ---------------------------------
2019-03-19 02:24:44,933 [INFO] Summary:
2019-03-19 02:24:44,934 [INFO] Batch 56000, worst loss 0.061396 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-19 02:24:44,935 [INFO] Regularization: 3573.164307 * 0.0000010000 = 0.0035731643
2019-03-19 02:24:44,937 [INFO] Sum of grad norms: 0.140915
2019-03-19 02:24:44,938 [INFO] ---------------------------------
2019-03-19 02:25:03,416 [INFO] ---------------------------------
2019-03-19 02:25:03,417 [INFO] Summary:
2019-03-19 02:25:03,418 [INFO] Batch 57000, worst loss 0.061459 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-19 02:25:03,418 [INFO] Regularization: 3558.939453 * 0.0000010000 = 0.0035589396
2019-03-19 02:25:03,419 [INFO] Sum of grad norms: 0.039734
2019-03-19 02:25:03,419 [INFO] ---------------------------------
2019-03-19 02:25:21,919 [INFO] ---------------------------------
2019-03-19 02:25:21,920 [INFO] Summary:
2019-03-19 02:25:21,920 [INFO] Batch 58000, worst loss 0.061416 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-19 02:25:21,921 [INFO] Regularization: 3543.434814 * 0.0000010000 = 0.0035434349
2019-03-19 02:25:21,921 [INFO] Sum of grad norms: 0.111456
2019-03-19 02:25:21,922 [INFO] ---------------------------------
2019-03-19 02:25:40,405 [INFO] ---------------------------------
2019-03-19 02:25:40,406 [INFO] Summary:
2019-03-19 02:25:40,407 [INFO] Batch 59000, worst loss 0.061332 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-19 02:25:40,408 [INFO] Regularization: 3528.839355 * 0.0000010000 = 0.0035288394
2019-03-19 02:25:40,408 [INFO] Sum of grad norms: 0.091951
2019-03-19 02:25:40,409 [INFO] ---------------------------------
2019-03-19 02:25:59,331 [INFO] ---------------------------------
2019-03-19 02:25:59,332 [INFO] Summary:
2019-03-19 02:25:59,332 [INFO] Batch 60000, worst loss 0.061540 (incl. reg.) of 1000 batches, learning rate 0.000157 @cl.-depth 1
2019-03-19 02:25:59,333 [INFO] Regularization: 3514.689209 * 0.0000010000 = 0.0035146892
2019-03-19 02:25:59,333 [INFO] Sum of grad norms: 0.097450
2019-03-19 02:25:59,334 [INFO] ---------------------------------
2019-03-19 02:26:04,231 [INFO] ---------------------------------
2019-03-19 02:26:04,232 [INFO] Evaluation:
2019-03-19 02:26:04,233 [INFO] Batch 60000, worst loss 0.057674 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:26:04,234 [INFO] ---------------------------------
2019-03-19 02:26:23,062 [INFO] ---------------------------------
2019-03-19 02:26:23,063 [INFO] Summary:
2019-03-19 02:26:23,064 [INFO] Batch 61000, worst loss 0.061278 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 02:26:23,064 [INFO] Regularization: 3500.120850 * 0.0000010000 = 0.0035001209
2019-03-19 02:26:23,065 [INFO] Sum of grad norms: 0.098636
2019-03-19 02:26:23,065 [INFO] ---------------------------------
2019-03-19 02:26:41,738 [INFO] ---------------------------------
2019-03-19 02:26:41,739 [INFO] Summary:
2019-03-19 02:26:41,739 [INFO] Batch 62000, worst loss 0.061313 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 02:26:41,740 [INFO] Regularization: 3490.958496 * 0.0000010000 = 0.0034909586
2019-03-19 02:26:41,741 [INFO] Sum of grad norms: 0.060076
2019-03-19 02:26:41,741 [INFO] ---------------------------------
2019-03-19 02:27:00,375 [INFO] ---------------------------------
2019-03-19 02:27:00,376 [INFO] Summary:
2019-03-19 02:27:00,377 [INFO] Batch 63000, worst loss 0.061305 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 02:27:00,377 [INFO] Regularization: 3483.104248 * 0.0000010000 = 0.0034831043
2019-03-19 02:27:00,378 [INFO] Sum of grad norms: 0.094342
2019-03-19 02:27:00,378 [INFO] ---------------------------------
2019-03-19 02:27:19,079 [INFO] ---------------------------------
2019-03-19 02:27:19,080 [INFO] Summary:
2019-03-19 02:27:19,080 [INFO] Batch 64000, worst loss 0.061320 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 02:27:19,081 [INFO] Regularization: 3475.379639 * 0.0000010000 = 0.0034753797
2019-03-19 02:27:19,082 [INFO] Sum of grad norms: 0.112588
2019-03-19 02:27:19,082 [INFO] ---------------------------------
2019-03-19 02:27:38,374 [INFO] ---------------------------------
2019-03-19 02:27:38,375 [INFO] Summary:
2019-03-19 02:27:38,376 [INFO] Batch 65000, worst loss 0.061154 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 02:27:38,376 [INFO] Regularization: 3467.150146 * 0.0000010000 = 0.0034671500
2019-03-19 02:27:38,377 [INFO] Sum of grad norms: 0.018272
2019-03-19 02:27:38,377 [INFO] ---------------------------------
2019-03-19 02:27:57,130 [INFO] ---------------------------------
2019-03-19 02:27:57,131 [INFO] Summary:
2019-03-19 02:27:57,131 [INFO] Batch 66000, worst loss 0.061345 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 02:27:57,132 [INFO] Regularization: 3459.907715 * 0.0000010000 = 0.0034599076
2019-03-19 02:27:57,132 [INFO] Sum of grad norms: 0.051244
2019-03-19 02:27:57,133 [INFO] ---------------------------------
2019-03-19 02:28:16,009 [INFO] ---------------------------------
2019-03-19 02:28:16,010 [INFO] Summary:
2019-03-19 02:28:16,010 [INFO] Batch 67000, worst loss 0.061230 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 02:28:16,011 [INFO] Regularization: 3452.160645 * 0.0000010000 = 0.0034521606
2019-03-19 02:28:16,011 [INFO] Sum of grad norms: 0.087285
2019-03-19 02:28:16,012 [INFO] ---------------------------------
2019-03-19 02:28:35,164 [INFO] ---------------------------------
2019-03-19 02:28:35,165 [INFO] Summary:
2019-03-19 02:28:35,166 [INFO] Batch 68000, worst loss 0.061259 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 02:28:35,166 [INFO] Regularization: 3444.604980 * 0.0000010000 = 0.0034446050
2019-03-19 02:28:35,167 [INFO] Sum of grad norms: 0.092190
2019-03-19 02:28:35,167 [INFO] ---------------------------------
2019-03-19 02:28:54,444 [INFO] ---------------------------------
2019-03-19 02:28:54,445 [INFO] Summary:
2019-03-19 02:28:54,446 [INFO] Batch 69000, worst loss 0.061285 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 02:28:54,446 [INFO] Regularization: 3436.209229 * 0.0000010000 = 0.0034362092
2019-03-19 02:28:54,447 [INFO] Sum of grad norms: 0.047262
2019-03-19 02:28:54,447 [INFO] ---------------------------------
2019-03-19 02:29:13,185 [INFO] ---------------------------------
2019-03-19 02:29:13,186 [INFO] Summary:
2019-03-19 02:29:13,186 [INFO] Batch 70000, worst loss 0.061084 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 02:29:13,187 [INFO] Regularization: 3428.439209 * 0.0000010000 = 0.0034284391
2019-03-19 02:29:13,187 [INFO] Sum of grad norms: 0.120948
2019-03-19 02:29:13,188 [INFO] ---------------------------------
2019-03-19 02:29:18,117 [INFO] ---------------------------------
2019-03-19 02:29:18,121 [INFO] Evaluation:
2019-03-19 02:29:18,122 [INFO] Batch 70000, worst loss 0.057700 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:29:18,123 [INFO] ---------------------------------
2019-03-19 02:29:36,673 [INFO] ---------------------------------
2019-03-19 02:29:36,674 [INFO] Summary:
2019-03-19 02:29:36,675 [INFO] Batch 71000, worst loss 0.060982 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 02:29:36,675 [INFO] Regularization: 3420.408203 * 0.0000010000 = 0.0034204081
2019-03-19 02:29:36,676 [INFO] Sum of grad norms: 0.101650
2019-03-19 02:29:36,677 [INFO] ---------------------------------
2019-03-19 02:29:55,327 [INFO] ---------------------------------
2019-03-19 02:29:55,328 [INFO] Summary:
2019-03-19 02:29:55,329 [INFO] Batch 72000, worst loss 0.061106 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 02:29:55,329 [INFO] Regularization: 3415.645508 * 0.0000010000 = 0.0034156456
2019-03-19 02:29:55,330 [INFO] Sum of grad norms: 0.074687
2019-03-19 02:29:55,330 [INFO] ---------------------------------
2019-03-19 02:30:14,039 [INFO] ---------------------------------
2019-03-19 02:30:14,040 [INFO] Summary:
2019-03-19 02:30:14,041 [INFO] Batch 73000, worst loss 0.061106 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 02:30:14,042 [INFO] Regularization: 3411.418701 * 0.0000010000 = 0.0034114188
2019-03-19 02:30:14,042 [INFO] Sum of grad norms: 0.023384
2019-03-19 02:30:14,043 [INFO] ---------------------------------
2019-03-19 02:30:32,927 [INFO] ---------------------------------
2019-03-19 02:30:32,928 [INFO] Summary:
2019-03-19 02:30:32,930 [INFO] Batch 74000, worst loss 0.061231 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 02:30:32,931 [INFO] Regularization: 3407.250977 * 0.0000010000 = 0.0034072509
2019-03-19 02:30:32,931 [INFO] Sum of grad norms: 0.031977
2019-03-19 02:30:32,932 [INFO] ---------------------------------
2019-03-19 02:30:51,762 [INFO] ---------------------------------
2019-03-19 02:30:51,763 [INFO] Summary:
2019-03-19 02:30:51,764 [INFO] Batch 75000, worst loss 0.061236 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 02:30:51,765 [INFO] Regularization: 3402.869141 * 0.0000010000 = 0.0034028692
2019-03-19 02:30:51,766 [INFO] Sum of grad norms: 0.026874
2019-03-19 02:30:51,766 [INFO] ---------------------------------
2019-03-19 02:31:10,422 [INFO] ---------------------------------
2019-03-19 02:31:10,423 [INFO] Summary:
2019-03-19 02:31:10,423 [INFO] Batch 76000, worst loss 0.060969 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 02:31:10,424 [INFO] Regularization: 3398.601318 * 0.0000010000 = 0.0033986012
2019-03-19 02:31:10,424 [INFO] Sum of grad norms: 0.053546
2019-03-19 02:31:10,425 [INFO] ---------------------------------
2019-03-19 02:31:29,060 [INFO] ---------------------------------
2019-03-19 02:31:29,061 [INFO] Summary:
2019-03-19 02:31:29,062 [INFO] Batch 77000, worst loss 0.061123 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 02:31:29,063 [INFO] Regularization: 3394.348145 * 0.0000010000 = 0.0033943481
2019-03-19 02:31:29,063 [INFO] Sum of grad norms: 0.069632
2019-03-19 02:31:29,064 [INFO] ---------------------------------
2019-03-19 02:31:47,616 [INFO] ---------------------------------
2019-03-19 02:31:47,617 [INFO] Summary:
2019-03-19 02:31:47,618 [INFO] Batch 78000, worst loss 0.061132 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 02:31:47,618 [INFO] Regularization: 3390.208740 * 0.0000010000 = 0.0033902088
2019-03-19 02:31:47,619 [INFO] Sum of grad norms: 0.042012
2019-03-19 02:31:47,619 [INFO] ---------------------------------
2019-03-19 02:32:06,230 [INFO] ---------------------------------
2019-03-19 02:32:06,231 [INFO] Summary:
2019-03-19 02:32:06,231 [INFO] Batch 79000, worst loss 0.061186 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 02:32:06,232 [INFO] Regularization: 3385.906006 * 0.0000010000 = 0.0033859061
2019-03-19 02:32:06,232 [INFO] Sum of grad norms: 0.025843
2019-03-19 02:32:06,233 [INFO] ---------------------------------
2019-03-19 02:32:25,254 [INFO] ---------------------------------
2019-03-19 02:32:25,255 [INFO] Summary:
2019-03-19 02:32:25,256 [INFO] Batch 80000, worst loss 0.061200 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 02:32:25,256 [INFO] Regularization: 3381.568604 * 0.0000010000 = 0.0033815687
2019-03-19 02:32:25,257 [INFO] Sum of grad norms: 0.068414
2019-03-19 02:32:25,257 [INFO] ---------------------------------
2019-03-19 02:32:30,206 [INFO] ---------------------------------
2019-03-19 02:32:30,207 [INFO] Evaluation:
2019-03-19 02:32:30,208 [INFO] Batch 80000, worst loss 0.057653 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:32:30,211 [INFO] ---------------------------------
2019-03-19 02:32:49,107 [INFO] ---------------------------------
2019-03-19 02:32:49,108 [INFO] Summary:
2019-03-19 02:32:49,108 [INFO] Batch 81000, worst loss 0.061074 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 02:32:49,109 [INFO] Regularization: 3377.330566 * 0.0000010000 = 0.0033773305
2019-03-19 02:32:49,109 [INFO] Sum of grad norms: 0.063815
2019-03-19 02:32:49,110 [INFO] ---------------------------------
2019-03-19 02:33:07,720 [INFO] ---------------------------------
2019-03-19 02:33:07,721 [INFO] Summary:
2019-03-19 02:33:07,722 [INFO] Batch 82000, worst loss 0.061041 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 02:33:07,722 [INFO] Regularization: 3374.359619 * 0.0000010000 = 0.0033743596
2019-03-19 02:33:07,723 [INFO] Sum of grad norms: 0.069115
2019-03-19 02:33:07,723 [INFO] ---------------------------------
2019-03-19 02:33:26,401 [INFO] ---------------------------------
2019-03-19 02:33:26,402 [INFO] Summary:
2019-03-19 02:33:26,403 [INFO] Batch 83000, worst loss 0.061024 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 02:33:26,403 [INFO] Regularization: 3372.337646 * 0.0000010000 = 0.0033723377
2019-03-19 02:33:26,404 [INFO] Sum of grad norms: 0.060518
2019-03-19 02:33:26,404 [INFO] ---------------------------------
2019-03-19 02:33:45,075 [INFO] ---------------------------------
2019-03-19 02:33:45,076 [INFO] Summary:
2019-03-19 02:33:45,076 [INFO] Batch 84000, worst loss 0.061027 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 02:33:45,077 [INFO] Regularization: 3370.176758 * 0.0000010000 = 0.0033701768
2019-03-19 02:33:45,077 [INFO] Sum of grad norms: 0.046547
2019-03-19 02:33:45,078 [INFO] ---------------------------------
2019-03-19 02:34:03,745 [INFO] ---------------------------------
2019-03-19 02:34:03,746 [INFO] Summary:
2019-03-19 02:34:03,747 [INFO] Batch 85000, worst loss 0.061047 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 02:34:03,747 [INFO] Regularization: 3367.850342 * 0.0000010000 = 0.0033678503
2019-03-19 02:34:03,748 [INFO] Sum of grad norms: 0.034393
2019-03-19 02:34:03,748 [INFO] ---------------------------------
2019-03-19 02:34:22,228 [INFO] ---------------------------------
2019-03-19 02:34:22,228 [INFO] Summary:
2019-03-19 02:34:22,229 [INFO] Batch 86000, worst loss 0.061090 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 02:34:22,229 [INFO] Regularization: 3365.769287 * 0.0000010000 = 0.0033657693
2019-03-19 02:34:22,230 [INFO] Sum of grad norms: 0.031368
2019-03-19 02:34:22,231 [INFO] ---------------------------------
2019-03-19 02:34:40,991 [INFO] ---------------------------------
2019-03-19 02:34:40,991 [INFO] Summary:
2019-03-19 02:34:40,992 [INFO] Batch 87000, worst loss 0.061096 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 02:34:40,992 [INFO] Regularization: 3363.496094 * 0.0000010000 = 0.0033634962
2019-03-19 02:34:40,993 [INFO] Sum of grad norms: 0.043023
2019-03-19 02:34:40,994 [INFO] ---------------------------------
2019-03-19 02:35:00,061 [INFO] ---------------------------------
2019-03-19 02:35:00,062 [INFO] Summary:
2019-03-19 02:35:00,062 [INFO] Batch 88000, worst loss 0.060880 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 02:35:00,063 [INFO] Regularization: 3361.244141 * 0.0000010000 = 0.0033612442
2019-03-19 02:35:00,064 [INFO] Sum of grad norms: 0.060757
2019-03-19 02:35:00,064 [INFO] ---------------------------------
2019-03-19 02:35:18,925 [INFO] ---------------------------------
2019-03-19 02:35:18,925 [INFO] Summary:
2019-03-19 02:35:18,926 [INFO] Batch 89000, worst loss 0.060996 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 02:35:18,927 [INFO] Regularization: 3359.077637 * 0.0000010000 = 0.0033590777
2019-03-19 02:35:18,927 [INFO] Sum of grad norms: 0.081598
2019-03-19 02:35:18,928 [INFO] ---------------------------------
2019-03-19 02:35:37,593 [INFO] ---------------------------------
2019-03-19 02:35:37,594 [INFO] Summary:
2019-03-19 02:35:37,595 [INFO] Batch 90000, worst loss 0.060943 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 02:35:37,596 [INFO] Regularization: 3357.010742 * 0.0000010000 = 0.0033570107
2019-03-19 02:35:37,597 [INFO] Sum of grad norms: 0.075208
2019-03-19 02:35:37,598 [INFO] ---------------------------------
2019-03-19 02:35:42,633 [INFO] ---------------------------------
2019-03-19 02:35:42,634 [INFO] Evaluation:
2019-03-19 02:35:42,635 [INFO] Batch 90000, worst loss 0.057602 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:35:42,637 [INFO] ---------------------------------
2019-03-19 02:36:00,940 [INFO] ---------------------------------
2019-03-19 02:36:00,941 [INFO] Summary:
2019-03-19 02:36:00,942 [INFO] Batch 91000, worst loss 0.061026 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 02:36:00,942 [INFO] Regularization: 3354.907959 * 0.0000010000 = 0.0033549080
2019-03-19 02:36:00,943 [INFO] Sum of grad norms: 0.025183
2019-03-19 02:36:00,943 [INFO] ---------------------------------
2019-03-19 02:36:20,064 [INFO] ---------------------------------
2019-03-19 02:36:20,065 [INFO] Summary:
2019-03-19 02:36:20,066 [INFO] Batch 92000, worst loss 0.061005 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 02:36:20,066 [INFO] Regularization: 3353.617920 * 0.0000010000 = 0.0033536179
2019-03-19 02:36:20,067 [INFO] Sum of grad norms: 0.025735
2019-03-19 02:36:20,068 [INFO] ---------------------------------
2019-03-19 02:36:38,697 [INFO] ---------------------------------
2019-03-19 02:36:38,698 [INFO] Summary:
2019-03-19 02:36:38,699 [INFO] Batch 93000, worst loss 0.061041 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 02:36:38,699 [INFO] Regularization: 3352.500732 * 0.0000010000 = 0.0033525007
2019-03-19 02:36:38,700 [INFO] Sum of grad norms: 0.032552
2019-03-19 02:36:38,701 [INFO] ---------------------------------
2019-03-19 02:36:57,145 [INFO] ---------------------------------
2019-03-19 02:36:57,146 [INFO] Summary:
2019-03-19 02:36:57,146 [INFO] Batch 94000, worst loss 0.060903 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 02:36:57,147 [INFO] Regularization: 3351.462402 * 0.0000010000 = 0.0033514623
2019-03-19 02:36:57,147 [INFO] Sum of grad norms: 0.053955
2019-03-19 02:36:57,148 [INFO] ---------------------------------
2019-03-19 02:37:15,791 [INFO] ---------------------------------
2019-03-19 02:37:15,792 [INFO] Summary:
2019-03-19 02:37:15,793 [INFO] Batch 95000, worst loss 0.061079 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 02:37:15,793 [INFO] Regularization: 3350.199463 * 0.0000010000 = 0.0033501994
2019-03-19 02:37:15,794 [INFO] Sum of grad norms: 0.049195
2019-03-19 02:37:15,794 [INFO] ---------------------------------
2019-03-19 02:37:34,259 [INFO] ---------------------------------
2019-03-19 02:37:34,260 [INFO] Summary:
2019-03-19 02:37:34,262 [INFO] Batch 96000, worst loss 0.061078 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 02:37:34,263 [INFO] Regularization: 3349.014893 * 0.0000010000 = 0.0033490148
2019-03-19 02:37:34,264 [INFO] Sum of grad norms: 0.054437
2019-03-19 02:37:34,264 [INFO] ---------------------------------
2019-03-19 02:37:53,094 [INFO] ---------------------------------
2019-03-19 02:37:53,095 [INFO] Summary:
2019-03-19 02:37:53,096 [INFO] Batch 97000, worst loss 0.060876 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 02:37:53,096 [INFO] Regularization: 3348.081055 * 0.0000010000 = 0.0033480811
2019-03-19 02:37:53,097 [INFO] Sum of grad norms: 0.026678
2019-03-19 02:37:53,098 [INFO] ---------------------------------
2019-03-19 02:38:11,637 [INFO] ---------------------------------
2019-03-19 02:38:11,638 [INFO] Summary:
2019-03-19 02:38:11,638 [INFO] Batch 98000, worst loss 0.061088 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 02:38:11,639 [INFO] Regularization: 3347.084717 * 0.0000010000 = 0.0033470846
2019-03-19 02:38:11,639 [INFO] Sum of grad norms: 0.031127
2019-03-19 02:38:11,640 [INFO] ---------------------------------
2019-03-19 02:38:30,193 [INFO] ---------------------------------
2019-03-19 02:38:30,194 [INFO] Summary:
2019-03-19 02:38:30,195 [INFO] Batch 99000, worst loss 0.061088 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 02:38:30,195 [INFO] Regularization: 3346.019043 * 0.0000010000 = 0.0033460190
2019-03-19 02:38:30,196 [INFO] Sum of grad norms: 0.073031
2019-03-19 02:38:30,196 [INFO] ---------------------------------
2019-03-19 02:38:48,652 [INFO] ---------------------------------
2019-03-19 02:38:48,653 [INFO] Summary:
2019-03-19 02:38:48,654 [INFO] Batch 100000, worst loss 0.061166 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 02:38:48,654 [INFO] Regularization: 3345.102539 * 0.0000010000 = 0.0033451025
2019-03-19 02:38:48,655 [INFO] Sum of grad norms: 0.018547
2019-03-19 02:38:48,655 [INFO] ---------------------------------
2019-03-19 02:38:53,607 [INFO] ---------------------------------
2019-03-19 02:38:53,608 [INFO] Evaluation:
2019-03-19 02:38:53,609 [INFO] Batch 100000, worst loss 0.057805 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:38:53,610 [INFO] ---------------------------------
2019-03-19 02:39:12,242 [INFO] ---------------------------------
2019-03-19 02:39:12,242 [INFO] Summary:
2019-03-19 02:39:12,243 [INFO] Batch 101000, worst loss 0.061035 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 02:39:12,244 [INFO] Regularization: 3343.983643 * 0.0000010000 = 0.0033439836
2019-03-19 02:39:12,244 [INFO] Sum of grad norms: 0.060094
2019-03-19 02:39:12,245 [INFO] ---------------------------------
2019-03-19 02:39:30,948 [INFO] ---------------------------------
2019-03-19 02:39:30,948 [INFO] Summary:
2019-03-19 02:39:30,949 [INFO] Batch 102000, worst loss 0.061063 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 02:39:30,950 [INFO] Regularization: 3343.409180 * 0.0000010000 = 0.0033434092
2019-03-19 02:39:30,950 [INFO] Sum of grad norms: 0.032029
2019-03-19 02:39:30,951 [INFO] ---------------------------------
2019-03-19 02:39:49,595 [INFO] ---------------------------------
2019-03-19 02:39:49,596 [INFO] Summary:
2019-03-19 02:39:49,597 [INFO] Batch 103000, worst loss 0.061079 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 02:39:49,597 [INFO] Regularization: 3342.857910 * 0.0000010000 = 0.0033428578
2019-03-19 02:39:49,598 [INFO] Sum of grad norms: 0.038738
2019-03-19 02:39:49,599 [INFO] ---------------------------------
2019-03-19 02:40:08,323 [INFO] ---------------------------------
2019-03-19 02:40:08,323 [INFO] Summary:
2019-03-19 02:40:08,324 [INFO] Batch 104000, worst loss 0.061074 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 02:40:08,325 [INFO] Regularization: 3342.302002 * 0.0000010000 = 0.0033423021
2019-03-19 02:40:08,325 [INFO] Sum of grad norms: 0.044486
2019-03-19 02:40:08,326 [INFO] ---------------------------------
2019-03-19 02:40:26,880 [INFO] ---------------------------------
2019-03-19 02:40:26,881 [INFO] Summary:
2019-03-19 02:40:26,881 [INFO] Batch 105000, worst loss 0.060982 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 02:40:26,882 [INFO] Regularization: 3341.803955 * 0.0000010000 = 0.0033418040
2019-03-19 02:40:26,882 [INFO] Sum of grad norms: 0.033870
2019-03-19 02:40:26,883 [INFO] ---------------------------------
2019-03-19 02:40:45,632 [INFO] ---------------------------------
2019-03-19 02:40:45,633 [INFO] Summary:
2019-03-19 02:40:45,634 [INFO] Batch 106000, worst loss 0.060913 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 02:40:45,634 [INFO] Regularization: 3341.307617 * 0.0000010000 = 0.0033413076
2019-03-19 02:40:45,635 [INFO] Sum of grad norms: 0.049437
2019-03-19 02:40:45,635 [INFO] ---------------------------------
2019-03-19 02:41:04,417 [INFO] ---------------------------------
2019-03-19 02:41:04,418 [INFO] Summary:
2019-03-19 02:41:04,418 [INFO] Batch 107000, worst loss 0.060912 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 02:41:04,419 [INFO] Regularization: 3340.761963 * 0.0000010000 = 0.0033407619
2019-03-19 02:41:04,419 [INFO] Sum of grad norms: 0.035289
2019-03-19 02:41:04,420 [INFO] ---------------------------------
2019-03-19 02:41:23,120 [INFO] ---------------------------------
2019-03-19 02:41:23,121 [INFO] Summary:
2019-03-19 02:41:23,121 [INFO] Batch 108000, worst loss 0.060880 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 02:41:23,122 [INFO] Regularization: 3340.198242 * 0.0000010000 = 0.0033401982
2019-03-19 02:41:23,122 [INFO] Sum of grad norms: 0.032151
2019-03-19 02:41:23,123 [INFO] ---------------------------------
2019-03-19 02:41:41,884 [INFO] ---------------------------------
2019-03-19 02:41:41,885 [INFO] Summary:
2019-03-19 02:41:41,885 [INFO] Batch 109000, worst loss 0.061053 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 02:41:41,886 [INFO] Regularization: 3339.615967 * 0.0000010000 = 0.0033396159
2019-03-19 02:41:41,886 [INFO] Sum of grad norms: 0.037625
2019-03-19 02:41:41,887 [INFO] ---------------------------------
2019-03-19 02:42:00,583 [INFO] ---------------------------------
2019-03-19 02:42:00,583 [INFO] Summary:
2019-03-19 02:42:00,584 [INFO] Batch 110000, worst loss 0.061055 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 02:42:00,585 [INFO] Regularization: 3339.118164 * 0.0000010000 = 0.0033391181
2019-03-19 02:42:00,585 [INFO] Sum of grad norms: 0.030189
2019-03-19 02:42:00,586 [INFO] ---------------------------------
2019-03-19 02:42:05,435 [INFO] ---------------------------------
2019-03-19 02:42:05,436 [INFO] Evaluation:
2019-03-19 02:42:05,437 [INFO] Batch 110000, worst loss 0.057583 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:42:05,438 [INFO] ---------------------------------
2019-03-19 02:42:24,290 [INFO] ---------------------------------
2019-03-19 02:42:24,291 [INFO] Summary:
2019-03-19 02:42:24,291 [INFO] Batch 111000, worst loss 0.060921 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 02:42:24,292 [INFO] Regularization: 3338.616211 * 0.0000010000 = 0.0033386161
2019-03-19 02:42:24,292 [INFO] Sum of grad norms: 0.023580
2019-03-19 02:42:24,293 [INFO] ---------------------------------
2019-03-19 02:42:43,295 [INFO] ---------------------------------
2019-03-19 02:42:43,297 [INFO] Summary:
2019-03-19 02:42:43,297 [INFO] Batch 112000, worst loss 0.060954 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 02:42:43,298 [INFO] Regularization: 3338.255859 * 0.0000010000 = 0.0033382559
2019-03-19 02:42:43,298 [INFO] Sum of grad norms: 0.045213
2019-03-19 02:42:43,299 [INFO] ---------------------------------
2019-03-19 02:43:02,031 [INFO] ---------------------------------
2019-03-19 02:43:02,032 [INFO] Summary:
2019-03-19 02:43:02,033 [INFO] Batch 113000, worst loss 0.061264 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 02:43:02,033 [INFO] Regularization: 3337.946045 * 0.0000010000 = 0.0033379460
2019-03-19 02:43:02,034 [INFO] Sum of grad norms: 0.050874
2019-03-19 02:43:02,034 [INFO] ---------------------------------
2019-03-19 02:43:20,686 [INFO] ---------------------------------
2019-03-19 02:43:20,687 [INFO] Summary:
2019-03-19 02:43:20,688 [INFO] Batch 114000, worst loss 0.061265 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 02:43:20,689 [INFO] Regularization: 3337.652344 * 0.0000010000 = 0.0033376524
2019-03-19 02:43:20,689 [INFO] Sum of grad norms: 0.033211
2019-03-19 02:43:20,690 [INFO] ---------------------------------
2019-03-19 02:43:39,588 [INFO] ---------------------------------
2019-03-19 02:43:39,589 [INFO] Summary:
2019-03-19 02:43:39,589 [INFO] Batch 115000, worst loss 0.060983 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 02:43:39,590 [INFO] Regularization: 3337.413330 * 0.0000010000 = 0.0033374133
2019-03-19 02:43:39,590 [INFO] Sum of grad norms: 0.044594
2019-03-19 02:43:39,591 [INFO] ---------------------------------
2019-03-19 02:43:58,485 [INFO] ---------------------------------
2019-03-19 02:43:58,485 [INFO] Summary:
2019-03-19 02:43:58,486 [INFO] Batch 116000, worst loss 0.060988 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 02:43:58,487 [INFO] Regularization: 3337.125000 * 0.0000010000 = 0.0033371251
2019-03-19 02:43:58,488 [INFO] Sum of grad norms: 0.030884
2019-03-19 02:43:58,488 [INFO] ---------------------------------
2019-03-19 02:44:17,092 [INFO] ---------------------------------
2019-03-19 02:44:17,093 [INFO] Summary:
2019-03-19 02:44:17,093 [INFO] Batch 117000, worst loss 0.061001 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 02:44:17,094 [INFO] Regularization: 3336.853271 * 0.0000010000 = 0.0033368533
2019-03-19 02:44:17,094 [INFO] Sum of grad norms: 0.022790
2019-03-19 02:44:17,095 [INFO] ---------------------------------
2019-03-19 02:44:35,832 [INFO] ---------------------------------
2019-03-19 02:44:35,834 [INFO] Summary:
2019-03-19 02:44:35,834 [INFO] Batch 118000, worst loss 0.061059 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 02:44:35,835 [INFO] Regularization: 3336.555664 * 0.0000010000 = 0.0033365556
2019-03-19 02:44:35,835 [INFO] Sum of grad norms: 0.020291
2019-03-19 02:44:35,836 [INFO] ---------------------------------
2019-03-19 02:44:54,443 [INFO] ---------------------------------
2019-03-19 02:44:54,445 [INFO] Summary:
2019-03-19 02:44:54,446 [INFO] Batch 119000, worst loss 0.061019 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 02:44:54,447 [INFO] Regularization: 3336.326172 * 0.0000010000 = 0.0033363262
2019-03-19 02:44:54,448 [INFO] Sum of grad norms: 0.025404
2019-03-19 02:44:54,449 [INFO] ---------------------------------
2019-03-19 02:45:12,859 [INFO] ---------------------------------
2019-03-19 02:45:12,860 [INFO] Summary:
2019-03-19 02:45:12,860 [INFO] Batch 120000, worst loss 0.061017 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 02:45:12,861 [INFO] Regularization: 3336.036621 * 0.0000010000 = 0.0033360366
2019-03-19 02:45:12,861 [INFO] Sum of grad norms: 0.049305
2019-03-19 02:45:12,862 [INFO] ---------------------------------
2019-03-19 02:45:17,730 [INFO] ---------------------------------
2019-03-19 02:45:17,731 [INFO] Evaluation:
2019-03-19 02:45:17,732 [INFO] Batch 120000, worst loss 0.057541 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:45:17,733 [INFO] ---------------------------------
2019-03-19 02:45:36,768 [INFO] ---------------------------------
2019-03-19 02:45:36,769 [INFO] Summary:
2019-03-19 02:45:36,770 [INFO] Batch 121000, worst loss 0.060877 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:45:36,771 [INFO] Regularization: 3335.689209 * 0.0000010000 = 0.0033356892
2019-03-19 02:45:36,771 [INFO] Sum of grad norms: 0.027457
2019-03-19 02:45:36,772 [INFO] ---------------------------------
2019-03-19 02:45:55,943 [INFO] ---------------------------------
2019-03-19 02:45:55,944 [INFO] Summary:
2019-03-19 02:45:55,945 [INFO] Batch 122000, worst loss 0.060845 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:45:55,945 [INFO] Regularization: 3335.490967 * 0.0000010000 = 0.0033354911
2019-03-19 02:45:55,946 [INFO] Sum of grad norms: 0.028333
2019-03-19 02:45:55,946 [INFO] ---------------------------------
2019-03-19 02:46:14,790 [INFO] ---------------------------------
2019-03-19 02:46:14,791 [INFO] Summary:
2019-03-19 02:46:14,791 [INFO] Batch 123000, worst loss 0.061099 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:46:14,792 [INFO] Regularization: 3335.367432 * 0.0000010000 = 0.0033353674
2019-03-19 02:46:14,792 [INFO] Sum of grad norms: 0.021557
2019-03-19 02:46:14,793 [INFO] ---------------------------------
2019-03-19 02:46:33,268 [INFO] ---------------------------------
2019-03-19 02:46:33,269 [INFO] Summary:
2019-03-19 02:46:33,270 [INFO] Batch 124000, worst loss 0.061097 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:46:33,270 [INFO] Regularization: 3335.193848 * 0.0000010000 = 0.0033351937
2019-03-19 02:46:33,271 [INFO] Sum of grad norms: 0.023123
2019-03-19 02:46:33,272 [INFO] ---------------------------------
2019-03-19 02:46:52,105 [INFO] ---------------------------------
2019-03-19 02:46:52,106 [INFO] Summary:
2019-03-19 02:46:52,106 [INFO] Batch 125000, worst loss 0.060936 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:46:52,107 [INFO] Regularization: 3335.060303 * 0.0000010000 = 0.0033350603
2019-03-19 02:46:52,107 [INFO] Sum of grad norms: 0.035893
2019-03-19 02:46:52,108 [INFO] ---------------------------------
2019-03-19 02:47:10,631 [INFO] ---------------------------------
2019-03-19 02:47:10,632 [INFO] Summary:
2019-03-19 02:47:10,633 [INFO] Batch 126000, worst loss 0.060949 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:47:10,633 [INFO] Regularization: 3334.953125 * 0.0000010000 = 0.0033349532
2019-03-19 02:47:10,634 [INFO] Sum of grad norms: 0.041170
2019-03-19 02:47:10,634 [INFO] ---------------------------------
2019-03-19 02:47:29,286 [INFO] ---------------------------------
2019-03-19 02:47:29,287 [INFO] Summary:
2019-03-19 02:47:29,287 [INFO] Batch 127000, worst loss 0.060985 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:47:29,288 [INFO] Regularization: 3334.821045 * 0.0000010000 = 0.0033348210
2019-03-19 02:47:29,288 [INFO] Sum of grad norms: 0.039491
2019-03-19 02:47:29,289 [INFO] ---------------------------------
2019-03-19 02:47:48,318 [INFO] ---------------------------------
2019-03-19 02:47:48,319 [INFO] Summary:
2019-03-19 02:47:48,320 [INFO] Batch 128000, worst loss 0.060862 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:47:48,320 [INFO] Regularization: 3334.688477 * 0.0000010000 = 0.0033346885
2019-03-19 02:47:48,321 [INFO] Sum of grad norms: 0.027951
2019-03-19 02:47:48,321 [INFO] ---------------------------------
2019-03-19 02:48:06,877 [INFO] ---------------------------------
2019-03-19 02:48:06,878 [INFO] Summary:
2019-03-19 02:48:06,879 [INFO] Batch 129000, worst loss 0.060957 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:48:06,879 [INFO] Regularization: 3334.543945 * 0.0000010000 = 0.0033345439
2019-03-19 02:48:06,880 [INFO] Sum of grad norms: 0.019294
2019-03-19 02:48:06,880 [INFO] ---------------------------------
2019-03-19 02:48:25,350 [INFO] ---------------------------------
2019-03-19 02:48:25,351 [INFO] Summary:
2019-03-19 02:48:25,352 [INFO] Batch 130000, worst loss 0.061053 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:48:25,353 [INFO] Regularization: 3334.404053 * 0.0000010000 = 0.0033344040
2019-03-19 02:48:25,353 [INFO] Sum of grad norms: 0.063312
2019-03-19 02:48:25,354 [INFO] ---------------------------------
2019-03-19 02:48:30,284 [INFO] ---------------------------------
2019-03-19 02:48:30,285 [INFO] Evaluation:
2019-03-19 02:48:30,286 [INFO] Batch 130000, worst loss 0.057718 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:48:30,288 [INFO] ---------------------------------
2019-03-19 02:48:49,327 [INFO] ---------------------------------
2019-03-19 02:48:49,328 [INFO] Summary:
2019-03-19 02:48:49,329 [INFO] Batch 131000, worst loss 0.061013 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:48:49,329 [INFO] Regularization: 3334.251465 * 0.0000010000 = 0.0033342515
2019-03-19 02:48:49,330 [INFO] Sum of grad norms: 0.032524
2019-03-19 02:48:49,330 [INFO] ---------------------------------
2019-03-19 02:49:08,300 [INFO] ---------------------------------
2019-03-19 02:49:08,301 [INFO] Summary:
2019-03-19 02:49:08,301 [INFO] Batch 132000, worst loss 0.061013 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:49:08,302 [INFO] Regularization: 3334.151611 * 0.0000010000 = 0.0033341516
2019-03-19 02:49:08,302 [INFO] Sum of grad norms: 0.032752
2019-03-19 02:49:08,303 [INFO] ---------------------------------
2019-03-19 02:49:27,123 [INFO] ---------------------------------
2019-03-19 02:49:27,124 [INFO] Summary:
2019-03-19 02:49:27,125 [INFO] Batch 133000, worst loss 0.061061 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:49:27,125 [INFO] Regularization: 3334.086670 * 0.0000010000 = 0.0033340866
2019-03-19 02:49:27,126 [INFO] Sum of grad norms: 0.025802
2019-03-19 02:49:27,126 [INFO] ---------------------------------
2019-03-19 02:49:46,400 [INFO] ---------------------------------
2019-03-19 02:49:46,401 [INFO] Summary:
2019-03-19 02:49:46,401 [INFO] Batch 134000, worst loss 0.060875 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:49:46,402 [INFO] Regularization: 3334.014893 * 0.0000010000 = 0.0033340149
2019-03-19 02:49:46,402 [INFO] Sum of grad norms: 0.029309
2019-03-19 02:49:46,403 [INFO] ---------------------------------
2019-03-19 02:50:05,374 [INFO] ---------------------------------
2019-03-19 02:50:05,375 [INFO] Summary:
2019-03-19 02:50:05,375 [INFO] Batch 135000, worst loss 0.060859 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:50:05,376 [INFO] Regularization: 3333.937988 * 0.0000010000 = 0.0033339381
2019-03-19 02:50:05,376 [INFO] Sum of grad norms: 0.042040
2019-03-19 02:50:05,377 [INFO] ---------------------------------
2019-03-19 02:50:24,104 [INFO] ---------------------------------
2019-03-19 02:50:24,105 [INFO] Summary:
2019-03-19 02:50:24,105 [INFO] Batch 136000, worst loss 0.060898 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:50:24,106 [INFO] Regularization: 3333.862305 * 0.0000010000 = 0.0033338624
2019-03-19 02:50:24,107 [INFO] Sum of grad norms: 0.031995
2019-03-19 02:50:24,107 [INFO] ---------------------------------
2019-03-19 02:50:42,634 [INFO] ---------------------------------
2019-03-19 02:50:42,635 [INFO] Summary:
2019-03-19 02:50:42,636 [INFO] Batch 137000, worst loss 0.060935 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:50:42,637 [INFO] Regularization: 3333.797607 * 0.0000010000 = 0.0033337977
2019-03-19 02:50:42,637 [INFO] Sum of grad norms: 0.028251
2019-03-19 02:50:42,638 [INFO] ---------------------------------
2019-03-19 02:51:01,356 [INFO] ---------------------------------
2019-03-19 02:51:01,357 [INFO] Summary:
2019-03-19 02:51:01,358 [INFO] Batch 138000, worst loss 0.060934 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:51:01,358 [INFO] Regularization: 3333.740479 * 0.0000010000 = 0.0033337404
2019-03-19 02:51:01,359 [INFO] Sum of grad norms: 0.076269
2019-03-19 02:51:01,360 [INFO] ---------------------------------
2019-03-19 02:51:20,369 [INFO] ---------------------------------
2019-03-19 02:51:20,369 [INFO] Summary:
2019-03-19 02:51:20,370 [INFO] Batch 139000, worst loss 0.060906 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:51:20,371 [INFO] Regularization: 3333.670654 * 0.0000010000 = 0.0033336706
2019-03-19 02:51:20,371 [INFO] Sum of grad norms: 0.052955
2019-03-19 02:51:20,372 [INFO] ---------------------------------
2019-03-19 02:51:39,176 [INFO] ---------------------------------
2019-03-19 02:51:39,178 [INFO] Summary:
2019-03-19 02:51:39,178 [INFO] Batch 140000, worst loss 0.060849 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 02:51:39,179 [INFO] Regularization: 3333.600830 * 0.0000010000 = 0.0033336009
2019-03-19 02:51:39,179 [INFO] Sum of grad norms: 0.051648
2019-03-19 02:51:39,180 [INFO] ---------------------------------
2019-03-19 02:51:44,092 [INFO] ---------------------------------
2019-03-19 02:51:44,093 [INFO] Evaluation:
2019-03-19 02:51:44,095 [INFO] Batch 140000, worst loss 0.057592 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:51:44,096 [INFO] ---------------------------------
2019-03-19 02:52:02,895 [INFO] ---------------------------------
2019-03-19 02:52:02,896 [INFO] Summary:
2019-03-19 02:52:02,897 [INFO] Batch 141000, worst loss 0.060925 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:52:02,898 [INFO] Regularization: 3333.538330 * 0.0000010000 = 0.0033335383
2019-03-19 02:52:02,898 [INFO] Sum of grad norms: 0.062403
2019-03-19 02:52:02,899 [INFO] ---------------------------------
2019-03-19 02:52:21,784 [INFO] ---------------------------------
2019-03-19 02:52:21,785 [INFO] Summary:
2019-03-19 02:52:21,785 [INFO] Batch 142000, worst loss 0.060872 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:52:21,786 [INFO] Regularization: 3333.497559 * 0.0000010000 = 0.0033334976
2019-03-19 02:52:21,786 [INFO] Sum of grad norms: 0.018109
2019-03-19 02:52:21,787 [INFO] ---------------------------------
2019-03-19 02:52:40,443 [INFO] ---------------------------------
2019-03-19 02:52:40,444 [INFO] Summary:
2019-03-19 02:52:40,445 [INFO] Batch 143000, worst loss 0.060858 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:52:40,445 [INFO] Regularization: 3333.458496 * 0.0000010000 = 0.0033334584
2019-03-19 02:52:40,446 [INFO] Sum of grad norms: 0.091198
2019-03-19 02:52:40,447 [INFO] ---------------------------------
2019-03-19 02:52:59,195 [INFO] ---------------------------------
2019-03-19 02:52:59,196 [INFO] Summary:
2019-03-19 02:52:59,196 [INFO] Batch 144000, worst loss 0.060894 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:52:59,197 [INFO] Regularization: 3333.415283 * 0.0000010000 = 0.0033334154
2019-03-19 02:52:59,197 [INFO] Sum of grad norms: 0.080014
2019-03-19 02:52:59,198 [INFO] ---------------------------------
2019-03-19 02:53:18,005 [INFO] ---------------------------------
2019-03-19 02:53:18,006 [INFO] Summary:
2019-03-19 02:53:18,007 [INFO] Batch 145000, worst loss 0.060894 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:53:18,007 [INFO] Regularization: 3333.376953 * 0.0000010000 = 0.0033333770
2019-03-19 02:53:18,008 [INFO] Sum of grad norms: 0.046186
2019-03-19 02:53:18,008 [INFO] ---------------------------------
2019-03-19 02:53:37,094 [INFO] ---------------------------------
2019-03-19 02:53:37,095 [INFO] Summary:
2019-03-19 02:53:37,095 [INFO] Batch 146000, worst loss 0.060852 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:53:37,096 [INFO] Regularization: 3333.339355 * 0.0000010000 = 0.0033333392
2019-03-19 02:53:37,096 [INFO] Sum of grad norms: 0.031592
2019-03-19 02:53:37,097 [INFO] ---------------------------------
2019-03-19 02:53:55,922 [INFO] ---------------------------------
2019-03-19 02:53:55,923 [INFO] Summary:
2019-03-19 02:53:55,923 [INFO] Batch 147000, worst loss 0.061012 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:53:55,924 [INFO] Regularization: 3333.299805 * 0.0000010000 = 0.0033332999
2019-03-19 02:53:55,924 [INFO] Sum of grad norms: 0.023168
2019-03-19 02:53:55,925 [INFO] ---------------------------------
2019-03-19 02:54:14,797 [INFO] ---------------------------------
2019-03-19 02:54:14,798 [INFO] Summary:
2019-03-19 02:54:14,799 [INFO] Batch 148000, worst loss 0.061011 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:54:14,799 [INFO] Regularization: 3333.261963 * 0.0000010000 = 0.0033332619
2019-03-19 02:54:14,800 [INFO] Sum of grad norms: 0.041615
2019-03-19 02:54:14,800 [INFO] ---------------------------------
2019-03-19 02:54:33,507 [INFO] ---------------------------------
2019-03-19 02:54:33,508 [INFO] Summary:
2019-03-19 02:54:33,509 [INFO] Batch 149000, worst loss 0.060938 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:54:33,509 [INFO] Regularization: 3333.224365 * 0.0000010000 = 0.0033332244
2019-03-19 02:54:33,510 [INFO] Sum of grad norms: 0.028890
2019-03-19 02:54:33,511 [INFO] ---------------------------------
2019-03-19 02:54:52,205 [INFO] ---------------------------------
2019-03-19 02:54:52,206 [INFO] Summary:
2019-03-19 02:54:52,207 [INFO] Batch 150000, worst loss 0.060903 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:54:52,207 [INFO] Regularization: 3333.188232 * 0.0000010000 = 0.0033331881
2019-03-19 02:54:52,208 [INFO] Sum of grad norms: 0.036755
2019-03-19 02:54:52,209 [INFO] ---------------------------------
2019-03-19 02:54:57,129 [INFO] ---------------------------------
2019-03-19 02:54:57,130 [INFO] Evaluation:
2019-03-19 02:54:57,130 [INFO] Batch 150000, worst loss 0.057563 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:54:57,131 [INFO] ---------------------------------
2019-03-19 02:55:16,202 [INFO] ---------------------------------
2019-03-19 02:55:16,203 [INFO] Summary:
2019-03-19 02:55:16,204 [INFO] Batch 151000, worst loss 0.061000 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:55:16,204 [INFO] Regularization: 3333.153564 * 0.0000010000 = 0.0033331537
2019-03-19 02:55:16,205 [INFO] Sum of grad norms: 0.026792
2019-03-19 02:55:16,205 [INFO] ---------------------------------
2019-03-19 02:55:35,113 [INFO] ---------------------------------
2019-03-19 02:55:35,114 [INFO] Summary:
2019-03-19 02:55:35,114 [INFO] Batch 152000, worst loss 0.060999 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:55:35,115 [INFO] Regularization: 3333.129395 * 0.0000010000 = 0.0033331295
2019-03-19 02:55:35,116 [INFO] Sum of grad norms: 0.025041
2019-03-19 02:55:35,116 [INFO] ---------------------------------
2019-03-19 02:55:53,540 [INFO] ---------------------------------
2019-03-19 02:55:53,541 [INFO] Summary:
2019-03-19 02:55:53,541 [INFO] Batch 153000, worst loss 0.060999 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:55:53,542 [INFO] Regularization: 3333.111572 * 0.0000010000 = 0.0033331115
2019-03-19 02:55:53,543 [INFO] Sum of grad norms: 0.026087
2019-03-19 02:55:53,543 [INFO] ---------------------------------
2019-03-19 02:56:12,268 [INFO] ---------------------------------
2019-03-19 02:56:12,269 [INFO] Summary:
2019-03-19 02:56:12,270 [INFO] Batch 154000, worst loss 0.060983 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:56:12,271 [INFO] Regularization: 3333.093262 * 0.0000010000 = 0.0033330934
2019-03-19 02:56:12,272 [INFO] Sum of grad norms: 0.024430
2019-03-19 02:56:12,273 [INFO] ---------------------------------
2019-03-19 02:56:30,709 [INFO] ---------------------------------
2019-03-19 02:56:30,710 [INFO] Summary:
2019-03-19 02:56:30,711 [INFO] Batch 155000, worst loss 0.061006 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:56:30,711 [INFO] Regularization: 3333.079834 * 0.0000010000 = 0.0033330799
2019-03-19 02:56:30,712 [INFO] Sum of grad norms: 0.076211
2019-03-19 02:56:30,713 [INFO] ---------------------------------
2019-03-19 02:56:49,695 [INFO] ---------------------------------
2019-03-19 02:56:49,696 [INFO] Summary:
2019-03-19 02:56:49,697 [INFO] Batch 156000, worst loss 0.061006 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:56:49,697 [INFO] Regularization: 3333.066162 * 0.0000010000 = 0.0033330661
2019-03-19 02:56:49,698 [INFO] Sum of grad norms: 0.015868
2019-03-19 02:56:49,698 [INFO] ---------------------------------
2019-03-19 02:57:08,087 [INFO] ---------------------------------
2019-03-19 02:57:08,088 [INFO] Summary:
2019-03-19 02:57:08,089 [INFO] Batch 157000, worst loss 0.060957 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:57:08,089 [INFO] Regularization: 3333.053223 * 0.0000010000 = 0.0033330533
2019-03-19 02:57:08,090 [INFO] Sum of grad norms: 0.027349
2019-03-19 02:57:08,090 [INFO] ---------------------------------
2019-03-19 02:57:26,382 [INFO] ---------------------------------
2019-03-19 02:57:26,383 [INFO] Summary:
2019-03-19 02:57:26,383 [INFO] Batch 158000, worst loss 0.061006 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:57:26,384 [INFO] Regularization: 3333.032959 * 0.0000010000 = 0.0033330331
2019-03-19 02:57:26,384 [INFO] Sum of grad norms: 0.057666
2019-03-19 02:57:26,385 [INFO] ---------------------------------
2019-03-19 02:57:45,438 [INFO] ---------------------------------
2019-03-19 02:57:45,439 [INFO] Summary:
2019-03-19 02:57:45,440 [INFO] Batch 159000, worst loss 0.061004 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:57:45,440 [INFO] Regularization: 3333.013916 * 0.0000010000 = 0.0033330140
2019-03-19 02:57:45,441 [INFO] Sum of grad norms: 0.024038
2019-03-19 02:57:45,441 [INFO] ---------------------------------
2019-03-19 02:58:04,212 [INFO] ---------------------------------
2019-03-19 02:58:04,213 [INFO] Summary:
2019-03-19 02:58:04,214 [INFO] Batch 160000, worst loss 0.060930 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:58:04,214 [INFO] Regularization: 3332.996826 * 0.0000010000 = 0.0033329967
2019-03-19 02:58:04,215 [INFO] Sum of grad norms: 0.033175
2019-03-19 02:58:04,215 [INFO] ---------------------------------
2019-03-19 02:58:09,214 [INFO] ---------------------------------
2019-03-19 02:58:09,215 [INFO] Evaluation:
2019-03-19 02:58:09,216 [INFO] Batch 160000, worst loss 0.057588 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 02:58:09,218 [INFO] ---------------------------------
2019-03-19 02:58:28,057 [INFO] ---------------------------------
2019-03-19 02:58:28,058 [INFO] Summary:
2019-03-19 02:58:28,058 [INFO] Batch 161000, worst loss 0.060870 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:58:28,059 [INFO] Regularization: 3332.979004 * 0.0000010000 = 0.0033329790
2019-03-19 02:58:28,059 [INFO] Sum of grad norms: 0.039685
2019-03-19 02:58:28,060 [INFO] ---------------------------------
2019-03-19 02:58:46,760 [INFO] ---------------------------------
2019-03-19 02:58:46,761 [INFO] Summary:
2019-03-19 02:58:46,761 [INFO] Batch 162000, worst loss 0.061067 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:58:46,762 [INFO] Regularization: 3332.966553 * 0.0000010000 = 0.0033329665
2019-03-19 02:58:46,762 [INFO] Sum of grad norms: 0.065830
2019-03-19 02:58:46,763 [INFO] ---------------------------------
2019-03-19 02:59:05,496 [INFO] ---------------------------------
2019-03-19 02:59:05,497 [INFO] Summary:
2019-03-19 02:59:05,498 [INFO] Batch 163000, worst loss 0.061067 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:59:05,498 [INFO] Regularization: 3332.958008 * 0.0000010000 = 0.0033329581
2019-03-19 02:59:05,499 [INFO] Sum of grad norms: 0.036491
2019-03-19 02:59:05,499 [INFO] ---------------------------------
2019-03-19 02:59:24,375 [INFO] ---------------------------------
2019-03-19 02:59:24,376 [INFO] Summary:
2019-03-19 02:59:24,377 [INFO] Batch 164000, worst loss 0.061067 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:59:24,378 [INFO] Regularization: 3332.949219 * 0.0000010000 = 0.0033329492
2019-03-19 02:59:24,378 [INFO] Sum of grad norms: 0.027566
2019-03-19 02:59:24,379 [INFO] ---------------------------------
2019-03-19 02:59:43,699 [INFO] ---------------------------------
2019-03-19 02:59:43,700 [INFO] Summary:
2019-03-19 02:59:43,700 [INFO] Batch 165000, worst loss 0.061056 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 02:59:43,701 [INFO] Regularization: 3332.939697 * 0.0000010000 = 0.0033329397
2019-03-19 02:59:43,701 [INFO] Sum of grad norms: 0.028948
2019-03-19 02:59:43,702 [INFO] ---------------------------------
2019-03-19 03:00:02,514 [INFO] ---------------------------------
2019-03-19 03:00:02,515 [INFO] Summary:
2019-03-19 03:00:02,516 [INFO] Batch 166000, worst loss 0.061057 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:00:02,517 [INFO] Regularization: 3332.931641 * 0.0000010000 = 0.0033329315
2019-03-19 03:00:02,518 [INFO] Sum of grad norms: 0.050992
2019-03-19 03:00:02,519 [INFO] ---------------------------------
2019-03-19 03:00:21,306 [INFO] ---------------------------------
2019-03-19 03:00:21,307 [INFO] Summary:
2019-03-19 03:00:21,308 [INFO] Batch 167000, worst loss 0.061013 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:00:21,308 [INFO] Regularization: 3332.925293 * 0.0000010000 = 0.0033329253
2019-03-19 03:00:21,309 [INFO] Sum of grad norms: 0.033515
2019-03-19 03:00:21,309 [INFO] ---------------------------------
2019-03-19 03:00:40,207 [INFO] ---------------------------------
2019-03-19 03:00:40,208 [INFO] Summary:
2019-03-19 03:00:40,209 [INFO] Batch 168000, worst loss 0.061025 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:00:40,209 [INFO] Regularization: 3332.918213 * 0.0000010000 = 0.0033329183
2019-03-19 03:00:40,210 [INFO] Sum of grad norms: 0.036313
2019-03-19 03:00:40,210 [INFO] ---------------------------------
2019-03-19 03:00:59,223 [INFO] ---------------------------------
2019-03-19 03:00:59,224 [INFO] Summary:
2019-03-19 03:00:59,225 [INFO] Batch 169000, worst loss 0.061025 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:00:59,225 [INFO] Regularization: 3332.910645 * 0.0000010000 = 0.0033329106
2019-03-19 03:00:59,226 [INFO] Sum of grad norms: 0.026485
2019-03-19 03:00:59,226 [INFO] ---------------------------------
2019-03-19 03:01:18,310 [INFO] ---------------------------------
2019-03-19 03:01:18,310 [INFO] Summary:
2019-03-19 03:01:18,311 [INFO] Batch 170000, worst loss 0.061024 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:01:18,312 [INFO] Regularization: 3332.902588 * 0.0000010000 = 0.0033329027
2019-03-19 03:01:18,312 [INFO] Sum of grad norms: 0.036748
2019-03-19 03:01:18,313 [INFO] ---------------------------------
2019-03-19 03:01:23,258 [INFO] ---------------------------------
2019-03-19 03:01:23,259 [INFO] Evaluation:
2019-03-19 03:01:23,260 [INFO] Batch 170000, worst loss 0.057607 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:01:23,261 [INFO] ---------------------------------
2019-03-19 03:01:41,983 [INFO] ---------------------------------
2019-03-19 03:01:41,984 [INFO] Summary:
2019-03-19 03:01:41,985 [INFO] Batch 171000, worst loss 0.060940 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:01:41,986 [INFO] Regularization: 3332.894531 * 0.0000010000 = 0.0033328945
2019-03-19 03:01:41,986 [INFO] Sum of grad norms: 0.025725
2019-03-19 03:01:41,987 [INFO] ---------------------------------
2019-03-19 03:02:00,698 [INFO] ---------------------------------
2019-03-19 03:02:00,699 [INFO] Summary:
2019-03-19 03:02:00,699 [INFO] Batch 172000, worst loss 0.060950 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:02:00,700 [INFO] Regularization: 3332.891357 * 0.0000010000 = 0.0033328913
2019-03-19 03:02:00,700 [INFO] Sum of grad norms: 0.037147
2019-03-19 03:02:00,701 [INFO] ---------------------------------
2019-03-19 03:02:19,030 [INFO] ---------------------------------
2019-03-19 03:02:19,031 [INFO] Summary:
2019-03-19 03:02:19,032 [INFO] Batch 173000, worst loss 0.060950 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:02:19,032 [INFO] Regularization: 3332.888184 * 0.0000010000 = 0.0033328882
2019-03-19 03:02:19,033 [INFO] Sum of grad norms: 0.029323
2019-03-19 03:02:19,034 [INFO] ---------------------------------
2019-03-19 03:02:37,527 [INFO] ---------------------------------
2019-03-19 03:02:37,528 [INFO] Summary:
2019-03-19 03:02:37,529 [INFO] Batch 174000, worst loss 0.061125 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:02:37,529 [INFO] Regularization: 3332.885254 * 0.0000010000 = 0.0033328852
2019-03-19 03:02:37,530 [INFO] Sum of grad norms: 0.021488
2019-03-19 03:02:37,531 [INFO] ---------------------------------
2019-03-19 03:02:56,447 [INFO] ---------------------------------
2019-03-19 03:02:56,448 [INFO] Summary:
2019-03-19 03:02:56,449 [INFO] Batch 175000, worst loss 0.061125 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:02:56,450 [INFO] Regularization: 3332.881592 * 0.0000010000 = 0.0033328815
2019-03-19 03:02:56,450 [INFO] Sum of grad norms: 0.032246
2019-03-19 03:02:56,451 [INFO] ---------------------------------
2019-03-19 03:03:15,506 [INFO] ---------------------------------
2019-03-19 03:03:15,507 [INFO] Summary:
2019-03-19 03:03:15,508 [INFO] Batch 176000, worst loss 0.061056 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:03:15,508 [INFO] Regularization: 3332.879395 * 0.0000010000 = 0.0033328794
2019-03-19 03:03:15,509 [INFO] Sum of grad norms: 0.026712
2019-03-19 03:03:15,510 [INFO] ---------------------------------
2019-03-19 03:03:34,297 [INFO] ---------------------------------
2019-03-19 03:03:34,298 [INFO] Summary:
2019-03-19 03:03:34,298 [INFO] Batch 177000, worst loss 0.060880 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:03:34,299 [INFO] Regularization: 3332.876465 * 0.0000010000 = 0.0033328764
2019-03-19 03:03:34,300 [INFO] Sum of grad norms: 0.034019
2019-03-19 03:03:34,300 [INFO] ---------------------------------
2019-03-19 03:03:53,126 [INFO] ---------------------------------
2019-03-19 03:03:53,127 [INFO] Summary:
2019-03-19 03:03:53,127 [INFO] Batch 178000, worst loss 0.061012 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:03:53,128 [INFO] Regularization: 3332.873047 * 0.0000010000 = 0.0033328731
2019-03-19 03:03:53,128 [INFO] Sum of grad norms: 0.044049
2019-03-19 03:03:53,129 [INFO] ---------------------------------
2019-03-19 03:04:11,451 [INFO] ---------------------------------
2019-03-19 03:04:11,452 [INFO] Summary:
2019-03-19 03:04:11,453 [INFO] Batch 179000, worst loss 0.061012 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:04:11,453 [INFO] Regularization: 3332.870605 * 0.0000010000 = 0.0033328705
2019-03-19 03:04:11,454 [INFO] Sum of grad norms: 0.025238
2019-03-19 03:04:11,454 [INFO] ---------------------------------
2019-03-19 03:04:29,988 [INFO] ---------------------------------
2019-03-19 03:04:29,989 [INFO] Summary:
2019-03-19 03:04:29,990 [INFO] Batch 180000, worst loss 0.061011 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:04:29,991 [INFO] Regularization: 3332.868652 * 0.0000010000 = 0.0033328687
2019-03-19 03:04:29,991 [INFO] Sum of grad norms: 0.047690
2019-03-19 03:04:29,992 [INFO] ---------------------------------
2019-03-19 03:04:34,824 [INFO] ---------------------------------
2019-03-19 03:04:34,825 [INFO] Evaluation:
2019-03-19 03:04:34,826 [INFO] Batch 180000, worst loss 0.057562 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:04:34,830 [INFO] ---------------------------------
2019-03-19 03:04:53,171 [INFO] ---------------------------------
2019-03-19 03:04:53,172 [INFO] Summary:
2019-03-19 03:04:53,172 [INFO] Batch 181000, worst loss 0.060920 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:04:53,173 [INFO] Regularization: 3332.866211 * 0.0000010000 = 0.0033328661
2019-03-19 03:04:53,173 [INFO] Sum of grad norms: 0.021726
2019-03-19 03:04:53,174 [INFO] ---------------------------------
2019-03-19 03:05:11,755 [INFO] ---------------------------------
2019-03-19 03:05:11,756 [INFO] Summary:
2019-03-19 03:05:11,756 [INFO] Batch 182000, worst loss 0.060920 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:05:11,757 [INFO] Regularization: 3332.865479 * 0.0000010000 = 0.0033328654
2019-03-19 03:05:11,757 [INFO] Sum of grad norms: 0.078932
2019-03-19 03:05:11,758 [INFO] ---------------------------------
2019-03-19 03:05:30,596 [INFO] ---------------------------------
2019-03-19 03:05:30,602 [INFO] Summary:
2019-03-19 03:05:30,602 [INFO] Batch 183000, worst loss 0.060845 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:05:30,603 [INFO] Regularization: 3332.864502 * 0.0000010000 = 0.0033328645
2019-03-19 03:05:30,604 [INFO] Sum of grad norms: 0.035055
2019-03-19 03:05:30,604 [INFO] ---------------------------------
2019-03-19 03:05:50,053 [INFO] ---------------------------------
2019-03-19 03:05:50,054 [INFO] Summary:
2019-03-19 03:05:50,055 [INFO] Batch 184000, worst loss 0.060915 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:05:50,055 [INFO] Regularization: 3332.863525 * 0.0000010000 = 0.0033328636
2019-03-19 03:05:50,056 [INFO] Sum of grad norms: 0.053331
2019-03-19 03:05:50,057 [INFO] ---------------------------------
2019-03-19 03:06:08,656 [INFO] ---------------------------------
2019-03-19 03:06:08,657 [INFO] Summary:
2019-03-19 03:06:08,657 [INFO] Batch 185000, worst loss 0.060914 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:06:08,658 [INFO] Regularization: 3332.862793 * 0.0000010000 = 0.0033328629
2019-03-19 03:06:08,658 [INFO] Sum of grad norms: 0.033994
2019-03-19 03:06:08,659 [INFO] ---------------------------------
2019-03-19 03:06:27,182 [INFO] ---------------------------------
2019-03-19 03:06:27,183 [INFO] Summary:
2019-03-19 03:06:27,183 [INFO] Batch 186000, worst loss 0.060915 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:06:27,184 [INFO] Regularization: 3332.861816 * 0.0000010000 = 0.0033328617
2019-03-19 03:06:27,185 [INFO] Sum of grad norms: 0.023503
2019-03-19 03:06:27,185 [INFO] ---------------------------------
2019-03-19 03:06:46,305 [INFO] ---------------------------------
2019-03-19 03:06:46,306 [INFO] Summary:
2019-03-19 03:06:46,306 [INFO] Batch 187000, worst loss 0.060946 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:06:46,307 [INFO] Regularization: 3332.862061 * 0.0000010000 = 0.0033328622
2019-03-19 03:06:46,307 [INFO] Sum of grad norms: 0.023098
2019-03-19 03:06:46,308 [INFO] ---------------------------------
2019-03-19 03:07:05,063 [INFO] ---------------------------------
2019-03-19 03:07:05,064 [INFO] Summary:
2019-03-19 03:07:05,064 [INFO] Batch 188000, worst loss 0.061102 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:07:05,065 [INFO] Regularization: 3332.861328 * 0.0000010000 = 0.0033328612
2019-03-19 03:07:05,066 [INFO] Sum of grad norms: 0.044757
2019-03-19 03:07:05,066 [INFO] ---------------------------------
2019-03-19 03:07:23,982 [INFO] ---------------------------------
2019-03-19 03:07:23,983 [INFO] Summary:
2019-03-19 03:07:23,983 [INFO] Batch 189000, worst loss 0.061043 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:07:23,984 [INFO] Regularization: 3332.860840 * 0.0000010000 = 0.0033328608
2019-03-19 03:07:23,984 [INFO] Sum of grad norms: 0.031013
2019-03-19 03:07:23,985 [INFO] ---------------------------------
2019-03-19 03:07:42,439 [INFO] ---------------------------------
2019-03-19 03:07:42,440 [INFO] Summary:
2019-03-19 03:07:42,441 [INFO] Batch 190000, worst loss 0.061102 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:07:42,441 [INFO] Regularization: 3332.859619 * 0.0000010000 = 0.0033328596
2019-03-19 03:07:42,442 [INFO] Sum of grad norms: 0.042077
2019-03-19 03:07:42,443 [INFO] ---------------------------------
2019-03-19 03:07:47,321 [INFO] ---------------------------------
2019-03-19 03:07:47,322 [INFO] Evaluation:
2019-03-19 03:07:47,322 [INFO] Batch 190000, worst loss 0.057558 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:07:47,324 [INFO] ---------------------------------
2019-03-19 03:08:06,314 [INFO] ---------------------------------
2019-03-19 03:08:06,314 [INFO] Summary:
2019-03-19 03:08:06,315 [INFO] Batch 191000, worst loss 0.061134 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:08:06,316 [INFO] Regularization: 3332.859375 * 0.0000010000 = 0.0033328594
2019-03-19 03:08:06,316 [INFO] Sum of grad norms: 0.043559
2019-03-19 03:08:06,317 [INFO] ---------------------------------
2019-03-19 03:08:24,929 [INFO] ---------------------------------
2019-03-19 03:08:24,930 [INFO] Summary:
2019-03-19 03:08:24,931 [INFO] Batch 192000, worst loss 0.060902 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:08:24,932 [INFO] Regularization: 3332.858887 * 0.0000010000 = 0.0033328589
2019-03-19 03:08:24,932 [INFO] Sum of grad norms: 0.055698
2019-03-19 03:08:24,933 [INFO] ---------------------------------
2019-03-19 03:08:43,747 [INFO] ---------------------------------
2019-03-19 03:08:43,748 [INFO] Summary:
2019-03-19 03:08:43,748 [INFO] Batch 193000, worst loss 0.061134 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:08:43,749 [INFO] Regularization: 3332.858398 * 0.0000010000 = 0.0033328584
2019-03-19 03:08:43,750 [INFO] Sum of grad norms: 0.014839
2019-03-19 03:08:43,750 [INFO] ---------------------------------
2019-03-19 03:09:02,643 [INFO] ---------------------------------
2019-03-19 03:09:02,644 [INFO] Summary:
2019-03-19 03:09:02,645 [INFO] Batch 194000, worst loss 0.060951 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:09:02,645 [INFO] Regularization: 3332.858154 * 0.0000010000 = 0.0033328582
2019-03-19 03:09:02,646 [INFO] Sum of grad norms: 0.029320
2019-03-19 03:09:02,646 [INFO] ---------------------------------
2019-03-19 03:09:21,172 [INFO] ---------------------------------
2019-03-19 03:09:21,173 [INFO] Summary:
2019-03-19 03:09:21,174 [INFO] Batch 195000, worst loss 0.060951 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:09:21,175 [INFO] Regularization: 3332.857422 * 0.0000010000 = 0.0033328575
2019-03-19 03:09:21,175 [INFO] Sum of grad norms: 0.031433
2019-03-19 03:09:21,176 [INFO] ---------------------------------
2019-03-19 03:09:39,847 [INFO] ---------------------------------
2019-03-19 03:09:39,849 [INFO] Summary:
2019-03-19 03:09:39,849 [INFO] Batch 196000, worst loss 0.061037 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:09:39,850 [INFO] Regularization: 3332.857178 * 0.0000010000 = 0.0033328573
2019-03-19 03:09:39,851 [INFO] Sum of grad norms: 0.048504
2019-03-19 03:09:39,852 [INFO] ---------------------------------
2019-03-19 03:09:58,538 [INFO] ---------------------------------
2019-03-19 03:09:58,539 [INFO] Summary:
2019-03-19 03:09:58,540 [INFO] Batch 197000, worst loss 0.060920 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:09:58,541 [INFO] Regularization: 3332.856934 * 0.0000010000 = 0.0033328568
2019-03-19 03:09:58,541 [INFO] Sum of grad norms: 0.031490
2019-03-19 03:09:58,542 [INFO] ---------------------------------
2019-03-19 03:10:17,159 [INFO] ---------------------------------
2019-03-19 03:10:17,160 [INFO] Summary:
2019-03-19 03:10:17,160 [INFO] Batch 198000, worst loss 0.061038 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:10:17,161 [INFO] Regularization: 3332.856445 * 0.0000010000 = 0.0033328563
2019-03-19 03:10:17,162 [INFO] Sum of grad norms: 0.029853
2019-03-19 03:10:17,162 [INFO] ---------------------------------
2019-03-19 03:10:35,872 [INFO] ---------------------------------
2019-03-19 03:10:35,873 [INFO] Summary:
2019-03-19 03:10:35,874 [INFO] Batch 199000, worst loss 0.061088 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:10:35,874 [INFO] Regularization: 3332.856445 * 0.0000010000 = 0.0033328563
2019-03-19 03:10:35,875 [INFO] Sum of grad norms: 0.019673
2019-03-19 03:10:35,875 [INFO] ---------------------------------
2019-03-19 03:10:54,629 [INFO] ---------------------------------
2019-03-19 03:10:54,630 [INFO] Summary:
2019-03-19 03:10:54,631 [INFO] Batch 200000, worst loss 0.060960 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:10:54,631 [INFO] Regularization: 3332.855957 * 0.0000010000 = 0.0033328559
2019-03-19 03:10:54,632 [INFO] Sum of grad norms: 0.019045
2019-03-19 03:10:54,632 [INFO] ---------------------------------
2019-03-19 03:10:59,589 [INFO] ---------------------------------
2019-03-19 03:10:59,590 [INFO] Evaluation:
2019-03-19 03:10:59,591 [INFO] Batch 200000, worst loss 0.057755 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:10:59,592 [INFO] ---------------------------------
2019-03-19 03:11:18,298 [INFO] ---------------------------------
2019-03-19 03:11:18,299 [INFO] Summary:
2019-03-19 03:11:18,299 [INFO] Batch 201000, worst loss 0.061013 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:11:18,300 [INFO] Regularization: 3332.855957 * 0.0000010000 = 0.0033328559
2019-03-19 03:11:18,300 [INFO] Sum of grad norms: 0.031302
2019-03-19 03:11:18,301 [INFO] ---------------------------------
2019-03-19 03:11:37,055 [INFO] ---------------------------------
2019-03-19 03:11:37,056 [INFO] Summary:
2019-03-19 03:11:37,056 [INFO] Batch 202000, worst loss 0.060962 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:11:37,057 [INFO] Regularization: 3332.855469 * 0.0000010000 = 0.0033328554
2019-03-19 03:11:37,057 [INFO] Sum of grad norms: 0.036937
2019-03-19 03:11:37,058 [INFO] ---------------------------------
2019-03-19 03:11:56,017 [INFO] ---------------------------------
2019-03-19 03:11:56,019 [INFO] Summary:
2019-03-19 03:11:56,019 [INFO] Batch 203000, worst loss 0.060962 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:11:56,020 [INFO] Regularization: 3332.855713 * 0.0000010000 = 0.0033328556
2019-03-19 03:11:56,020 [INFO] Sum of grad norms: 0.022074
2019-03-19 03:11:56,021 [INFO] ---------------------------------
2019-03-19 03:12:14,911 [INFO] ---------------------------------
2019-03-19 03:12:14,912 [INFO] Summary:
2019-03-19 03:12:14,913 [INFO] Batch 204000, worst loss 0.061133 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:12:14,913 [INFO] Regularization: 3332.855469 * 0.0000010000 = 0.0033328554
2019-03-19 03:12:14,914 [INFO] Sum of grad norms: 0.025039
2019-03-19 03:12:14,914 [INFO] ---------------------------------
2019-03-19 03:12:33,687 [INFO] ---------------------------------
2019-03-19 03:12:33,688 [INFO] Summary:
2019-03-19 03:12:33,688 [INFO] Batch 205000, worst loss 0.061133 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:12:33,689 [INFO] Regularization: 3332.855469 * 0.0000010000 = 0.0033328554
2019-03-19 03:12:33,689 [INFO] Sum of grad norms: 0.026044
2019-03-19 03:12:33,690 [INFO] ---------------------------------
2019-03-19 03:12:52,794 [INFO] ---------------------------------
2019-03-19 03:12:52,795 [INFO] Summary:
2019-03-19 03:12:52,796 [INFO] Batch 206000, worst loss 0.061133 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:12:52,797 [INFO] Regularization: 3332.855469 * 0.0000010000 = 0.0033328554
2019-03-19 03:12:52,798 [INFO] Sum of grad norms: 0.019078
2019-03-19 03:12:52,798 [INFO] ---------------------------------
2019-03-19 03:13:11,604 [INFO] ---------------------------------
2019-03-19 03:13:11,605 [INFO] Summary:
2019-03-19 03:13:11,605 [INFO] Batch 207000, worst loss 0.060969 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:13:11,606 [INFO] Regularization: 3332.855469 * 0.0000010000 = 0.0033328554
2019-03-19 03:13:11,606 [INFO] Sum of grad norms: 0.033698
2019-03-19 03:13:11,607 [INFO] ---------------------------------
2019-03-19 03:13:30,372 [INFO] ---------------------------------
2019-03-19 03:13:30,373 [INFO] Summary:
2019-03-19 03:13:30,374 [INFO] Batch 208000, worst loss 0.060943 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:13:30,374 [INFO] Regularization: 3332.855225 * 0.0000010000 = 0.0033328552
2019-03-19 03:13:30,375 [INFO] Sum of grad norms: 0.039122
2019-03-19 03:13:30,375 [INFO] ---------------------------------
2019-03-19 03:13:48,916 [INFO] ---------------------------------
2019-03-19 03:13:48,917 [INFO] Summary:
2019-03-19 03:13:48,918 [INFO] Batch 209000, worst loss 0.060917 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:13:48,918 [INFO] Regularization: 3332.855225 * 0.0000010000 = 0.0033328552
2019-03-19 03:13:48,919 [INFO] Sum of grad norms: 0.026010
2019-03-19 03:13:48,919 [INFO] ---------------------------------
2019-03-19 03:14:07,848 [INFO] ---------------------------------
2019-03-19 03:14:07,849 [INFO] Summary:
2019-03-19 03:14:07,849 [INFO] Batch 210000, worst loss 0.060937 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:14:07,850 [INFO] Regularization: 3332.854980 * 0.0000010000 = 0.0033328549
2019-03-19 03:14:07,850 [INFO] Sum of grad norms: 0.028732
2019-03-19 03:14:07,851 [INFO] ---------------------------------
2019-03-19 03:14:12,771 [INFO] ---------------------------------
2019-03-19 03:14:12,772 [INFO] Evaluation:
2019-03-19 03:14:12,773 [INFO] Batch 210000, worst loss 0.057630 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:14:12,774 [INFO] ---------------------------------
2019-03-19 03:14:31,759 [INFO] ---------------------------------
2019-03-19 03:14:31,760 [INFO] Summary:
2019-03-19 03:14:31,760 [INFO] Batch 211000, worst loss 0.060964 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:14:31,761 [INFO] Regularization: 3332.854492 * 0.0000010000 = 0.0033328545
2019-03-19 03:14:31,761 [INFO] Sum of grad norms: 0.039688
2019-03-19 03:14:31,762 [INFO] ---------------------------------
2019-03-19 03:14:50,634 [INFO] ---------------------------------
2019-03-19 03:14:50,636 [INFO] Summary:
2019-03-19 03:14:50,636 [INFO] Batch 212000, worst loss 0.060963 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:14:50,637 [INFO] Regularization: 3332.854492 * 0.0000010000 = 0.0033328545
2019-03-19 03:14:50,637 [INFO] Sum of grad norms: 0.059504
2019-03-19 03:14:50,638 [INFO] ---------------------------------
2019-03-19 03:15:09,473 [INFO] ---------------------------------
2019-03-19 03:15:09,474 [INFO] Summary:
2019-03-19 03:15:09,474 [INFO] Batch 213000, worst loss 0.061030 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:15:09,475 [INFO] Regularization: 3332.854492 * 0.0000010000 = 0.0033328545
2019-03-19 03:15:09,475 [INFO] Sum of grad norms: 0.052439
2019-03-19 03:15:09,476 [INFO] ---------------------------------
2019-03-19 03:15:28,029 [INFO] ---------------------------------
2019-03-19 03:15:28,029 [INFO] Summary:
2019-03-19 03:15:28,030 [INFO] Batch 214000, worst loss 0.060913 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:15:28,031 [INFO] Regularization: 3332.854492 * 0.0000010000 = 0.0033328545
2019-03-19 03:15:28,031 [INFO] Sum of grad norms: 0.026957
2019-03-19 03:15:28,032 [INFO] ---------------------------------
2019-03-19 03:15:46,447 [INFO] ---------------------------------
2019-03-19 03:15:46,448 [INFO] Summary:
2019-03-19 03:15:46,449 [INFO] Batch 215000, worst loss 0.061030 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:15:46,449 [INFO] Regularization: 3332.854492 * 0.0000010000 = 0.0033328545
2019-03-19 03:15:46,450 [INFO] Sum of grad norms: 0.022997
2019-03-19 03:15:46,451 [INFO] ---------------------------------
2019-03-19 03:16:05,135 [INFO] ---------------------------------
2019-03-19 03:16:05,136 [INFO] Summary:
2019-03-19 03:16:05,137 [INFO] Batch 216000, worst loss 0.061062 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:16:05,137 [INFO] Regularization: 3332.854492 * 0.0000010000 = 0.0033328545
2019-03-19 03:16:05,138 [INFO] Sum of grad norms: 0.024272
2019-03-19 03:16:05,138 [INFO] ---------------------------------
2019-03-19 03:16:23,831 [INFO] ---------------------------------
2019-03-19 03:16:23,832 [INFO] Summary:
2019-03-19 03:16:23,833 [INFO] Batch 217000, worst loss 0.061062 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:16:23,833 [INFO] Regularization: 3332.854248 * 0.0000010000 = 0.0033328542
2019-03-19 03:16:23,834 [INFO] Sum of grad norms: 0.042230
2019-03-19 03:16:23,834 [INFO] ---------------------------------
2019-03-19 03:16:42,678 [INFO] ---------------------------------
2019-03-19 03:16:42,679 [INFO] Summary:
2019-03-19 03:16:42,679 [INFO] Batch 218000, worst loss 0.061062 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:16:42,680 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:16:42,680 [INFO] Sum of grad norms: 0.016508
2019-03-19 03:16:42,681 [INFO] ---------------------------------
2019-03-19 03:17:00,970 [INFO] ---------------------------------
2019-03-19 03:17:00,971 [INFO] Summary:
2019-03-19 03:17:00,972 [INFO] Batch 219000, worst loss 0.061126 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:17:00,972 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:17:00,973 [INFO] Sum of grad norms: 0.030279
2019-03-19 03:17:00,973 [INFO] ---------------------------------
2019-03-19 03:17:19,497 [INFO] ---------------------------------
2019-03-19 03:17:19,498 [INFO] Summary:
2019-03-19 03:17:19,498 [INFO] Batch 220000, worst loss 0.061126 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:17:19,499 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:17:19,499 [INFO] Sum of grad norms: 0.028179
2019-03-19 03:17:19,500 [INFO] ---------------------------------
2019-03-19 03:17:24,462 [INFO] ---------------------------------
2019-03-19 03:17:24,463 [INFO] Evaluation:
2019-03-19 03:17:24,464 [INFO] Batch 220000, worst loss 0.057793 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:17:24,465 [INFO] ---------------------------------
2019-03-19 03:17:43,067 [INFO] ---------------------------------
2019-03-19 03:17:43,068 [INFO] Summary:
2019-03-19 03:17:43,069 [INFO] Batch 221000, worst loss 0.061060 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:17:43,070 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:17:43,071 [INFO] Sum of grad norms: 0.045838
2019-03-19 03:17:43,072 [INFO] ---------------------------------
2019-03-19 03:18:01,479 [INFO] ---------------------------------
2019-03-19 03:18:01,480 [INFO] Summary:
2019-03-19 03:18:01,480 [INFO] Batch 222000, worst loss 0.061060 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:18:01,481 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:18:01,481 [INFO] Sum of grad norms: 0.022718
2019-03-19 03:18:01,482 [INFO] ---------------------------------
2019-03-19 03:18:20,016 [INFO] ---------------------------------
2019-03-19 03:18:20,017 [INFO] Summary:
2019-03-19 03:18:20,017 [INFO] Batch 223000, worst loss 0.061060 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:18:20,018 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:18:20,018 [INFO] Sum of grad norms: 0.030434
2019-03-19 03:18:20,019 [INFO] ---------------------------------
2019-03-19 03:18:38,898 [INFO] ---------------------------------
2019-03-19 03:18:38,899 [INFO] Summary:
2019-03-19 03:18:38,900 [INFO] Batch 224000, worst loss 0.060972 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:18:38,900 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:18:38,901 [INFO] Sum of grad norms: 0.017913
2019-03-19 03:18:38,902 [INFO] ---------------------------------
2019-03-19 03:18:57,259 [INFO] ---------------------------------
2019-03-19 03:18:57,260 [INFO] Summary:
2019-03-19 03:18:57,261 [INFO] Batch 225000, worst loss 0.060898 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:18:57,261 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:18:57,262 [INFO] Sum of grad norms: 0.033960
2019-03-19 03:18:57,262 [INFO] ---------------------------------
2019-03-19 03:19:16,208 [INFO] ---------------------------------
2019-03-19 03:19:16,209 [INFO] Summary:
2019-03-19 03:19:16,210 [INFO] Batch 226000, worst loss 0.061042 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:19:16,211 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:19:16,212 [INFO] Sum of grad norms: 0.024429
2019-03-19 03:19:16,213 [INFO] ---------------------------------
2019-03-19 03:19:34,865 [INFO] ---------------------------------
2019-03-19 03:19:34,866 [INFO] Summary:
2019-03-19 03:19:34,867 [INFO] Batch 227000, worst loss 0.061053 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:19:34,867 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:19:34,868 [INFO] Sum of grad norms: 0.026732
2019-03-19 03:19:34,868 [INFO] ---------------------------------
2019-03-19 03:19:54,017 [INFO] ---------------------------------
2019-03-19 03:19:54,018 [INFO] Summary:
2019-03-19 03:19:54,018 [INFO] Batch 228000, worst loss 0.061042 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:19:54,019 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:19:54,020 [INFO] Sum of grad norms: 0.032427
2019-03-19 03:19:54,020 [INFO] ---------------------------------
2019-03-19 03:20:12,740 [INFO] ---------------------------------
2019-03-19 03:20:12,741 [INFO] Summary:
2019-03-19 03:20:12,742 [INFO] Batch 229000, worst loss 0.061053 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:20:12,742 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:20:12,743 [INFO] Sum of grad norms: 0.019862
2019-03-19 03:20:12,743 [INFO] ---------------------------------
2019-03-19 03:20:31,392 [INFO] ---------------------------------
2019-03-19 03:20:31,393 [INFO] Summary:
2019-03-19 03:20:31,394 [INFO] Batch 230000, worst loss 0.061000 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:20:31,394 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:20:31,395 [INFO] Sum of grad norms: 0.051381
2019-03-19 03:20:31,395 [INFO] ---------------------------------
2019-03-19 03:20:36,327 [INFO] ---------------------------------
2019-03-19 03:20:36,328 [INFO] Evaluation:
2019-03-19 03:20:36,331 [INFO] Batch 230000, worst loss 0.057670 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:20:36,333 [INFO] ---------------------------------
2019-03-19 03:20:55,145 [INFO] ---------------------------------
2019-03-19 03:20:55,146 [INFO] Summary:
2019-03-19 03:20:55,147 [INFO] Batch 231000, worst loss 0.060961 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:20:55,148 [INFO] Regularization: 3332.854004 * 0.0000010000 = 0.0033328540
2019-03-19 03:20:55,149 [INFO] Sum of grad norms: 0.027922
2019-03-19 03:20:55,150 [INFO] ---------------------------------
2019-03-19 03:21:13,934 [INFO] ---------------------------------
2019-03-19 03:21:13,935 [INFO] Summary:
2019-03-19 03:21:13,936 [INFO] Batch 232000, worst loss 0.060961 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:21:13,936 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:21:13,937 [INFO] Sum of grad norms: 0.039034
2019-03-19 03:21:13,937 [INFO] ---------------------------------
2019-03-19 03:21:32,899 [INFO] ---------------------------------
2019-03-19 03:21:32,900 [INFO] Summary:
2019-03-19 03:21:32,900 [INFO] Batch 233000, worst loss 0.060939 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:21:32,901 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:21:32,901 [INFO] Sum of grad norms: 0.020809
2019-03-19 03:21:32,902 [INFO] ---------------------------------
2019-03-19 03:21:51,681 [INFO] ---------------------------------
2019-03-19 03:21:51,682 [INFO] Summary:
2019-03-19 03:21:51,683 [INFO] Batch 234000, worst loss 0.060959 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:21:51,683 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:21:51,684 [INFO] Sum of grad norms: 0.028563
2019-03-19 03:21:51,685 [INFO] ---------------------------------
2019-03-19 03:22:10,113 [INFO] ---------------------------------
2019-03-19 03:22:10,114 [INFO] Summary:
2019-03-19 03:22:10,115 [INFO] Batch 235000, worst loss 0.061012 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:22:10,115 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:22:10,116 [INFO] Sum of grad norms: 0.035836
2019-03-19 03:22:10,116 [INFO] ---------------------------------
2019-03-19 03:22:28,618 [INFO] ---------------------------------
2019-03-19 03:22:28,619 [INFO] Summary:
2019-03-19 03:22:28,620 [INFO] Batch 236000, worst loss 0.060902 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:22:28,620 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:22:28,621 [INFO] Sum of grad norms: 0.038567
2019-03-19 03:22:28,621 [INFO] ---------------------------------
2019-03-19 03:22:47,266 [INFO] ---------------------------------
2019-03-19 03:22:47,267 [INFO] Summary:
2019-03-19 03:22:47,267 [INFO] Batch 237000, worst loss 0.061114 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:22:47,268 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:22:47,268 [INFO] Sum of grad norms: 0.031797
2019-03-19 03:22:47,269 [INFO] ---------------------------------
2019-03-19 03:23:06,154 [INFO] ---------------------------------
2019-03-19 03:23:06,155 [INFO] Summary:
2019-03-19 03:23:06,155 [INFO] Batch 238000, worst loss 0.060928 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:23:06,156 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:23:06,156 [INFO] Sum of grad norms: 0.054217
2019-03-19 03:23:06,157 [INFO] ---------------------------------
2019-03-19 03:23:24,673 [INFO] ---------------------------------
2019-03-19 03:23:24,674 [INFO] Summary:
2019-03-19 03:23:24,674 [INFO] Batch 239000, worst loss 0.061114 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:23:24,675 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:23:24,676 [INFO] Sum of grad norms: 0.082724
2019-03-19 03:23:24,676 [INFO] ---------------------------------
2019-03-19 03:23:43,328 [INFO] ---------------------------------
2019-03-19 03:23:43,329 [INFO] Summary:
2019-03-19 03:23:43,330 [INFO] Batch 240000, worst loss 0.060928 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:23:43,330 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:23:43,331 [INFO] Sum of grad norms: 0.021114
2019-03-19 03:23:43,331 [INFO] ---------------------------------
2019-03-19 03:23:48,330 [INFO] ---------------------------------
2019-03-19 03:23:48,331 [INFO] Evaluation:
2019-03-19 03:23:48,331 [INFO] Batch 240000, worst loss 0.057618 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:23:48,332 [INFO] ---------------------------------
2019-03-19 03:24:07,179 [INFO] ---------------------------------
2019-03-19 03:24:07,180 [INFO] Summary:
2019-03-19 03:24:07,181 [INFO] Batch 241000, worst loss 0.060911 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:24:07,181 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:24:07,182 [INFO] Sum of grad norms: 0.047490
2019-03-19 03:24:07,182 [INFO] ---------------------------------
2019-03-19 03:24:26,050 [INFO] ---------------------------------
2019-03-19 03:24:26,051 [INFO] Summary:
2019-03-19 03:24:26,052 [INFO] Batch 242000, worst loss 0.061058 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:24:26,052 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:24:26,053 [INFO] Sum of grad norms: 0.031831
2019-03-19 03:24:26,053 [INFO] ---------------------------------
2019-03-19 03:24:45,016 [INFO] ---------------------------------
2019-03-19 03:24:45,017 [INFO] Summary:
2019-03-19 03:24:45,018 [INFO] Batch 243000, worst loss 0.060911 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:24:45,018 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:24:45,019 [INFO] Sum of grad norms: 0.061702
2019-03-19 03:24:45,019 [INFO] ---------------------------------
2019-03-19 03:25:03,688 [INFO] ---------------------------------
2019-03-19 03:25:03,689 [INFO] Summary:
2019-03-19 03:25:03,690 [INFO] Batch 244000, worst loss 0.061058 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:25:03,690 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:25:03,691 [INFO] Sum of grad norms: 0.033552
2019-03-19 03:25:03,692 [INFO] ---------------------------------
2019-03-19 03:25:22,300 [INFO] ---------------------------------
2019-03-19 03:25:22,301 [INFO] Summary:
2019-03-19 03:25:22,302 [INFO] Batch 245000, worst loss 0.060884 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:25:22,302 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:25:22,303 [INFO] Sum of grad norms: 0.023326
2019-03-19 03:25:22,303 [INFO] ---------------------------------
2019-03-19 03:25:41,090 [INFO] ---------------------------------
2019-03-19 03:25:41,091 [INFO] Summary:
2019-03-19 03:25:41,092 [INFO] Batch 246000, worst loss 0.060893 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:25:41,092 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:25:41,093 [INFO] Sum of grad norms: 0.027835
2019-03-19 03:25:41,093 [INFO] ---------------------------------
2019-03-19 03:25:59,720 [INFO] ---------------------------------
2019-03-19 03:25:59,721 [INFO] Summary:
2019-03-19 03:25:59,722 [INFO] Batch 247000, worst loss 0.060944 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:25:59,723 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:25:59,724 [INFO] Sum of grad norms: 0.038069
2019-03-19 03:25:59,725 [INFO] ---------------------------------
2019-03-19 03:26:18,552 [INFO] ---------------------------------
2019-03-19 03:26:18,553 [INFO] Summary:
2019-03-19 03:26:18,554 [INFO] Batch 248000, worst loss 0.060920 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:26:18,554 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:26:18,555 [INFO] Sum of grad norms: 0.023219
2019-03-19 03:26:18,555 [INFO] ---------------------------------
2019-03-19 03:26:37,173 [INFO] ---------------------------------
2019-03-19 03:26:37,174 [INFO] Summary:
2019-03-19 03:26:37,175 [INFO] Batch 249000, worst loss 0.060944 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:26:37,176 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:26:37,177 [INFO] Sum of grad norms: 0.029703
2019-03-19 03:26:37,178 [INFO] ---------------------------------
2019-03-19 03:26:56,093 [INFO] ---------------------------------
2019-03-19 03:26:56,094 [INFO] Summary:
2019-03-19 03:26:56,094 [INFO] Batch 250000, worst loss 0.060968 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 03:26:56,095 [INFO] Regularization: 3332.853760 * 0.0000010000 = 0.0033328538
2019-03-19 03:26:56,096 [INFO] Sum of grad norms: 0.061826
2019-03-19 03:26:56,096 [INFO] ---------------------------------
2019-03-19 03:27:01,085 [INFO] ---------------------------------
2019-03-19 03:27:01,086 [INFO] Evaluation:
2019-03-19 03:27:01,087 [INFO] Batch 250000, worst loss 0.057636 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:27:01,087 [INFO] ---------------------------------
2019-03-19 03:27:01,088 [INFO] Finished training, saved to file classifier/1552933539/1552962421_5_classifier_final.pth
2019-03-19 03:27:01,255 [INFO] ---------------------------------
2019-03-19 03:27:01,257 [INFO] Training model #6: (1, 64, 201) @ 1
2019-03-19 03:27:17,136 [INFO] ---------------------------------
2019-03-19 03:27:17,137 [INFO] Summary:
2019-03-19 03:27:17,138 [INFO] Batch 1000, worst loss 16.866194 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 03:27:17,138 [INFO] Regularization: 9030.421875 * 0.0000010000 = 0.0090304222
2019-03-19 03:27:17,139 [INFO] Sum of grad norms: 0.799721
2019-03-19 03:27:17,139 [INFO] ---------------------------------
2019-03-19 03:27:33,298 [INFO] ---------------------------------
2019-03-19 03:27:33,299 [INFO] Summary:
2019-03-19 03:27:33,299 [INFO] Batch 2000, worst loss 0.146307 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 03:27:33,300 [INFO] Regularization: 8019.996582 * 0.0000010000 = 0.0080199968
2019-03-19 03:27:33,300 [INFO] Sum of grad norms: 2.081369
2019-03-19 03:27:33,301 [INFO] ---------------------------------
2019-03-19 03:27:52,284 [INFO] ---------------------------------
2019-03-19 03:27:52,285 [INFO] Summary:
2019-03-19 03:27:52,286 [INFO] Batch 3000, worst loss 0.088220 (incl. reg.) of 1000 batches, learning rate 0.001463 @cl.-depth 1
2019-03-19 03:27:52,286 [INFO] Regularization: 7361.020020 * 0.0000010000 = 0.0073610200
2019-03-19 03:27:52,287 [INFO] Sum of grad norms: 0.360756
2019-03-19 03:27:52,287 [INFO] ---------------------------------
2019-03-19 03:28:11,140 [INFO] ---------------------------------
2019-03-19 03:28:11,141 [INFO] Summary:
2019-03-19 03:28:11,141 [INFO] Batch 4000, worst loss 0.080350 (incl. reg.) of 1000 batches, learning rate 0.000882 @cl.-depth 1
2019-03-19 03:28:11,142 [INFO] Regularization: 6924.830078 * 0.0000010000 = 0.0069248299
2019-03-19 03:28:11,142 [INFO] Sum of grad norms: 0.659111
2019-03-19 03:28:11,143 [INFO] ---------------------------------
2019-03-19 03:28:30,083 [INFO] ---------------------------------
2019-03-19 03:28:30,084 [INFO] Summary:
2019-03-19 03:28:30,085 [INFO] Batch 5000, worst loss 0.077286 (incl. reg.) of 1000 batches, learning rate 0.000803 @cl.-depth 1
2019-03-19 03:28:30,085 [INFO] Regularization: 6567.333496 * 0.0000010000 = 0.0065673334
2019-03-19 03:28:30,086 [INFO] Sum of grad norms: 0.195982
2019-03-19 03:28:30,087 [INFO] ---------------------------------
2019-03-19 03:28:48,914 [INFO] ---------------------------------
2019-03-19 03:28:48,915 [INFO] Summary:
2019-03-19 03:28:48,916 [INFO] Batch 6000, worst loss 0.073343 (incl. reg.) of 1000 batches, learning rate 0.000773 @cl.-depth 1
2019-03-19 03:28:48,917 [INFO] Regularization: 6224.068848 * 0.0000010000 = 0.0062240688
2019-03-19 03:28:48,917 [INFO] Sum of grad norms: 1.139638
2019-03-19 03:28:48,918 [INFO] ---------------------------------
2019-03-19 03:29:07,920 [INFO] ---------------------------------
2019-03-19 03:29:07,921 [INFO] Summary:
2019-03-19 03:29:07,922 [INFO] Batch 7000, worst loss 0.072974 (incl. reg.) of 1000 batches, learning rate 0.000733 @cl.-depth 1
2019-03-19 03:29:07,922 [INFO] Regularization: 5942.543945 * 0.0000010000 = 0.0059425440
2019-03-19 03:29:07,923 [INFO] Sum of grad norms: 0.146038
2019-03-19 03:29:07,924 [INFO] ---------------------------------
2019-03-19 03:29:26,911 [INFO] ---------------------------------
2019-03-19 03:29:26,912 [INFO] Summary:
2019-03-19 03:29:26,913 [INFO] Batch 8000, worst loss 0.072597 (incl. reg.) of 1000 batches, learning rate 0.000730 @cl.-depth 1
2019-03-19 03:29:26,914 [INFO] Regularization: 5722.655273 * 0.0000010000 = 0.0057226554
2019-03-19 03:29:26,914 [INFO] Sum of grad norms: 0.865453
2019-03-19 03:29:26,915 [INFO] ---------------------------------
2019-03-19 03:29:45,767 [INFO] ---------------------------------
2019-03-19 03:29:45,768 [INFO] Summary:
2019-03-19 03:29:45,768 [INFO] Batch 9000, worst loss 0.073980 (incl. reg.) of 1000 batches, learning rate 0.000726 @cl.-depth 1
2019-03-19 03:29:45,769 [INFO] Regularization: 5558.750488 * 0.0000010000 = 0.0055587506
2019-03-19 03:29:45,769 [INFO] Sum of grad norms: 0.275123
2019-03-19 03:29:45,770 [INFO] ---------------------------------
2019-03-19 03:30:04,541 [INFO] ---------------------------------
2019-03-19 03:30:04,542 [INFO] Summary:
2019-03-19 03:30:04,543 [INFO] Batch 10000, worst loss 0.070473 (incl. reg.) of 1000 batches, learning rate 0.000726 @cl.-depth 1
2019-03-19 03:30:04,543 [INFO] Regularization: 5414.612793 * 0.0000010000 = 0.0054146126
2019-03-19 03:30:04,544 [INFO] Sum of grad norms: 0.265036
2019-03-19 03:30:04,545 [INFO] ---------------------------------
2019-03-19 03:30:09,420 [INFO] ---------------------------------
2019-03-19 03:30:09,420 [INFO] Evaluation:
2019-03-19 03:30:09,421 [INFO] Batch 10000, worst loss 0.063756 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:30:09,422 [INFO] ---------------------------------
2019-03-19 03:30:28,514 [INFO] ---------------------------------
2019-03-19 03:30:28,515 [INFO] Summary:
2019-03-19 03:30:28,516 [INFO] Batch 11000, worst loss 0.073372 (incl. reg.) of 1000 batches, learning rate 0.000705 @cl.-depth 1
2019-03-19 03:30:28,516 [INFO] Regularization: 5319.237305 * 0.0000010000 = 0.0053192372
2019-03-19 03:30:28,517 [INFO] Sum of grad norms: 0.109067
2019-03-19 03:30:28,517 [INFO] ---------------------------------
2019-03-19 03:30:47,258 [INFO] ---------------------------------
2019-03-19 03:30:47,259 [INFO] Summary:
2019-03-19 03:30:47,260 [INFO] Batch 12000, worst loss 0.069002 (incl. reg.) of 1000 batches, learning rate 0.000705 @cl.-depth 1
2019-03-19 03:30:47,260 [INFO] Regularization: 5229.144531 * 0.0000010000 = 0.0052291444
2019-03-19 03:30:47,261 [INFO] Sum of grad norms: 0.302507
2019-03-19 03:30:47,261 [INFO] ---------------------------------
2019-03-19 03:31:06,071 [INFO] ---------------------------------
2019-03-19 03:31:06,072 [INFO] Summary:
2019-03-19 03:31:06,073 [INFO] Batch 13000, worst loss 0.069398 (incl. reg.) of 1000 batches, learning rate 0.000690 @cl.-depth 1
2019-03-19 03:31:06,073 [INFO] Regularization: 5155.985840 * 0.0000010000 = 0.0051559857
2019-03-19 03:31:06,074 [INFO] Sum of grad norms: 0.573071
2019-03-19 03:31:06,074 [INFO] ---------------------------------
2019-03-19 03:31:24,987 [INFO] ---------------------------------
2019-03-19 03:31:24,988 [INFO] Summary:
2019-03-19 03:31:24,988 [INFO] Batch 14000, worst loss 0.069387 (incl. reg.) of 1000 batches, learning rate 0.000690 @cl.-depth 1
2019-03-19 03:31:24,989 [INFO] Regularization: 5078.710449 * 0.0000010000 = 0.0050787106
2019-03-19 03:31:24,989 [INFO] Sum of grad norms: 1.155746
2019-03-19 03:31:24,990 [INFO] ---------------------------------
2019-03-19 03:31:44,465 [INFO] ---------------------------------
2019-03-19 03:31:44,466 [INFO] Summary:
2019-03-19 03:31:44,467 [INFO] Batch 15000, worst loss 0.072070 (incl. reg.) of 1000 batches, learning rate 0.000690 @cl.-depth 1
2019-03-19 03:31:44,468 [INFO] Regularization: 5010.825195 * 0.0000010000 = 0.0050108251
2019-03-19 03:31:44,468 [INFO] Sum of grad norms: 0.546809
2019-03-19 03:31:44,469 [INFO] ---------------------------------
2019-03-19 03:32:03,371 [INFO] ---------------------------------
2019-03-19 03:32:03,372 [INFO] Summary:
2019-03-19 03:32:03,373 [INFO] Batch 16000, worst loss 0.068965 (incl. reg.) of 1000 batches, learning rate 0.000690 @cl.-depth 1
2019-03-19 03:32:03,374 [INFO] Regularization: 4948.534180 * 0.0000010000 = 0.0049485341
2019-03-19 03:32:03,374 [INFO] Sum of grad norms: 0.138400
2019-03-19 03:32:03,375 [INFO] ---------------------------------
2019-03-19 03:32:22,574 [INFO] ---------------------------------
2019-03-19 03:32:22,575 [INFO] Summary:
2019-03-19 03:32:22,576 [INFO] Batch 17000, worst loss 0.068921 (incl. reg.) of 1000 batches, learning rate 0.000690 @cl.-depth 1
2019-03-19 03:32:22,576 [INFO] Regularization: 4890.688965 * 0.0000010000 = 0.0048906892
2019-03-19 03:32:22,577 [INFO] Sum of grad norms: 0.516330
2019-03-19 03:32:22,577 [INFO] ---------------------------------
2019-03-19 03:32:41,571 [INFO] ---------------------------------
2019-03-19 03:32:41,572 [INFO] Summary:
2019-03-19 03:32:41,573 [INFO] Batch 18000, worst loss 0.068054 (incl. reg.) of 1000 batches, learning rate 0.000689 @cl.-depth 1
2019-03-19 03:32:41,573 [INFO] Regularization: 4858.625488 * 0.0000010000 = 0.0048586256
2019-03-19 03:32:41,574 [INFO] Sum of grad norms: 0.493231
2019-03-19 03:32:41,574 [INFO] ---------------------------------
2019-03-19 03:33:00,930 [INFO] ---------------------------------
2019-03-19 03:33:00,931 [INFO] Summary:
2019-03-19 03:33:00,932 [INFO] Batch 19000, worst loss 0.069928 (incl. reg.) of 1000 batches, learning rate 0.000681 @cl.-depth 1
2019-03-19 03:33:00,932 [INFO] Regularization: 4796.500488 * 0.0000010000 = 0.0047965003
2019-03-19 03:33:00,933 [INFO] Sum of grad norms: 0.419555
2019-03-19 03:33:00,933 [INFO] ---------------------------------
2019-03-19 03:33:20,220 [INFO] ---------------------------------
2019-03-19 03:33:20,221 [INFO] Summary:
2019-03-19 03:33:20,221 [INFO] Batch 20000, worst loss 0.067191 (incl. reg.) of 1000 batches, learning rate 0.000681 @cl.-depth 1
2019-03-19 03:33:20,222 [INFO] Regularization: 4741.480469 * 0.0000010000 = 0.0047414806
2019-03-19 03:33:20,223 [INFO] Sum of grad norms: 0.084841
2019-03-19 03:33:20,223 [INFO] ---------------------------------
2019-03-19 03:33:25,121 [INFO] ---------------------------------
2019-03-19 03:33:25,122 [INFO] Evaluation:
2019-03-19 03:33:25,123 [INFO] Batch 20000, worst loss 0.060841 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:33:25,124 [INFO] ---------------------------------
2019-03-19 03:33:44,324 [INFO] ---------------------------------
2019-03-19 03:33:44,326 [INFO] Summary:
2019-03-19 03:33:44,327 [INFO] Batch 21000, worst loss 0.066946 (incl. reg.) of 1000 batches, learning rate 0.000672 @cl.-depth 1
2019-03-19 03:33:44,327 [INFO] Regularization: 4683.878418 * 0.0000010000 = 0.0046838783
2019-03-19 03:33:44,328 [INFO] Sum of grad norms: 0.088444
2019-03-19 03:33:44,329 [INFO] ---------------------------------
2019-03-19 03:34:02,989 [INFO] ---------------------------------
2019-03-19 03:34:02,990 [INFO] Summary:
2019-03-19 03:34:02,990 [INFO] Batch 22000, worst loss 0.066660 (incl. reg.) of 1000 batches, learning rate 0.000669 @cl.-depth 1
2019-03-19 03:34:02,991 [INFO] Regularization: 4635.662598 * 0.0000010000 = 0.0046356628
2019-03-19 03:34:02,991 [INFO] Sum of grad norms: 0.258756
2019-03-19 03:34:02,992 [INFO] ---------------------------------
2019-03-19 03:34:21,634 [INFO] ---------------------------------
2019-03-19 03:34:21,636 [INFO] Summary:
2019-03-19 03:34:21,637 [INFO] Batch 23000, worst loss 0.066340 (incl. reg.) of 1000 batches, learning rate 0.000667 @cl.-depth 1
2019-03-19 03:34:21,637 [INFO] Regularization: 4575.847656 * 0.0000010000 = 0.0045758476
2019-03-19 03:34:21,638 [INFO] Sum of grad norms: 0.234071
2019-03-19 03:34:21,639 [INFO] ---------------------------------
2019-03-19 03:34:40,589 [INFO] ---------------------------------
2019-03-19 03:34:40,590 [INFO] Summary:
2019-03-19 03:34:40,591 [INFO] Batch 24000, worst loss 0.070058 (incl. reg.) of 1000 batches, learning rate 0.000663 @cl.-depth 1
2019-03-19 03:34:40,591 [INFO] Regularization: 4529.347656 * 0.0000010000 = 0.0045293476
2019-03-19 03:34:40,592 [INFO] Sum of grad norms: 0.550970
2019-03-19 03:34:40,593 [INFO] ---------------------------------
2019-03-19 03:34:59,500 [INFO] ---------------------------------
2019-03-19 03:34:59,501 [INFO] Summary:
2019-03-19 03:34:59,501 [INFO] Batch 25000, worst loss 0.065681 (incl. reg.) of 1000 batches, learning rate 0.000663 @cl.-depth 1
2019-03-19 03:34:59,502 [INFO] Regularization: 4483.904785 * 0.0000010000 = 0.0044839047
2019-03-19 03:34:59,502 [INFO] Sum of grad norms: 0.503521
2019-03-19 03:34:59,503 [INFO] ---------------------------------
2019-03-19 03:35:18,425 [INFO] ---------------------------------
2019-03-19 03:35:18,426 [INFO] Summary:
2019-03-19 03:35:18,427 [INFO] Batch 26000, worst loss 0.065167 (incl. reg.) of 1000 batches, learning rate 0.000657 @cl.-depth 1
2019-03-19 03:35:18,427 [INFO] Regularization: 4444.326172 * 0.0000010000 = 0.0044443263
2019-03-19 03:35:18,428 [INFO] Sum of grad norms: 0.456752
2019-03-19 03:35:18,429 [INFO] ---------------------------------
2019-03-19 03:35:37,574 [INFO] ---------------------------------
2019-03-19 03:35:37,575 [INFO] Summary:
2019-03-19 03:35:37,576 [INFO] Batch 27000, worst loss 0.065880 (incl. reg.) of 1000 batches, learning rate 0.000652 @cl.-depth 1
2019-03-19 03:35:37,576 [INFO] Regularization: 4400.586426 * 0.0000010000 = 0.0044005862
2019-03-19 03:35:37,577 [INFO] Sum of grad norms: 0.446365
2019-03-19 03:35:37,577 [INFO] ---------------------------------
2019-03-19 03:35:56,851 [INFO] ---------------------------------
2019-03-19 03:35:56,853 [INFO] Summary:
2019-03-19 03:35:56,853 [INFO] Batch 28000, worst loss 0.065641 (incl. reg.) of 1000 batches, learning rate 0.000652 @cl.-depth 1
2019-03-19 03:35:56,854 [INFO] Regularization: 4370.428223 * 0.0000010000 = 0.0043704282
2019-03-19 03:35:56,854 [INFO] Sum of grad norms: 0.480582
2019-03-19 03:35:56,855 [INFO] ---------------------------------
2019-03-19 03:36:15,572 [INFO] ---------------------------------
2019-03-19 03:36:15,572 [INFO] Summary:
2019-03-19 03:36:15,573 [INFO] Batch 29000, worst loss 0.065555 (incl. reg.) of 1000 batches, learning rate 0.000652 @cl.-depth 1
2019-03-19 03:36:15,574 [INFO] Regularization: 4307.894043 * 0.0000010000 = 0.0043078940
2019-03-19 03:36:15,574 [INFO] Sum of grad norms: 0.081651
2019-03-19 03:36:15,575 [INFO] ---------------------------------
2019-03-19 03:36:34,925 [INFO] ---------------------------------
2019-03-19 03:36:34,925 [INFO] Summary:
2019-03-19 03:36:34,926 [INFO] Batch 30000, worst loss 0.066911 (incl. reg.) of 1000 batches, learning rate 0.000652 @cl.-depth 1
2019-03-19 03:36:34,927 [INFO] Regularization: 4265.055176 * 0.0000010000 = 0.0042650551
2019-03-19 03:36:34,927 [INFO] Sum of grad norms: 0.321511
2019-03-19 03:36:34,928 [INFO] ---------------------------------
2019-03-19 03:36:39,825 [INFO] ---------------------------------
2019-03-19 03:36:39,826 [INFO] Evaluation:
2019-03-19 03:36:39,826 [INFO] Batch 30000, worst loss 0.059203 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:36:39,827 [INFO] ---------------------------------
2019-03-19 03:36:58,880 [INFO] ---------------------------------
2019-03-19 03:36:58,881 [INFO] Summary:
2019-03-19 03:36:58,882 [INFO] Batch 31000, worst loss 0.064710 (incl. reg.) of 1000 batches, learning rate 0.000652 @cl.-depth 1
2019-03-19 03:36:58,882 [INFO] Regularization: 4210.325195 * 0.0000010000 = 0.0042103250
2019-03-19 03:36:58,883 [INFO] Sum of grad norms: 0.358309
2019-03-19 03:36:58,884 [INFO] ---------------------------------
2019-03-19 03:37:17,967 [INFO] ---------------------------------
2019-03-19 03:37:17,968 [INFO] Summary:
2019-03-19 03:37:17,969 [INFO] Batch 32000, worst loss 0.064448 (incl. reg.) of 1000 batches, learning rate 0.000647 @cl.-depth 1
2019-03-19 03:37:17,970 [INFO] Regularization: 4164.270020 * 0.0000010000 = 0.0041642701
2019-03-19 03:37:17,970 [INFO] Sum of grad norms: 0.091974
2019-03-19 03:37:17,971 [INFO] ---------------------------------
2019-03-19 03:37:37,043 [INFO] ---------------------------------
2019-03-19 03:37:37,044 [INFO] Summary:
2019-03-19 03:37:37,044 [INFO] Batch 33000, worst loss 0.063752 (incl. reg.) of 1000 batches, learning rate 0.000644 @cl.-depth 1
2019-03-19 03:37:37,045 [INFO] Regularization: 4115.170410 * 0.0000010000 = 0.0041151703
2019-03-19 03:37:37,045 [INFO] Sum of grad norms: 0.084659
2019-03-19 03:37:37,046 [INFO] ---------------------------------
2019-03-19 03:37:56,048 [INFO] ---------------------------------
2019-03-19 03:37:56,049 [INFO] Summary:
2019-03-19 03:37:56,049 [INFO] Batch 34000, worst loss 0.064642 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-19 03:37:56,050 [INFO] Regularization: 4091.175293 * 0.0000010000 = 0.0040911753
2019-03-19 03:37:56,051 [INFO] Sum of grad norms: 0.090325
2019-03-19 03:37:56,051 [INFO] ---------------------------------
2019-03-19 03:38:14,963 [INFO] ---------------------------------
2019-03-19 03:38:14,964 [INFO] Summary:
2019-03-19 03:38:14,965 [INFO] Batch 35000, worst loss 0.064715 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-19 03:38:14,965 [INFO] Regularization: 4023.424072 * 0.0000010000 = 0.0040234239
2019-03-19 03:38:14,966 [INFO] Sum of grad norms: 1.340209
2019-03-19 03:38:14,966 [INFO] ---------------------------------
2019-03-19 03:38:34,103 [INFO] ---------------------------------
2019-03-19 03:38:34,105 [INFO] Summary:
2019-03-19 03:38:34,105 [INFO] Batch 36000, worst loss 0.065620 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-19 03:38:34,106 [INFO] Regularization: 3972.933105 * 0.0000010000 = 0.0039729332
2019-03-19 03:38:34,107 [INFO] Sum of grad norms: 0.430825
2019-03-19 03:38:34,108 [INFO] ---------------------------------
2019-03-19 03:38:53,106 [INFO] ---------------------------------
2019-03-19 03:38:53,107 [INFO] Summary:
2019-03-19 03:38:53,108 [INFO] Batch 37000, worst loss 0.063875 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-19 03:38:53,108 [INFO] Regularization: 3923.040771 * 0.0000010000 = 0.0039230408
2019-03-19 03:38:53,109 [INFO] Sum of grad norms: 0.987586
2019-03-19 03:38:53,109 [INFO] ---------------------------------
2019-03-19 03:39:12,184 [INFO] ---------------------------------
2019-03-19 03:39:12,185 [INFO] Summary:
2019-03-19 03:39:12,185 [INFO] Batch 38000, worst loss 0.064088 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-19 03:39:12,186 [INFO] Regularization: 3871.931641 * 0.0000010000 = 0.0038719317
2019-03-19 03:39:12,186 [INFO] Sum of grad norms: 0.189178
2019-03-19 03:39:12,187 [INFO] ---------------------------------
2019-03-19 03:39:31,082 [INFO] ---------------------------------
2019-03-19 03:39:31,083 [INFO] Summary:
2019-03-19 03:39:31,083 [INFO] Batch 39000, worst loss 0.064147 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-19 03:39:31,084 [INFO] Regularization: 3834.859375 * 0.0000010000 = 0.0038348595
2019-03-19 03:39:31,084 [INFO] Sum of grad norms: 0.637006
2019-03-19 03:39:31,085 [INFO] ---------------------------------
2019-03-19 03:39:50,100 [INFO] ---------------------------------
2019-03-19 03:39:50,101 [INFO] Summary:
2019-03-19 03:39:50,102 [INFO] Batch 40000, worst loss 0.063613 (incl. reg.) of 1000 batches, learning rate 0.000638 @cl.-depth 1
2019-03-19 03:39:50,103 [INFO] Regularization: 3789.904297 * 0.0000010000 = 0.0037899043
2019-03-19 03:39:50,103 [INFO] Sum of grad norms: 0.303106
2019-03-19 03:39:50,104 [INFO] ---------------------------------
2019-03-19 03:39:55,078 [INFO] ---------------------------------
2019-03-19 03:39:55,078 [INFO] Evaluation:
2019-03-19 03:39:55,079 [INFO] Batch 40000, worst loss 0.058663 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:39:55,080 [INFO] ---------------------------------
2019-03-19 03:40:14,720 [INFO] ---------------------------------
2019-03-19 03:40:14,721 [INFO] Summary:
2019-03-19 03:40:14,722 [INFO] Batch 41000, worst loss 0.062959 (incl. reg.) of 1000 batches, learning rate 0.000318 @cl.-depth 1
2019-03-19 03:40:14,723 [INFO] Regularization: 3742.246094 * 0.0000010000 = 0.0037422462
2019-03-19 03:40:14,723 [INFO] Sum of grad norms: 0.389588
2019-03-19 03:40:14,724 [INFO] ---------------------------------
2019-03-19 03:40:33,505 [INFO] ---------------------------------
2019-03-19 03:40:33,506 [INFO] Summary:
2019-03-19 03:40:33,507 [INFO] Batch 42000, worst loss 0.062018 (incl. reg.) of 1000 batches, learning rate 0.000318 @cl.-depth 1
2019-03-19 03:40:33,507 [INFO] Regularization: 3705.757080 * 0.0000010000 = 0.0037057570
2019-03-19 03:40:33,508 [INFO] Sum of grad norms: 0.274857
2019-03-19 03:40:33,508 [INFO] ---------------------------------
2019-03-19 03:40:52,362 [INFO] ---------------------------------
2019-03-19 03:40:52,363 [INFO] Summary:
2019-03-19 03:40:52,364 [INFO] Batch 43000, worst loss 0.061958 (incl. reg.) of 1000 batches, learning rate 0.000318 @cl.-depth 1
2019-03-19 03:40:52,365 [INFO] Regularization: 3676.569092 * 0.0000010000 = 0.0036765691
2019-03-19 03:40:52,365 [INFO] Sum of grad norms: 0.085163
2019-03-19 03:40:52,366 [INFO] ---------------------------------
2019-03-19 03:41:11,301 [INFO] ---------------------------------
2019-03-19 03:41:11,301 [INFO] Summary:
2019-03-19 03:41:11,302 [INFO] Batch 44000, worst loss 0.062005 (incl. reg.) of 1000 batches, learning rate 0.000318 @cl.-depth 1
2019-03-19 03:41:11,302 [INFO] Regularization: 3649.503174 * 0.0000010000 = 0.0036495032
2019-03-19 03:41:11,303 [INFO] Sum of grad norms: 0.174555
2019-03-19 03:41:11,304 [INFO] ---------------------------------
2019-03-19 03:41:30,102 [INFO] ---------------------------------
2019-03-19 03:41:30,103 [INFO] Summary:
2019-03-19 03:41:30,104 [INFO] Batch 45000, worst loss 0.062129 (incl. reg.) of 1000 batches, learning rate 0.000318 @cl.-depth 1
2019-03-19 03:41:30,104 [INFO] Regularization: 3620.620361 * 0.0000010000 = 0.0036206204
2019-03-19 03:41:30,105 [INFO] Sum of grad norms: 0.212063
2019-03-19 03:41:30,106 [INFO] ---------------------------------
2019-03-19 03:41:49,162 [INFO] ---------------------------------
2019-03-19 03:41:49,164 [INFO] Summary:
2019-03-19 03:41:49,164 [INFO] Batch 46000, worst loss 0.061979 (incl. reg.) of 1000 batches, learning rate 0.000318 @cl.-depth 1
2019-03-19 03:41:49,165 [INFO] Regularization: 3593.965088 * 0.0000010000 = 0.0035939650
2019-03-19 03:41:49,165 [INFO] Sum of grad norms: 0.219653
2019-03-19 03:41:49,166 [INFO] ---------------------------------
2019-03-19 03:42:08,125 [INFO] ---------------------------------
2019-03-19 03:42:08,126 [INFO] Summary:
2019-03-19 03:42:08,127 [INFO] Batch 47000, worst loss 0.061760 (incl. reg.) of 1000 batches, learning rate 0.000318 @cl.-depth 1
2019-03-19 03:42:08,128 [INFO] Regularization: 3567.471191 * 0.0000010000 = 0.0035674712
2019-03-19 03:42:08,128 [INFO] Sum of grad norms: 0.420648
2019-03-19 03:42:08,129 [INFO] ---------------------------------
2019-03-19 03:42:26,958 [INFO] ---------------------------------
2019-03-19 03:42:26,959 [INFO] Summary:
2019-03-19 03:42:26,959 [INFO] Batch 48000, worst loss 0.062124 (incl. reg.) of 1000 batches, learning rate 0.000318 @cl.-depth 1
2019-03-19 03:42:26,960 [INFO] Regularization: 3539.437500 * 0.0000010000 = 0.0035394374
2019-03-19 03:42:26,960 [INFO] Sum of grad norms: 0.167342
2019-03-19 03:42:26,961 [INFO] ---------------------------------
2019-03-19 03:42:45,930 [INFO] ---------------------------------
2019-03-19 03:42:45,931 [INFO] Summary:
2019-03-19 03:42:45,932 [INFO] Batch 49000, worst loss 0.061997 (incl. reg.) of 1000 batches, learning rate 0.000318 @cl.-depth 1
2019-03-19 03:42:45,932 [INFO] Regularization: 3509.441895 * 0.0000010000 = 0.0035094419
2019-03-19 03:42:45,933 [INFO] Sum of grad norms: 0.278177
2019-03-19 03:42:45,933 [INFO] ---------------------------------
2019-03-19 03:43:04,706 [INFO] ---------------------------------
2019-03-19 03:43:04,707 [INFO] Summary:
2019-03-19 03:43:04,707 [INFO] Batch 50000, worst loss 0.061821 (incl. reg.) of 1000 batches, learning rate 0.000318 @cl.-depth 1
2019-03-19 03:43:04,708 [INFO] Regularization: 3482.932617 * 0.0000010000 = 0.0034829327
2019-03-19 03:43:04,708 [INFO] Sum of grad norms: 0.091158
2019-03-19 03:43:04,709 [INFO] ---------------------------------
2019-03-19 03:43:09,691 [INFO] ---------------------------------
2019-03-19 03:43:09,692 [INFO] Evaluation:
2019-03-19 03:43:09,693 [INFO] Batch 50000, worst loss 0.057960 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:43:09,693 [INFO] ---------------------------------
2019-03-19 03:43:28,323 [INFO] ---------------------------------
2019-03-19 03:43:28,324 [INFO] Summary:
2019-03-19 03:43:28,324 [INFO] Batch 51000, worst loss 0.061554 (incl. reg.) of 1000 batches, learning rate 0.000159 @cl.-depth 1
2019-03-19 03:43:28,325 [INFO] Regularization: 3456.706543 * 0.0000010000 = 0.0034567066
2019-03-19 03:43:28,325 [INFO] Sum of grad norms: 0.069383
2019-03-19 03:43:28,326 [INFO] ---------------------------------
2019-03-19 03:43:47,307 [INFO] ---------------------------------
2019-03-19 03:43:47,308 [INFO] Summary:
2019-03-19 03:43:47,309 [INFO] Batch 52000, worst loss 0.061298 (incl. reg.) of 1000 batches, learning rate 0.000159 @cl.-depth 1
2019-03-19 03:43:47,309 [INFO] Regularization: 3437.780273 * 0.0000010000 = 0.0034377803
2019-03-19 03:43:47,310 [INFO] Sum of grad norms: 0.063528
2019-03-19 03:43:47,311 [INFO] ---------------------------------
2019-03-19 03:44:06,074 [INFO] ---------------------------------
2019-03-19 03:44:06,075 [INFO] Summary:
2019-03-19 03:44:06,076 [INFO] Batch 53000, worst loss 0.061319 (incl. reg.) of 1000 batches, learning rate 0.000159 @cl.-depth 1
2019-03-19 03:44:06,076 [INFO] Regularization: 3422.783203 * 0.0000010000 = 0.0034227832
2019-03-19 03:44:06,077 [INFO] Sum of grad norms: 0.149374
2019-03-19 03:44:06,077 [INFO] ---------------------------------
2019-03-19 03:44:24,883 [INFO] ---------------------------------
2019-03-19 03:44:24,884 [INFO] Summary:
2019-03-19 03:44:24,885 [INFO] Batch 54000, worst loss 0.061257 (incl. reg.) of 1000 batches, learning rate 0.000159 @cl.-depth 1
2019-03-19 03:44:24,885 [INFO] Regularization: 3409.236084 * 0.0000010000 = 0.0034092360
2019-03-19 03:44:24,886 [INFO] Sum of grad norms: 0.219558
2019-03-19 03:44:24,886 [INFO] ---------------------------------
2019-03-19 03:44:43,746 [INFO] ---------------------------------
2019-03-19 03:44:43,747 [INFO] Summary:
2019-03-19 03:44:43,747 [INFO] Batch 55000, worst loss 0.061239 (incl. reg.) of 1000 batches, learning rate 0.000159 @cl.-depth 1
2019-03-19 03:44:43,748 [INFO] Regularization: 3393.631836 * 0.0000010000 = 0.0033936319
2019-03-19 03:44:43,748 [INFO] Sum of grad norms: 0.075424
2019-03-19 03:44:43,749 [INFO] ---------------------------------
2019-03-19 03:45:02,349 [INFO] ---------------------------------
2019-03-19 03:45:02,350 [INFO] Summary:
2019-03-19 03:45:02,351 [INFO] Batch 56000, worst loss 0.061247 (incl. reg.) of 1000 batches, learning rate 0.000159 @cl.-depth 1
2019-03-19 03:45:02,352 [INFO] Regularization: 3380.061279 * 0.0000010000 = 0.0033800614
2019-03-19 03:45:02,353 [INFO] Sum of grad norms: 0.163008
2019-03-19 03:45:02,354 [INFO] ---------------------------------
2019-03-19 03:45:21,059 [INFO] ---------------------------------
2019-03-19 03:45:21,060 [INFO] Summary:
2019-03-19 03:45:21,060 [INFO] Batch 57000, worst loss 0.061282 (incl. reg.) of 1000 batches, learning rate 0.000159 @cl.-depth 1
2019-03-19 03:45:21,061 [INFO] Regularization: 3365.925537 * 0.0000010000 = 0.0033659255
2019-03-19 03:45:21,061 [INFO] Sum of grad norms: 0.048229
2019-03-19 03:45:21,062 [INFO] ---------------------------------
2019-03-19 03:45:39,858 [INFO] ---------------------------------
2019-03-19 03:45:39,859 [INFO] Summary:
2019-03-19 03:45:39,859 [INFO] Batch 58000, worst loss 0.060995 (incl. reg.) of 1000 batches, learning rate 0.000159 @cl.-depth 1
2019-03-19 03:45:39,860 [INFO] Regularization: 3350.791504 * 0.0000010000 = 0.0033507915
2019-03-19 03:45:39,860 [INFO] Sum of grad norms: 0.058940
2019-03-19 03:45:39,861 [INFO] ---------------------------------
2019-03-19 03:45:58,590 [INFO] ---------------------------------
2019-03-19 03:45:58,591 [INFO] Summary:
2019-03-19 03:45:58,592 [INFO] Batch 59000, worst loss 0.061168 (incl. reg.) of 1000 batches, learning rate 0.000159 @cl.-depth 1
2019-03-19 03:45:58,592 [INFO] Regularization: 3336.709961 * 0.0000010000 = 0.0033367099
2019-03-19 03:45:58,593 [INFO] Sum of grad norms: 0.025194
2019-03-19 03:45:58,594 [INFO] ---------------------------------
2019-03-19 03:46:17,525 [INFO] ---------------------------------
2019-03-19 03:46:17,526 [INFO] Summary:
2019-03-19 03:46:17,526 [INFO] Batch 60000, worst loss 0.061312 (incl. reg.) of 1000 batches, learning rate 0.000159 @cl.-depth 1
2019-03-19 03:46:17,527 [INFO] Regularization: 3322.452148 * 0.0000010000 = 0.0033224521
2019-03-19 03:46:17,527 [INFO] Sum of grad norms: 0.209576
2019-03-19 03:46:17,528 [INFO] ---------------------------------
2019-03-19 03:46:22,423 [INFO] ---------------------------------
2019-03-19 03:46:22,424 [INFO] Evaluation:
2019-03-19 03:46:22,427 [INFO] Batch 60000, worst loss 0.057715 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:46:22,428 [INFO] ---------------------------------
2019-03-19 03:46:41,365 [INFO] ---------------------------------
2019-03-19 03:46:41,366 [INFO] Summary:
2019-03-19 03:46:41,367 [INFO] Batch 61000, worst loss 0.060979 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 03:46:41,367 [INFO] Regularization: 3308.580566 * 0.0000010000 = 0.0033085805
2019-03-19 03:46:41,368 [INFO] Sum of grad norms: 0.115747
2019-03-19 03:46:41,368 [INFO] ---------------------------------
2019-03-19 03:47:00,290 [INFO] ---------------------------------
2019-03-19 03:47:00,292 [INFO] Summary:
2019-03-19 03:47:00,292 [INFO] Batch 62000, worst loss 0.061101 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 03:47:00,293 [INFO] Regularization: 3299.444580 * 0.0000010000 = 0.0032994447
2019-03-19 03:47:00,293 [INFO] Sum of grad norms: 0.104072
2019-03-19 03:47:00,294 [INFO] ---------------------------------
2019-03-19 03:47:19,184 [INFO] ---------------------------------
2019-03-19 03:47:19,185 [INFO] Summary:
2019-03-19 03:47:19,185 [INFO] Batch 63000, worst loss 0.061130 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 03:47:19,186 [INFO] Regularization: 3291.397217 * 0.0000010000 = 0.0032913971
2019-03-19 03:47:19,186 [INFO] Sum of grad norms: 0.026770
2019-03-19 03:47:19,187 [INFO] ---------------------------------
2019-03-19 03:47:38,211 [INFO] ---------------------------------
2019-03-19 03:47:38,212 [INFO] Summary:
2019-03-19 03:47:38,213 [INFO] Batch 64000, worst loss 0.061025 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 03:47:38,213 [INFO] Regularization: 3283.895020 * 0.0000010000 = 0.0032838951
2019-03-19 03:47:38,214 [INFO] Sum of grad norms: 0.034660
2019-03-19 03:47:38,215 [INFO] ---------------------------------
2019-03-19 03:47:57,030 [INFO] ---------------------------------
2019-03-19 03:47:57,031 [INFO] Summary:
2019-03-19 03:47:57,032 [INFO] Batch 65000, worst loss 0.061073 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 03:47:57,032 [INFO] Regularization: 3276.177246 * 0.0000010000 = 0.0032761772
2019-03-19 03:47:57,033 [INFO] Sum of grad norms: 0.106309
2019-03-19 03:47:57,033 [INFO] ---------------------------------
2019-03-19 03:48:16,383 [INFO] ---------------------------------
2019-03-19 03:48:16,384 [INFO] Summary:
2019-03-19 03:48:16,385 [INFO] Batch 66000, worst loss 0.061015 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 03:48:16,385 [INFO] Regularization: 3268.821533 * 0.0000010000 = 0.0032688216
2019-03-19 03:48:16,386 [INFO] Sum of grad norms: 0.049869
2019-03-19 03:48:16,386 [INFO] ---------------------------------
2019-03-19 03:48:35,199 [INFO] ---------------------------------
2019-03-19 03:48:35,200 [INFO] Summary:
2019-03-19 03:48:35,201 [INFO] Batch 67000, worst loss 0.060902 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 03:48:35,201 [INFO] Regularization: 3261.156494 * 0.0000010000 = 0.0032611564
2019-03-19 03:48:35,202 [INFO] Sum of grad norms: 0.034200
2019-03-19 03:48:35,202 [INFO] ---------------------------------
2019-03-19 03:48:53,854 [INFO] ---------------------------------
2019-03-19 03:48:53,855 [INFO] Summary:
2019-03-19 03:48:53,855 [INFO] Batch 68000, worst loss 0.060975 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 03:48:53,856 [INFO] Regularization: 3253.479980 * 0.0000010000 = 0.0032534800
2019-03-19 03:48:53,856 [INFO] Sum of grad norms: 0.060824
2019-03-19 03:48:53,857 [INFO] ---------------------------------
2019-03-19 03:49:12,406 [INFO] ---------------------------------
2019-03-19 03:49:12,407 [INFO] Summary:
2019-03-19 03:49:12,408 [INFO] Batch 69000, worst loss 0.060975 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 03:49:12,408 [INFO] Regularization: 3245.936035 * 0.0000010000 = 0.0032459360
2019-03-19 03:49:12,409 [INFO] Sum of grad norms: 0.056727
2019-03-19 03:49:12,410 [INFO] ---------------------------------
2019-03-19 03:49:31,274 [INFO] ---------------------------------
2019-03-19 03:49:31,276 [INFO] Summary:
2019-03-19 03:49:31,276 [INFO] Batch 70000, worst loss 0.061107 (incl. reg.) of 1000 batches, learning rate 0.000080 @cl.-depth 1
2019-03-19 03:49:31,277 [INFO] Regularization: 3238.498535 * 0.0000010000 = 0.0032384985
2019-03-19 03:49:31,277 [INFO] Sum of grad norms: 0.084473
2019-03-19 03:49:31,278 [INFO] ---------------------------------
2019-03-19 03:49:36,168 [INFO] ---------------------------------
2019-03-19 03:49:36,169 [INFO] Evaluation:
2019-03-19 03:49:36,170 [INFO] Batch 70000, worst loss 0.057677 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:49:36,170 [INFO] ---------------------------------
2019-03-19 03:49:54,829 [INFO] ---------------------------------
2019-03-19 03:49:54,830 [INFO] Summary:
2019-03-19 03:49:54,831 [INFO] Batch 71000, worst loss 0.060873 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 03:49:54,832 [INFO] Regularization: 3230.955566 * 0.0000010000 = 0.0032309555
2019-03-19 03:49:54,832 [INFO] Sum of grad norms: 0.025284
2019-03-19 03:49:54,833 [INFO] ---------------------------------
2019-03-19 03:50:13,914 [INFO] ---------------------------------
2019-03-19 03:50:13,915 [INFO] Summary:
2019-03-19 03:50:13,915 [INFO] Batch 72000, worst loss 0.060802 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 03:50:13,916 [INFO] Regularization: 3226.090576 * 0.0000010000 = 0.0032260905
2019-03-19 03:50:13,916 [INFO] Sum of grad norms: 0.024655
2019-03-19 03:50:13,917 [INFO] ---------------------------------
2019-03-19 03:50:32,482 [INFO] ---------------------------------
2019-03-19 03:50:32,483 [INFO] Summary:
2019-03-19 03:50:32,484 [INFO] Batch 73000, worst loss 0.060810 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 03:50:32,484 [INFO] Regularization: 3222.146240 * 0.0000010000 = 0.0032221463
2019-03-19 03:50:32,485 [INFO] Sum of grad norms: 0.045102
2019-03-19 03:50:32,486 [INFO] ---------------------------------
2019-03-19 03:50:51,718 [INFO] ---------------------------------
2019-03-19 03:50:51,719 [INFO] Summary:
2019-03-19 03:50:51,720 [INFO] Batch 74000, worst loss 0.061051 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 03:50:51,720 [INFO] Regularization: 3218.209961 * 0.0000010000 = 0.0032182098
2019-03-19 03:50:51,721 [INFO] Sum of grad norms: 0.043712
2019-03-19 03:50:51,722 [INFO] ---------------------------------
2019-03-19 03:51:10,561 [INFO] ---------------------------------
2019-03-19 03:51:10,562 [INFO] Summary:
2019-03-19 03:51:10,562 [INFO] Batch 75000, worst loss 0.061025 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 03:51:10,563 [INFO] Regularization: 3213.896729 * 0.0000010000 = 0.0032138966
2019-03-19 03:51:10,564 [INFO] Sum of grad norms: 0.028922
2019-03-19 03:51:10,565 [INFO] ---------------------------------
2019-03-19 03:51:29,290 [INFO] ---------------------------------
2019-03-19 03:51:29,291 [INFO] Summary:
2019-03-19 03:51:29,291 [INFO] Batch 76000, worst loss 0.060797 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 03:51:29,292 [INFO] Regularization: 3209.735107 * 0.0000010000 = 0.0032097350
2019-03-19 03:51:29,292 [INFO] Sum of grad norms: 0.078043
2019-03-19 03:51:29,293 [INFO] ---------------------------------
2019-03-19 03:51:48,525 [INFO] ---------------------------------
2019-03-19 03:51:48,526 [INFO] Summary:
2019-03-19 03:51:48,526 [INFO] Batch 77000, worst loss 0.060814 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 03:51:48,527 [INFO] Regularization: 3205.563965 * 0.0000010000 = 0.0032055639
2019-03-19 03:51:48,527 [INFO] Sum of grad norms: 0.109287
2019-03-19 03:51:48,528 [INFO] ---------------------------------
2019-03-19 03:52:07,460 [INFO] ---------------------------------
2019-03-19 03:52:07,461 [INFO] Summary:
2019-03-19 03:52:07,462 [INFO] Batch 78000, worst loss 0.060975 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 03:52:07,462 [INFO] Regularization: 3201.486328 * 0.0000010000 = 0.0032014863
2019-03-19 03:52:07,463 [INFO] Sum of grad norms: 0.067858
2019-03-19 03:52:07,464 [INFO] ---------------------------------
2019-03-19 03:52:26,388 [INFO] ---------------------------------
2019-03-19 03:52:26,389 [INFO] Summary:
2019-03-19 03:52:26,389 [INFO] Batch 79000, worst loss 0.060945 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 03:52:26,390 [INFO] Regularization: 3197.242432 * 0.0000010000 = 0.0031972425
2019-03-19 03:52:26,390 [INFO] Sum of grad norms: 0.029924
2019-03-19 03:52:26,391 [INFO] ---------------------------------
2019-03-19 03:52:45,151 [INFO] ---------------------------------
2019-03-19 03:52:45,151 [INFO] Summary:
2019-03-19 03:52:45,152 [INFO] Batch 80000, worst loss 0.060760 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 03:52:45,153 [INFO] Regularization: 3193.059814 * 0.0000010000 = 0.0031930597
2019-03-19 03:52:45,153 [INFO] Sum of grad norms: 0.057591
2019-03-19 03:52:45,154 [INFO] ---------------------------------
2019-03-19 03:52:50,101 [INFO] ---------------------------------
2019-03-19 03:52:50,102 [INFO] Evaluation:
2019-03-19 03:52:50,103 [INFO] Batch 80000, worst loss 0.057520 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:52:50,103 [INFO] ---------------------------------
2019-03-19 03:53:09,028 [INFO] ---------------------------------
2019-03-19 03:53:09,029 [INFO] Summary:
2019-03-19 03:53:09,029 [INFO] Batch 81000, worst loss 0.060681 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 03:53:09,030 [INFO] Regularization: 3188.770264 * 0.0000010000 = 0.0031887703
2019-03-19 03:53:09,030 [INFO] Sum of grad norms: 0.032897
2019-03-19 03:53:09,031 [INFO] ---------------------------------
2019-03-19 03:53:28,004 [INFO] ---------------------------------
2019-03-19 03:53:28,005 [INFO] Summary:
2019-03-19 03:53:28,006 [INFO] Batch 82000, worst loss 0.060856 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 03:53:28,007 [INFO] Regularization: 3186.150879 * 0.0000010000 = 0.0031861509
2019-03-19 03:53:28,007 [INFO] Sum of grad norms: 0.026145
2019-03-19 03:53:28,008 [INFO] ---------------------------------
2019-03-19 03:53:46,611 [INFO] ---------------------------------
2019-03-19 03:53:46,612 [INFO] Summary:
2019-03-19 03:53:46,613 [INFO] Batch 83000, worst loss 0.060739 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 03:53:46,613 [INFO] Regularization: 3184.281250 * 0.0000010000 = 0.0031842813
2019-03-19 03:53:46,614 [INFO] Sum of grad norms: 0.031043
2019-03-19 03:53:46,615 [INFO] ---------------------------------
2019-03-19 03:54:05,318 [INFO] ---------------------------------
2019-03-19 03:54:05,319 [INFO] Summary:
2019-03-19 03:54:05,320 [INFO] Batch 84000, worst loss 0.060776 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 03:54:05,320 [INFO] Regularization: 3181.943359 * 0.0000010000 = 0.0031819434
2019-03-19 03:54:05,321 [INFO] Sum of grad norms: 0.030547
2019-03-19 03:54:05,322 [INFO] ---------------------------------
2019-03-19 03:54:24,149 [INFO] ---------------------------------
2019-03-19 03:54:24,150 [INFO] Summary:
2019-03-19 03:54:24,151 [INFO] Batch 85000, worst loss 0.060859 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 03:54:24,151 [INFO] Regularization: 3179.766357 * 0.0000010000 = 0.0031797662
2019-03-19 03:54:24,152 [INFO] Sum of grad norms: 0.042398
2019-03-19 03:54:24,153 [INFO] ---------------------------------
2019-03-19 03:54:42,898 [INFO] ---------------------------------
2019-03-19 03:54:42,899 [INFO] Summary:
2019-03-19 03:54:42,900 [INFO] Batch 86000, worst loss 0.060872 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 03:54:42,900 [INFO] Regularization: 3177.568115 * 0.0000010000 = 0.0031775681
2019-03-19 03:54:42,901 [INFO] Sum of grad norms: 0.032940
2019-03-19 03:54:42,901 [INFO] ---------------------------------
2019-03-19 03:55:01,251 [INFO] ---------------------------------
2019-03-19 03:55:01,252 [INFO] Summary:
2019-03-19 03:55:01,253 [INFO] Batch 87000, worst loss 0.060784 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 03:55:01,253 [INFO] Regularization: 3175.304443 * 0.0000010000 = 0.0031753045
2019-03-19 03:55:01,254 [INFO] Sum of grad norms: 0.048450
2019-03-19 03:55:01,255 [INFO] ---------------------------------
2019-03-19 03:55:20,111 [INFO] ---------------------------------
2019-03-19 03:55:20,112 [INFO] Summary:
2019-03-19 03:55:20,113 [INFO] Batch 88000, worst loss 0.060731 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 03:55:20,113 [INFO] Regularization: 3173.089111 * 0.0000010000 = 0.0031730891
2019-03-19 03:55:20,114 [INFO] Sum of grad norms: 0.031533
2019-03-19 03:55:20,114 [INFO] ---------------------------------
2019-03-19 03:55:38,688 [INFO] ---------------------------------
2019-03-19 03:55:38,689 [INFO] Summary:
2019-03-19 03:55:38,690 [INFO] Batch 89000, worst loss 0.060739 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 03:55:38,691 [INFO] Regularization: 3170.770996 * 0.0000010000 = 0.0031707711
2019-03-19 03:55:38,691 [INFO] Sum of grad norms: 0.036016
2019-03-19 03:55:38,692 [INFO] ---------------------------------
2019-03-19 03:55:57,744 [INFO] ---------------------------------
2019-03-19 03:55:57,745 [INFO] Summary:
2019-03-19 03:55:57,746 [INFO] Batch 90000, worst loss 0.060749 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 03:55:57,747 [INFO] Regularization: 3168.700928 * 0.0000010000 = 0.0031687010
2019-03-19 03:55:57,747 [INFO] Sum of grad norms: 0.069905
2019-03-19 03:55:57,748 [INFO] ---------------------------------
2019-03-19 03:56:02,758 [INFO] ---------------------------------
2019-03-19 03:56:02,759 [INFO] Evaluation:
2019-03-19 03:56:02,760 [INFO] Batch 90000, worst loss 0.057476 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:56:02,761 [INFO] New best loss 0.057476, saved to file classifier/1552933539/1552964162_6_classifier_90000.pth
2019-03-19 03:56:02,772 [INFO] Target
2019-03-19 03:56:02,773 [INFO] [[0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-19 03:56:02,776 [INFO] Classifier output
2019-03-19 03:56:02,776 [INFO] [[0.990393 0.99019  0.990432 ... 0.988754 0.989165 0.989559]
 [0.990303 0.990122 0.990279 ... 0.989118 0.989405 0.989643]
 [0.990243 0.990076 0.990175 ... 0.989355 0.989561 0.989698]
 ...
 [0.989523 0.989874 0.990031 ... 0.016924 0.017816 0.019988]
 [0.990323 0.990619 0.98941  ... 0.009874 0.010604 0.009627]
 [0.990423 0.990213 0.990483 ... 0.98863  0.989084 0.98953 ]]
2019-03-19 03:56:02,778 [INFO] ---------------------------------
2019-03-19 03:56:21,570 [INFO] ---------------------------------
2019-03-19 03:56:21,571 [INFO] Summary:
2019-03-19 03:56:21,572 [INFO] Batch 91000, worst loss 0.060814 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 03:56:21,573 [INFO] Regularization: 3166.437256 * 0.0000010000 = 0.0031664371
2019-03-19 03:56:21,573 [INFO] Sum of grad norms: 0.044572
2019-03-19 03:56:21,574 [INFO] ---------------------------------
2019-03-19 03:56:40,447 [INFO] ---------------------------------
2019-03-19 03:56:40,448 [INFO] Summary:
2019-03-19 03:56:40,449 [INFO] Batch 92000, worst loss 0.060775 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 03:56:40,449 [INFO] Regularization: 3165.115234 * 0.0000010000 = 0.0031651151
2019-03-19 03:56:40,450 [INFO] Sum of grad norms: 0.026655
2019-03-19 03:56:40,450 [INFO] ---------------------------------
2019-03-19 03:56:59,391 [INFO] ---------------------------------
2019-03-19 03:56:59,392 [INFO] Summary:
2019-03-19 03:56:59,393 [INFO] Batch 93000, worst loss 0.060767 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 03:56:59,393 [INFO] Regularization: 3163.982910 * 0.0000010000 = 0.0031639829
2019-03-19 03:56:59,394 [INFO] Sum of grad norms: 0.035085
2019-03-19 03:56:59,394 [INFO] ---------------------------------
2019-03-19 03:57:18,208 [INFO] ---------------------------------
2019-03-19 03:57:18,209 [INFO] Summary:
2019-03-19 03:57:18,210 [INFO] Batch 94000, worst loss 0.060660 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 03:57:18,210 [INFO] Regularization: 3162.916748 * 0.0000010000 = 0.0031629167
2019-03-19 03:57:18,211 [INFO] Sum of grad norms: 0.041204
2019-03-19 03:57:18,211 [INFO] ---------------------------------
2019-03-19 03:57:36,799 [INFO] ---------------------------------
2019-03-19 03:57:36,800 [INFO] Summary:
2019-03-19 03:57:36,801 [INFO] Batch 95000, worst loss 0.060771 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 03:57:36,801 [INFO] Regularization: 3161.701172 * 0.0000010000 = 0.0031617011
2019-03-19 03:57:36,802 [INFO] Sum of grad norms: 0.026096
2019-03-19 03:57:36,803 [INFO] ---------------------------------
2019-03-19 03:57:55,758 [INFO] ---------------------------------
2019-03-19 03:57:55,759 [INFO] Summary:
2019-03-19 03:57:55,759 [INFO] Batch 96000, worst loss 0.060780 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 03:57:55,760 [INFO] Regularization: 3160.497803 * 0.0000010000 = 0.0031604979
2019-03-19 03:57:55,760 [INFO] Sum of grad norms: 0.026068
2019-03-19 03:57:55,761 [INFO] ---------------------------------
2019-03-19 03:58:14,410 [INFO] ---------------------------------
2019-03-19 03:58:14,410 [INFO] Summary:
2019-03-19 03:58:14,411 [INFO] Batch 97000, worst loss 0.060671 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 03:58:14,412 [INFO] Regularization: 3159.484375 * 0.0000010000 = 0.0031594844
2019-03-19 03:58:14,412 [INFO] Sum of grad norms: 0.024029
2019-03-19 03:58:14,413 [INFO] ---------------------------------
2019-03-19 03:58:33,250 [INFO] ---------------------------------
2019-03-19 03:58:33,251 [INFO] Summary:
2019-03-19 03:58:33,251 [INFO] Batch 98000, worst loss 0.060702 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 03:58:33,252 [INFO] Regularization: 3158.317627 * 0.0000010000 = 0.0031583176
2019-03-19 03:58:33,252 [INFO] Sum of grad norms: 0.021095
2019-03-19 03:58:33,253 [INFO] ---------------------------------
2019-03-19 03:58:52,481 [INFO] ---------------------------------
2019-03-19 03:58:52,482 [INFO] Summary:
2019-03-19 03:58:52,482 [INFO] Batch 99000, worst loss 0.060685 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 03:58:52,483 [INFO] Regularization: 3157.129639 * 0.0000010000 = 0.0031571297
2019-03-19 03:58:52,483 [INFO] Sum of grad norms: 0.022538
2019-03-19 03:58:52,484 [INFO] ---------------------------------
2019-03-19 03:59:11,262 [INFO] ---------------------------------
2019-03-19 03:59:11,263 [INFO] Summary:
2019-03-19 03:59:11,264 [INFO] Batch 100000, worst loss 0.060641 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 03:59:11,264 [INFO] Regularization: 3156.189453 * 0.0000010000 = 0.0031561893
2019-03-19 03:59:11,265 [INFO] Sum of grad norms: 0.031051
2019-03-19 03:59:11,266 [INFO] ---------------------------------
2019-03-19 03:59:16,358 [INFO] ---------------------------------
2019-03-19 03:59:16,359 [INFO] Evaluation:
2019-03-19 03:59:16,359 [INFO] Batch 100000, worst loss 0.057575 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 03:59:16,361 [INFO] ---------------------------------
2019-03-19 03:59:35,494 [INFO] ---------------------------------
2019-03-19 03:59:35,495 [INFO] Summary:
2019-03-19 03:59:35,496 [INFO] Batch 101000, worst loss 0.060820 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 03:59:35,496 [INFO] Regularization: 3155.021729 * 0.0000010000 = 0.0031550217
2019-03-19 03:59:35,497 [INFO] Sum of grad norms: 0.020299
2019-03-19 03:59:35,498 [INFO] ---------------------------------
2019-03-19 03:59:54,179 [INFO] ---------------------------------
2019-03-19 03:59:54,180 [INFO] Summary:
2019-03-19 03:59:54,180 [INFO] Batch 102000, worst loss 0.060818 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 03:59:54,181 [INFO] Regularization: 3154.354004 * 0.0000010000 = 0.0031543539
2019-03-19 03:59:54,181 [INFO] Sum of grad norms: 0.032431
2019-03-19 03:59:54,182 [INFO] ---------------------------------
2019-03-19 04:00:12,774 [INFO] ---------------------------------
2019-03-19 04:00:12,775 [INFO] Summary:
2019-03-19 04:00:12,776 [INFO] Batch 103000, worst loss 0.060961 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 04:00:12,776 [INFO] Regularization: 3153.781494 * 0.0000010000 = 0.0031537814
2019-03-19 04:00:12,777 [INFO] Sum of grad norms: 0.052938
2019-03-19 04:00:12,778 [INFO] ---------------------------------
2019-03-19 04:00:31,611 [INFO] ---------------------------------
2019-03-19 04:00:31,612 [INFO] Summary:
2019-03-19 04:00:31,612 [INFO] Batch 104000, worst loss 0.060714 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 04:00:31,613 [INFO] Regularization: 3153.169922 * 0.0000010000 = 0.0031531700
2019-03-19 04:00:31,613 [INFO] Sum of grad norms: 0.037281
2019-03-19 04:00:31,614 [INFO] ---------------------------------
2019-03-19 04:00:50,302 [INFO] ---------------------------------
2019-03-19 04:00:50,303 [INFO] Summary:
2019-03-19 04:00:50,304 [INFO] Batch 105000, worst loss 0.060739 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 04:00:50,305 [INFO] Regularization: 3152.608398 * 0.0000010000 = 0.0031526084
2019-03-19 04:00:50,306 [INFO] Sum of grad norms: 0.042844
2019-03-19 04:00:50,307 [INFO] ---------------------------------
2019-03-19 04:01:09,240 [INFO] ---------------------------------
2019-03-19 04:01:09,241 [INFO] Summary:
2019-03-19 04:01:09,241 [INFO] Batch 106000, worst loss 0.060710 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 04:01:09,242 [INFO] Regularization: 3152.046143 * 0.0000010000 = 0.0031520461
2019-03-19 04:01:09,242 [INFO] Sum of grad norms: 0.025414
2019-03-19 04:01:09,243 [INFO] ---------------------------------
2019-03-19 04:01:28,325 [INFO] ---------------------------------
2019-03-19 04:01:28,325 [INFO] Summary:
2019-03-19 04:01:28,326 [INFO] Batch 107000, worst loss 0.060716 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 04:01:28,326 [INFO] Regularization: 3151.474609 * 0.0000010000 = 0.0031514745
2019-03-19 04:01:28,327 [INFO] Sum of grad norms: 0.018671
2019-03-19 04:01:28,328 [INFO] ---------------------------------
2019-03-19 04:01:47,322 [INFO] ---------------------------------
2019-03-19 04:01:47,323 [INFO] Summary:
2019-03-19 04:01:47,324 [INFO] Batch 108000, worst loss 0.060597 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 04:01:47,324 [INFO] Regularization: 3150.876709 * 0.0000010000 = 0.0031508766
2019-03-19 04:01:47,325 [INFO] Sum of grad norms: 0.018414
2019-03-19 04:01:47,325 [INFO] ---------------------------------
2019-03-19 04:02:06,509 [INFO] ---------------------------------
2019-03-19 04:02:06,510 [INFO] Summary:
2019-03-19 04:02:06,511 [INFO] Batch 109000, worst loss 0.060649 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 04:02:06,511 [INFO] Regularization: 3150.230225 * 0.0000010000 = 0.0031502303
2019-03-19 04:02:06,512 [INFO] Sum of grad norms: 0.038877
2019-03-19 04:02:06,513 [INFO] ---------------------------------
2019-03-19 04:02:25,330 [INFO] ---------------------------------
2019-03-19 04:02:25,331 [INFO] Summary:
2019-03-19 04:02:25,331 [INFO] Batch 110000, worst loss 0.060740 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 04:02:25,332 [INFO] Regularization: 3149.731934 * 0.0000010000 = 0.0031497320
2019-03-19 04:02:25,332 [INFO] Sum of grad norms: 0.023757
2019-03-19 04:02:25,333 [INFO] ---------------------------------
2019-03-19 04:02:30,268 [INFO] ---------------------------------
2019-03-19 04:02:30,269 [INFO] Evaluation:
2019-03-19 04:02:30,270 [INFO] Batch 110000, worst loss 0.057696 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:02:30,271 [INFO] ---------------------------------
2019-03-19 04:02:49,281 [INFO] ---------------------------------
2019-03-19 04:02:49,282 [INFO] Summary:
2019-03-19 04:02:49,283 [INFO] Batch 111000, worst loss 0.060843 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 04:02:49,283 [INFO] Regularization: 3149.194336 * 0.0000010000 = 0.0031491944
2019-03-19 04:02:49,284 [INFO] Sum of grad norms: 0.043946
2019-03-19 04:02:49,284 [INFO] ---------------------------------
2019-03-19 04:03:07,680 [INFO] ---------------------------------
2019-03-19 04:03:07,681 [INFO] Summary:
2019-03-19 04:03:07,682 [INFO] Batch 112000, worst loss 0.060724 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 04:03:07,682 [INFO] Regularization: 3148.800781 * 0.0000010000 = 0.0031488007
2019-03-19 04:03:07,683 [INFO] Sum of grad norms: 0.019614
2019-03-19 04:03:07,683 [INFO] ---------------------------------
2019-03-19 04:03:26,424 [INFO] ---------------------------------
2019-03-19 04:03:26,425 [INFO] Summary:
2019-03-19 04:03:26,427 [INFO] Batch 113000, worst loss 0.060828 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 04:03:26,427 [INFO] Regularization: 3148.499756 * 0.0000010000 = 0.0031484996
2019-03-19 04:03:26,428 [INFO] Sum of grad norms: 0.026286
2019-03-19 04:03:26,428 [INFO] ---------------------------------
2019-03-19 04:03:45,075 [INFO] ---------------------------------
2019-03-19 04:03:45,076 [INFO] Summary:
2019-03-19 04:03:45,077 [INFO] Batch 114000, worst loss 0.060821 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 04:03:45,077 [INFO] Regularization: 3148.232666 * 0.0000010000 = 0.0031482326
2019-03-19 04:03:45,078 [INFO] Sum of grad norms: 0.041843
2019-03-19 04:03:45,078 [INFO] ---------------------------------
2019-03-19 04:04:04,081 [INFO] ---------------------------------
2019-03-19 04:04:04,082 [INFO] Summary:
2019-03-19 04:04:04,083 [INFO] Batch 115000, worst loss 0.060640 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 04:04:04,083 [INFO] Regularization: 3147.928467 * 0.0000010000 = 0.0031479285
2019-03-19 04:04:04,084 [INFO] Sum of grad norms: 0.049305
2019-03-19 04:04:04,084 [INFO] ---------------------------------
2019-03-19 04:04:22,826 [INFO] ---------------------------------
2019-03-19 04:04:22,827 [INFO] Summary:
2019-03-19 04:04:22,828 [INFO] Batch 116000, worst loss 0.060760 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 04:04:22,829 [INFO] Regularization: 3147.656982 * 0.0000010000 = 0.0031476570
2019-03-19 04:04:22,829 [INFO] Sum of grad norms: 0.051696
2019-03-19 04:04:22,830 [INFO] ---------------------------------
2019-03-19 04:04:41,462 [INFO] ---------------------------------
2019-03-19 04:04:41,463 [INFO] Summary:
2019-03-19 04:04:41,464 [INFO] Batch 117000, worst loss 0.060783 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 04:04:41,464 [INFO] Regularization: 3147.372314 * 0.0000010000 = 0.0031473723
2019-03-19 04:04:41,465 [INFO] Sum of grad norms: 0.032345
2019-03-19 04:04:41,465 [INFO] ---------------------------------
2019-03-19 04:05:00,110 [INFO] ---------------------------------
2019-03-19 04:05:00,111 [INFO] Summary:
2019-03-19 04:05:00,112 [INFO] Batch 118000, worst loss 0.060884 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 04:05:00,113 [INFO] Regularization: 3147.038086 * 0.0000010000 = 0.0031470382
2019-03-19 04:05:00,113 [INFO] Sum of grad norms: 0.016156
2019-03-19 04:05:00,114 [INFO] ---------------------------------
2019-03-19 04:05:18,823 [INFO] ---------------------------------
2019-03-19 04:05:18,824 [INFO] Summary:
2019-03-19 04:05:18,824 [INFO] Batch 119000, worst loss 0.060659 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 04:05:18,825 [INFO] Regularization: 3146.737793 * 0.0000010000 = 0.0031467378
2019-03-19 04:05:18,825 [INFO] Sum of grad norms: 0.026738
2019-03-19 04:05:18,826 [INFO] ---------------------------------
2019-03-19 04:05:37,586 [INFO] ---------------------------------
2019-03-19 04:05:37,587 [INFO] Summary:
2019-03-19 04:05:37,587 [INFO] Batch 120000, worst loss 0.060605 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 04:05:37,588 [INFO] Regularization: 3146.450684 * 0.0000010000 = 0.0031464507
2019-03-19 04:05:37,588 [INFO] Sum of grad norms: 0.040702
2019-03-19 04:05:37,589 [INFO] ---------------------------------
2019-03-19 04:05:42,457 [INFO] ---------------------------------
2019-03-19 04:05:42,457 [INFO] Evaluation:
2019-03-19 04:05:42,459 [INFO] Batch 120000, worst loss 0.057476 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:05:42,460 [INFO] New best loss 0.057476, saved to file classifier/1552933539/1552964742_6_classifier_120000.pth
2019-03-19 04:05:42,471 [INFO] Target
2019-03-19 04:05:42,472 [INFO] [[0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 ...
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-19 04:05:42,474 [INFO] Classifier output
2019-03-19 04:05:42,474 [INFO] [[0.011881 0.012603 0.011644 ... 0.989289 0.990608 0.98932 ]
 [0.011881 0.012603 0.011644 ... 0.989289 0.990608 0.98932 ]
 [0.990591 0.990521 0.98873  ... 0.006896 0.00733  0.007639]
 ...
 [0.989684 0.989057 0.990045 ... 0.016136 0.006491 0.011882]
 [0.990591 0.990521 0.98873  ... 0.006896 0.00733  0.007639]
 [0.989733 0.989833 0.989826 ... 0.988273 0.987664 0.986375]]
2019-03-19 04:05:42,476 [INFO] ---------------------------------
2019-03-19 04:06:01,704 [INFO] ---------------------------------
2019-03-19 04:06:01,705 [INFO] Summary:
2019-03-19 04:06:01,706 [INFO] Batch 121000, worst loss 0.060536 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:06:01,707 [INFO] Regularization: 3146.152100 * 0.0000010000 = 0.0031461520
2019-03-19 04:06:01,707 [INFO] Sum of grad norms: 0.037033
2019-03-19 04:06:01,708 [INFO] ---------------------------------
2019-03-19 04:06:20,855 [INFO] ---------------------------------
2019-03-19 04:06:20,856 [INFO] Summary:
2019-03-19 04:06:20,857 [INFO] Batch 122000, worst loss 0.060532 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:06:20,857 [INFO] Regularization: 3145.949707 * 0.0000010000 = 0.0031459497
2019-03-19 04:06:20,858 [INFO] Sum of grad norms: 0.020069
2019-03-19 04:06:20,859 [INFO] ---------------------------------
2019-03-19 04:06:39,747 [INFO] ---------------------------------
2019-03-19 04:06:39,748 [INFO] Summary:
2019-03-19 04:06:39,748 [INFO] Batch 123000, worst loss 0.060615 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:06:39,749 [INFO] Regularization: 3145.798340 * 0.0000010000 = 0.0031457983
2019-03-19 04:06:39,749 [INFO] Sum of grad norms: 0.021527
2019-03-19 04:06:39,750 [INFO] ---------------------------------
2019-03-19 04:06:58,425 [INFO] ---------------------------------
2019-03-19 04:06:58,426 [INFO] Summary:
2019-03-19 04:06:58,426 [INFO] Batch 124000, worst loss 0.060829 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:06:58,427 [INFO] Regularization: 3145.627686 * 0.0000010000 = 0.0031456277
2019-03-19 04:06:58,427 [INFO] Sum of grad norms: 0.051371
2019-03-19 04:06:58,428 [INFO] ---------------------------------
2019-03-19 04:07:17,526 [INFO] ---------------------------------
2019-03-19 04:07:17,527 [INFO] Summary:
2019-03-19 04:07:17,527 [INFO] Batch 125000, worst loss 0.060641 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:07:17,528 [INFO] Regularization: 3145.480469 * 0.0000010000 = 0.0031454805
2019-03-19 04:07:17,528 [INFO] Sum of grad norms: 0.022152
2019-03-19 04:07:17,529 [INFO] ---------------------------------
2019-03-19 04:07:36,339 [INFO] ---------------------------------
2019-03-19 04:07:36,340 [INFO] Summary:
2019-03-19 04:07:36,340 [INFO] Batch 126000, worst loss 0.060735 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:07:36,341 [INFO] Regularization: 3145.363770 * 0.0000010000 = 0.0031453636
2019-03-19 04:07:36,342 [INFO] Sum of grad norms: 0.019522
2019-03-19 04:07:36,342 [INFO] ---------------------------------
2019-03-19 04:07:55,080 [INFO] ---------------------------------
2019-03-19 04:07:55,080 [INFO] Summary:
2019-03-19 04:07:55,081 [INFO] Batch 127000, worst loss 0.060662 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:07:55,082 [INFO] Regularization: 3145.187012 * 0.0000010000 = 0.0031451869
2019-03-19 04:07:55,082 [INFO] Sum of grad norms: 0.020477
2019-03-19 04:07:55,083 [INFO] ---------------------------------
2019-03-19 04:08:13,769 [INFO] ---------------------------------
2019-03-19 04:08:13,770 [INFO] Summary:
2019-03-19 04:08:13,772 [INFO] Batch 128000, worst loss 0.060791 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:08:13,773 [INFO] Regularization: 3145.026855 * 0.0000010000 = 0.0031450267
2019-03-19 04:08:13,773 [INFO] Sum of grad norms: 0.036580
2019-03-19 04:08:13,774 [INFO] ---------------------------------
2019-03-19 04:08:32,701 [INFO] ---------------------------------
2019-03-19 04:08:32,702 [INFO] Summary:
2019-03-19 04:08:32,703 [INFO] Batch 129000, worst loss 0.060761 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:08:32,703 [INFO] Regularization: 3144.885742 * 0.0000010000 = 0.0031448856
2019-03-19 04:08:32,704 [INFO] Sum of grad norms: 0.020863
2019-03-19 04:08:32,705 [INFO] ---------------------------------
2019-03-19 04:08:51,433 [INFO] ---------------------------------
2019-03-19 04:08:51,434 [INFO] Summary:
2019-03-19 04:08:51,434 [INFO] Batch 130000, worst loss 0.060659 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:08:51,435 [INFO] Regularization: 3144.744629 * 0.0000010000 = 0.0031447446
2019-03-19 04:08:51,435 [INFO] Sum of grad norms: 0.030859
2019-03-19 04:08:51,436 [INFO] ---------------------------------
2019-03-19 04:08:56,333 [INFO] ---------------------------------
2019-03-19 04:08:56,334 [INFO] Evaluation:
2019-03-19 04:08:56,335 [INFO] Batch 130000, worst loss 0.057617 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:08:56,336 [INFO] ---------------------------------
2019-03-19 04:09:15,012 [INFO] ---------------------------------
2019-03-19 04:09:15,013 [INFO] Summary:
2019-03-19 04:09:15,013 [INFO] Batch 131000, worst loss 0.060634 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:09:15,014 [INFO] Regularization: 3144.614746 * 0.0000010000 = 0.0031446146
2019-03-19 04:09:15,014 [INFO] Sum of grad norms: 0.031951
2019-03-19 04:09:15,015 [INFO] ---------------------------------
2019-03-19 04:09:33,878 [INFO] ---------------------------------
2019-03-19 04:09:33,879 [INFO] Summary:
2019-03-19 04:09:33,879 [INFO] Batch 132000, worst loss 0.060732 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:09:33,880 [INFO] Regularization: 3144.510498 * 0.0000010000 = 0.0031445106
2019-03-19 04:09:33,880 [INFO] Sum of grad norms: 0.021141
2019-03-19 04:09:33,881 [INFO] ---------------------------------
2019-03-19 04:09:53,230 [INFO] ---------------------------------
2019-03-19 04:09:53,231 [INFO] Summary:
2019-03-19 04:09:53,232 [INFO] Batch 133000, worst loss 0.060823 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:09:53,232 [INFO] Regularization: 3144.443359 * 0.0000010000 = 0.0031444433
2019-03-19 04:09:53,233 [INFO] Sum of grad norms: 0.024051
2019-03-19 04:09:53,234 [INFO] ---------------------------------
2019-03-19 04:10:11,883 [INFO] ---------------------------------
2019-03-19 04:10:11,884 [INFO] Summary:
2019-03-19 04:10:11,885 [INFO] Batch 134000, worst loss 0.060716 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:10:11,885 [INFO] Regularization: 3144.372803 * 0.0000010000 = 0.0031443727
2019-03-19 04:10:11,886 [INFO] Sum of grad norms: 0.029898
2019-03-19 04:10:11,886 [INFO] ---------------------------------
2019-03-19 04:10:30,766 [INFO] ---------------------------------
2019-03-19 04:10:30,767 [INFO] Summary:
2019-03-19 04:10:30,768 [INFO] Batch 135000, worst loss 0.060655 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:10:30,768 [INFO] Regularization: 3144.275879 * 0.0000010000 = 0.0031442759
2019-03-19 04:10:30,769 [INFO] Sum of grad norms: 0.046484
2019-03-19 04:10:30,770 [INFO] ---------------------------------
2019-03-19 04:10:49,382 [INFO] ---------------------------------
2019-03-19 04:10:49,383 [INFO] Summary:
2019-03-19 04:10:49,383 [INFO] Batch 136000, worst loss 0.060655 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:10:49,384 [INFO] Regularization: 3144.188232 * 0.0000010000 = 0.0031441883
2019-03-19 04:10:49,384 [INFO] Sum of grad norms: 0.024778
2019-03-19 04:10:49,385 [INFO] ---------------------------------
2019-03-19 04:11:08,262 [INFO] ---------------------------------
2019-03-19 04:11:08,263 [INFO] Summary:
2019-03-19 04:11:08,264 [INFO] Batch 137000, worst loss 0.060822 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:11:08,265 [INFO] Regularization: 3144.122070 * 0.0000010000 = 0.0031441220
2019-03-19 04:11:08,265 [INFO] Sum of grad norms: 0.017915
2019-03-19 04:11:08,266 [INFO] ---------------------------------
2019-03-19 04:11:27,233 [INFO] ---------------------------------
2019-03-19 04:11:27,234 [INFO] Summary:
2019-03-19 04:11:27,235 [INFO] Batch 138000, worst loss 0.060752 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:11:27,235 [INFO] Regularization: 3144.055664 * 0.0000010000 = 0.0031440556
2019-03-19 04:11:27,236 [INFO] Sum of grad norms: 0.033771
2019-03-19 04:11:27,236 [INFO] ---------------------------------
2019-03-19 04:11:46,374 [INFO] ---------------------------------
2019-03-19 04:11:46,375 [INFO] Summary:
2019-03-19 04:11:46,375 [INFO] Batch 139000, worst loss 0.060574 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:11:46,376 [INFO] Regularization: 3143.996338 * 0.0000010000 = 0.0031439962
2019-03-19 04:11:46,377 [INFO] Sum of grad norms: 0.018441
2019-03-19 04:11:46,377 [INFO] ---------------------------------
2019-03-19 04:12:05,018 [INFO] ---------------------------------
2019-03-19 04:12:05,020 [INFO] Summary:
2019-03-19 04:12:05,020 [INFO] Batch 140000, worst loss 0.060622 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 04:12:05,021 [INFO] Regularization: 3143.908691 * 0.0000010000 = 0.0031439087
2019-03-19 04:12:05,021 [INFO] Sum of grad norms: 0.022141
2019-03-19 04:12:05,022 [INFO] ---------------------------------
2019-03-19 04:12:09,934 [INFO] ---------------------------------
2019-03-19 04:12:09,935 [INFO] Evaluation:
2019-03-19 04:12:09,936 [INFO] Batch 140000, worst loss 0.057560 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:12:09,937 [INFO] ---------------------------------
2019-03-19 04:12:28,775 [INFO] ---------------------------------
2019-03-19 04:12:28,776 [INFO] Summary:
2019-03-19 04:12:28,777 [INFO] Batch 141000, worst loss 0.060704 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:12:28,777 [INFO] Regularization: 3143.834717 * 0.0000010000 = 0.0031438346
2019-03-19 04:12:28,778 [INFO] Sum of grad norms: 0.019436
2019-03-19 04:12:28,778 [INFO] ---------------------------------
2019-03-19 04:12:47,803 [INFO] ---------------------------------
2019-03-19 04:12:47,803 [INFO] Summary:
2019-03-19 04:12:47,804 [INFO] Batch 142000, worst loss 0.060725 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:12:47,805 [INFO] Regularization: 3143.793701 * 0.0000010000 = 0.0031437937
2019-03-19 04:12:47,805 [INFO] Sum of grad norms: 0.023915
2019-03-19 04:12:47,806 [INFO] ---------------------------------
2019-03-19 04:13:06,532 [INFO] ---------------------------------
2019-03-19 04:13:06,533 [INFO] Summary:
2019-03-19 04:13:06,533 [INFO] Batch 143000, worst loss 0.060666 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:13:06,534 [INFO] Regularization: 3143.749268 * 0.0000010000 = 0.0031437492
2019-03-19 04:13:06,534 [INFO] Sum of grad norms: 0.029517
2019-03-19 04:13:06,535 [INFO] ---------------------------------
2019-03-19 04:13:25,640 [INFO] ---------------------------------
2019-03-19 04:13:25,642 [INFO] Summary:
2019-03-19 04:13:25,642 [INFO] Batch 144000, worst loss 0.060754 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:13:25,643 [INFO] Regularization: 3143.704590 * 0.0000010000 = 0.0031437045
2019-03-19 04:13:25,643 [INFO] Sum of grad norms: 0.022380
2019-03-19 04:13:25,644 [INFO] ---------------------------------
2019-03-19 04:13:44,931 [INFO] ---------------------------------
2019-03-19 04:13:44,932 [INFO] Summary:
2019-03-19 04:13:44,933 [INFO] Batch 145000, worst loss 0.060726 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:13:44,933 [INFO] Regularization: 3143.665283 * 0.0000010000 = 0.0031436654
2019-03-19 04:13:44,934 [INFO] Sum of grad norms: 0.021423
2019-03-19 04:13:44,935 [INFO] ---------------------------------
2019-03-19 04:14:03,847 [INFO] ---------------------------------
2019-03-19 04:14:03,848 [INFO] Summary:
2019-03-19 04:14:03,849 [INFO] Batch 146000, worst loss 0.060839 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:14:03,849 [INFO] Regularization: 3143.632568 * 0.0000010000 = 0.0031436326
2019-03-19 04:14:03,850 [INFO] Sum of grad norms: 0.021070
2019-03-19 04:14:03,851 [INFO] ---------------------------------
2019-03-19 04:14:22,613 [INFO] ---------------------------------
2019-03-19 04:14:22,614 [INFO] Summary:
2019-03-19 04:14:22,615 [INFO] Batch 147000, worst loss 0.060624 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:14:22,615 [INFO] Regularization: 3143.588135 * 0.0000010000 = 0.0031435881
2019-03-19 04:14:22,616 [INFO] Sum of grad norms: 0.034274
2019-03-19 04:14:22,617 [INFO] ---------------------------------
2019-03-19 04:14:41,428 [INFO] ---------------------------------
2019-03-19 04:14:41,429 [INFO] Summary:
2019-03-19 04:14:41,430 [INFO] Batch 148000, worst loss 0.060638 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:14:41,430 [INFO] Regularization: 3143.553467 * 0.0000010000 = 0.0031435534
2019-03-19 04:14:41,431 [INFO] Sum of grad norms: 0.026916
2019-03-19 04:14:41,431 [INFO] ---------------------------------
2019-03-19 04:14:59,878 [INFO] ---------------------------------
2019-03-19 04:14:59,879 [INFO] Summary:
2019-03-19 04:14:59,880 [INFO] Batch 149000, worst loss 0.060678 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:14:59,880 [INFO] Regularization: 3143.515381 * 0.0000010000 = 0.0031435154
2019-03-19 04:14:59,881 [INFO] Sum of grad norms: 0.030562
2019-03-19 04:14:59,882 [INFO] ---------------------------------
2019-03-19 04:15:18,592 [INFO] ---------------------------------
2019-03-19 04:15:18,593 [INFO] Summary:
2019-03-19 04:15:18,594 [INFO] Batch 150000, worst loss 0.060678 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:15:18,595 [INFO] Regularization: 3143.479980 * 0.0000010000 = 0.0031434800
2019-03-19 04:15:18,595 [INFO] Sum of grad norms: 0.032022
2019-03-19 04:15:18,596 [INFO] ---------------------------------
2019-03-19 04:15:23,544 [INFO] ---------------------------------
2019-03-19 04:15:23,545 [INFO] Evaluation:
2019-03-19 04:15:23,548 [INFO] Batch 150000, worst loss 0.057485 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:15:23,550 [INFO] ---------------------------------
2019-03-19 04:15:42,674 [INFO] ---------------------------------
2019-03-19 04:15:42,675 [INFO] Summary:
2019-03-19 04:15:42,675 [INFO] Batch 151000, worst loss 0.060656 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:15:42,676 [INFO] Regularization: 3143.438721 * 0.0000010000 = 0.0031434386
2019-03-19 04:15:42,676 [INFO] Sum of grad norms: 0.056161
2019-03-19 04:15:42,677 [INFO] ---------------------------------
2019-03-19 04:16:01,629 [INFO] ---------------------------------
2019-03-19 04:16:01,630 [INFO] Summary:
2019-03-19 04:16:01,630 [INFO] Batch 152000, worst loss 0.060649 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:16:01,631 [INFO] Regularization: 3143.413574 * 0.0000010000 = 0.0031434135
2019-03-19 04:16:01,631 [INFO] Sum of grad norms: 0.047362
2019-03-19 04:16:01,632 [INFO] ---------------------------------
2019-03-19 04:16:20,226 [INFO] ---------------------------------
2019-03-19 04:16:20,227 [INFO] Summary:
2019-03-19 04:16:20,228 [INFO] Batch 153000, worst loss 0.060649 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:16:20,228 [INFO] Regularization: 3143.396484 * 0.0000010000 = 0.0031433965
2019-03-19 04:16:20,229 [INFO] Sum of grad norms: 0.024404
2019-03-19 04:16:20,230 [INFO] ---------------------------------
2019-03-19 04:16:39,257 [INFO] ---------------------------------
2019-03-19 04:16:39,258 [INFO] Summary:
2019-03-19 04:16:39,259 [INFO] Batch 154000, worst loss 0.060640 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:16:39,259 [INFO] Regularization: 3143.381348 * 0.0000010000 = 0.0031433813
2019-03-19 04:16:39,260 [INFO] Sum of grad norms: 0.033896
2019-03-19 04:16:39,260 [INFO] ---------------------------------
2019-03-19 04:16:58,005 [INFO] ---------------------------------
2019-03-19 04:16:58,006 [INFO] Summary:
2019-03-19 04:16:58,006 [INFO] Batch 155000, worst loss 0.060765 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:16:58,007 [INFO] Regularization: 3143.363770 * 0.0000010000 = 0.0031433639
2019-03-19 04:16:58,007 [INFO] Sum of grad norms: 0.024140
2019-03-19 04:16:58,008 [INFO] ---------------------------------
2019-03-19 04:17:17,187 [INFO] ---------------------------------
2019-03-19 04:17:17,188 [INFO] Summary:
2019-03-19 04:17:17,188 [INFO] Batch 156000, worst loss 0.060765 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:17:17,189 [INFO] Regularization: 3143.345947 * 0.0000010000 = 0.0031433459
2019-03-19 04:17:17,189 [INFO] Sum of grad norms: 0.029896
2019-03-19 04:17:17,190 [INFO] ---------------------------------
2019-03-19 04:17:35,922 [INFO] ---------------------------------
2019-03-19 04:17:35,923 [INFO] Summary:
2019-03-19 04:17:35,924 [INFO] Batch 157000, worst loss 0.060694 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:17:35,924 [INFO] Regularization: 3143.330322 * 0.0000010000 = 0.0031433303
2019-03-19 04:17:35,925 [INFO] Sum of grad norms: 0.023540
2019-03-19 04:17:35,926 [INFO] ---------------------------------
2019-03-19 04:17:54,712 [INFO] ---------------------------------
2019-03-19 04:17:54,713 [INFO] Summary:
2019-03-19 04:17:54,714 [INFO] Batch 158000, worst loss 0.060693 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:17:54,714 [INFO] Regularization: 3143.308350 * 0.0000010000 = 0.0031433085
2019-03-19 04:17:54,715 [INFO] Sum of grad norms: 0.045206
2019-03-19 04:17:54,716 [INFO] ---------------------------------
2019-03-19 04:18:13,686 [INFO] ---------------------------------
2019-03-19 04:18:13,687 [INFO] Summary:
2019-03-19 04:18:13,688 [INFO] Batch 159000, worst loss 0.060707 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:18:13,688 [INFO] Regularization: 3143.293213 * 0.0000010000 = 0.0031432933
2019-03-19 04:18:13,689 [INFO] Sum of grad norms: 0.032480
2019-03-19 04:18:13,689 [INFO] ---------------------------------
2019-03-19 04:18:32,765 [INFO] ---------------------------------
2019-03-19 04:18:32,766 [INFO] Summary:
2019-03-19 04:18:32,766 [INFO] Batch 160000, worst loss 0.060707 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:18:32,767 [INFO] Regularization: 3143.273193 * 0.0000010000 = 0.0031432733
2019-03-19 04:18:32,767 [INFO] Sum of grad norms: 0.052580
2019-03-19 04:18:32,768 [INFO] ---------------------------------
2019-03-19 04:18:37,715 [INFO] ---------------------------------
2019-03-19 04:18:37,716 [INFO] Evaluation:
2019-03-19 04:18:37,717 [INFO] Batch 160000, worst loss 0.057385 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:18:37,718 [INFO] New best loss 0.057385, saved to file classifier/1552933539/1552965517_6_classifier_160000.pth
2019-03-19 04:18:37,728 [INFO] Target
2019-03-19 04:18:37,729 [INFO] [[0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 ...
 [0.01 0.01 0.01 ... 0.99 0.99 0.99]
 [0.99 0.99 0.99 ... 0.01 0.01 0.01]
 [0.99 0.99 0.99 ... 0.99 0.99 0.99]]
2019-03-19 04:18:37,731 [INFO] Classifier output
2019-03-19 04:18:37,732 [INFO] [[0.989631 0.990487 0.990187 ... 0.01598  0.015024 0.016576]
 [0.990039 0.990785 0.989739 ... 0.009064 0.011562 0.009211]
 [0.008426 0.009066 0.00804  ... 0.990343 0.990874 0.992735]
 ...
 [0.010492 0.010099 0.009867 ... 0.99121  0.99006  0.991386]
 [0.990453 0.990433 0.990109 ... 0.01364  0.012656 0.013207]
 [0.990224 0.990056 0.990228 ... 0.989438 0.989688 0.98992 ]]
2019-03-19 04:18:37,734 [INFO] ---------------------------------
2019-03-19 04:18:56,536 [INFO] ---------------------------------
2019-03-19 04:18:56,537 [INFO] Summary:
2019-03-19 04:18:56,537 [INFO] Batch 161000, worst loss 0.060722 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:18:56,538 [INFO] Regularization: 3143.255127 * 0.0000010000 = 0.0031432551
2019-03-19 04:18:56,539 [INFO] Sum of grad norms: 0.035534
2019-03-19 04:18:56,539 [INFO] ---------------------------------
2019-03-19 04:19:15,412 [INFO] ---------------------------------
2019-03-19 04:19:15,413 [INFO] Summary:
2019-03-19 04:19:15,413 [INFO] Batch 162000, worst loss 0.060722 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:19:15,414 [INFO] Regularization: 3143.241699 * 0.0000010000 = 0.0031432416
2019-03-19 04:19:15,414 [INFO] Sum of grad norms: 0.032164
2019-03-19 04:19:15,415 [INFO] ---------------------------------
2019-03-19 04:19:34,174 [INFO] ---------------------------------
2019-03-19 04:19:34,175 [INFO] Summary:
2019-03-19 04:19:34,176 [INFO] Batch 163000, worst loss 0.060698 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:19:34,177 [INFO] Regularization: 3143.230225 * 0.0000010000 = 0.0031432302
2019-03-19 04:19:34,177 [INFO] Sum of grad norms: 0.030626
2019-03-19 04:19:34,178 [INFO] ---------------------------------
2019-03-19 04:19:53,424 [INFO] ---------------------------------
2019-03-19 04:19:53,425 [INFO] Summary:
2019-03-19 04:19:53,425 [INFO] Batch 164000, worst loss 0.060801 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:19:53,426 [INFO] Regularization: 3143.221436 * 0.0000010000 = 0.0031432214
2019-03-19 04:19:53,426 [INFO] Sum of grad norms: 0.045261
2019-03-19 04:19:53,427 [INFO] ---------------------------------
2019-03-19 04:20:12,207 [INFO] ---------------------------------
2019-03-19 04:20:12,208 [INFO] Summary:
2019-03-19 04:20:12,209 [INFO] Batch 165000, worst loss 0.060708 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:20:12,209 [INFO] Regularization: 3143.211914 * 0.0000010000 = 0.0031432118
2019-03-19 04:20:12,210 [INFO] Sum of grad norms: 0.031794
2019-03-19 04:20:12,211 [INFO] ---------------------------------
2019-03-19 04:20:31,209 [INFO] ---------------------------------
2019-03-19 04:20:31,210 [INFO] Summary:
2019-03-19 04:20:31,211 [INFO] Batch 166000, worst loss 0.060720 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:20:31,211 [INFO] Regularization: 3143.204590 * 0.0000010000 = 0.0031432046
2019-03-19 04:20:31,212 [INFO] Sum of grad norms: 0.031227
2019-03-19 04:20:31,212 [INFO] ---------------------------------
2019-03-19 04:20:50,005 [INFO] ---------------------------------
2019-03-19 04:20:50,006 [INFO] Summary:
2019-03-19 04:20:50,007 [INFO] Batch 167000, worst loss 0.060733 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:20:50,008 [INFO] Regularization: 3143.196777 * 0.0000010000 = 0.0031431967
2019-03-19 04:20:50,008 [INFO] Sum of grad norms: 0.024821
2019-03-19 04:20:50,009 [INFO] ---------------------------------
2019-03-19 04:21:08,809 [INFO] ---------------------------------
2019-03-19 04:21:08,810 [INFO] Summary:
2019-03-19 04:21:08,811 [INFO] Batch 168000, worst loss 0.060678 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:21:08,811 [INFO] Regularization: 3143.186279 * 0.0000010000 = 0.0031431862
2019-03-19 04:21:08,812 [INFO] Sum of grad norms: 0.017175
2019-03-19 04:21:08,812 [INFO] ---------------------------------
2019-03-19 04:21:27,170 [INFO] ---------------------------------
2019-03-19 04:21:27,171 [INFO] Summary:
2019-03-19 04:21:27,172 [INFO] Batch 169000, worst loss 0.060678 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:21:27,172 [INFO] Regularization: 3143.178711 * 0.0000010000 = 0.0031431788
2019-03-19 04:21:27,173 [INFO] Sum of grad norms: 0.017086
2019-03-19 04:21:27,173 [INFO] ---------------------------------
2019-03-19 04:21:46,053 [INFO] ---------------------------------
2019-03-19 04:21:46,054 [INFO] Summary:
2019-03-19 04:21:46,055 [INFO] Batch 170000, worst loss 0.060706 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:21:46,056 [INFO] Regularization: 3143.170898 * 0.0000010000 = 0.0031431708
2019-03-19 04:21:46,056 [INFO] Sum of grad norms: 0.046665
2019-03-19 04:21:46,057 [INFO] ---------------------------------
2019-03-19 04:21:50,956 [INFO] ---------------------------------
2019-03-19 04:21:50,957 [INFO] Evaluation:
2019-03-19 04:21:50,958 [INFO] Batch 170000, worst loss 0.057496 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:21:50,959 [INFO] ---------------------------------
2019-03-19 04:22:10,163 [INFO] ---------------------------------
2019-03-19 04:22:10,163 [INFO] Summary:
2019-03-19 04:22:10,164 [INFO] Batch 171000, worst loss 0.060703 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:22:10,165 [INFO] Regularization: 3143.164551 * 0.0000010000 = 0.0031431646
2019-03-19 04:22:10,165 [INFO] Sum of grad norms: 0.032756
2019-03-19 04:22:10,166 [INFO] ---------------------------------
2019-03-19 04:22:29,724 [INFO] ---------------------------------
2019-03-19 04:22:29,725 [INFO] Summary:
2019-03-19 04:22:29,726 [INFO] Batch 172000, worst loss 0.060651 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:22:29,727 [INFO] Regularization: 3143.159424 * 0.0000010000 = 0.0031431594
2019-03-19 04:22:29,727 [INFO] Sum of grad norms: 0.016484
2019-03-19 04:22:29,728 [INFO] ---------------------------------
2019-03-19 04:22:48,753 [INFO] ---------------------------------
2019-03-19 04:22:48,754 [INFO] Summary:
2019-03-19 04:22:48,754 [INFO] Batch 173000, worst loss 0.060679 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:22:48,755 [INFO] Regularization: 3143.154785 * 0.0000010000 = 0.0031431548
2019-03-19 04:22:48,756 [INFO] Sum of grad norms: 0.034670
2019-03-19 04:22:48,756 [INFO] ---------------------------------
2019-03-19 04:23:07,377 [INFO] ---------------------------------
2019-03-19 04:23:07,378 [INFO] Summary:
2019-03-19 04:23:07,379 [INFO] Batch 174000, worst loss 0.060893 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:23:07,379 [INFO] Regularization: 3143.151367 * 0.0000010000 = 0.0031431513
2019-03-19 04:23:07,380 [INFO] Sum of grad norms: 0.023292
2019-03-19 04:23:07,380 [INFO] ---------------------------------
2019-03-19 04:23:26,045 [INFO] ---------------------------------
2019-03-19 04:23:26,046 [INFO] Summary:
2019-03-19 04:23:26,047 [INFO] Batch 175000, worst loss 0.060631 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:23:26,047 [INFO] Regularization: 3143.148438 * 0.0000010000 = 0.0031431485
2019-03-19 04:23:26,048 [INFO] Sum of grad norms: 0.032072
2019-03-19 04:23:26,049 [INFO] ---------------------------------
2019-03-19 04:23:45,288 [INFO] ---------------------------------
2019-03-19 04:23:45,290 [INFO] Summary:
2019-03-19 04:23:45,290 [INFO] Batch 176000, worst loss 0.060631 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:23:45,291 [INFO] Regularization: 3143.144531 * 0.0000010000 = 0.0031431445
2019-03-19 04:23:45,291 [INFO] Sum of grad norms: 0.029140
2019-03-19 04:23:45,292 [INFO] ---------------------------------
2019-03-19 04:24:04,036 [INFO] ---------------------------------
2019-03-19 04:24:04,038 [INFO] Summary:
2019-03-19 04:24:04,038 [INFO] Batch 177000, worst loss 0.060584 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:24:04,039 [INFO] Regularization: 3143.141113 * 0.0000010000 = 0.0031431410
2019-03-19 04:24:04,039 [INFO] Sum of grad norms: 0.050207
2019-03-19 04:24:04,040 [INFO] ---------------------------------
2019-03-19 04:24:22,645 [INFO] ---------------------------------
2019-03-19 04:24:22,646 [INFO] Summary:
2019-03-19 04:24:22,647 [INFO] Batch 178000, worst loss 0.060751 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:24:22,647 [INFO] Regularization: 3143.136475 * 0.0000010000 = 0.0031431364
2019-03-19 04:24:22,648 [INFO] Sum of grad norms: 0.041096
2019-03-19 04:24:22,649 [INFO] ---------------------------------
2019-03-19 04:24:41,200 [INFO] ---------------------------------
2019-03-19 04:24:41,201 [INFO] Summary:
2019-03-19 04:24:41,201 [INFO] Batch 179000, worst loss 0.060751 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:24:41,202 [INFO] Regularization: 3143.132812 * 0.0000010000 = 0.0031431329
2019-03-19 04:24:41,202 [INFO] Sum of grad norms: 0.037595
2019-03-19 04:24:41,203 [INFO] ---------------------------------
2019-03-19 04:24:59,951 [INFO] ---------------------------------
2019-03-19 04:24:59,952 [INFO] Summary:
2019-03-19 04:24:59,953 [INFO] Batch 180000, worst loss 0.060714 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:24:59,953 [INFO] Regularization: 3143.130615 * 0.0000010000 = 0.0031431306
2019-03-19 04:24:59,954 [INFO] Sum of grad norms: 0.021276
2019-03-19 04:24:59,955 [INFO] ---------------------------------
2019-03-19 04:25:04,932 [INFO] ---------------------------------
2019-03-19 04:25:04,933 [INFO] Evaluation:
2019-03-19 04:25:04,934 [INFO] Batch 180000, worst loss 0.057651 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:25:04,938 [INFO] ---------------------------------
2019-03-19 04:25:23,914 [INFO] ---------------------------------
2019-03-19 04:25:23,915 [INFO] Summary:
2019-03-19 04:25:23,915 [INFO] Batch 181000, worst loss 0.060794 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:25:23,916 [INFO] Regularization: 3143.127930 * 0.0000010000 = 0.0031431280
2019-03-19 04:25:23,917 [INFO] Sum of grad norms: 0.047017
2019-03-19 04:25:23,917 [INFO] ---------------------------------
2019-03-19 04:25:42,924 [INFO] ---------------------------------
2019-03-19 04:25:42,925 [INFO] Summary:
2019-03-19 04:25:42,926 [INFO] Batch 182000, worst loss 0.060678 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:25:42,927 [INFO] Regularization: 3143.126953 * 0.0000010000 = 0.0031431268
2019-03-19 04:25:42,927 [INFO] Sum of grad norms: 0.023942
2019-03-19 04:25:42,928 [INFO] ---------------------------------
2019-03-19 04:26:01,449 [INFO] ---------------------------------
2019-03-19 04:26:01,450 [INFO] Summary:
2019-03-19 04:26:01,451 [INFO] Batch 183000, worst loss 0.060627 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:26:01,452 [INFO] Regularization: 3143.124756 * 0.0000010000 = 0.0031431247
2019-03-19 04:26:01,452 [INFO] Sum of grad norms: 0.021820
2019-03-19 04:26:01,453 [INFO] ---------------------------------
2019-03-19 04:26:20,290 [INFO] ---------------------------------
2019-03-19 04:26:20,291 [INFO] Summary:
2019-03-19 04:26:20,292 [INFO] Batch 184000, worst loss 0.060655 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:26:20,293 [INFO] Regularization: 3143.124756 * 0.0000010000 = 0.0031431247
2019-03-19 04:26:20,293 [INFO] Sum of grad norms: 0.019324
2019-03-19 04:26:20,294 [INFO] ---------------------------------
2019-03-19 04:26:39,240 [INFO] ---------------------------------
2019-03-19 04:26:39,240 [INFO] Summary:
2019-03-19 04:26:39,241 [INFO] Batch 185000, worst loss 0.060668 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:26:39,241 [INFO] Regularization: 3143.123535 * 0.0000010000 = 0.0031431236
2019-03-19 04:26:39,242 [INFO] Sum of grad norms: 0.022439
2019-03-19 04:26:39,243 [INFO] ---------------------------------
2019-03-19 04:26:57,977 [INFO] ---------------------------------
2019-03-19 04:26:57,978 [INFO] Summary:
2019-03-19 04:26:57,978 [INFO] Batch 186000, worst loss 0.060724 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:26:57,979 [INFO] Regularization: 3143.122803 * 0.0000010000 = 0.0031431229
2019-03-19 04:26:57,979 [INFO] Sum of grad norms: 0.034544
2019-03-19 04:26:57,980 [INFO] ---------------------------------
2019-03-19 04:27:16,656 [INFO] ---------------------------------
2019-03-19 04:27:16,656 [INFO] Summary:
2019-03-19 04:27:16,657 [INFO] Batch 187000, worst loss 0.060724 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:27:16,657 [INFO] Regularization: 3143.121826 * 0.0000010000 = 0.0031431217
2019-03-19 04:27:16,658 [INFO] Sum of grad norms: 0.030364
2019-03-19 04:27:16,658 [INFO] ---------------------------------
2019-03-19 04:27:35,706 [INFO] ---------------------------------
2019-03-19 04:27:35,707 [INFO] Summary:
2019-03-19 04:27:35,707 [INFO] Batch 188000, worst loss 0.060702 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:27:35,708 [INFO] Regularization: 3143.121094 * 0.0000010000 = 0.0031431210
2019-03-19 04:27:35,709 [INFO] Sum of grad norms: 0.021255
2019-03-19 04:27:35,709 [INFO] ---------------------------------
2019-03-19 04:27:54,265 [INFO] ---------------------------------
2019-03-19 04:27:54,266 [INFO] Summary:
2019-03-19 04:27:54,267 [INFO] Batch 189000, worst loss 0.060709 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:27:54,267 [INFO] Regularization: 3143.119873 * 0.0000010000 = 0.0031431199
2019-03-19 04:27:54,268 [INFO] Sum of grad norms: 0.026132
2019-03-19 04:27:54,268 [INFO] ---------------------------------
2019-03-19 04:28:13,184 [INFO] ---------------------------------
2019-03-19 04:28:13,185 [INFO] Summary:
2019-03-19 04:28:13,186 [INFO] Batch 190000, worst loss 0.060771 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:28:13,186 [INFO] Regularization: 3143.118652 * 0.0000010000 = 0.0031431187
2019-03-19 04:28:13,187 [INFO] Sum of grad norms: 0.021718
2019-03-19 04:28:13,188 [INFO] ---------------------------------
2019-03-19 04:28:18,175 [INFO] ---------------------------------
2019-03-19 04:28:18,176 [INFO] Evaluation:
2019-03-19 04:28:18,178 [INFO] Batch 190000, worst loss 0.057608 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:28:18,179 [INFO] ---------------------------------
2019-03-19 04:28:36,776 [INFO] ---------------------------------
2019-03-19 04:28:36,777 [INFO] Summary:
2019-03-19 04:28:36,778 [INFO] Batch 191000, worst loss 0.060763 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:28:36,778 [INFO] Regularization: 3143.118164 * 0.0000010000 = 0.0031431182
2019-03-19 04:28:36,779 [INFO] Sum of grad norms: 0.022555
2019-03-19 04:28:36,779 [INFO] ---------------------------------
2019-03-19 04:28:55,366 [INFO] ---------------------------------
2019-03-19 04:28:55,367 [INFO] Summary:
2019-03-19 04:28:55,368 [INFO] Batch 192000, worst loss 0.060738 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:28:55,368 [INFO] Regularization: 3143.117432 * 0.0000010000 = 0.0031431175
2019-03-19 04:28:55,369 [INFO] Sum of grad norms: 0.027546
2019-03-19 04:28:55,370 [INFO] ---------------------------------
2019-03-19 04:29:14,126 [INFO] ---------------------------------
2019-03-19 04:29:14,127 [INFO] Summary:
2019-03-19 04:29:14,128 [INFO] Batch 193000, worst loss 0.060771 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:29:14,128 [INFO] Regularization: 3143.117188 * 0.0000010000 = 0.0031431171
2019-03-19 04:29:14,129 [INFO] Sum of grad norms: 0.018905
2019-03-19 04:29:14,129 [INFO] ---------------------------------
2019-03-19 04:29:32,797 [INFO] ---------------------------------
2019-03-19 04:29:32,798 [INFO] Summary:
2019-03-19 04:29:32,799 [INFO] Batch 194000, worst loss 0.060677 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:29:32,800 [INFO] Regularization: 3143.117188 * 0.0000010000 = 0.0031431171
2019-03-19 04:29:32,800 [INFO] Sum of grad norms: 0.023354
2019-03-19 04:29:32,801 [INFO] ---------------------------------
2019-03-19 04:29:51,438 [INFO] ---------------------------------
2019-03-19 04:29:51,439 [INFO] Summary:
2019-03-19 04:29:51,440 [INFO] Batch 195000, worst loss 0.060629 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:29:51,440 [INFO] Regularization: 3143.116943 * 0.0000010000 = 0.0031431168
2019-03-19 04:29:51,441 [INFO] Sum of grad norms: 0.040211
2019-03-19 04:29:51,442 [INFO] ---------------------------------
2019-03-19 04:30:10,487 [INFO] ---------------------------------
2019-03-19 04:30:10,488 [INFO] Summary:
2019-03-19 04:30:10,489 [INFO] Batch 196000, worst loss 0.060721 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:30:10,489 [INFO] Regularization: 3143.116211 * 0.0000010000 = 0.0031431161
2019-03-19 04:30:10,490 [INFO] Sum of grad norms: 0.015426
2019-03-19 04:30:10,491 [INFO] ---------------------------------
2019-03-19 04:30:29,079 [INFO] ---------------------------------
2019-03-19 04:30:29,080 [INFO] Summary:
2019-03-19 04:30:29,081 [INFO] Batch 197000, worst loss 0.060595 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:30:29,082 [INFO] Regularization: 3143.115723 * 0.0000010000 = 0.0031431157
2019-03-19 04:30:29,082 [INFO] Sum of grad norms: 0.038485
2019-03-19 04:30:29,083 [INFO] ---------------------------------
2019-03-19 04:30:47,705 [INFO] ---------------------------------
2019-03-19 04:30:47,706 [INFO] Summary:
2019-03-19 04:30:47,707 [INFO] Batch 198000, worst loss 0.060554 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:30:47,707 [INFO] Regularization: 3143.115234 * 0.0000010000 = 0.0031431152
2019-03-19 04:30:47,708 [INFO] Sum of grad norms: 0.023600
2019-03-19 04:30:47,708 [INFO] ---------------------------------
2019-03-19 04:31:06,438 [INFO] ---------------------------------
2019-03-19 04:31:06,439 [INFO] Summary:
2019-03-19 04:31:06,440 [INFO] Batch 199000, worst loss 0.060952 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:31:06,440 [INFO] Regularization: 3143.115234 * 0.0000010000 = 0.0031431152
2019-03-19 04:31:06,441 [INFO] Sum of grad norms: 0.038112
2019-03-19 04:31:06,441 [INFO] ---------------------------------
2019-03-19 04:31:24,942 [INFO] ---------------------------------
2019-03-19 04:31:24,943 [INFO] Summary:
2019-03-19 04:31:24,943 [INFO] Batch 200000, worst loss 0.060798 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:31:24,944 [INFO] Regularization: 3143.115234 * 0.0000010000 = 0.0031431152
2019-03-19 04:31:24,944 [INFO] Sum of grad norms: 0.026565
2019-03-19 04:31:24,945 [INFO] ---------------------------------
2019-03-19 04:31:29,912 [INFO] ---------------------------------
2019-03-19 04:31:29,913 [INFO] Evaluation:
2019-03-19 04:31:29,914 [INFO] Batch 200000, worst loss 0.057597 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:31:29,915 [INFO] ---------------------------------
2019-03-19 04:31:48,617 [INFO] ---------------------------------
2019-03-19 04:31:48,618 [INFO] Summary:
2019-03-19 04:31:48,618 [INFO] Batch 201000, worst loss 0.060824 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:31:48,619 [INFO] Regularization: 3143.114990 * 0.0000010000 = 0.0031431150
2019-03-19 04:31:48,619 [INFO] Sum of grad norms: 0.041778
2019-03-19 04:31:48,620 [INFO] ---------------------------------
2019-03-19 04:32:07,403 [INFO] ---------------------------------
2019-03-19 04:32:07,404 [INFO] Summary:
2019-03-19 04:32:07,405 [INFO] Batch 202000, worst loss 0.060823 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:32:07,405 [INFO] Regularization: 3143.114746 * 0.0000010000 = 0.0031431147
2019-03-19 04:32:07,406 [INFO] Sum of grad norms: 0.032917
2019-03-19 04:32:07,406 [INFO] ---------------------------------
2019-03-19 04:32:26,329 [INFO] ---------------------------------
2019-03-19 04:32:26,330 [INFO] Summary:
2019-03-19 04:32:26,331 [INFO] Batch 203000, worst loss 0.060723 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:32:26,331 [INFO] Regularization: 3143.114502 * 0.0000010000 = 0.0031431145
2019-03-19 04:32:26,332 [INFO] Sum of grad norms: 0.019369
2019-03-19 04:32:26,333 [INFO] ---------------------------------
2019-03-19 04:32:44,746 [INFO] ---------------------------------
2019-03-19 04:32:44,747 [INFO] Summary:
2019-03-19 04:32:44,747 [INFO] Batch 204000, worst loss 0.060667 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:32:44,748 [INFO] Regularization: 3143.114502 * 0.0000010000 = 0.0031431145
2019-03-19 04:32:44,748 [INFO] Sum of grad norms: 0.026800
2019-03-19 04:32:44,749 [INFO] ---------------------------------
2019-03-19 04:33:03,726 [INFO] ---------------------------------
2019-03-19 04:33:03,727 [INFO] Summary:
2019-03-19 04:33:03,727 [INFO] Batch 205000, worst loss 0.060689 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:33:03,728 [INFO] Regularization: 3143.114502 * 0.0000010000 = 0.0031431145
2019-03-19 04:33:03,728 [INFO] Sum of grad norms: 0.028889
2019-03-19 04:33:03,729 [INFO] ---------------------------------
2019-03-19 04:33:22,636 [INFO] ---------------------------------
2019-03-19 04:33:22,638 [INFO] Summary:
2019-03-19 04:33:22,639 [INFO] Batch 206000, worst loss 0.060656 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:33:22,639 [INFO] Regularization: 3143.114502 * 0.0000010000 = 0.0031431145
2019-03-19 04:33:22,640 [INFO] Sum of grad norms: 0.037154
2019-03-19 04:33:22,641 [INFO] ---------------------------------
2019-03-19 04:33:41,492 [INFO] ---------------------------------
2019-03-19 04:33:41,493 [INFO] Summary:
2019-03-19 04:33:41,493 [INFO] Batch 207000, worst loss 0.060786 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:33:41,494 [INFO] Regularization: 3143.114502 * 0.0000010000 = 0.0031431145
2019-03-19 04:33:41,494 [INFO] Sum of grad norms: 0.046313
2019-03-19 04:33:41,495 [INFO] ---------------------------------
2019-03-19 04:33:59,816 [INFO] ---------------------------------
2019-03-19 04:33:59,817 [INFO] Summary:
2019-03-19 04:33:59,818 [INFO] Batch 208000, worst loss 0.060786 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:33:59,818 [INFO] Regularization: 3143.114258 * 0.0000010000 = 0.0031431143
2019-03-19 04:33:59,819 [INFO] Sum of grad norms: 0.025014
2019-03-19 04:33:59,819 [INFO] ---------------------------------
2019-03-19 04:34:18,203 [INFO] ---------------------------------
2019-03-19 04:34:18,203 [INFO] Summary:
2019-03-19 04:34:18,204 [INFO] Batch 209000, worst loss 0.060702 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:34:18,204 [INFO] Regularization: 3143.114258 * 0.0000010000 = 0.0031431143
2019-03-19 04:34:18,205 [INFO] Sum of grad norms: 0.020291
2019-03-19 04:34:18,205 [INFO] ---------------------------------
2019-03-19 04:34:37,190 [INFO] ---------------------------------
2019-03-19 04:34:37,191 [INFO] Summary:
2019-03-19 04:34:37,192 [INFO] Batch 210000, worst loss 0.060629 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:34:37,192 [INFO] Regularization: 3143.114258 * 0.0000010000 = 0.0031431143
2019-03-19 04:34:37,193 [INFO] Sum of grad norms: 0.045509
2019-03-19 04:34:37,193 [INFO] ---------------------------------
2019-03-19 04:34:42,106 [INFO] ---------------------------------
2019-03-19 04:34:42,107 [INFO] Evaluation:
2019-03-19 04:34:42,110 [INFO] Batch 210000, worst loss 0.057557 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:34:42,112 [INFO] ---------------------------------
2019-03-19 04:35:00,781 [INFO] ---------------------------------
2019-03-19 04:35:00,782 [INFO] Summary:
2019-03-19 04:35:00,783 [INFO] Batch 211000, worst loss 0.060972 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:35:00,784 [INFO] Regularization: 3143.114014 * 0.0000010000 = 0.0031431140
2019-03-19 04:35:00,784 [INFO] Sum of grad norms: 0.020018
2019-03-19 04:35:00,785 [INFO] ---------------------------------
2019-03-19 04:35:19,728 [INFO] ---------------------------------
2019-03-19 04:35:19,729 [INFO] Summary:
2019-03-19 04:35:19,729 [INFO] Batch 212000, worst loss 0.060972 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:35:19,730 [INFO] Regularization: 3143.114014 * 0.0000010000 = 0.0031431140
2019-03-19 04:35:19,731 [INFO] Sum of grad norms: 0.022982
2019-03-19 04:35:19,731 [INFO] ---------------------------------
2019-03-19 04:35:38,546 [INFO] ---------------------------------
2019-03-19 04:35:38,547 [INFO] Summary:
2019-03-19 04:35:38,547 [INFO] Batch 213000, worst loss 0.060697 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:35:38,548 [INFO] Regularization: 3143.114014 * 0.0000010000 = 0.0031431140
2019-03-19 04:35:38,549 [INFO] Sum of grad norms: 0.022189
2019-03-19 04:35:38,549 [INFO] ---------------------------------
2019-03-19 04:35:57,383 [INFO] ---------------------------------
2019-03-19 04:35:57,384 [INFO] Summary:
2019-03-19 04:35:57,385 [INFO] Batch 214000, worst loss 0.060738 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:35:57,385 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:35:57,386 [INFO] Sum of grad norms: 0.046553
2019-03-19 04:35:57,386 [INFO] ---------------------------------
2019-03-19 04:36:15,951 [INFO] ---------------------------------
2019-03-19 04:36:15,951 [INFO] Summary:
2019-03-19 04:36:15,952 [INFO] Batch 215000, worst loss 0.060745 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:36:15,952 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:36:15,953 [INFO] Sum of grad norms: 0.028891
2019-03-19 04:36:15,953 [INFO] ---------------------------------
2019-03-19 04:36:34,459 [INFO] ---------------------------------
2019-03-19 04:36:34,460 [INFO] Summary:
2019-03-19 04:36:34,461 [INFO] Batch 216000, worst loss 0.060760 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:36:34,461 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:36:34,462 [INFO] Sum of grad norms: 0.041055
2019-03-19 04:36:34,462 [INFO] ---------------------------------
2019-03-19 04:36:53,031 [INFO] ---------------------------------
2019-03-19 04:36:53,032 [INFO] Summary:
2019-03-19 04:36:53,033 [INFO] Batch 217000, worst loss 0.060626 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:36:53,033 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:36:53,034 [INFO] Sum of grad norms: 0.030971
2019-03-19 04:36:53,035 [INFO] ---------------------------------
2019-03-19 04:37:11,861 [INFO] ---------------------------------
2019-03-19 04:37:11,862 [INFO] Summary:
2019-03-19 04:37:11,862 [INFO] Batch 218000, worst loss 0.060612 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:37:11,863 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:37:11,863 [INFO] Sum of grad norms: 0.042474
2019-03-19 04:37:11,864 [INFO] ---------------------------------
2019-03-19 04:37:30,815 [INFO] ---------------------------------
2019-03-19 04:37:30,816 [INFO] Summary:
2019-03-19 04:37:30,817 [INFO] Batch 219000, worst loss 0.060756 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:37:30,817 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:37:30,818 [INFO] Sum of grad norms: 0.025220
2019-03-19 04:37:30,818 [INFO] ---------------------------------
2019-03-19 04:37:49,742 [INFO] ---------------------------------
2019-03-19 04:37:49,743 [INFO] Summary:
2019-03-19 04:37:49,744 [INFO] Batch 220000, worst loss 0.061071 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:37:49,744 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:37:49,745 [INFO] Sum of grad norms: 0.036034
2019-03-19 04:37:49,745 [INFO] ---------------------------------
2019-03-19 04:37:54,699 [INFO] ---------------------------------
2019-03-19 04:37:54,700 [INFO] Evaluation:
2019-03-19 04:37:54,701 [INFO] Batch 220000, worst loss 0.057596 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:37:54,703 [INFO] ---------------------------------
2019-03-19 04:38:13,846 [INFO] ---------------------------------
2019-03-19 04:38:13,847 [INFO] Summary:
2019-03-19 04:38:13,848 [INFO] Batch 221000, worst loss 0.060652 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:38:13,848 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:38:13,849 [INFO] Sum of grad norms: 0.021070
2019-03-19 04:38:13,849 [INFO] ---------------------------------
2019-03-19 04:38:32,428 [INFO] ---------------------------------
2019-03-19 04:38:32,429 [INFO] Summary:
2019-03-19 04:38:32,430 [INFO] Batch 222000, worst loss 0.060751 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:38:32,430 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:38:32,431 [INFO] Sum of grad norms: 0.035299
2019-03-19 04:38:32,431 [INFO] ---------------------------------
2019-03-19 04:38:51,339 [INFO] ---------------------------------
2019-03-19 04:38:51,340 [INFO] Summary:
2019-03-19 04:38:51,341 [INFO] Batch 223000, worst loss 0.060747 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:38:51,342 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:38:51,343 [INFO] Sum of grad norms: 0.029467
2019-03-19 04:38:51,344 [INFO] ---------------------------------
2019-03-19 04:39:10,267 [INFO] ---------------------------------
2019-03-19 04:39:10,268 [INFO] Summary:
2019-03-19 04:39:10,269 [INFO] Batch 224000, worst loss 0.060872 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:39:10,269 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:39:10,270 [INFO] Sum of grad norms: 0.039155
2019-03-19 04:39:10,271 [INFO] ---------------------------------
2019-03-19 04:39:28,899 [INFO] ---------------------------------
2019-03-19 04:39:28,900 [INFO] Summary:
2019-03-19 04:39:28,901 [INFO] Batch 225000, worst loss 0.060732 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:39:28,902 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:39:28,902 [INFO] Sum of grad norms: 0.038867
2019-03-19 04:39:28,903 [INFO] ---------------------------------
2019-03-19 04:39:47,550 [INFO] ---------------------------------
2019-03-19 04:39:47,551 [INFO] Summary:
2019-03-19 04:39:47,552 [INFO] Batch 226000, worst loss 0.060732 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:39:47,552 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:39:47,553 [INFO] Sum of grad norms: 0.020993
2019-03-19 04:39:47,554 [INFO] ---------------------------------
2019-03-19 04:40:06,780 [INFO] ---------------------------------
2019-03-19 04:40:06,781 [INFO] Summary:
2019-03-19 04:40:06,781 [INFO] Batch 227000, worst loss 0.060798 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:40:06,782 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:40:06,782 [INFO] Sum of grad norms: 0.022544
2019-03-19 04:40:06,783 [INFO] ---------------------------------
2019-03-19 04:40:25,346 [INFO] ---------------------------------
2019-03-19 04:40:25,347 [INFO] Summary:
2019-03-19 04:40:25,347 [INFO] Batch 228000, worst loss 0.060798 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:40:25,348 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:40:25,349 [INFO] Sum of grad norms: 0.019199
2019-03-19 04:40:25,349 [INFO] ---------------------------------
2019-03-19 04:40:43,940 [INFO] ---------------------------------
2019-03-19 04:40:43,941 [INFO] Summary:
2019-03-19 04:40:43,942 [INFO] Batch 229000, worst loss 0.060779 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:40:43,942 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:40:43,943 [INFO] Sum of grad norms: 0.017634
2019-03-19 04:40:43,943 [INFO] ---------------------------------
2019-03-19 04:41:02,877 [INFO] ---------------------------------
2019-03-19 04:41:02,878 [INFO] Summary:
2019-03-19 04:41:02,879 [INFO] Batch 230000, worst loss 0.060672 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:41:02,880 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:41:02,880 [INFO] Sum of grad norms: 0.035374
2019-03-19 04:41:02,881 [INFO] ---------------------------------
2019-03-19 04:41:07,774 [INFO] ---------------------------------
2019-03-19 04:41:07,775 [INFO] Evaluation:
2019-03-19 04:41:07,776 [INFO] Batch 230000, worst loss 0.057618 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:41:07,777 [INFO] ---------------------------------
2019-03-19 04:41:27,200 [INFO] ---------------------------------
2019-03-19 04:41:27,201 [INFO] Summary:
2019-03-19 04:41:27,201 [INFO] Batch 231000, worst loss 0.060771 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:41:27,202 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:41:27,202 [INFO] Sum of grad norms: 0.024990
2019-03-19 04:41:27,203 [INFO] ---------------------------------
2019-03-19 04:41:45,989 [INFO] ---------------------------------
2019-03-19 04:41:45,990 [INFO] Summary:
2019-03-19 04:41:45,991 [INFO] Batch 232000, worst loss 0.060681 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:41:45,991 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:41:45,992 [INFO] Sum of grad norms: 0.041163
2019-03-19 04:41:45,993 [INFO] ---------------------------------
2019-03-19 04:42:04,525 [INFO] ---------------------------------
2019-03-19 04:42:04,526 [INFO] Summary:
2019-03-19 04:42:04,526 [INFO] Batch 233000, worst loss 0.060688 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:42:04,527 [INFO] Regularization: 3143.113770 * 0.0000010000 = 0.0031431138
2019-03-19 04:42:04,527 [INFO] Sum of grad norms: 0.026901
2019-03-19 04:42:04,528 [INFO] ---------------------------------
2019-03-19 04:42:23,475 [INFO] ---------------------------------
2019-03-19 04:42:23,476 [INFO] Summary:
2019-03-19 04:42:23,478 [INFO] Batch 234000, worst loss 0.060753 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:42:23,478 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:42:23,479 [INFO] Sum of grad norms: 0.047176
2019-03-19 04:42:23,480 [INFO] ---------------------------------
2019-03-19 04:42:42,218 [INFO] ---------------------------------
2019-03-19 04:42:42,219 [INFO] Summary:
2019-03-19 04:42:42,219 [INFO] Batch 235000, worst loss 0.060753 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:42:42,220 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:42:42,220 [INFO] Sum of grad norms: 0.044363
2019-03-19 04:42:42,221 [INFO] ---------------------------------
2019-03-19 04:43:00,475 [INFO] ---------------------------------
2019-03-19 04:43:00,476 [INFO] Summary:
2019-03-19 04:43:00,476 [INFO] Batch 236000, worst loss 0.060880 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:43:00,477 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:43:00,477 [INFO] Sum of grad norms: 0.018704
2019-03-19 04:43:00,478 [INFO] ---------------------------------
2019-03-19 04:43:19,478 [INFO] ---------------------------------
2019-03-19 04:43:19,479 [INFO] Summary:
2019-03-19 04:43:19,480 [INFO] Batch 237000, worst loss 0.060799 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:43:19,480 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:43:19,481 [INFO] Sum of grad norms: 0.028910
2019-03-19 04:43:19,482 [INFO] ---------------------------------
2019-03-19 04:43:38,258 [INFO] ---------------------------------
2019-03-19 04:43:38,259 [INFO] Summary:
2019-03-19 04:43:38,259 [INFO] Batch 238000, worst loss 0.060799 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:43:38,260 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:43:38,260 [INFO] Sum of grad norms: 0.042873
2019-03-19 04:43:38,261 [INFO] ---------------------------------
2019-03-19 04:43:56,592 [INFO] ---------------------------------
2019-03-19 04:43:56,593 [INFO] Summary:
2019-03-19 04:43:56,594 [INFO] Batch 239000, worst loss 0.060724 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:43:56,595 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:43:56,595 [INFO] Sum of grad norms: 0.027092
2019-03-19 04:43:56,596 [INFO] ---------------------------------
2019-03-19 04:44:15,756 [INFO] ---------------------------------
2019-03-19 04:44:15,757 [INFO] Summary:
2019-03-19 04:44:15,757 [INFO] Batch 240000, worst loss 0.060735 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:44:15,758 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:44:15,758 [INFO] Sum of grad norms: 0.034360
2019-03-19 04:44:15,759 [INFO] ---------------------------------
2019-03-19 04:44:20,669 [INFO] ---------------------------------
2019-03-19 04:44:20,669 [INFO] Evaluation:
2019-03-19 04:44:20,670 [INFO] Batch 240000, worst loss 0.057592 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:44:20,671 [INFO] ---------------------------------
2019-03-19 04:44:39,970 [INFO] ---------------------------------
2019-03-19 04:44:39,971 [INFO] Summary:
2019-03-19 04:44:39,972 [INFO] Batch 241000, worst loss 0.060639 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:44:39,972 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:44:39,973 [INFO] Sum of grad norms: 0.027968
2019-03-19 04:44:39,973 [INFO] ---------------------------------
2019-03-19 04:44:58,898 [INFO] ---------------------------------
2019-03-19 04:44:58,899 [INFO] Summary:
2019-03-19 04:44:58,899 [INFO] Batch 242000, worst loss 0.060786 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:44:58,900 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:44:58,901 [INFO] Sum of grad norms: 0.018010
2019-03-19 04:44:58,901 [INFO] ---------------------------------
2019-03-19 04:45:17,421 [INFO] ---------------------------------
2019-03-19 04:45:17,422 [INFO] Summary:
2019-03-19 04:45:17,423 [INFO] Batch 243000, worst loss 0.060786 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:45:17,423 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:45:17,424 [INFO] Sum of grad norms: 0.019389
2019-03-19 04:45:17,424 [INFO] ---------------------------------
2019-03-19 04:45:36,288 [INFO] ---------------------------------
2019-03-19 04:45:36,289 [INFO] Summary:
2019-03-19 04:45:36,290 [INFO] Batch 244000, worst loss 0.060636 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:45:36,290 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:45:36,291 [INFO] Sum of grad norms: 0.031140
2019-03-19 04:45:36,291 [INFO] ---------------------------------
2019-03-19 04:45:55,778 [INFO] ---------------------------------
2019-03-19 04:45:55,779 [INFO] Summary:
2019-03-19 04:45:55,780 [INFO] Batch 245000, worst loss 0.060820 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:45:55,780 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:45:55,781 [INFO] Sum of grad norms: 0.055518
2019-03-19 04:45:55,782 [INFO] ---------------------------------
2019-03-19 04:46:14,815 [INFO] ---------------------------------
2019-03-19 04:46:14,816 [INFO] Summary:
2019-03-19 04:46:14,817 [INFO] Batch 246000, worst loss 0.060820 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:46:14,818 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:46:14,818 [INFO] Sum of grad norms: 0.041492
2019-03-19 04:46:14,819 [INFO] ---------------------------------
2019-03-19 04:46:33,614 [INFO] ---------------------------------
2019-03-19 04:46:33,614 [INFO] Summary:
2019-03-19 04:46:33,615 [INFO] Batch 247000, worst loss 0.060788 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:46:33,616 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:46:33,617 [INFO] Sum of grad norms: 0.036297
2019-03-19 04:46:33,617 [INFO] ---------------------------------
2019-03-19 04:46:52,385 [INFO] ---------------------------------
2019-03-19 04:46:52,386 [INFO] Summary:
2019-03-19 04:46:52,386 [INFO] Batch 248000, worst loss 0.060724 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:46:52,387 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:46:52,387 [INFO] Sum of grad norms: 0.023591
2019-03-19 04:46:52,388 [INFO] ---------------------------------
2019-03-19 04:47:11,296 [INFO] ---------------------------------
2019-03-19 04:47:11,297 [INFO] Summary:
2019-03-19 04:47:11,298 [INFO] Batch 249000, worst loss 0.060789 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:47:11,298 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:47:11,299 [INFO] Sum of grad norms: 0.029013
2019-03-19 04:47:11,299 [INFO] ---------------------------------
2019-03-19 04:47:29,854 [INFO] ---------------------------------
2019-03-19 04:47:29,854 [INFO] Summary:
2019-03-19 04:47:29,855 [INFO] Batch 250000, worst loss 0.060789 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 04:47:29,856 [INFO] Regularization: 3143.113525 * 0.0000010000 = 0.0031431136
2019-03-19 04:47:29,856 [INFO] Sum of grad norms: 0.021466
2019-03-19 04:47:29,857 [INFO] ---------------------------------
2019-03-19 04:47:34,826 [INFO] ---------------------------------
2019-03-19 04:47:34,826 [INFO] Evaluation:
2019-03-19 04:47:34,830 [INFO] Batch 250000, worst loss 0.057526 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:47:34,831 [INFO] ---------------------------------
2019-03-19 04:47:34,833 [INFO] Finished training, saved to file classifier/1552933539/1552967254_6_classifier_final.pth
2019-03-19 04:47:34,988 [INFO] ---------------------------------
2019-03-19 04:47:34,990 [INFO] Training model #7: (1, 64, 201) @ 1
2019-03-19 04:47:50,993 [INFO] ---------------------------------
2019-03-19 04:47:50,994 [INFO] Summary:
2019-03-19 04:47:50,995 [INFO] Batch 1000, worst loss 16.360174 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 04:47:50,996 [INFO] Regularization: 9483.389648 * 0.0000010000 = 0.0094833896
2019-03-19 04:47:50,996 [INFO] Sum of grad norms: 1.087834
2019-03-19 04:47:50,997 [INFO] ---------------------------------
2019-03-19 04:48:06,913 [INFO] ---------------------------------
2019-03-19 04:48:06,914 [INFO] Summary:
2019-03-19 04:48:06,914 [INFO] Batch 2000, worst loss 0.150601 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 04:48:06,915 [INFO] Regularization: 8562.052734 * 0.0000010000 = 0.0085620526
2019-03-19 04:48:06,915 [INFO] Sum of grad norms: 1.197559
2019-03-19 04:48:06,916 [INFO] ---------------------------------
2019-03-19 04:48:25,543 [INFO] ---------------------------------
2019-03-19 04:48:25,543 [INFO] Summary:
2019-03-19 04:48:25,544 [INFO] Batch 3000, worst loss 0.083159 (incl. reg.) of 1000 batches, learning rate 0.001506 @cl.-depth 1
2019-03-19 04:48:25,544 [INFO] Regularization: 7772.494141 * 0.0000010000 = 0.0077724941
2019-03-19 04:48:25,545 [INFO] Sum of grad norms: 2.547009
2019-03-19 04:48:25,546 [INFO] ---------------------------------
2019-03-19 04:48:45,011 [INFO] ---------------------------------
2019-03-19 04:48:45,012 [INFO] Summary:
2019-03-19 04:48:45,013 [INFO] Batch 4000, worst loss 0.076786 (incl. reg.) of 1000 batches, learning rate 0.000832 @cl.-depth 1
2019-03-19 04:48:45,013 [INFO] Regularization: 7397.790039 * 0.0000010000 = 0.0073977900
2019-03-19 04:48:45,014 [INFO] Sum of grad norms: 0.208119
2019-03-19 04:48:45,014 [INFO] ---------------------------------
2019-03-19 04:49:04,026 [INFO] ---------------------------------
2019-03-19 04:49:04,027 [INFO] Summary:
2019-03-19 04:49:04,028 [INFO] Batch 5000, worst loss 0.074903 (incl. reg.) of 1000 batches, learning rate 0.000768 @cl.-depth 1
2019-03-19 04:49:04,028 [INFO] Regularization: 7044.069824 * 0.0000010000 = 0.0070440699
2019-03-19 04:49:04,029 [INFO] Sum of grad norms: 0.225842
2019-03-19 04:49:04,029 [INFO] ---------------------------------
2019-03-19 04:49:23,565 [INFO] ---------------------------------
2019-03-19 04:49:23,566 [INFO] Summary:
2019-03-19 04:49:23,567 [INFO] Batch 6000, worst loss 0.074680 (incl. reg.) of 1000 batches, learning rate 0.000749 @cl.-depth 1
2019-03-19 04:49:23,568 [INFO] Regularization: 6717.356445 * 0.0000010000 = 0.0067173564
2019-03-19 04:49:23,568 [INFO] Sum of grad norms: 0.257641
2019-03-19 04:49:23,569 [INFO] ---------------------------------
2019-03-19 04:49:42,440 [INFO] ---------------------------------
2019-03-19 04:49:42,441 [INFO] Summary:
2019-03-19 04:49:42,441 [INFO] Batch 7000, worst loss 0.072244 (incl. reg.) of 1000 batches, learning rate 0.000747 @cl.-depth 1
2019-03-19 04:49:42,442 [INFO] Regularization: 6443.164062 * 0.0000010000 = 0.0064431638
2019-03-19 04:49:42,442 [INFO] Sum of grad norms: 0.815340
2019-03-19 04:49:42,443 [INFO] ---------------------------------
2019-03-19 04:50:01,236 [INFO] ---------------------------------
2019-03-19 04:50:01,237 [INFO] Summary:
2019-03-19 04:50:01,237 [INFO] Batch 8000, worst loss 0.070784 (incl. reg.) of 1000 batches, learning rate 0.000722 @cl.-depth 1
2019-03-19 04:50:01,238 [INFO] Regularization: 6192.583008 * 0.0000010000 = 0.0061925831
2019-03-19 04:50:01,239 [INFO] Sum of grad norms: 0.476641
2019-03-19 04:50:01,239 [INFO] ---------------------------------
2019-03-19 04:50:20,100 [INFO] ---------------------------------
2019-03-19 04:50:20,101 [INFO] Summary:
2019-03-19 04:50:20,102 [INFO] Batch 9000, worst loss 0.071475 (incl. reg.) of 1000 batches, learning rate 0.000708 @cl.-depth 1
2019-03-19 04:50:20,103 [INFO] Regularization: 6011.163086 * 0.0000010000 = 0.0060111629
2019-03-19 04:50:20,103 [INFO] Sum of grad norms: 0.366272
2019-03-19 04:50:20,104 [INFO] ---------------------------------
2019-03-19 04:50:39,590 [INFO] ---------------------------------
2019-03-19 04:50:39,591 [INFO] Summary:
2019-03-19 04:50:39,591 [INFO] Batch 10000, worst loss 0.071383 (incl. reg.) of 1000 batches, learning rate 0.000708 @cl.-depth 1
2019-03-19 04:50:39,592 [INFO] Regularization: 5839.750488 * 0.0000010000 = 0.0058397506
2019-03-19 04:50:39,592 [INFO] Sum of grad norms: 0.437756
2019-03-19 04:50:39,593 [INFO] ---------------------------------
2019-03-19 04:50:44,516 [INFO] ---------------------------------
2019-03-19 04:50:44,517 [INFO] Evaluation:
2019-03-19 04:50:44,518 [INFO] Batch 10000, worst loss 0.063156 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:50:44,519 [INFO] ---------------------------------
2019-03-19 04:51:03,046 [INFO] ---------------------------------
2019-03-19 04:51:03,047 [INFO] Summary:
2019-03-19 04:51:03,047 [INFO] Batch 11000, worst loss 0.071046 (incl. reg.) of 1000 batches, learning rate 0.000708 @cl.-depth 1
2019-03-19 04:51:03,048 [INFO] Regularization: 5694.248535 * 0.0000010000 = 0.0056942487
2019-03-19 04:51:03,049 [INFO] Sum of grad norms: 0.378953
2019-03-19 04:51:03,049 [INFO] ---------------------------------
2019-03-19 04:51:22,026 [INFO] ---------------------------------
2019-03-19 04:51:22,027 [INFO] Summary:
2019-03-19 04:51:22,028 [INFO] Batch 12000, worst loss 0.069434 (incl. reg.) of 1000 batches, learning rate 0.000708 @cl.-depth 1
2019-03-19 04:51:22,028 [INFO] Regularization: 5584.636230 * 0.0000010000 = 0.0055846362
2019-03-19 04:51:22,029 [INFO] Sum of grad norms: 0.657064
2019-03-19 04:51:22,030 [INFO] ---------------------------------
2019-03-19 04:51:40,959 [INFO] ---------------------------------
2019-03-19 04:51:40,960 [INFO] Summary:
2019-03-19 04:51:40,960 [INFO] Batch 13000, worst loss 0.068372 (incl. reg.) of 1000 batches, learning rate 0.000694 @cl.-depth 1
2019-03-19 04:51:40,961 [INFO] Regularization: 5480.376953 * 0.0000010000 = 0.0054803770
2019-03-19 04:51:40,961 [INFO] Sum of grad norms: 0.983193
2019-03-19 04:51:40,962 [INFO] ---------------------------------
2019-03-19 04:51:59,552 [INFO] ---------------------------------
2019-03-19 04:51:59,553 [INFO] Summary:
2019-03-19 04:51:59,553 [INFO] Batch 14000, worst loss 0.069863 (incl. reg.) of 1000 batches, learning rate 0.000684 @cl.-depth 1
2019-03-19 04:51:59,554 [INFO] Regularization: 5392.175781 * 0.0000010000 = 0.0053921756
2019-03-19 04:51:59,554 [INFO] Sum of grad norms: 0.902307
2019-03-19 04:51:59,555 [INFO] ---------------------------------
2019-03-19 04:52:18,296 [INFO] ---------------------------------
2019-03-19 04:52:18,297 [INFO] Summary:
2019-03-19 04:52:18,298 [INFO] Batch 15000, worst loss 0.068117 (incl. reg.) of 1000 batches, learning rate 0.000684 @cl.-depth 1
2019-03-19 04:52:18,299 [INFO] Regularization: 5300.501953 * 0.0000010000 = 0.0053005018
2019-03-19 04:52:18,299 [INFO] Sum of grad norms: 0.443562
2019-03-19 04:52:18,300 [INFO] ---------------------------------
2019-03-19 04:52:36,886 [INFO] ---------------------------------
2019-03-19 04:52:36,887 [INFO] Summary:
2019-03-19 04:52:36,888 [INFO] Batch 16000, worst loss 0.069533 (incl. reg.) of 1000 batches, learning rate 0.000681 @cl.-depth 1
2019-03-19 04:52:36,888 [INFO] Regularization: 5218.167480 * 0.0000010000 = 0.0052181673
2019-03-19 04:52:36,889 [INFO] Sum of grad norms: 0.657992
2019-03-19 04:52:36,890 [INFO] ---------------------------------
2019-03-19 04:52:55,835 [INFO] ---------------------------------
2019-03-19 04:52:55,836 [INFO] Summary:
2019-03-19 04:52:55,837 [INFO] Batch 17000, worst loss 0.069983 (incl. reg.) of 1000 batches, learning rate 0.000681 @cl.-depth 1
2019-03-19 04:52:55,838 [INFO] Regularization: 5131.580078 * 0.0000010000 = 0.0051315799
2019-03-19 04:52:55,838 [INFO] Sum of grad norms: 0.343537
2019-03-19 04:52:55,839 [INFO] ---------------------------------
2019-03-19 04:53:14,774 [INFO] ---------------------------------
2019-03-19 04:53:14,775 [INFO] Summary:
2019-03-19 04:53:14,775 [INFO] Batch 18000, worst loss 0.067836 (incl. reg.) of 1000 batches, learning rate 0.000681 @cl.-depth 1
2019-03-19 04:53:14,776 [INFO] Regularization: 5066.960938 * 0.0000010000 = 0.0050669611
2019-03-19 04:53:14,776 [INFO] Sum of grad norms: 0.134222
2019-03-19 04:53:14,777 [INFO] ---------------------------------
2019-03-19 04:53:33,351 [INFO] ---------------------------------
2019-03-19 04:53:33,352 [INFO] Summary:
2019-03-19 04:53:33,353 [INFO] Batch 19000, worst loss 0.068047 (incl. reg.) of 1000 batches, learning rate 0.000678 @cl.-depth 1
2019-03-19 04:53:33,353 [INFO] Regularization: 4991.081543 * 0.0000010000 = 0.0049910815
2019-03-19 04:53:33,354 [INFO] Sum of grad norms: 2.039593
2019-03-19 04:53:33,354 [INFO] ---------------------------------
2019-03-19 04:53:52,192 [INFO] ---------------------------------
2019-03-19 04:53:52,193 [INFO] Summary:
2019-03-19 04:53:52,193 [INFO] Batch 20000, worst loss 0.067898 (incl. reg.) of 1000 batches, learning rate 0.000678 @cl.-depth 1
2019-03-19 04:53:52,194 [INFO] Regularization: 4916.555664 * 0.0000010000 = 0.0049165557
2019-03-19 04:53:52,194 [INFO] Sum of grad norms: 0.350541
2019-03-19 04:53:52,195 [INFO] ---------------------------------
2019-03-19 04:53:57,137 [INFO] ---------------------------------
2019-03-19 04:53:57,137 [INFO] Evaluation:
2019-03-19 04:53:57,138 [INFO] Batch 20000, worst loss 0.060470 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:53:57,139 [INFO] ---------------------------------
2019-03-19 04:54:16,124 [INFO] ---------------------------------
2019-03-19 04:54:16,125 [INFO] Summary:
2019-03-19 04:54:16,126 [INFO] Batch 21000, worst loss 0.066441 (incl. reg.) of 1000 batches, learning rate 0.000678 @cl.-depth 1
2019-03-19 04:54:16,127 [INFO] Regularization: 4850.997559 * 0.0000010000 = 0.0048509976
2019-03-19 04:54:16,127 [INFO] Sum of grad norms: 0.343774
2019-03-19 04:54:16,128 [INFO] ---------------------------------
2019-03-19 04:54:35,401 [INFO] ---------------------------------
2019-03-19 04:54:35,402 [INFO] Summary:
2019-03-19 04:54:35,402 [INFO] Batch 22000, worst loss 0.066517 (incl. reg.) of 1000 batches, learning rate 0.000664 @cl.-depth 1
2019-03-19 04:54:35,403 [INFO] Regularization: 4786.069336 * 0.0000010000 = 0.0047860695
2019-03-19 04:54:35,403 [INFO] Sum of grad norms: 0.540803
2019-03-19 04:54:35,404 [INFO] ---------------------------------
2019-03-19 04:54:54,515 [INFO] ---------------------------------
2019-03-19 04:54:54,516 [INFO] Summary:
2019-03-19 04:54:54,517 [INFO] Batch 23000, worst loss 0.065696 (incl. reg.) of 1000 batches, learning rate 0.000664 @cl.-depth 1
2019-03-19 04:54:54,517 [INFO] Regularization: 4704.269531 * 0.0000010000 = 0.0047042696
2019-03-19 04:54:54,518 [INFO] Sum of grad norms: 0.174511
2019-03-19 04:54:54,519 [INFO] ---------------------------------
2019-03-19 04:55:13,286 [INFO] ---------------------------------
2019-03-19 04:55:13,287 [INFO] Summary:
2019-03-19 04:55:13,288 [INFO] Batch 24000, worst loss 0.065953 (incl. reg.) of 1000 batches, learning rate 0.000657 @cl.-depth 1
2019-03-19 04:55:13,288 [INFO] Regularization: 4639.497070 * 0.0000010000 = 0.0046394970
2019-03-19 04:55:13,289 [INFO] Sum of grad norms: 0.085049
2019-03-19 04:55:13,289 [INFO] ---------------------------------
2019-03-19 04:55:32,648 [INFO] ---------------------------------
2019-03-19 04:55:32,649 [INFO] Summary:
2019-03-19 04:55:32,650 [INFO] Batch 25000, worst loss 0.065742 (incl. reg.) of 1000 batches, learning rate 0.000657 @cl.-depth 1
2019-03-19 04:55:32,650 [INFO] Regularization: 4577.103027 * 0.0000010000 = 0.0045771031
2019-03-19 04:55:32,651 [INFO] Sum of grad norms: 0.416319
2019-03-19 04:55:32,651 [INFO] ---------------------------------
2019-03-19 04:55:51,752 [INFO] ---------------------------------
2019-03-19 04:55:51,753 [INFO] Summary:
2019-03-19 04:55:51,754 [INFO] Batch 26000, worst loss 0.065028 (incl. reg.) of 1000 batches, learning rate 0.000657 @cl.-depth 1
2019-03-19 04:55:51,755 [INFO] Regularization: 4508.520020 * 0.0000010000 = 0.0045085200
2019-03-19 04:55:51,756 [INFO] Sum of grad norms: 0.041082
2019-03-19 04:55:51,757 [INFO] ---------------------------------
2019-03-19 04:56:10,696 [INFO] ---------------------------------
2019-03-19 04:56:10,697 [INFO] Summary:
2019-03-19 04:56:10,698 [INFO] Batch 27000, worst loss 0.065431 (incl. reg.) of 1000 batches, learning rate 0.000650 @cl.-depth 1
2019-03-19 04:56:10,698 [INFO] Regularization: 4449.896973 * 0.0000010000 = 0.0044498970
2019-03-19 04:56:10,699 [INFO] Sum of grad norms: 0.473510
2019-03-19 04:56:10,700 [INFO] ---------------------------------
2019-03-19 04:56:29,549 [INFO] ---------------------------------
2019-03-19 04:56:29,550 [INFO] Summary:
2019-03-19 04:56:29,551 [INFO] Batch 28000, worst loss 0.064668 (incl. reg.) of 1000 batches, learning rate 0.000650 @cl.-depth 1
2019-03-19 04:56:29,552 [INFO] Regularization: 4391.900391 * 0.0000010000 = 0.0043919003
2019-03-19 04:56:29,552 [INFO] Sum of grad norms: 0.931531
2019-03-19 04:56:29,553 [INFO] ---------------------------------
2019-03-19 04:56:48,207 [INFO] ---------------------------------
2019-03-19 04:56:48,208 [INFO] Summary:
2019-03-19 04:56:48,208 [INFO] Batch 29000, worst loss 0.066030 (incl. reg.) of 1000 batches, learning rate 0.000647 @cl.-depth 1
2019-03-19 04:56:48,209 [INFO] Regularization: 4332.411133 * 0.0000010000 = 0.0043324111
2019-03-19 04:56:48,209 [INFO] Sum of grad norms: 0.354965
2019-03-19 04:56:48,210 [INFO] ---------------------------------
2019-03-19 04:57:07,034 [INFO] ---------------------------------
2019-03-19 04:57:07,035 [INFO] Summary:
2019-03-19 04:57:07,036 [INFO] Batch 30000, worst loss 0.066890 (incl. reg.) of 1000 batches, learning rate 0.000647 @cl.-depth 1
2019-03-19 04:57:07,036 [INFO] Regularization: 4271.922363 * 0.0000010000 = 0.0042719222
2019-03-19 04:57:07,037 [INFO] Sum of grad norms: 0.737429
2019-03-19 04:57:07,038 [INFO] ---------------------------------
2019-03-19 04:57:11,881 [INFO] ---------------------------------
2019-03-19 04:57:11,882 [INFO] Evaluation:
2019-03-19 04:57:11,883 [INFO] Batch 30000, worst loss 0.060463 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 04:57:11,887 [INFO] ---------------------------------
2019-03-19 04:57:30,664 [INFO] ---------------------------------
2019-03-19 04:57:30,665 [INFO] Summary:
2019-03-19 04:57:30,666 [INFO] Batch 31000, worst loss 0.066328 (incl. reg.) of 1000 batches, learning rate 0.000647 @cl.-depth 1
2019-03-19 04:57:30,666 [INFO] Regularization: 4212.027344 * 0.0000010000 = 0.0042120274
2019-03-19 04:57:30,667 [INFO] Sum of grad norms: 0.225163
2019-03-19 04:57:30,667 [INFO] ---------------------------------
2019-03-19 04:57:49,390 [INFO] ---------------------------------
2019-03-19 04:57:49,392 [INFO] Summary:
2019-03-19 04:57:49,392 [INFO] Batch 32000, worst loss 0.064395 (incl. reg.) of 1000 batches, learning rate 0.000647 @cl.-depth 1
2019-03-19 04:57:49,393 [INFO] Regularization: 4155.536133 * 0.0000010000 = 0.0041555362
2019-03-19 04:57:49,394 [INFO] Sum of grad norms: 0.142318
2019-03-19 04:57:49,395 [INFO] ---------------------------------
2019-03-19 04:58:08,601 [INFO] ---------------------------------
2019-03-19 04:58:08,602 [INFO] Summary:
2019-03-19 04:58:08,603 [INFO] Batch 33000, worst loss 0.064104 (incl. reg.) of 1000 batches, learning rate 0.000644 @cl.-depth 1
2019-03-19 04:58:08,604 [INFO] Regularization: 4097.894531 * 0.0000010000 = 0.0040978943
2019-03-19 04:58:08,605 [INFO] Sum of grad norms: 0.081533
2019-03-19 04:58:08,606 [INFO] ---------------------------------
2019-03-19 04:58:27,408 [INFO] ---------------------------------
2019-03-19 04:58:27,409 [INFO] Summary:
2019-03-19 04:58:27,410 [INFO] Batch 34000, worst loss 0.064916 (incl. reg.) of 1000 batches, learning rate 0.000641 @cl.-depth 1
2019-03-19 04:58:27,410 [INFO] Regularization: 4045.896729 * 0.0000010000 = 0.0040458967
2019-03-19 04:58:27,411 [INFO] Sum of grad norms: 0.166346
2019-03-19 04:58:27,411 [INFO] ---------------------------------
2019-03-19 04:58:46,188 [INFO] ---------------------------------
2019-03-19 04:58:46,189 [INFO] Summary:
2019-03-19 04:58:46,189 [INFO] Batch 35000, worst loss 0.063914 (incl. reg.) of 1000 batches, learning rate 0.000641 @cl.-depth 1
2019-03-19 04:58:46,190 [INFO] Regularization: 3990.808838 * 0.0000010000 = 0.0039908090
2019-03-19 04:58:46,190 [INFO] Sum of grad norms: 0.232967
2019-03-19 04:58:46,191 [INFO] ---------------------------------
2019-03-19 04:59:05,322 [INFO] ---------------------------------
2019-03-19 04:59:05,323 [INFO] Summary:
2019-03-19 04:59:05,323 [INFO] Batch 36000, worst loss 0.064742 (incl. reg.) of 1000 batches, learning rate 0.000639 @cl.-depth 1
2019-03-19 04:59:05,324 [INFO] Regularization: 3931.251221 * 0.0000010000 = 0.0039312514
2019-03-19 04:59:05,324 [INFO] Sum of grad norms: 0.131300
2019-03-19 04:59:05,325 [INFO] ---------------------------------
2019-03-19 04:59:24,723 [INFO] ---------------------------------
2019-03-19 04:59:24,724 [INFO] Summary:
2019-03-19 04:59:24,725 [INFO] Batch 37000, worst loss 0.063918 (incl. reg.) of 1000 batches, learning rate 0.000639 @cl.-depth 1
2019-03-19 04:59:24,725 [INFO] Regularization: 3880.285400 * 0.0000010000 = 0.0038802854
2019-03-19 04:59:24,726 [INFO] Sum of grad norms: 0.634367
2019-03-19 04:59:24,726 [INFO] ---------------------------------
2019-03-19 04:59:44,035 [INFO] ---------------------------------
2019-03-19 04:59:44,036 [INFO] Summary:
2019-03-19 04:59:44,037 [INFO] Batch 38000, worst loss 0.063658 (incl. reg.) of 1000 batches, learning rate 0.000639 @cl.-depth 1
2019-03-19 04:59:44,038 [INFO] Regularization: 3824.615234 * 0.0000010000 = 0.0038246152
2019-03-19 04:59:44,039 [INFO] Sum of grad norms: 0.351391
2019-03-19 04:59:44,039 [INFO] ---------------------------------
2019-03-19 05:00:02,775 [INFO] ---------------------------------
2019-03-19 05:00:02,776 [INFO] Summary:
2019-03-19 05:00:02,776 [INFO] Batch 39000, worst loss 0.063421 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-19 05:00:02,777 [INFO] Regularization: 3773.896484 * 0.0000010000 = 0.0037738965
2019-03-19 05:00:02,777 [INFO] Sum of grad norms: 0.093240
2019-03-19 05:00:02,778 [INFO] ---------------------------------
2019-03-19 05:00:21,629 [INFO] ---------------------------------
2019-03-19 05:00:21,630 [INFO] Summary:
2019-03-19 05:00:21,630 [INFO] Batch 40000, worst loss 0.063213 (incl. reg.) of 1000 batches, learning rate 0.000634 @cl.-depth 1
2019-03-19 05:00:21,631 [INFO] Regularization: 3730.253174 * 0.0000010000 = 0.0037302531
2019-03-19 05:00:21,632 [INFO] Sum of grad norms: 0.268527
2019-03-19 05:00:21,632 [INFO] ---------------------------------
2019-03-19 05:00:26,553 [INFO] ---------------------------------
2019-03-19 05:00:26,554 [INFO] Evaluation:
2019-03-19 05:00:26,555 [INFO] Batch 40000, worst loss 0.058335 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:00:26,556 [INFO] ---------------------------------
2019-03-19 05:00:45,243 [INFO] ---------------------------------
2019-03-19 05:00:45,244 [INFO] Summary:
2019-03-19 05:00:45,244 [INFO] Batch 41000, worst loss 0.064104 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-19 05:00:45,245 [INFO] Regularization: 3679.383057 * 0.0000010000 = 0.0036793831
2019-03-19 05:00:45,246 [INFO] Sum of grad norms: 0.033614
2019-03-19 05:00:45,246 [INFO] ---------------------------------
2019-03-19 05:01:03,830 [INFO] ---------------------------------
2019-03-19 05:01:03,831 [INFO] Summary:
2019-03-19 05:01:03,832 [INFO] Batch 42000, worst loss 0.061849 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-19 05:01:03,832 [INFO] Regularization: 3640.425781 * 0.0000010000 = 0.0036404259
2019-03-19 05:01:03,833 [INFO] Sum of grad norms: 0.199523
2019-03-19 05:01:03,833 [INFO] ---------------------------------
2019-03-19 05:01:23,019 [INFO] ---------------------------------
2019-03-19 05:01:23,020 [INFO] Summary:
2019-03-19 05:01:23,021 [INFO] Batch 43000, worst loss 0.061807 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-19 05:01:23,022 [INFO] Regularization: 3610.772705 * 0.0000010000 = 0.0036107728
2019-03-19 05:01:23,022 [INFO] Sum of grad norms: 0.052660
2019-03-19 05:01:23,023 [INFO] ---------------------------------
2019-03-19 05:01:41,694 [INFO] ---------------------------------
2019-03-19 05:01:41,695 [INFO] Summary:
2019-03-19 05:01:41,696 [INFO] Batch 44000, worst loss 0.061752 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-19 05:01:41,697 [INFO] Regularization: 3578.864746 * 0.0000010000 = 0.0035788647
2019-03-19 05:01:41,697 [INFO] Sum of grad norms: 0.410189
2019-03-19 05:01:41,698 [INFO] ---------------------------------
2019-03-19 05:02:00,843 [INFO] ---------------------------------
2019-03-19 05:02:00,844 [INFO] Summary:
2019-03-19 05:02:00,844 [INFO] Batch 45000, worst loss 0.061980 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-19 05:02:00,845 [INFO] Regularization: 3551.025635 * 0.0000010000 = 0.0035510256
2019-03-19 05:02:00,845 [INFO] Sum of grad norms: 0.095055
2019-03-19 05:02:00,846 [INFO] ---------------------------------
2019-03-19 05:02:19,561 [INFO] ---------------------------------
2019-03-19 05:02:19,562 [INFO] Summary:
2019-03-19 05:02:19,562 [INFO] Batch 46000, worst loss 0.061678 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-19 05:02:19,563 [INFO] Regularization: 3521.298828 * 0.0000010000 = 0.0035212988
2019-03-19 05:02:19,563 [INFO] Sum of grad norms: 0.032859
2019-03-19 05:02:19,564 [INFO] ---------------------------------
2019-03-19 05:02:38,039 [INFO] ---------------------------------
2019-03-19 05:02:38,040 [INFO] Summary:
2019-03-19 05:02:38,040 [INFO] Batch 47000, worst loss 0.061586 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-19 05:02:38,041 [INFO] Regularization: 3489.859619 * 0.0000010000 = 0.0034898596
2019-03-19 05:02:38,041 [INFO] Sum of grad norms: 0.174421
2019-03-19 05:02:38,042 [INFO] ---------------------------------
2019-03-19 05:02:56,586 [INFO] ---------------------------------
2019-03-19 05:02:56,587 [INFO] Summary:
2019-03-19 05:02:56,588 [INFO] Batch 48000, worst loss 0.061467 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-19 05:02:56,589 [INFO] Regularization: 3458.721436 * 0.0000010000 = 0.0034587213
2019-03-19 05:02:56,589 [INFO] Sum of grad norms: 0.034176
2019-03-19 05:02:56,590 [INFO] ---------------------------------
2019-03-19 05:03:15,068 [INFO] ---------------------------------
2019-03-19 05:03:15,069 [INFO] Summary:
2019-03-19 05:03:15,070 [INFO] Batch 49000, worst loss 0.061545 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-19 05:03:15,071 [INFO] Regularization: 3430.895264 * 0.0000010000 = 0.0034308953
2019-03-19 05:03:15,071 [INFO] Sum of grad norms: 0.097876
2019-03-19 05:03:15,072 [INFO] ---------------------------------
2019-03-19 05:03:34,359 [INFO] ---------------------------------
2019-03-19 05:03:34,360 [INFO] Summary:
2019-03-19 05:03:34,361 [INFO] Batch 50000, worst loss 0.061571 (incl. reg.) of 1000 batches, learning rate 0.000316 @cl.-depth 1
2019-03-19 05:03:34,361 [INFO] Regularization: 3403.624756 * 0.0000010000 = 0.0034036248
2019-03-19 05:03:34,362 [INFO] Sum of grad norms: 0.103947
2019-03-19 05:03:34,362 [INFO] ---------------------------------
2019-03-19 05:03:39,312 [INFO] ---------------------------------
2019-03-19 05:03:39,312 [INFO] Evaluation:
2019-03-19 05:03:39,313 [INFO] Batch 50000, worst loss 0.057811 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:03:39,314 [INFO] ---------------------------------
2019-03-19 05:03:58,205 [INFO] ---------------------------------
2019-03-19 05:03:58,206 [INFO] Summary:
2019-03-19 05:03:58,207 [INFO] Batch 51000, worst loss 0.061315 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 05:03:58,207 [INFO] Regularization: 3374.944824 * 0.0000010000 = 0.0033749449
2019-03-19 05:03:58,208 [INFO] Sum of grad norms: 0.082490
2019-03-19 05:03:58,208 [INFO] ---------------------------------
2019-03-19 05:04:16,676 [INFO] ---------------------------------
2019-03-19 05:04:16,677 [INFO] Summary:
2019-03-19 05:04:16,677 [INFO] Batch 52000, worst loss 0.061165 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 05:04:16,678 [INFO] Regularization: 3355.134521 * 0.0000010000 = 0.0033551345
2019-03-19 05:04:16,678 [INFO] Sum of grad norms: 0.209033
2019-03-19 05:04:16,679 [INFO] ---------------------------------
2019-03-19 05:04:35,374 [INFO] ---------------------------------
2019-03-19 05:04:35,375 [INFO] Summary:
2019-03-19 05:04:35,375 [INFO] Batch 53000, worst loss 0.061097 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 05:04:35,376 [INFO] Regularization: 3337.785645 * 0.0000010000 = 0.0033377856
2019-03-19 05:04:35,376 [INFO] Sum of grad norms: 0.094016
2019-03-19 05:04:35,377 [INFO] ---------------------------------
2019-03-19 05:04:54,607 [INFO] ---------------------------------
2019-03-19 05:04:54,608 [INFO] Summary:
2019-03-19 05:04:54,609 [INFO] Batch 54000, worst loss 0.061111 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 05:04:54,609 [INFO] Regularization: 3321.807861 * 0.0000010000 = 0.0033218078
2019-03-19 05:04:54,610 [INFO] Sum of grad norms: 0.251468
2019-03-19 05:04:54,610 [INFO] ---------------------------------
2019-03-19 05:05:13,298 [INFO] ---------------------------------
2019-03-19 05:05:13,299 [INFO] Summary:
2019-03-19 05:05:13,300 [INFO] Batch 55000, worst loss 0.061139 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 05:05:13,300 [INFO] Regularization: 3305.780273 * 0.0000010000 = 0.0033057802
2019-03-19 05:05:13,301 [INFO] Sum of grad norms: 0.048970
2019-03-19 05:05:13,301 [INFO] ---------------------------------
2019-03-19 05:05:32,269 [INFO] ---------------------------------
2019-03-19 05:05:32,270 [INFO] Summary:
2019-03-19 05:05:32,271 [INFO] Batch 56000, worst loss 0.061002 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 05:05:32,272 [INFO] Regularization: 3289.082764 * 0.0000010000 = 0.0032890828
2019-03-19 05:05:32,272 [INFO] Sum of grad norms: 0.042166
2019-03-19 05:05:32,273 [INFO] ---------------------------------
2019-03-19 05:05:51,275 [INFO] ---------------------------------
2019-03-19 05:05:51,276 [INFO] Summary:
2019-03-19 05:05:51,276 [INFO] Batch 57000, worst loss 0.061115 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 05:05:51,277 [INFO] Regularization: 3274.818848 * 0.0000010000 = 0.0032748189
2019-03-19 05:05:51,277 [INFO] Sum of grad norms: 0.069353
2019-03-19 05:05:51,278 [INFO] ---------------------------------
2019-03-19 05:06:10,090 [INFO] ---------------------------------
2019-03-19 05:06:10,091 [INFO] Summary:
2019-03-19 05:06:10,091 [INFO] Batch 58000, worst loss 0.061103 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 05:06:10,092 [INFO] Regularization: 3261.147461 * 0.0000010000 = 0.0032611475
2019-03-19 05:06:10,092 [INFO] Sum of grad norms: 0.039999
2019-03-19 05:06:10,093 [INFO] ---------------------------------
2019-03-19 05:06:28,858 [INFO] ---------------------------------
2019-03-19 05:06:28,859 [INFO] Summary:
2019-03-19 05:06:28,859 [INFO] Batch 59000, worst loss 0.061083 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 05:06:28,860 [INFO] Regularization: 3246.214600 * 0.0000010000 = 0.0032462147
2019-03-19 05:06:28,860 [INFO] Sum of grad norms: 0.163179
2019-03-19 05:06:28,861 [INFO] ---------------------------------
2019-03-19 05:06:47,418 [INFO] ---------------------------------
2019-03-19 05:06:47,419 [INFO] Summary:
2019-03-19 05:06:47,420 [INFO] Batch 60000, worst loss 0.061223 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 05:06:47,420 [INFO] Regularization: 3231.219971 * 0.0000010000 = 0.0032312199
2019-03-19 05:06:47,421 [INFO] Sum of grad norms: 0.095643
2019-03-19 05:06:47,422 [INFO] ---------------------------------
2019-03-19 05:06:52,313 [INFO] ---------------------------------
2019-03-19 05:06:52,314 [INFO] Evaluation:
2019-03-19 05:06:52,315 [INFO] Batch 60000, worst loss 0.057744 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:06:52,316 [INFO] ---------------------------------
2019-03-19 05:07:10,993 [INFO] ---------------------------------
2019-03-19 05:07:10,994 [INFO] Summary:
2019-03-19 05:07:10,994 [INFO] Batch 61000, worst loss 0.060817 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 05:07:10,995 [INFO] Regularization: 3215.743408 * 0.0000010000 = 0.0032157435
2019-03-19 05:07:10,995 [INFO] Sum of grad norms: 0.047370
2019-03-19 05:07:10,996 [INFO] ---------------------------------
2019-03-19 05:07:29,873 [INFO] ---------------------------------
2019-03-19 05:07:29,874 [INFO] Summary:
2019-03-19 05:07:29,875 [INFO] Batch 62000, worst loss 0.061000 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 05:07:29,876 [INFO] Regularization: 3206.836670 * 0.0000010000 = 0.0032068368
2019-03-19 05:07:29,877 [INFO] Sum of grad norms: 0.098445
2019-03-19 05:07:29,877 [INFO] ---------------------------------
2019-03-19 05:07:48,540 [INFO] ---------------------------------
2019-03-19 05:07:48,540 [INFO] Summary:
2019-03-19 05:07:48,541 [INFO] Batch 63000, worst loss 0.060911 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 05:07:48,541 [INFO] Regularization: 3198.712891 * 0.0000010000 = 0.0031987128
2019-03-19 05:07:48,542 [INFO] Sum of grad norms: 0.023638
2019-03-19 05:07:48,543 [INFO] ---------------------------------
2019-03-19 05:08:07,042 [INFO] ---------------------------------
2019-03-19 05:08:07,043 [INFO] Summary:
2019-03-19 05:08:07,043 [INFO] Batch 64000, worst loss 0.060903 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 05:08:07,044 [INFO] Regularization: 3191.447998 * 0.0000010000 = 0.0031914480
2019-03-19 05:08:07,044 [INFO] Sum of grad norms: 0.077717
2019-03-19 05:08:07,045 [INFO] ---------------------------------
2019-03-19 05:08:25,735 [INFO] ---------------------------------
2019-03-19 05:08:25,736 [INFO] Summary:
2019-03-19 05:08:25,737 [INFO] Batch 65000, worst loss 0.060952 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 05:08:25,737 [INFO] Regularization: 3183.555420 * 0.0000010000 = 0.0031835553
2019-03-19 05:08:25,738 [INFO] Sum of grad norms: 0.251352
2019-03-19 05:08:25,739 [INFO] ---------------------------------
2019-03-19 05:08:44,652 [INFO] ---------------------------------
2019-03-19 05:08:44,653 [INFO] Summary:
2019-03-19 05:08:44,654 [INFO] Batch 66000, worst loss 0.060984 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 05:08:44,655 [INFO] Regularization: 3175.572021 * 0.0000010000 = 0.0031755720
2019-03-19 05:08:44,656 [INFO] Sum of grad norms: 0.044113
2019-03-19 05:08:44,657 [INFO] ---------------------------------
2019-03-19 05:09:03,085 [INFO] ---------------------------------
2019-03-19 05:09:03,086 [INFO] Summary:
2019-03-19 05:09:03,087 [INFO] Batch 67000, worst loss 0.060768 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 05:09:03,087 [INFO] Regularization: 3167.847656 * 0.0000010000 = 0.0031678476
2019-03-19 05:09:03,088 [INFO] Sum of grad norms: 0.073462
2019-03-19 05:09:03,088 [INFO] ---------------------------------
2019-03-19 05:09:21,852 [INFO] ---------------------------------
2019-03-19 05:09:21,853 [INFO] Summary:
2019-03-19 05:09:21,854 [INFO] Batch 68000, worst loss 0.060930 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 05:09:21,854 [INFO] Regularization: 3160.027100 * 0.0000010000 = 0.0031600271
2019-03-19 05:09:21,855 [INFO] Sum of grad norms: 0.040537
2019-03-19 05:09:21,856 [INFO] ---------------------------------
2019-03-19 05:09:41,088 [INFO] ---------------------------------
2019-03-19 05:09:41,089 [INFO] Summary:
2019-03-19 05:09:41,090 [INFO] Batch 69000, worst loss 0.060823 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 05:09:41,090 [INFO] Regularization: 3151.969238 * 0.0000010000 = 0.0031519693
2019-03-19 05:09:41,091 [INFO] Sum of grad norms: 0.056311
2019-03-19 05:09:41,091 [INFO] ---------------------------------
2019-03-19 05:10:00,076 [INFO] ---------------------------------
2019-03-19 05:10:00,077 [INFO] Summary:
2019-03-19 05:10:00,078 [INFO] Batch 70000, worst loss 0.060756 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 05:10:00,078 [INFO] Regularization: 3145.383545 * 0.0000010000 = 0.0031453834
2019-03-19 05:10:00,079 [INFO] Sum of grad norms: 0.131026
2019-03-19 05:10:00,080 [INFO] ---------------------------------
2019-03-19 05:10:05,009 [INFO] ---------------------------------
2019-03-19 05:10:05,012 [INFO] Evaluation:
2019-03-19 05:10:05,014 [INFO] Batch 70000, worst loss 0.057712 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:10:05,017 [INFO] ---------------------------------
2019-03-19 05:10:23,857 [INFO] ---------------------------------
2019-03-19 05:10:23,858 [INFO] Summary:
2019-03-19 05:10:23,859 [INFO] Batch 71000, worst loss 0.060834 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 05:10:23,859 [INFO] Regularization: 3137.553711 * 0.0000010000 = 0.0031375538
2019-03-19 05:10:23,860 [INFO] Sum of grad norms: 0.047060
2019-03-19 05:10:23,861 [INFO] ---------------------------------
2019-03-19 05:10:42,559 [INFO] ---------------------------------
2019-03-19 05:10:42,560 [INFO] Summary:
2019-03-19 05:10:42,561 [INFO] Batch 72000, worst loss 0.060850 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 05:10:42,561 [INFO] Regularization: 3132.412354 * 0.0000010000 = 0.0031324124
2019-03-19 05:10:42,562 [INFO] Sum of grad norms: 0.023316
2019-03-19 05:10:42,562 [INFO] ---------------------------------
2019-03-19 05:11:01,228 [INFO] ---------------------------------
2019-03-19 05:11:01,229 [INFO] Summary:
2019-03-19 05:11:01,230 [INFO] Batch 73000, worst loss 0.060705 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 05:11:01,230 [INFO] Regularization: 3128.404541 * 0.0000010000 = 0.0031284045
2019-03-19 05:11:01,231 [INFO] Sum of grad norms: 0.024959
2019-03-19 05:11:01,231 [INFO] ---------------------------------
2019-03-19 05:11:19,964 [INFO] ---------------------------------
2019-03-19 05:11:19,965 [INFO] Summary:
2019-03-19 05:11:19,965 [INFO] Batch 74000, worst loss 0.060720 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 05:11:19,966 [INFO] Regularization: 3124.496582 * 0.0000010000 = 0.0031244967
2019-03-19 05:11:19,966 [INFO] Sum of grad norms: 0.025083
2019-03-19 05:11:19,967 [INFO] ---------------------------------
2019-03-19 05:11:39,110 [INFO] ---------------------------------
2019-03-19 05:11:39,111 [INFO] Summary:
2019-03-19 05:11:39,112 [INFO] Batch 75000, worst loss 0.060727 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 05:11:39,112 [INFO] Regularization: 3120.148438 * 0.0000010000 = 0.0031201483
2019-03-19 05:11:39,113 [INFO] Sum of grad norms: 0.047963
2019-03-19 05:11:39,113 [INFO] ---------------------------------
2019-03-19 05:11:57,669 [INFO] ---------------------------------
2019-03-19 05:11:57,670 [INFO] Summary:
2019-03-19 05:11:57,671 [INFO] Batch 76000, worst loss 0.060759 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 05:11:57,671 [INFO] Regularization: 3115.979980 * 0.0000010000 = 0.0031159800
2019-03-19 05:11:57,672 [INFO] Sum of grad norms: 0.083191
2019-03-19 05:11:57,672 [INFO] ---------------------------------
2019-03-19 05:12:16,349 [INFO] ---------------------------------
2019-03-19 05:12:16,350 [INFO] Summary:
2019-03-19 05:12:16,351 [INFO] Batch 77000, worst loss 0.060665 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 05:12:16,351 [INFO] Regularization: 3112.206299 * 0.0000010000 = 0.0031122062
2019-03-19 05:12:16,352 [INFO] Sum of grad norms: 0.041761
2019-03-19 05:12:16,352 [INFO] ---------------------------------
2019-03-19 05:12:34,884 [INFO] ---------------------------------
2019-03-19 05:12:34,885 [INFO] Summary:
2019-03-19 05:12:34,886 [INFO] Batch 78000, worst loss 0.060638 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 05:12:34,886 [INFO] Regularization: 3108.276367 * 0.0000010000 = 0.0031082763
2019-03-19 05:12:34,887 [INFO] Sum of grad norms: 0.023379
2019-03-19 05:12:34,887 [INFO] ---------------------------------
2019-03-19 05:12:53,812 [INFO] ---------------------------------
2019-03-19 05:12:53,813 [INFO] Summary:
2019-03-19 05:12:53,813 [INFO] Batch 79000, worst loss 0.060779 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 05:12:53,814 [INFO] Regularization: 3104.083496 * 0.0000010000 = 0.0031040835
2019-03-19 05:12:53,814 [INFO] Sum of grad norms: 0.090675
2019-03-19 05:12:53,815 [INFO] ---------------------------------
2019-03-19 05:13:12,336 [INFO] ---------------------------------
2019-03-19 05:13:12,337 [INFO] Summary:
2019-03-19 05:13:12,338 [INFO] Batch 80000, worst loss 0.060712 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 05:13:12,338 [INFO] Regularization: 3099.940918 * 0.0000010000 = 0.0030999410
2019-03-19 05:13:12,339 [INFO] Sum of grad norms: 0.051452
2019-03-19 05:13:12,340 [INFO] ---------------------------------
2019-03-19 05:13:17,349 [INFO] ---------------------------------
2019-03-19 05:13:17,350 [INFO] Evaluation:
2019-03-19 05:13:17,351 [INFO] Batch 80000, worst loss 0.057540 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:13:17,352 [INFO] ---------------------------------
2019-03-19 05:13:36,248 [INFO] ---------------------------------
2019-03-19 05:13:36,248 [INFO] Summary:
2019-03-19 05:13:36,249 [INFO] Batch 81000, worst loss 0.060687 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 05:13:36,250 [INFO] Regularization: 3096.004150 * 0.0000010000 = 0.0030960042
2019-03-19 05:13:36,251 [INFO] Sum of grad norms: 0.060650
2019-03-19 05:13:36,252 [INFO] ---------------------------------
2019-03-19 05:13:55,089 [INFO] ---------------------------------
2019-03-19 05:13:55,090 [INFO] Summary:
2019-03-19 05:13:55,091 [INFO] Batch 82000, worst loss 0.060742 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 05:13:55,092 [INFO] Regularization: 3093.375732 * 0.0000010000 = 0.0030933758
2019-03-19 05:13:55,093 [INFO] Sum of grad norms: 0.047325
2019-03-19 05:13:55,094 [INFO] ---------------------------------
2019-03-19 05:14:13,977 [INFO] ---------------------------------
2019-03-19 05:14:13,978 [INFO] Summary:
2019-03-19 05:14:13,979 [INFO] Batch 83000, worst loss 0.060682 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 05:14:13,980 [INFO] Regularization: 3091.348145 * 0.0000010000 = 0.0030913481
2019-03-19 05:14:13,980 [INFO] Sum of grad norms: 0.064547
2019-03-19 05:14:13,981 [INFO] ---------------------------------
2019-03-19 05:14:32,524 [INFO] ---------------------------------
2019-03-19 05:14:32,525 [INFO] Summary:
2019-03-19 05:14:32,525 [INFO] Batch 84000, worst loss 0.060785 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 05:14:32,526 [INFO] Regularization: 3089.233887 * 0.0000010000 = 0.0030892338
2019-03-19 05:14:32,526 [INFO] Sum of grad norms: 0.030624
2019-03-19 05:14:32,527 [INFO] ---------------------------------
2019-03-19 05:14:51,349 [INFO] ---------------------------------
2019-03-19 05:14:51,350 [INFO] Summary:
2019-03-19 05:14:51,351 [INFO] Batch 85000, worst loss 0.060783 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 05:14:51,351 [INFO] Regularization: 3087.035889 * 0.0000010000 = 0.0030870358
2019-03-19 05:14:51,352 [INFO] Sum of grad norms: 0.027078
2019-03-19 05:14:51,352 [INFO] ---------------------------------
2019-03-19 05:15:10,078 [INFO] ---------------------------------
2019-03-19 05:15:10,079 [INFO] Summary:
2019-03-19 05:15:10,079 [INFO] Batch 86000, worst loss 0.060630 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 05:15:10,080 [INFO] Regularization: 3084.972168 * 0.0000010000 = 0.0030849723
2019-03-19 05:15:10,081 [INFO] Sum of grad norms: 0.025754
2019-03-19 05:15:10,082 [INFO] ---------------------------------
2019-03-19 05:15:28,947 [INFO] ---------------------------------
2019-03-19 05:15:28,948 [INFO] Summary:
2019-03-19 05:15:28,949 [INFO] Batch 87000, worst loss 0.060657 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 05:15:28,950 [INFO] Regularization: 3082.823730 * 0.0000010000 = 0.0030828237
2019-03-19 05:15:28,950 [INFO] Sum of grad norms: 0.082895
2019-03-19 05:15:28,951 [INFO] ---------------------------------
2019-03-19 05:15:47,797 [INFO] ---------------------------------
2019-03-19 05:15:47,798 [INFO] Summary:
2019-03-19 05:15:47,799 [INFO] Batch 88000, worst loss 0.060763 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 05:15:47,800 [INFO] Regularization: 3080.650635 * 0.0000010000 = 0.0030806507
2019-03-19 05:15:47,800 [INFO] Sum of grad norms: 0.095474
2019-03-19 05:15:47,801 [INFO] ---------------------------------
2019-03-19 05:16:06,474 [INFO] ---------------------------------
2019-03-19 05:16:06,475 [INFO] Summary:
2019-03-19 05:16:06,476 [INFO] Batch 89000, worst loss 0.060663 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 05:16:06,476 [INFO] Regularization: 3078.422607 * 0.0000010000 = 0.0030784225
2019-03-19 05:16:06,477 [INFO] Sum of grad norms: 0.043821
2019-03-19 05:16:06,478 [INFO] ---------------------------------
2019-03-19 05:16:25,381 [INFO] ---------------------------------
2019-03-19 05:16:25,383 [INFO] Summary:
2019-03-19 05:16:25,383 [INFO] Batch 90000, worst loss 0.060653 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 05:16:25,384 [INFO] Regularization: 3076.586914 * 0.0000010000 = 0.0030765869
2019-03-19 05:16:25,384 [INFO] Sum of grad norms: 0.046339
2019-03-19 05:16:25,385 [INFO] ---------------------------------
2019-03-19 05:16:30,328 [INFO] ---------------------------------
2019-03-19 05:16:30,329 [INFO] Evaluation:
2019-03-19 05:16:30,330 [INFO] Batch 90000, worst loss 0.057556 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:16:30,331 [INFO] ---------------------------------
2019-03-19 05:16:49,654 [INFO] ---------------------------------
2019-03-19 05:16:49,655 [INFO] Summary:
2019-03-19 05:16:49,656 [INFO] Batch 91000, worst loss 0.060666 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 05:16:49,657 [INFO] Regularization: 3074.623535 * 0.0000010000 = 0.0030746234
2019-03-19 05:16:49,657 [INFO] Sum of grad norms: 0.023542
2019-03-19 05:16:49,658 [INFO] ---------------------------------
2019-03-19 05:17:08,381 [INFO] ---------------------------------
2019-03-19 05:17:08,382 [INFO] Summary:
2019-03-19 05:17:08,383 [INFO] Batch 92000, worst loss 0.060619 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 05:17:08,383 [INFO] Regularization: 3073.251465 * 0.0000010000 = 0.0030732516
2019-03-19 05:17:08,384 [INFO] Sum of grad norms: 0.027259
2019-03-19 05:17:08,384 [INFO] ---------------------------------
2019-03-19 05:17:27,151 [INFO] ---------------------------------
2019-03-19 05:17:27,152 [INFO] Summary:
2019-03-19 05:17:27,153 [INFO] Batch 93000, worst loss 0.060611 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 05:17:27,153 [INFO] Regularization: 3072.041504 * 0.0000010000 = 0.0030720416
2019-03-19 05:17:27,154 [INFO] Sum of grad norms: 0.077094
2019-03-19 05:17:27,154 [INFO] ---------------------------------
2019-03-19 05:17:45,789 [INFO] ---------------------------------
2019-03-19 05:17:45,790 [INFO] Summary:
2019-03-19 05:17:45,790 [INFO] Batch 94000, worst loss 0.060655 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 05:17:45,791 [INFO] Regularization: 3071.070312 * 0.0000010000 = 0.0030710704
2019-03-19 05:17:45,792 [INFO] Sum of grad norms: 0.033685
2019-03-19 05:17:45,792 [INFO] ---------------------------------
2019-03-19 05:18:04,371 [INFO] ---------------------------------
2019-03-19 05:18:04,372 [INFO] Summary:
2019-03-19 05:18:04,373 [INFO] Batch 95000, worst loss 0.060684 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 05:18:04,373 [INFO] Regularization: 3069.866943 * 0.0000010000 = 0.0030698669
2019-03-19 05:18:04,374 [INFO] Sum of grad norms: 0.104538
2019-03-19 05:18:04,375 [INFO] ---------------------------------
2019-03-19 05:18:22,874 [INFO] ---------------------------------
2019-03-19 05:18:22,875 [INFO] Summary:
2019-03-19 05:18:22,876 [INFO] Batch 96000, worst loss 0.060660 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 05:18:22,876 [INFO] Regularization: 3068.734863 * 0.0000010000 = 0.0030687349
2019-03-19 05:18:22,877 [INFO] Sum of grad norms: 0.023226
2019-03-19 05:18:22,877 [INFO] ---------------------------------
2019-03-19 05:18:41,929 [INFO] ---------------------------------
2019-03-19 05:18:41,930 [INFO] Summary:
2019-03-19 05:18:41,930 [INFO] Batch 97000, worst loss 0.060552 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 05:18:41,931 [INFO] Regularization: 3067.844482 * 0.0000010000 = 0.0030678445
2019-03-19 05:18:41,932 [INFO] Sum of grad norms: 0.020063
2019-03-19 05:18:41,932 [INFO] ---------------------------------
2019-03-19 05:19:00,901 [INFO] ---------------------------------
2019-03-19 05:19:00,902 [INFO] Summary:
2019-03-19 05:19:00,902 [INFO] Batch 98000, worst loss 0.060573 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 05:19:00,903 [INFO] Regularization: 3066.710449 * 0.0000010000 = 0.0030667104
2019-03-19 05:19:00,903 [INFO] Sum of grad norms: 0.026630
2019-03-19 05:19:00,904 [INFO] ---------------------------------
2019-03-19 05:19:19,381 [INFO] ---------------------------------
2019-03-19 05:19:19,382 [INFO] Summary:
2019-03-19 05:19:19,382 [INFO] Batch 99000, worst loss 0.060545 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 05:19:19,383 [INFO] Regularization: 3065.672119 * 0.0000010000 = 0.0030656720
2019-03-19 05:19:19,383 [INFO] Sum of grad norms: 0.025202
2019-03-19 05:19:19,384 [INFO] ---------------------------------
2019-03-19 05:19:38,431 [INFO] ---------------------------------
2019-03-19 05:19:38,432 [INFO] Summary:
2019-03-19 05:19:38,432 [INFO] Batch 100000, worst loss 0.060676 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 05:19:38,433 [INFO] Regularization: 3064.749268 * 0.0000010000 = 0.0030647493
2019-03-19 05:19:38,433 [INFO] Sum of grad norms: 0.027716
2019-03-19 05:19:38,434 [INFO] ---------------------------------
2019-03-19 05:19:43,387 [INFO] ---------------------------------
2019-03-19 05:19:43,388 [INFO] Evaluation:
2019-03-19 05:19:43,389 [INFO] Batch 100000, worst loss 0.057646 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:19:43,389 [INFO] ---------------------------------
2019-03-19 05:20:01,882 [INFO] ---------------------------------
2019-03-19 05:20:01,883 [INFO] Summary:
2019-03-19 05:20:01,883 [INFO] Batch 101000, worst loss 0.060717 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 05:20:01,884 [INFO] Regularization: 3063.854004 * 0.0000010000 = 0.0030638541
2019-03-19 05:20:01,884 [INFO] Sum of grad norms: 0.083048
2019-03-19 05:20:01,885 [INFO] ---------------------------------
2019-03-19 05:20:20,626 [INFO] ---------------------------------
2019-03-19 05:20:20,627 [INFO] Summary:
2019-03-19 05:20:20,628 [INFO] Batch 102000, worst loss 0.060653 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 05:20:20,628 [INFO] Regularization: 3062.995850 * 0.0000010000 = 0.0030629958
2019-03-19 05:20:20,629 [INFO] Sum of grad norms: 0.075022
2019-03-19 05:20:20,630 [INFO] ---------------------------------
2019-03-19 05:20:39,311 [INFO] ---------------------------------
2019-03-19 05:20:39,313 [INFO] Summary:
2019-03-19 05:20:39,313 [INFO] Batch 103000, worst loss 0.060712 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 05:20:39,314 [INFO] Regularization: 3062.400879 * 0.0000010000 = 0.0030624010
2019-03-19 05:20:39,314 [INFO] Sum of grad norms: 0.046289
2019-03-19 05:20:39,315 [INFO] ---------------------------------
2019-03-19 05:20:58,108 [INFO] ---------------------------------
2019-03-19 05:20:58,109 [INFO] Summary:
2019-03-19 05:20:58,110 [INFO] Batch 104000, worst loss 0.060684 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 05:20:58,110 [INFO] Regularization: 3061.811279 * 0.0000010000 = 0.0030618112
2019-03-19 05:20:58,111 [INFO] Sum of grad norms: 0.053437
2019-03-19 05:20:58,111 [INFO] ---------------------------------
2019-03-19 05:21:16,951 [INFO] ---------------------------------
2019-03-19 05:21:16,952 [INFO] Summary:
2019-03-19 05:21:16,952 [INFO] Batch 105000, worst loss 0.060604 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 05:21:16,953 [INFO] Regularization: 3061.346191 * 0.0000010000 = 0.0030613462
2019-03-19 05:21:16,953 [INFO] Sum of grad norms: 0.074975
2019-03-19 05:21:16,954 [INFO] ---------------------------------
2019-03-19 05:21:35,754 [INFO] ---------------------------------
2019-03-19 05:21:35,756 [INFO] Summary:
2019-03-19 05:21:35,756 [INFO] Batch 106000, worst loss 0.060514 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 05:21:35,757 [INFO] Regularization: 3060.794678 * 0.0000010000 = 0.0030607947
2019-03-19 05:21:35,757 [INFO] Sum of grad norms: 0.040732
2019-03-19 05:21:35,758 [INFO] ---------------------------------
2019-03-19 05:21:54,658 [INFO] ---------------------------------
2019-03-19 05:21:54,659 [INFO] Summary:
2019-03-19 05:21:54,660 [INFO] Batch 107000, worst loss 0.060620 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 05:21:54,661 [INFO] Regularization: 3060.161865 * 0.0000010000 = 0.0030601618
2019-03-19 05:21:54,662 [INFO] Sum of grad norms: 0.029205
2019-03-19 05:21:54,663 [INFO] ---------------------------------
2019-03-19 05:22:13,180 [INFO] ---------------------------------
2019-03-19 05:22:13,181 [INFO] Summary:
2019-03-19 05:22:13,181 [INFO] Batch 108000, worst loss 0.060596 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 05:22:13,182 [INFO] Regularization: 3059.775146 * 0.0000010000 = 0.0030597751
2019-03-19 05:22:13,182 [INFO] Sum of grad norms: 0.063192
2019-03-19 05:22:13,183 [INFO] ---------------------------------
2019-03-19 05:22:32,335 [INFO] ---------------------------------
2019-03-19 05:22:32,336 [INFO] Summary:
2019-03-19 05:22:32,336 [INFO] Batch 109000, worst loss 0.060517 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 05:22:32,337 [INFO] Regularization: 3059.191895 * 0.0000010000 = 0.0030591919
2019-03-19 05:22:32,337 [INFO] Sum of grad norms: 0.036069
2019-03-19 05:22:32,338 [INFO] ---------------------------------
2019-03-19 05:22:51,157 [INFO] ---------------------------------
2019-03-19 05:22:51,158 [INFO] Summary:
2019-03-19 05:22:51,158 [INFO] Batch 110000, worst loss 0.060592 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 05:22:51,159 [INFO] Regularization: 3058.764893 * 0.0000010000 = 0.0030587649
2019-03-19 05:22:51,159 [INFO] Sum of grad norms: 0.032289
2019-03-19 05:22:51,160 [INFO] ---------------------------------
2019-03-19 05:22:56,096 [INFO] ---------------------------------
2019-03-19 05:22:56,098 [INFO] Evaluation:
2019-03-19 05:22:56,099 [INFO] Batch 110000, worst loss 0.057512 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:22:56,100 [INFO] ---------------------------------
2019-03-19 05:23:14,966 [INFO] ---------------------------------
2019-03-19 05:23:14,967 [INFO] Summary:
2019-03-19 05:23:14,968 [INFO] Batch 111000, worst loss 0.060567 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 05:23:14,968 [INFO] Regularization: 3058.186035 * 0.0000010000 = 0.0030581860
2019-03-19 05:23:14,969 [INFO] Sum of grad norms: 0.038269
2019-03-19 05:23:14,969 [INFO] ---------------------------------
2019-03-19 05:23:33,688 [INFO] ---------------------------------
2019-03-19 05:23:33,689 [INFO] Summary:
2019-03-19 05:23:33,690 [INFO] Batch 112000, worst loss 0.060691 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 05:23:33,690 [INFO] Regularization: 3057.782715 * 0.0000010000 = 0.0030577828
2019-03-19 05:23:33,691 [INFO] Sum of grad norms: 0.025121
2019-03-19 05:23:33,691 [INFO] ---------------------------------
2019-03-19 05:23:52,645 [INFO] ---------------------------------
2019-03-19 05:23:52,646 [INFO] Summary:
2019-03-19 05:23:52,646 [INFO] Batch 113000, worst loss 0.060687 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 05:23:52,647 [INFO] Regularization: 3057.468018 * 0.0000010000 = 0.0030574680
2019-03-19 05:23:52,647 [INFO] Sum of grad norms: 0.076807
2019-03-19 05:23:52,648 [INFO] ---------------------------------
2019-03-19 05:24:11,657 [INFO] ---------------------------------
2019-03-19 05:24:11,658 [INFO] Summary:
2019-03-19 05:24:11,659 [INFO] Batch 114000, worst loss 0.060587 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 05:24:11,660 [INFO] Regularization: 3057.246582 * 0.0000010000 = 0.0030572466
2019-03-19 05:24:11,660 [INFO] Sum of grad norms: 0.044327
2019-03-19 05:24:11,661 [INFO] ---------------------------------
2019-03-19 05:24:30,911 [INFO] ---------------------------------
2019-03-19 05:24:30,912 [INFO] Summary:
2019-03-19 05:24:30,913 [INFO] Batch 115000, worst loss 0.060603 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 05:24:30,913 [INFO] Regularization: 3056.960938 * 0.0000010000 = 0.0030569609
2019-03-19 05:24:30,914 [INFO] Sum of grad norms: 0.031980
2019-03-19 05:24:30,915 [INFO] ---------------------------------
2019-03-19 05:24:49,790 [INFO] ---------------------------------
2019-03-19 05:24:49,791 [INFO] Summary:
2019-03-19 05:24:49,792 [INFO] Batch 116000, worst loss 0.060800 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 05:24:49,792 [INFO] Regularization: 3056.690430 * 0.0000010000 = 0.0030566903
2019-03-19 05:24:49,793 [INFO] Sum of grad norms: 0.040847
2019-03-19 05:24:49,793 [INFO] ---------------------------------
2019-03-19 05:25:08,393 [INFO] ---------------------------------
2019-03-19 05:25:08,394 [INFO] Summary:
2019-03-19 05:25:08,395 [INFO] Batch 117000, worst loss 0.060528 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 05:25:08,395 [INFO] Regularization: 3056.423340 * 0.0000010000 = 0.0030564233
2019-03-19 05:25:08,396 [INFO] Sum of grad norms: 0.026402
2019-03-19 05:25:08,396 [INFO] ---------------------------------
2019-03-19 05:25:27,232 [INFO] ---------------------------------
2019-03-19 05:25:27,232 [INFO] Summary:
2019-03-19 05:25:27,233 [INFO] Batch 118000, worst loss 0.060519 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 05:25:27,234 [INFO] Regularization: 3056.072754 * 0.0000010000 = 0.0030560726
2019-03-19 05:25:27,234 [INFO] Sum of grad norms: 0.026531
2019-03-19 05:25:27,235 [INFO] ---------------------------------
2019-03-19 05:25:45,835 [INFO] ---------------------------------
2019-03-19 05:25:45,835 [INFO] Summary:
2019-03-19 05:25:45,836 [INFO] Batch 119000, worst loss 0.060598 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 05:25:45,837 [INFO] Regularization: 3055.814209 * 0.0000010000 = 0.0030558142
2019-03-19 05:25:45,838 [INFO] Sum of grad norms: 0.054018
2019-03-19 05:25:45,839 [INFO] ---------------------------------
2019-03-19 05:26:04,601 [INFO] ---------------------------------
2019-03-19 05:26:04,602 [INFO] Summary:
2019-03-19 05:26:04,602 [INFO] Batch 120000, worst loss 0.060707 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 05:26:04,603 [INFO] Regularization: 3055.577148 * 0.0000010000 = 0.0030555772
2019-03-19 05:26:04,603 [INFO] Sum of grad norms: 0.035429
2019-03-19 05:26:04,604 [INFO] ---------------------------------
2019-03-19 05:26:09,634 [INFO] ---------------------------------
2019-03-19 05:26:09,635 [INFO] Evaluation:
2019-03-19 05:26:09,636 [INFO] Batch 120000, worst loss 0.057558 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:26:09,637 [INFO] ---------------------------------
2019-03-19 05:26:28,119 [INFO] ---------------------------------
2019-03-19 05:26:28,120 [INFO] Summary:
2019-03-19 05:26:28,120 [INFO] Batch 121000, worst loss 0.060591 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:26:28,121 [INFO] Regularization: 3055.322754 * 0.0000010000 = 0.0030553227
2019-03-19 05:26:28,121 [INFO] Sum of grad norms: 0.035044
2019-03-19 05:26:28,122 [INFO] ---------------------------------
2019-03-19 05:26:46,528 [INFO] ---------------------------------
2019-03-19 05:26:46,529 [INFO] Summary:
2019-03-19 05:26:46,529 [INFO] Batch 122000, worst loss 0.060633 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:26:46,530 [INFO] Regularization: 3055.109619 * 0.0000010000 = 0.0030551096
2019-03-19 05:26:46,530 [INFO] Sum of grad norms: 0.056101
2019-03-19 05:26:46,531 [INFO] ---------------------------------
2019-03-19 05:27:05,154 [INFO] ---------------------------------
2019-03-19 05:27:05,155 [INFO] Summary:
2019-03-19 05:27:05,155 [INFO] Batch 123000, worst loss 0.060514 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:27:05,156 [INFO] Regularization: 3054.957764 * 0.0000010000 = 0.0030549578
2019-03-19 05:27:05,156 [INFO] Sum of grad norms: 0.018813
2019-03-19 05:27:05,157 [INFO] ---------------------------------
2019-03-19 05:27:24,242 [INFO] ---------------------------------
2019-03-19 05:27:24,243 [INFO] Summary:
2019-03-19 05:27:24,244 [INFO] Batch 124000, worst loss 0.060631 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:27:24,245 [INFO] Regularization: 3054.782227 * 0.0000010000 = 0.0030547823
2019-03-19 05:27:24,246 [INFO] Sum of grad norms: 0.016533
2019-03-19 05:27:24,247 [INFO] ---------------------------------
2019-03-19 05:27:43,122 [INFO] ---------------------------------
2019-03-19 05:27:43,123 [INFO] Summary:
2019-03-19 05:27:43,124 [INFO] Batch 125000, worst loss 0.060631 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:27:43,124 [INFO] Regularization: 3054.673096 * 0.0000010000 = 0.0030546731
2019-03-19 05:27:43,125 [INFO] Sum of grad norms: 0.026318
2019-03-19 05:27:43,125 [INFO] ---------------------------------
2019-03-19 05:28:02,080 [INFO] ---------------------------------
2019-03-19 05:28:02,081 [INFO] Summary:
2019-03-19 05:28:02,082 [INFO] Batch 126000, worst loss 0.060603 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:28:02,082 [INFO] Regularization: 3054.545166 * 0.0000010000 = 0.0030545453
2019-03-19 05:28:02,083 [INFO] Sum of grad norms: 0.046895
2019-03-19 05:28:02,084 [INFO] ---------------------------------
2019-03-19 05:28:20,882 [INFO] ---------------------------------
2019-03-19 05:28:20,883 [INFO] Summary:
2019-03-19 05:28:20,884 [INFO] Batch 127000, worst loss 0.060654 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:28:20,884 [INFO] Regularization: 3054.375488 * 0.0000010000 = 0.0030543755
2019-03-19 05:28:20,885 [INFO] Sum of grad norms: 0.019556
2019-03-19 05:28:20,885 [INFO] ---------------------------------
2019-03-19 05:28:39,895 [INFO] ---------------------------------
2019-03-19 05:28:39,896 [INFO] Summary:
2019-03-19 05:28:39,896 [INFO] Batch 128000, worst loss 0.060758 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:28:39,897 [INFO] Regularization: 3054.247803 * 0.0000010000 = 0.0030542477
2019-03-19 05:28:39,897 [INFO] Sum of grad norms: 0.022671
2019-03-19 05:28:39,898 [INFO] ---------------------------------
2019-03-19 05:28:58,739 [INFO] ---------------------------------
2019-03-19 05:28:58,740 [INFO] Summary:
2019-03-19 05:28:58,740 [INFO] Batch 129000, worst loss 0.060621 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:28:58,741 [INFO] Regularization: 3054.081543 * 0.0000010000 = 0.0030540815
2019-03-19 05:28:58,741 [INFO] Sum of grad norms: 0.058993
2019-03-19 05:28:58,742 [INFO] ---------------------------------
2019-03-19 05:29:17,529 [INFO] ---------------------------------
2019-03-19 05:29:17,530 [INFO] Summary:
2019-03-19 05:29:17,530 [INFO] Batch 130000, worst loss 0.060548 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:29:17,531 [INFO] Regularization: 3053.962646 * 0.0000010000 = 0.0030539627
2019-03-19 05:29:17,531 [INFO] Sum of grad norms: 0.022710
2019-03-19 05:29:17,532 [INFO] ---------------------------------
2019-03-19 05:29:22,490 [INFO] ---------------------------------
2019-03-19 05:29:22,490 [INFO] Evaluation:
2019-03-19 05:29:22,491 [INFO] Batch 130000, worst loss 0.057517 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:29:22,492 [INFO] ---------------------------------
2019-03-19 05:29:41,597 [INFO] ---------------------------------
2019-03-19 05:29:41,597 [INFO] Summary:
2019-03-19 05:29:41,598 [INFO] Batch 131000, worst loss 0.060613 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:29:41,599 [INFO] Regularization: 3053.819824 * 0.0000010000 = 0.0030538198
2019-03-19 05:29:41,599 [INFO] Sum of grad norms: 0.040772
2019-03-19 05:29:41,600 [INFO] ---------------------------------
2019-03-19 05:30:00,787 [INFO] ---------------------------------
2019-03-19 05:30:00,788 [INFO] Summary:
2019-03-19 05:30:00,789 [INFO] Batch 132000, worst loss 0.060586 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:30:00,790 [INFO] Regularization: 3053.724365 * 0.0000010000 = 0.0030537243
2019-03-19 05:30:00,790 [INFO] Sum of grad norms: 0.021662
2019-03-19 05:30:00,791 [INFO] ---------------------------------
2019-03-19 05:30:19,327 [INFO] ---------------------------------
2019-03-19 05:30:19,328 [INFO] Summary:
2019-03-19 05:30:19,328 [INFO] Batch 133000, worst loss 0.060688 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:30:19,329 [INFO] Regularization: 3053.652588 * 0.0000010000 = 0.0030536526
2019-03-19 05:30:19,330 [INFO] Sum of grad norms: 0.034530
2019-03-19 05:30:19,330 [INFO] ---------------------------------
2019-03-19 05:30:38,124 [INFO] ---------------------------------
2019-03-19 05:30:38,125 [INFO] Summary:
2019-03-19 05:30:38,126 [INFO] Batch 134000, worst loss 0.060519 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:30:38,126 [INFO] Regularization: 3053.600586 * 0.0000010000 = 0.0030536007
2019-03-19 05:30:38,127 [INFO] Sum of grad norms: 0.026359
2019-03-19 05:30:38,127 [INFO] ---------------------------------
2019-03-19 05:30:56,887 [INFO] ---------------------------------
2019-03-19 05:30:56,888 [INFO] Summary:
2019-03-19 05:30:56,888 [INFO] Batch 135000, worst loss 0.060644 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:30:56,889 [INFO] Regularization: 3053.513672 * 0.0000010000 = 0.0030535136
2019-03-19 05:30:56,889 [INFO] Sum of grad norms: 0.026222
2019-03-19 05:30:56,890 [INFO] ---------------------------------
2019-03-19 05:31:15,646 [INFO] ---------------------------------
2019-03-19 05:31:15,647 [INFO] Summary:
2019-03-19 05:31:15,647 [INFO] Batch 136000, worst loss 0.060645 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:31:15,648 [INFO] Regularization: 3053.437988 * 0.0000010000 = 0.0030534379
2019-03-19 05:31:15,648 [INFO] Sum of grad norms: 0.020108
2019-03-19 05:31:15,649 [INFO] ---------------------------------
2019-03-19 05:31:34,625 [INFO] ---------------------------------
2019-03-19 05:31:34,626 [INFO] Summary:
2019-03-19 05:31:34,627 [INFO] Batch 137000, worst loss 0.060603 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:31:34,627 [INFO] Regularization: 3053.376709 * 0.0000010000 = 0.0030533767
2019-03-19 05:31:34,628 [INFO] Sum of grad norms: 0.024189
2019-03-19 05:31:34,628 [INFO] ---------------------------------
2019-03-19 05:31:53,258 [INFO] ---------------------------------
2019-03-19 05:31:53,259 [INFO] Summary:
2019-03-19 05:31:53,259 [INFO] Batch 138000, worst loss 0.060576 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:31:53,260 [INFO] Regularization: 3053.304443 * 0.0000010000 = 0.0030533045
2019-03-19 05:31:53,261 [INFO] Sum of grad norms: 0.045838
2019-03-19 05:31:53,261 [INFO] ---------------------------------
2019-03-19 05:32:12,302 [INFO] ---------------------------------
2019-03-19 05:32:12,303 [INFO] Summary:
2019-03-19 05:32:12,304 [INFO] Batch 139000, worst loss 0.060500 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:32:12,304 [INFO] Regularization: 3053.244629 * 0.0000010000 = 0.0030532447
2019-03-19 05:32:12,305 [INFO] Sum of grad norms: 0.040744
2019-03-19 05:32:12,305 [INFO] ---------------------------------
2019-03-19 05:32:31,045 [INFO] ---------------------------------
2019-03-19 05:32:31,046 [INFO] Summary:
2019-03-19 05:32:31,047 [INFO] Batch 140000, worst loss 0.060521 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 05:32:31,048 [INFO] Regularization: 3053.160645 * 0.0000010000 = 0.0030531606
2019-03-19 05:32:31,048 [INFO] Sum of grad norms: 0.045638
2019-03-19 05:32:31,049 [INFO] ---------------------------------
2019-03-19 05:32:36,003 [INFO] ---------------------------------
2019-03-19 05:32:36,004 [INFO] Evaluation:
2019-03-19 05:32:36,005 [INFO] Batch 140000, worst loss 0.057460 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:32:36,005 [INFO] ---------------------------------
2019-03-19 05:32:54,965 [INFO] ---------------------------------
2019-03-19 05:32:54,966 [INFO] Summary:
2019-03-19 05:32:54,966 [INFO] Batch 141000, worst loss 0.060551 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:32:54,967 [INFO] Regularization: 3053.107422 * 0.0000010000 = 0.0030531073
2019-03-19 05:32:54,967 [INFO] Sum of grad norms: 0.038300
2019-03-19 05:32:54,968 [INFO] ---------------------------------
2019-03-19 05:33:13,880 [INFO] ---------------------------------
2019-03-19 05:33:13,880 [INFO] Summary:
2019-03-19 05:33:13,881 [INFO] Batch 142000, worst loss 0.060640 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:33:13,882 [INFO] Regularization: 3053.062988 * 0.0000010000 = 0.0030530631
2019-03-19 05:33:13,882 [INFO] Sum of grad norms: 0.027697
2019-03-19 05:33:13,883 [INFO] ---------------------------------
2019-03-19 05:33:32,783 [INFO] ---------------------------------
2019-03-19 05:33:32,783 [INFO] Summary:
2019-03-19 05:33:32,784 [INFO] Batch 143000, worst loss 0.060653 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:33:32,785 [INFO] Regularization: 3053.019287 * 0.0000010000 = 0.0030530193
2019-03-19 05:33:32,785 [INFO] Sum of grad norms: 0.025830
2019-03-19 05:33:32,786 [INFO] ---------------------------------
2019-03-19 05:33:51,666 [INFO] ---------------------------------
2019-03-19 05:33:51,667 [INFO] Summary:
2019-03-19 05:33:51,668 [INFO] Batch 144000, worst loss 0.060652 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:33:51,668 [INFO] Regularization: 3052.983154 * 0.0000010000 = 0.0030529832
2019-03-19 05:33:51,669 [INFO] Sum of grad norms: 0.031722
2019-03-19 05:33:51,670 [INFO] ---------------------------------
2019-03-19 05:34:10,241 [INFO] ---------------------------------
2019-03-19 05:34:10,242 [INFO] Summary:
2019-03-19 05:34:10,242 [INFO] Batch 145000, worst loss 0.060506 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:34:10,243 [INFO] Regularization: 3052.942627 * 0.0000010000 = 0.0030529427
2019-03-19 05:34:10,244 [INFO] Sum of grad norms: 0.034765
2019-03-19 05:34:10,245 [INFO] ---------------------------------
2019-03-19 05:34:29,068 [INFO] ---------------------------------
2019-03-19 05:34:29,069 [INFO] Summary:
2019-03-19 05:34:29,070 [INFO] Batch 146000, worst loss 0.060674 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:34:29,070 [INFO] Regularization: 3052.909424 * 0.0000010000 = 0.0030529094
2019-03-19 05:34:29,071 [INFO] Sum of grad norms: 0.055550
2019-03-19 05:34:29,072 [INFO] ---------------------------------
2019-03-19 05:34:47,978 [INFO] ---------------------------------
2019-03-19 05:34:47,979 [INFO] Summary:
2019-03-19 05:34:47,980 [INFO] Batch 147000, worst loss 0.060629 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:34:47,980 [INFO] Regularization: 3052.864746 * 0.0000010000 = 0.0030528647
2019-03-19 05:34:47,980 [INFO] Sum of grad norms: 0.096049
2019-03-19 05:34:47,981 [INFO] ---------------------------------
2019-03-19 05:35:06,629 [INFO] ---------------------------------
2019-03-19 05:35:06,630 [INFO] Summary:
2019-03-19 05:35:06,630 [INFO] Batch 148000, worst loss 0.060629 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:35:06,631 [INFO] Regularization: 3052.830322 * 0.0000010000 = 0.0030528302
2019-03-19 05:35:06,631 [INFO] Sum of grad norms: 0.098311
2019-03-19 05:35:06,632 [INFO] ---------------------------------
2019-03-19 05:35:25,883 [INFO] ---------------------------------
2019-03-19 05:35:25,884 [INFO] Summary:
2019-03-19 05:35:25,885 [INFO] Batch 149000, worst loss 0.060623 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:35:25,885 [INFO] Regularization: 3052.794434 * 0.0000010000 = 0.0030527944
2019-03-19 05:35:25,886 [INFO] Sum of grad norms: 0.034371
2019-03-19 05:35:25,886 [INFO] ---------------------------------
2019-03-19 05:35:44,487 [INFO] ---------------------------------
2019-03-19 05:35:44,488 [INFO] Summary:
2019-03-19 05:35:44,489 [INFO] Batch 150000, worst loss 0.060497 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:35:44,489 [INFO] Regularization: 3052.748291 * 0.0000010000 = 0.0030527483
2019-03-19 05:35:44,490 [INFO] Sum of grad norms: 0.024420
2019-03-19 05:35:44,490 [INFO] ---------------------------------
2019-03-19 05:35:49,386 [INFO] ---------------------------------
2019-03-19 05:35:49,386 [INFO] Evaluation:
2019-03-19 05:35:49,387 [INFO] Batch 150000, worst loss 0.057495 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:35:49,388 [INFO] ---------------------------------
2019-03-19 05:36:08,169 [INFO] ---------------------------------
2019-03-19 05:36:08,170 [INFO] Summary:
2019-03-19 05:36:08,170 [INFO] Batch 151000, worst loss 0.060480 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:36:08,171 [INFO] Regularization: 3052.716309 * 0.0000010000 = 0.0030527164
2019-03-19 05:36:08,171 [INFO] Sum of grad norms: 0.050156
2019-03-19 05:36:08,172 [INFO] ---------------------------------
2019-03-19 05:36:27,010 [INFO] ---------------------------------
2019-03-19 05:36:27,011 [INFO] Summary:
2019-03-19 05:36:27,012 [INFO] Batch 152000, worst loss 0.060627 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:36:27,012 [INFO] Regularization: 3052.690430 * 0.0000010000 = 0.0030526905
2019-03-19 05:36:27,013 [INFO] Sum of grad norms: 0.039027
2019-03-19 05:36:27,013 [INFO] ---------------------------------
2019-03-19 05:36:45,992 [INFO] ---------------------------------
2019-03-19 05:36:45,993 [INFO] Summary:
2019-03-19 05:36:45,994 [INFO] Batch 153000, worst loss 0.060633 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:36:45,994 [INFO] Regularization: 3052.673096 * 0.0000010000 = 0.0030526731
2019-03-19 05:36:45,995 [INFO] Sum of grad norms: 0.072146
2019-03-19 05:36:45,995 [INFO] ---------------------------------
2019-03-19 05:37:04,834 [INFO] ---------------------------------
2019-03-19 05:37:04,835 [INFO] Summary:
2019-03-19 05:37:04,836 [INFO] Batch 154000, worst loss 0.060652 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:37:04,837 [INFO] Regularization: 3052.651855 * 0.0000010000 = 0.0030526519
2019-03-19 05:37:04,838 [INFO] Sum of grad norms: 0.021946
2019-03-19 05:37:04,839 [INFO] ---------------------------------
2019-03-19 05:37:23,643 [INFO] ---------------------------------
2019-03-19 05:37:23,644 [INFO] Summary:
2019-03-19 05:37:23,644 [INFO] Batch 155000, worst loss 0.060807 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:37:23,645 [INFO] Regularization: 3052.634521 * 0.0000010000 = 0.0030526344
2019-03-19 05:37:23,646 [INFO] Sum of grad norms: 0.020130
2019-03-19 05:37:23,647 [INFO] ---------------------------------
2019-03-19 05:37:42,546 [INFO] ---------------------------------
2019-03-19 05:37:42,547 [INFO] Summary:
2019-03-19 05:37:42,547 [INFO] Batch 156000, worst loss 0.060808 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:37:42,548 [INFO] Regularization: 3052.617432 * 0.0000010000 = 0.0030526174
2019-03-19 05:37:42,549 [INFO] Sum of grad norms: 0.032351
2019-03-19 05:37:42,549 [INFO] ---------------------------------
2019-03-19 05:38:01,359 [INFO] ---------------------------------
2019-03-19 05:38:01,360 [INFO] Summary:
2019-03-19 05:38:01,360 [INFO] Batch 157000, worst loss 0.060611 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:38:01,361 [INFO] Regularization: 3052.603027 * 0.0000010000 = 0.0030526030
2019-03-19 05:38:01,361 [INFO] Sum of grad norms: 0.057753
2019-03-19 05:38:01,362 [INFO] ---------------------------------
2019-03-19 05:38:19,771 [INFO] ---------------------------------
2019-03-19 05:38:19,772 [INFO] Summary:
2019-03-19 05:38:19,772 [INFO] Batch 158000, worst loss 0.060611 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:38:19,773 [INFO] Regularization: 3052.581543 * 0.0000010000 = 0.0030525816
2019-03-19 05:38:19,773 [INFO] Sum of grad norms: 0.071871
2019-03-19 05:38:19,774 [INFO] ---------------------------------
2019-03-19 05:38:38,505 [INFO] ---------------------------------
2019-03-19 05:38:38,506 [INFO] Summary:
2019-03-19 05:38:38,506 [INFO] Batch 159000, worst loss 0.060539 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:38:38,507 [INFO] Regularization: 3052.564453 * 0.0000010000 = 0.0030525643
2019-03-19 05:38:38,508 [INFO] Sum of grad norms: 0.023632
2019-03-19 05:38:38,508 [INFO] ---------------------------------
2019-03-19 05:38:57,073 [INFO] ---------------------------------
2019-03-19 05:38:57,074 [INFO] Summary:
2019-03-19 05:38:57,075 [INFO] Batch 160000, worst loss 0.060533 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:38:57,075 [INFO] Regularization: 3052.544434 * 0.0000010000 = 0.0030525443
2019-03-19 05:38:57,076 [INFO] Sum of grad norms: 0.052087
2019-03-19 05:38:57,076 [INFO] ---------------------------------
2019-03-19 05:39:01,962 [INFO] ---------------------------------
2019-03-19 05:39:01,963 [INFO] Evaluation:
2019-03-19 05:39:01,964 [INFO] Batch 160000, worst loss 0.057518 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:39:01,964 [INFO] ---------------------------------
2019-03-19 05:39:20,943 [INFO] ---------------------------------
2019-03-19 05:39:20,944 [INFO] Summary:
2019-03-19 05:39:20,945 [INFO] Batch 161000, worst loss 0.060563 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:39:20,945 [INFO] Regularization: 3052.527588 * 0.0000010000 = 0.0030525276
2019-03-19 05:39:20,946 [INFO] Sum of grad norms: 0.042112
2019-03-19 05:39:20,946 [INFO] ---------------------------------
2019-03-19 05:39:39,502 [INFO] ---------------------------------
2019-03-19 05:39:39,503 [INFO] Summary:
2019-03-19 05:39:39,504 [INFO] Batch 162000, worst loss 0.060576 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:39:39,504 [INFO] Regularization: 3052.510254 * 0.0000010000 = 0.0030525103
2019-03-19 05:39:39,505 [INFO] Sum of grad norms: 0.031934
2019-03-19 05:39:39,505 [INFO] ---------------------------------
2019-03-19 05:39:58,341 [INFO] ---------------------------------
2019-03-19 05:39:58,342 [INFO] Summary:
2019-03-19 05:39:58,343 [INFO] Batch 163000, worst loss 0.060910 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:39:58,343 [INFO] Regularization: 3052.500000 * 0.0000010000 = 0.0030525001
2019-03-19 05:39:58,344 [INFO] Sum of grad norms: 0.058061
2019-03-19 05:39:58,344 [INFO] ---------------------------------
2019-03-19 05:40:17,138 [INFO] ---------------------------------
2019-03-19 05:40:17,139 [INFO] Summary:
2019-03-19 05:40:17,139 [INFO] Batch 164000, worst loss 0.060537 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:40:17,140 [INFO] Regularization: 3052.489746 * 0.0000010000 = 0.0030524898
2019-03-19 05:40:17,140 [INFO] Sum of grad norms: 0.031199
2019-03-19 05:40:17,141 [INFO] ---------------------------------
2019-03-19 05:40:36,057 [INFO] ---------------------------------
2019-03-19 05:40:36,058 [INFO] Summary:
2019-03-19 05:40:36,059 [INFO] Batch 165000, worst loss 0.060580 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:40:36,059 [INFO] Regularization: 3052.481689 * 0.0000010000 = 0.0030524817
2019-03-19 05:40:36,060 [INFO] Sum of grad norms: 0.023221
2019-03-19 05:40:36,061 [INFO] ---------------------------------
2019-03-19 05:40:54,742 [INFO] ---------------------------------
2019-03-19 05:40:54,743 [INFO] Summary:
2019-03-19 05:40:54,743 [INFO] Batch 166000, worst loss 0.060580 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:40:54,744 [INFO] Regularization: 3052.473877 * 0.0000010000 = 0.0030524738
2019-03-19 05:40:54,744 [INFO] Sum of grad norms: 0.055885
2019-03-19 05:40:54,745 [INFO] ---------------------------------
2019-03-19 05:41:13,531 [INFO] ---------------------------------
2019-03-19 05:41:13,532 [INFO] Summary:
2019-03-19 05:41:13,533 [INFO] Batch 167000, worst loss 0.060545 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:41:13,533 [INFO] Regularization: 3052.468262 * 0.0000010000 = 0.0030524682
2019-03-19 05:41:13,534 [INFO] Sum of grad norms: 0.028897
2019-03-19 05:41:13,534 [INFO] ---------------------------------
2019-03-19 05:41:32,309 [INFO] ---------------------------------
2019-03-19 05:41:32,309 [INFO] Summary:
2019-03-19 05:41:32,310 [INFO] Batch 168000, worst loss 0.060622 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:41:32,311 [INFO] Regularization: 3052.460449 * 0.0000010000 = 0.0030524605
2019-03-19 05:41:32,312 [INFO] Sum of grad norms: 0.081270
2019-03-19 05:41:32,312 [INFO] ---------------------------------
2019-03-19 05:41:51,114 [INFO] ---------------------------------
2019-03-19 05:41:51,115 [INFO] Summary:
2019-03-19 05:41:51,115 [INFO] Batch 169000, worst loss 0.060622 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:41:51,116 [INFO] Regularization: 3052.452637 * 0.0000010000 = 0.0030524526
2019-03-19 05:41:51,116 [INFO] Sum of grad norms: 0.027644
2019-03-19 05:41:51,117 [INFO] ---------------------------------
2019-03-19 05:42:09,647 [INFO] ---------------------------------
2019-03-19 05:42:09,649 [INFO] Summary:
2019-03-19 05:42:09,649 [INFO] Batch 170000, worst loss 0.060547 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:42:09,650 [INFO] Regularization: 3052.442627 * 0.0000010000 = 0.0030524426
2019-03-19 05:42:09,650 [INFO] Sum of grad norms: 0.030397
2019-03-19 05:42:09,651 [INFO] ---------------------------------
2019-03-19 05:42:14,637 [INFO] ---------------------------------
2019-03-19 05:42:14,638 [INFO] Evaluation:
2019-03-19 05:42:14,639 [INFO] Batch 170000, worst loss 0.057534 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:42:14,640 [INFO] ---------------------------------
2019-03-19 05:42:33,365 [INFO] ---------------------------------
2019-03-19 05:42:33,366 [INFO] Summary:
2019-03-19 05:42:33,367 [INFO] Batch 171000, worst loss 0.060753 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:42:33,368 [INFO] Regularization: 3052.436035 * 0.0000010000 = 0.0030524360
2019-03-19 05:42:33,368 [INFO] Sum of grad norms: 0.021972
2019-03-19 05:42:33,369 [INFO] ---------------------------------
2019-03-19 05:42:52,422 [INFO] ---------------------------------
2019-03-19 05:42:52,423 [INFO] Summary:
2019-03-19 05:42:52,424 [INFO] Batch 172000, worst loss 0.060753 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:42:52,425 [INFO] Regularization: 3052.431641 * 0.0000010000 = 0.0030524316
2019-03-19 05:42:52,425 [INFO] Sum of grad norms: 0.017425
2019-03-19 05:42:52,426 [INFO] ---------------------------------
2019-03-19 05:43:11,001 [INFO] ---------------------------------
2019-03-19 05:43:11,002 [INFO] Summary:
2019-03-19 05:43:11,003 [INFO] Batch 173000, worst loss 0.060637 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:43:11,003 [INFO] Regularization: 3052.427246 * 0.0000010000 = 0.0030524272
2019-03-19 05:43:11,004 [INFO] Sum of grad norms: 0.040541
2019-03-19 05:43:11,005 [INFO] ---------------------------------
2019-03-19 05:43:29,598 [INFO] ---------------------------------
2019-03-19 05:43:29,599 [INFO] Summary:
2019-03-19 05:43:29,600 [INFO] Batch 174000, worst loss 0.060657 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:43:29,601 [INFO] Regularization: 3052.424805 * 0.0000010000 = 0.0030524249
2019-03-19 05:43:29,601 [INFO] Sum of grad norms: 0.028595
2019-03-19 05:43:29,602 [INFO] ---------------------------------
2019-03-19 05:43:48,897 [INFO] ---------------------------------
2019-03-19 05:43:48,898 [INFO] Summary:
2019-03-19 05:43:48,899 [INFO] Batch 175000, worst loss 0.060807 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:43:48,900 [INFO] Regularization: 3052.422363 * 0.0000010000 = 0.0030524223
2019-03-19 05:43:48,900 [INFO] Sum of grad norms: 0.035169
2019-03-19 05:43:48,901 [INFO] ---------------------------------
2019-03-19 05:44:07,628 [INFO] ---------------------------------
2019-03-19 05:44:07,629 [INFO] Summary:
2019-03-19 05:44:07,630 [INFO] Batch 176000, worst loss 0.060807 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:44:07,630 [INFO] Regularization: 3052.419189 * 0.0000010000 = 0.0030524193
2019-03-19 05:44:07,631 [INFO] Sum of grad norms: 0.022451
2019-03-19 05:44:07,631 [INFO] ---------------------------------
2019-03-19 05:44:26,177 [INFO] ---------------------------------
2019-03-19 05:44:26,178 [INFO] Summary:
2019-03-19 05:44:26,179 [INFO] Batch 177000, worst loss 0.060704 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:44:26,179 [INFO] Regularization: 3052.416504 * 0.0000010000 = 0.0030524165
2019-03-19 05:44:26,180 [INFO] Sum of grad norms: 0.054667
2019-03-19 05:44:26,181 [INFO] ---------------------------------
2019-03-19 05:44:45,482 [INFO] ---------------------------------
2019-03-19 05:44:45,483 [INFO] Summary:
2019-03-19 05:44:45,483 [INFO] Batch 178000, worst loss 0.060538 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:44:45,484 [INFO] Regularization: 3052.411621 * 0.0000010000 = 0.0030524116
2019-03-19 05:44:45,484 [INFO] Sum of grad norms: 0.050732
2019-03-19 05:44:45,485 [INFO] ---------------------------------
2019-03-19 05:45:04,030 [INFO] ---------------------------------
2019-03-19 05:45:04,031 [INFO] Summary:
2019-03-19 05:45:04,032 [INFO] Batch 179000, worst loss 0.060665 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:45:04,032 [INFO] Regularization: 3052.408447 * 0.0000010000 = 0.0030524083
2019-03-19 05:45:04,033 [INFO] Sum of grad norms: 0.045830
2019-03-19 05:45:04,033 [INFO] ---------------------------------
2019-03-19 05:45:22,465 [INFO] ---------------------------------
2019-03-19 05:45:22,466 [INFO] Summary:
2019-03-19 05:45:22,467 [INFO] Batch 180000, worst loss 0.060665 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:45:22,468 [INFO] Regularization: 3052.405273 * 0.0000010000 = 0.0030524053
2019-03-19 05:45:22,468 [INFO] Sum of grad norms: 0.032430
2019-03-19 05:45:22,469 [INFO] ---------------------------------
2019-03-19 05:45:27,349 [INFO] ---------------------------------
2019-03-19 05:45:27,350 [INFO] Evaluation:
2019-03-19 05:45:27,351 [INFO] Batch 180000, worst loss 0.057451 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:45:27,351 [INFO] ---------------------------------
2019-03-19 05:45:46,264 [INFO] ---------------------------------
2019-03-19 05:45:46,265 [INFO] Summary:
2019-03-19 05:45:46,266 [INFO] Batch 181000, worst loss 0.060564 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:45:46,267 [INFO] Regularization: 3052.402344 * 0.0000010000 = 0.0030524023
2019-03-19 05:45:46,268 [INFO] Sum of grad norms: 0.086720
2019-03-19 05:45:46,269 [INFO] ---------------------------------
2019-03-19 05:46:05,017 [INFO] ---------------------------------
2019-03-19 05:46:05,018 [INFO] Summary:
2019-03-19 05:46:05,019 [INFO] Batch 182000, worst loss 0.060564 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:46:05,019 [INFO] Regularization: 3052.400879 * 0.0000010000 = 0.0030524009
2019-03-19 05:46:05,020 [INFO] Sum of grad norms: 0.024243
2019-03-19 05:46:05,020 [INFO] ---------------------------------
2019-03-19 05:46:23,744 [INFO] ---------------------------------
2019-03-19 05:46:23,745 [INFO] Summary:
2019-03-19 05:46:23,745 [INFO] Batch 183000, worst loss 0.060502 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:46:23,746 [INFO] Regularization: 3052.399414 * 0.0000010000 = 0.0030523995
2019-03-19 05:46:23,746 [INFO] Sum of grad norms: 0.030357
2019-03-19 05:46:23,747 [INFO] ---------------------------------
2019-03-19 05:46:42,790 [INFO] ---------------------------------
2019-03-19 05:46:42,791 [INFO] Summary:
2019-03-19 05:46:42,791 [INFO] Batch 184000, worst loss 0.060618 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:46:42,792 [INFO] Regularization: 3052.398438 * 0.0000010000 = 0.0030523983
2019-03-19 05:46:42,792 [INFO] Sum of grad norms: 0.047471
2019-03-19 05:46:42,793 [INFO] ---------------------------------
2019-03-19 05:47:01,398 [INFO] ---------------------------------
2019-03-19 05:47:01,399 [INFO] Summary:
2019-03-19 05:47:01,400 [INFO] Batch 185000, worst loss 0.060657 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:47:01,400 [INFO] Regularization: 3052.397949 * 0.0000010000 = 0.0030523979
2019-03-19 05:47:01,401 [INFO] Sum of grad norms: 0.036758
2019-03-19 05:47:01,402 [INFO] ---------------------------------
2019-03-19 05:47:19,971 [INFO] ---------------------------------
2019-03-19 05:47:19,972 [INFO] Summary:
2019-03-19 05:47:19,973 [INFO] Batch 186000, worst loss 0.060678 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:47:19,974 [INFO] Regularization: 3052.396973 * 0.0000010000 = 0.0030523969
2019-03-19 05:47:19,975 [INFO] Sum of grad norms: 0.031735
2019-03-19 05:47:19,975 [INFO] ---------------------------------
2019-03-19 05:47:38,739 [INFO] ---------------------------------
2019-03-19 05:47:38,740 [INFO] Summary:
2019-03-19 05:47:38,741 [INFO] Batch 187000, worst loss 0.060678 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:47:38,741 [INFO] Regularization: 3052.396484 * 0.0000010000 = 0.0030523965
2019-03-19 05:47:38,742 [INFO] Sum of grad norms: 0.027477
2019-03-19 05:47:38,743 [INFO] ---------------------------------
2019-03-19 05:47:57,463 [INFO] ---------------------------------
2019-03-19 05:47:57,464 [INFO] Summary:
2019-03-19 05:47:57,464 [INFO] Batch 188000, worst loss 0.060597 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:47:57,465 [INFO] Regularization: 3052.395508 * 0.0000010000 = 0.0030523955
2019-03-19 05:47:57,465 [INFO] Sum of grad norms: 0.053121
2019-03-19 05:47:57,466 [INFO] ---------------------------------
2019-03-19 05:48:15,948 [INFO] ---------------------------------
2019-03-19 05:48:15,949 [INFO] Summary:
2019-03-19 05:48:15,950 [INFO] Batch 189000, worst loss 0.060655 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:48:15,951 [INFO] Regularization: 3052.394531 * 0.0000010000 = 0.0030523946
2019-03-19 05:48:15,952 [INFO] Sum of grad norms: 0.046436
2019-03-19 05:48:15,952 [INFO] ---------------------------------
2019-03-19 05:48:34,593 [INFO] ---------------------------------
2019-03-19 05:48:34,594 [INFO] Summary:
2019-03-19 05:48:34,595 [INFO] Batch 190000, worst loss 0.060642 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:48:34,596 [INFO] Regularization: 3052.393555 * 0.0000010000 = 0.0030523934
2019-03-19 05:48:34,597 [INFO] Sum of grad norms: 0.022715
2019-03-19 05:48:34,598 [INFO] ---------------------------------
2019-03-19 05:48:39,488 [INFO] ---------------------------------
2019-03-19 05:48:39,489 [INFO] Evaluation:
2019-03-19 05:48:39,490 [INFO] Batch 190000, worst loss 0.057590 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:48:39,491 [INFO] ---------------------------------
2019-03-19 05:48:58,462 [INFO] ---------------------------------
2019-03-19 05:48:58,463 [INFO] Summary:
2019-03-19 05:48:58,463 [INFO] Batch 191000, worst loss 0.060582 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:48:58,464 [INFO] Regularization: 3052.392578 * 0.0000010000 = 0.0030523925
2019-03-19 05:48:58,464 [INFO] Sum of grad norms: 0.022984
2019-03-19 05:48:58,465 [INFO] ---------------------------------
2019-03-19 05:49:17,163 [INFO] ---------------------------------
2019-03-19 05:49:17,164 [INFO] Summary:
2019-03-19 05:49:17,165 [INFO] Batch 192000, worst loss 0.060663 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:49:17,165 [INFO] Regularization: 3052.392090 * 0.0000010000 = 0.0030523920
2019-03-19 05:49:17,166 [INFO] Sum of grad norms: 0.046321
2019-03-19 05:49:17,166 [INFO] ---------------------------------
2019-03-19 05:49:36,181 [INFO] ---------------------------------
2019-03-19 05:49:36,182 [INFO] Summary:
2019-03-19 05:49:36,182 [INFO] Batch 193000, worst loss 0.060917 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:49:36,183 [INFO] Regularization: 3052.391846 * 0.0000010000 = 0.0030523918
2019-03-19 05:49:36,183 [INFO] Sum of grad norms: 0.039254
2019-03-19 05:49:36,184 [INFO] ---------------------------------
2019-03-19 05:49:54,748 [INFO] ---------------------------------
2019-03-19 05:49:54,749 [INFO] Summary:
2019-03-19 05:49:54,751 [INFO] Batch 194000, worst loss 0.060551 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:49:54,751 [INFO] Regularization: 3052.391113 * 0.0000010000 = 0.0030523911
2019-03-19 05:49:54,752 [INFO] Sum of grad norms: 0.022035
2019-03-19 05:49:54,752 [INFO] ---------------------------------
2019-03-19 05:50:13,404 [INFO] ---------------------------------
2019-03-19 05:50:13,405 [INFO] Summary:
2019-03-19 05:50:13,405 [INFO] Batch 195000, worst loss 0.060515 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:50:13,406 [INFO] Regularization: 3052.390625 * 0.0000010000 = 0.0030523906
2019-03-19 05:50:13,406 [INFO] Sum of grad norms: 0.039695
2019-03-19 05:50:13,407 [INFO] ---------------------------------
2019-03-19 05:50:32,077 [INFO] ---------------------------------
2019-03-19 05:50:32,078 [INFO] Summary:
2019-03-19 05:50:32,078 [INFO] Batch 196000, worst loss 0.060666 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:50:32,079 [INFO] Regularization: 3052.390625 * 0.0000010000 = 0.0030523906
2019-03-19 05:50:32,079 [INFO] Sum of grad norms: 0.020304
2019-03-19 05:50:32,080 [INFO] ---------------------------------
2019-03-19 05:50:50,935 [INFO] ---------------------------------
2019-03-19 05:50:50,936 [INFO] Summary:
2019-03-19 05:50:50,937 [INFO] Batch 197000, worst loss 0.060666 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:50:50,937 [INFO] Regularization: 3052.390137 * 0.0000010000 = 0.0030523902
2019-03-19 05:50:50,938 [INFO] Sum of grad norms: 0.048802
2019-03-19 05:50:50,938 [INFO] ---------------------------------
2019-03-19 05:51:09,544 [INFO] ---------------------------------
2019-03-19 05:51:09,545 [INFO] Summary:
2019-03-19 05:51:09,546 [INFO] Batch 198000, worst loss 0.060571 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:51:09,546 [INFO] Regularization: 3052.390137 * 0.0000010000 = 0.0030523902
2019-03-19 05:51:09,547 [INFO] Sum of grad norms: 0.022069
2019-03-19 05:51:09,548 [INFO] ---------------------------------
2019-03-19 05:51:28,340 [INFO] ---------------------------------
2019-03-19 05:51:28,341 [INFO] Summary:
2019-03-19 05:51:28,342 [INFO] Batch 199000, worst loss 0.060672 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:51:28,343 [INFO] Regularization: 3052.389648 * 0.0000010000 = 0.0030523897
2019-03-19 05:51:28,343 [INFO] Sum of grad norms: 0.073421
2019-03-19 05:51:28,344 [INFO] ---------------------------------
2019-03-19 05:51:47,324 [INFO] ---------------------------------
2019-03-19 05:51:47,325 [INFO] Summary:
2019-03-19 05:51:47,326 [INFO] Batch 200000, worst loss 0.060689 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:51:47,327 [INFO] Regularization: 3052.389648 * 0.0000010000 = 0.0030523897
2019-03-19 05:51:47,328 [INFO] Sum of grad norms: 0.023459
2019-03-19 05:51:47,329 [INFO] ---------------------------------
2019-03-19 05:51:52,341 [INFO] ---------------------------------
2019-03-19 05:51:52,342 [INFO] Evaluation:
2019-03-19 05:51:52,343 [INFO] Batch 200000, worst loss 0.057575 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:51:52,344 [INFO] ---------------------------------
2019-03-19 05:52:11,340 [INFO] ---------------------------------
2019-03-19 05:52:11,342 [INFO] Summary:
2019-03-19 05:52:11,343 [INFO] Batch 201000, worst loss 0.060630 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:52:11,343 [INFO] Regularization: 3052.389160 * 0.0000010000 = 0.0030523892
2019-03-19 05:52:11,344 [INFO] Sum of grad norms: 0.016121
2019-03-19 05:52:11,345 [INFO] ---------------------------------
2019-03-19 05:52:30,196 [INFO] ---------------------------------
2019-03-19 05:52:30,197 [INFO] Summary:
2019-03-19 05:52:30,197 [INFO] Batch 202000, worst loss 0.060604 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:52:30,198 [INFO] Regularization: 3052.389160 * 0.0000010000 = 0.0030523892
2019-03-19 05:52:30,199 [INFO] Sum of grad norms: 0.054293
2019-03-19 05:52:30,200 [INFO] ---------------------------------
2019-03-19 05:52:49,039 [INFO] ---------------------------------
2019-03-19 05:52:49,040 [INFO] Summary:
2019-03-19 05:52:49,041 [INFO] Batch 203000, worst loss 0.060573 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:52:49,041 [INFO] Regularization: 3052.389160 * 0.0000010000 = 0.0030523892
2019-03-19 05:52:49,042 [INFO] Sum of grad norms: 0.041445
2019-03-19 05:52:49,042 [INFO] ---------------------------------
2019-03-19 05:53:07,888 [INFO] ---------------------------------
2019-03-19 05:53:07,889 [INFO] Summary:
2019-03-19 05:53:07,890 [INFO] Batch 204000, worst loss 0.060580 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:53:07,890 [INFO] Regularization: 3052.388916 * 0.0000010000 = 0.0030523890
2019-03-19 05:53:07,891 [INFO] Sum of grad norms: 0.106311
2019-03-19 05:53:07,891 [INFO] ---------------------------------
2019-03-19 05:53:26,947 [INFO] ---------------------------------
2019-03-19 05:53:26,948 [INFO] Summary:
2019-03-19 05:53:26,949 [INFO] Batch 205000, worst loss 0.060713 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:53:26,949 [INFO] Regularization: 3052.388672 * 0.0000010000 = 0.0030523886
2019-03-19 05:53:26,950 [INFO] Sum of grad norms: 0.023653
2019-03-19 05:53:26,950 [INFO] ---------------------------------
2019-03-19 05:53:46,065 [INFO] ---------------------------------
2019-03-19 05:53:46,066 [INFO] Summary:
2019-03-19 05:53:46,067 [INFO] Batch 206000, worst loss 0.060595 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:53:46,067 [INFO] Regularization: 3052.388672 * 0.0000010000 = 0.0030523886
2019-03-19 05:53:46,068 [INFO] Sum of grad norms: 0.024800
2019-03-19 05:53:46,068 [INFO] ---------------------------------
2019-03-19 05:54:04,842 [INFO] ---------------------------------
2019-03-19 05:54:04,843 [INFO] Summary:
2019-03-19 05:54:04,844 [INFO] Batch 207000, worst loss 0.060620 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:54:04,844 [INFO] Regularization: 3052.388672 * 0.0000010000 = 0.0030523886
2019-03-19 05:54:04,845 [INFO] Sum of grad norms: 0.050556
2019-03-19 05:54:04,845 [INFO] ---------------------------------
2019-03-19 05:54:23,595 [INFO] ---------------------------------
2019-03-19 05:54:23,596 [INFO] Summary:
2019-03-19 05:54:23,597 [INFO] Batch 208000, worst loss 0.060620 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:54:23,597 [INFO] Regularization: 3052.388672 * 0.0000010000 = 0.0030523886
2019-03-19 05:54:23,598 [INFO] Sum of grad norms: 0.028174
2019-03-19 05:54:23,598 [INFO] ---------------------------------
2019-03-19 05:54:42,714 [INFO] ---------------------------------
2019-03-19 05:54:42,715 [INFO] Summary:
2019-03-19 05:54:42,716 [INFO] Batch 209000, worst loss 0.060572 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:54:42,716 [INFO] Regularization: 3052.388672 * 0.0000010000 = 0.0030523886
2019-03-19 05:54:42,717 [INFO] Sum of grad norms: 0.032912
2019-03-19 05:54:42,718 [INFO] ---------------------------------
2019-03-19 05:55:01,515 [INFO] ---------------------------------
2019-03-19 05:55:01,516 [INFO] Summary:
2019-03-19 05:55:01,517 [INFO] Batch 210000, worst loss 0.060611 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:55:01,518 [INFO] Regularization: 3052.388672 * 0.0000010000 = 0.0030523886
2019-03-19 05:55:01,518 [INFO] Sum of grad norms: 0.058978
2019-03-19 05:55:01,519 [INFO] ---------------------------------
2019-03-19 05:55:06,512 [INFO] ---------------------------------
2019-03-19 05:55:06,513 [INFO] Evaluation:
2019-03-19 05:55:06,514 [INFO] Batch 210000, worst loss 0.057667 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:55:06,515 [INFO] ---------------------------------
2019-03-19 05:55:25,251 [INFO] ---------------------------------
2019-03-19 05:55:25,252 [INFO] Summary:
2019-03-19 05:55:25,253 [INFO] Batch 211000, worst loss 0.060600 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:55:25,254 [INFO] Regularization: 3052.388184 * 0.0000010000 = 0.0030523881
2019-03-19 05:55:25,254 [INFO] Sum of grad norms: 0.036880
2019-03-19 05:55:25,255 [INFO] ---------------------------------
2019-03-19 05:55:43,795 [INFO] ---------------------------------
2019-03-19 05:55:43,796 [INFO] Summary:
2019-03-19 05:55:43,797 [INFO] Batch 212000, worst loss 0.060600 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:55:43,798 [INFO] Regularization: 3052.388184 * 0.0000010000 = 0.0030523881
2019-03-19 05:55:43,799 [INFO] Sum of grad norms: 0.068682
2019-03-19 05:55:43,800 [INFO] ---------------------------------
2019-03-19 05:56:02,705 [INFO] ---------------------------------
2019-03-19 05:56:02,707 [INFO] Summary:
2019-03-19 05:56:02,707 [INFO] Batch 213000, worst loss 0.060573 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:56:02,708 [INFO] Regularization: 3052.388184 * 0.0000010000 = 0.0030523881
2019-03-19 05:56:02,708 [INFO] Sum of grad norms: 0.029762
2019-03-19 05:56:02,709 [INFO] ---------------------------------
2019-03-19 05:56:22,128 [INFO] ---------------------------------
2019-03-19 05:56:22,129 [INFO] Summary:
2019-03-19 05:56:22,130 [INFO] Batch 214000, worst loss 0.060693 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:56:22,130 [INFO] Regularization: 3052.388184 * 0.0000010000 = 0.0030523881
2019-03-19 05:56:22,131 [INFO] Sum of grad norms: 0.042549
2019-03-19 05:56:22,132 [INFO] ---------------------------------
2019-03-19 05:56:40,698 [INFO] ---------------------------------
2019-03-19 05:56:40,699 [INFO] Summary:
2019-03-19 05:56:40,700 [INFO] Batch 215000, worst loss 0.060641 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:56:40,700 [INFO] Regularization: 3052.388184 * 0.0000010000 = 0.0030523881
2019-03-19 05:56:40,701 [INFO] Sum of grad norms: 0.067410
2019-03-19 05:56:40,701 [INFO] ---------------------------------
2019-03-19 05:56:59,580 [INFO] ---------------------------------
2019-03-19 05:56:59,581 [INFO] Summary:
2019-03-19 05:56:59,581 [INFO] Batch 216000, worst loss 0.060636 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:56:59,582 [INFO] Regularization: 3052.388184 * 0.0000010000 = 0.0030523881
2019-03-19 05:56:59,582 [INFO] Sum of grad norms: 0.017799
2019-03-19 05:56:59,583 [INFO] ---------------------------------
2019-03-19 05:57:18,341 [INFO] ---------------------------------
2019-03-19 05:57:18,342 [INFO] Summary:
2019-03-19 05:57:18,342 [INFO] Batch 217000, worst loss 0.060718 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:57:18,343 [INFO] Regularization: 3052.388184 * 0.0000010000 = 0.0030523881
2019-03-19 05:57:18,343 [INFO] Sum of grad norms: 0.020545
2019-03-19 05:57:18,344 [INFO] ---------------------------------
2019-03-19 05:57:36,922 [INFO] ---------------------------------
2019-03-19 05:57:36,923 [INFO] Summary:
2019-03-19 05:57:36,923 [INFO] Batch 218000, worst loss 0.060528 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:57:36,924 [INFO] Regularization: 3052.387695 * 0.0000010000 = 0.0030523876
2019-03-19 05:57:36,925 [INFO] Sum of grad norms: 0.094440
2019-03-19 05:57:36,925 [INFO] ---------------------------------
2019-03-19 05:57:56,161 [INFO] ---------------------------------
2019-03-19 05:57:56,162 [INFO] Summary:
2019-03-19 05:57:56,162 [INFO] Batch 219000, worst loss 0.060646 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:57:56,163 [INFO] Regularization: 3052.387695 * 0.0000010000 = 0.0030523876
2019-03-19 05:57:56,163 [INFO] Sum of grad norms: 0.020277
2019-03-19 05:57:56,164 [INFO] ---------------------------------
2019-03-19 05:58:14,710 [INFO] ---------------------------------
2019-03-19 05:58:14,711 [INFO] Summary:
2019-03-19 05:58:14,712 [INFO] Batch 220000, worst loss 0.060706 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:58:14,712 [INFO] Regularization: 3052.387695 * 0.0000010000 = 0.0030523876
2019-03-19 05:58:14,713 [INFO] Sum of grad norms: 0.032308
2019-03-19 05:58:14,713 [INFO] ---------------------------------
2019-03-19 05:58:19,640 [INFO] ---------------------------------
2019-03-19 05:58:19,641 [INFO] Evaluation:
2019-03-19 05:58:19,641 [INFO] Batch 220000, worst loss 0.057624 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 05:58:19,642 [INFO] ---------------------------------
2019-03-19 05:58:38,164 [INFO] ---------------------------------
2019-03-19 05:58:38,165 [INFO] Summary:
2019-03-19 05:58:38,166 [INFO] Batch 221000, worst loss 0.060642 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:58:38,166 [INFO] Regularization: 3052.387695 * 0.0000010000 = 0.0030523876
2019-03-19 05:58:38,167 [INFO] Sum of grad norms: 0.023430
2019-03-19 05:58:38,167 [INFO] ---------------------------------
2019-03-19 05:58:57,052 [INFO] ---------------------------------
2019-03-19 05:58:57,053 [INFO] Summary:
2019-03-19 05:58:57,055 [INFO] Batch 222000, worst loss 0.060631 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:58:57,055 [INFO] Regularization: 3052.387695 * 0.0000010000 = 0.0030523876
2019-03-19 05:58:57,056 [INFO] Sum of grad norms: 0.030953
2019-03-19 05:58:57,056 [INFO] ---------------------------------
2019-03-19 05:59:15,930 [INFO] ---------------------------------
2019-03-19 05:59:15,931 [INFO] Summary:
2019-03-19 05:59:15,931 [INFO] Batch 223000, worst loss 0.060631 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:59:15,932 [INFO] Regularization: 3052.387695 * 0.0000010000 = 0.0030523876
2019-03-19 05:59:15,932 [INFO] Sum of grad norms: 0.041748
2019-03-19 05:59:15,933 [INFO] ---------------------------------
2019-03-19 05:59:34,562 [INFO] ---------------------------------
2019-03-19 05:59:34,563 [INFO] Summary:
2019-03-19 05:59:34,564 [INFO] Batch 224000, worst loss 0.060628 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:59:34,564 [INFO] Regularization: 3052.387695 * 0.0000010000 = 0.0030523876
2019-03-19 05:59:34,565 [INFO] Sum of grad norms: 0.036164
2019-03-19 05:59:34,565 [INFO] ---------------------------------
2019-03-19 05:59:53,560 [INFO] ---------------------------------
2019-03-19 05:59:53,561 [INFO] Summary:
2019-03-19 05:59:53,562 [INFO] Batch 225000, worst loss 0.060532 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 05:59:53,562 [INFO] Regularization: 3052.387695 * 0.0000010000 = 0.0030523876
2019-03-19 05:59:53,563 [INFO] Sum of grad norms: 0.042240
2019-03-19 05:59:53,564 [INFO] ---------------------------------
2019-03-19 06:00:12,218 [INFO] ---------------------------------
2019-03-19 06:00:12,219 [INFO] Summary:
2019-03-19 06:00:12,220 [INFO] Batch 226000, worst loss 0.060593 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:00:12,220 [INFO] Regularization: 3052.387695 * 0.0000010000 = 0.0030523876
2019-03-19 06:00:12,221 [INFO] Sum of grad norms: 0.039564
2019-03-19 06:00:12,221 [INFO] ---------------------------------
2019-03-19 06:00:30,862 [INFO] ---------------------------------
2019-03-19 06:00:30,863 [INFO] Summary:
2019-03-19 06:00:30,863 [INFO] Batch 227000, worst loss 0.060596 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:00:30,864 [INFO] Regularization: 3052.387695 * 0.0000010000 = 0.0030523876
2019-03-19 06:00:30,865 [INFO] Sum of grad norms: 0.026974
2019-03-19 06:00:30,866 [INFO] ---------------------------------
2019-03-19 06:00:49,573 [INFO] ---------------------------------
2019-03-19 06:00:49,574 [INFO] Summary:
2019-03-19 06:00:49,575 [INFO] Batch 228000, worst loss 0.060560 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:00:49,575 [INFO] Regularization: 3052.387451 * 0.0000010000 = 0.0030523874
2019-03-19 06:00:49,576 [INFO] Sum of grad norms: 0.029364
2019-03-19 06:00:49,577 [INFO] ---------------------------------
2019-03-19 06:01:08,252 [INFO] ---------------------------------
2019-03-19 06:01:08,253 [INFO] Summary:
2019-03-19 06:01:08,254 [INFO] Batch 229000, worst loss 0.060702 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:01:08,255 [INFO] Regularization: 3052.387451 * 0.0000010000 = 0.0030523874
2019-03-19 06:01:08,255 [INFO] Sum of grad norms: 0.027892
2019-03-19 06:01:08,256 [INFO] ---------------------------------
2019-03-19 06:01:26,856 [INFO] ---------------------------------
2019-03-19 06:01:26,857 [INFO] Summary:
2019-03-19 06:01:26,858 [INFO] Batch 230000, worst loss 0.060564 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:01:26,858 [INFO] Regularization: 3052.387451 * 0.0000010000 = 0.0030523874
2019-03-19 06:01:26,859 [INFO] Sum of grad norms: 0.043133
2019-03-19 06:01:26,859 [INFO] ---------------------------------
2019-03-19 06:01:31,780 [INFO] ---------------------------------
2019-03-19 06:01:31,781 [INFO] Evaluation:
2019-03-19 06:01:31,782 [INFO] Batch 230000, worst loss 0.057512 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:01:31,784 [INFO] ---------------------------------
2019-03-19 06:01:50,495 [INFO] ---------------------------------
2019-03-19 06:01:50,496 [INFO] Summary:
2019-03-19 06:01:50,496 [INFO] Batch 231000, worst loss 0.060561 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:01:50,497 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:01:50,497 [INFO] Sum of grad norms: 0.062079
2019-03-19 06:01:50,498 [INFO] ---------------------------------
2019-03-19 06:02:09,342 [INFO] ---------------------------------
2019-03-19 06:02:09,343 [INFO] Summary:
2019-03-19 06:02:09,344 [INFO] Batch 232000, worst loss 0.060465 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:02:09,344 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:02:09,345 [INFO] Sum of grad norms: 0.037319
2019-03-19 06:02:09,346 [INFO] ---------------------------------
2019-03-19 06:02:28,268 [INFO] ---------------------------------
2019-03-19 06:02:28,269 [INFO] Summary:
2019-03-19 06:02:28,269 [INFO] Batch 233000, worst loss 0.060599 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:02:28,270 [INFO] Regularization: 3052.387451 * 0.0000010000 = 0.0030523874
2019-03-19 06:02:28,270 [INFO] Sum of grad norms: 0.065828
2019-03-19 06:02:28,271 [INFO] ---------------------------------
2019-03-19 06:02:46,957 [INFO] ---------------------------------
2019-03-19 06:02:46,958 [INFO] Summary:
2019-03-19 06:02:46,959 [INFO] Batch 234000, worst loss 0.060460 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:02:46,959 [INFO] Regularization: 3052.387451 * 0.0000010000 = 0.0030523874
2019-03-19 06:02:46,960 [INFO] Sum of grad norms: 0.060940
2019-03-19 06:02:46,960 [INFO] ---------------------------------
2019-03-19 06:03:06,002 [INFO] ---------------------------------
2019-03-19 06:03:06,003 [INFO] Summary:
2019-03-19 06:03:06,004 [INFO] Batch 235000, worst loss 0.060555 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:03:06,004 [INFO] Regularization: 3052.387451 * 0.0000010000 = 0.0030523874
2019-03-19 06:03:06,005 [INFO] Sum of grad norms: 0.031974
2019-03-19 06:03:06,005 [INFO] ---------------------------------
2019-03-19 06:03:25,320 [INFO] ---------------------------------
2019-03-19 06:03:25,321 [INFO] Summary:
2019-03-19 06:03:25,322 [INFO] Batch 236000, worst loss 0.060595 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:03:25,322 [INFO] Regularization: 3052.387451 * 0.0000010000 = 0.0030523874
2019-03-19 06:03:25,323 [INFO] Sum of grad norms: 0.083713
2019-03-19 06:03:25,323 [INFO] ---------------------------------
2019-03-19 06:03:43,783 [INFO] ---------------------------------
2019-03-19 06:03:43,784 [INFO] Summary:
2019-03-19 06:03:43,785 [INFO] Batch 237000, worst loss 0.060484 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:03:43,785 [INFO] Regularization: 3052.387451 * 0.0000010000 = 0.0030523874
2019-03-19 06:03:43,786 [INFO] Sum of grad norms: 0.026087
2019-03-19 06:03:43,786 [INFO] ---------------------------------
2019-03-19 06:04:02,550 [INFO] ---------------------------------
2019-03-19 06:04:02,550 [INFO] Summary:
2019-03-19 06:04:02,551 [INFO] Batch 238000, worst loss 0.060706 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:04:02,552 [INFO] Regularization: 3052.387451 * 0.0000010000 = 0.0030523874
2019-03-19 06:04:02,552 [INFO] Sum of grad norms: 0.020014
2019-03-19 06:04:02,553 [INFO] ---------------------------------
2019-03-19 06:04:21,259 [INFO] ---------------------------------
2019-03-19 06:04:21,259 [INFO] Summary:
2019-03-19 06:04:21,260 [INFO] Batch 239000, worst loss 0.060713 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:04:21,260 [INFO] Regularization: 3052.387451 * 0.0000010000 = 0.0030523874
2019-03-19 06:04:21,261 [INFO] Sum of grad norms: 0.022977
2019-03-19 06:04:21,262 [INFO] ---------------------------------
2019-03-19 06:04:40,637 [INFO] ---------------------------------
2019-03-19 06:04:40,637 [INFO] Summary:
2019-03-19 06:04:40,638 [INFO] Batch 240000, worst loss 0.060624 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:04:40,639 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:04:40,639 [INFO] Sum of grad norms: 0.061479
2019-03-19 06:04:40,640 [INFO] ---------------------------------
2019-03-19 06:04:45,497 [INFO] ---------------------------------
2019-03-19 06:04:45,498 [INFO] Evaluation:
2019-03-19 06:04:45,498 [INFO] Batch 240000, worst loss 0.057615 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:04:45,499 [INFO] ---------------------------------
2019-03-19 06:05:04,218 [INFO] ---------------------------------
2019-03-19 06:05:04,219 [INFO] Summary:
2019-03-19 06:05:04,220 [INFO] Batch 241000, worst loss 0.060667 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:05:04,220 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:05:04,221 [INFO] Sum of grad norms: 0.020054
2019-03-19 06:05:04,222 [INFO] ---------------------------------
2019-03-19 06:05:22,632 [INFO] ---------------------------------
2019-03-19 06:05:22,633 [INFO] Summary:
2019-03-19 06:05:22,634 [INFO] Batch 242000, worst loss 0.060621 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:05:22,635 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:05:22,635 [INFO] Sum of grad norms: 0.040358
2019-03-19 06:05:22,636 [INFO] ---------------------------------
2019-03-19 06:05:41,216 [INFO] ---------------------------------
2019-03-19 06:05:41,217 [INFO] Summary:
2019-03-19 06:05:41,218 [INFO] Batch 243000, worst loss 0.060614 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:05:41,218 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:05:41,219 [INFO] Sum of grad norms: 0.031752
2019-03-19 06:05:41,220 [INFO] ---------------------------------
2019-03-19 06:05:59,675 [INFO] ---------------------------------
2019-03-19 06:05:59,676 [INFO] Summary:
2019-03-19 06:05:59,677 [INFO] Batch 244000, worst loss 0.060612 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:05:59,677 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:05:59,678 [INFO] Sum of grad norms: 0.043956
2019-03-19 06:05:59,678 [INFO] ---------------------------------
2019-03-19 06:06:18,282 [INFO] ---------------------------------
2019-03-19 06:06:18,283 [INFO] Summary:
2019-03-19 06:06:18,283 [INFO] Batch 245000, worst loss 0.060621 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:06:18,284 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:06:18,284 [INFO] Sum of grad norms: 0.025994
2019-03-19 06:06:18,285 [INFO] ---------------------------------
2019-03-19 06:06:36,940 [INFO] ---------------------------------
2019-03-19 06:06:36,940 [INFO] Summary:
2019-03-19 06:06:36,941 [INFO] Batch 246000, worst loss 0.060621 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:06:36,942 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:06:36,942 [INFO] Sum of grad norms: 0.061282
2019-03-19 06:06:36,943 [INFO] ---------------------------------
2019-03-19 06:06:55,960 [INFO] ---------------------------------
2019-03-19 06:06:55,961 [INFO] Summary:
2019-03-19 06:06:55,961 [INFO] Batch 247000, worst loss 0.060630 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:06:55,962 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:06:55,962 [INFO] Sum of grad norms: 0.029786
2019-03-19 06:06:55,963 [INFO] ---------------------------------
2019-03-19 06:07:15,131 [INFO] ---------------------------------
2019-03-19 06:07:15,132 [INFO] Summary:
2019-03-19 06:07:15,133 [INFO] Batch 248000, worst loss 0.060691 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:07:15,133 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:07:15,134 [INFO] Sum of grad norms: 0.017232
2019-03-19 06:07:15,135 [INFO] ---------------------------------
2019-03-19 06:07:33,647 [INFO] ---------------------------------
2019-03-19 06:07:33,648 [INFO] Summary:
2019-03-19 06:07:33,648 [INFO] Batch 249000, worst loss 0.060691 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:07:33,649 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:07:33,649 [INFO] Sum of grad norms: 0.047586
2019-03-19 06:07:33,650 [INFO] ---------------------------------
2019-03-19 06:07:52,438 [INFO] ---------------------------------
2019-03-19 06:07:52,439 [INFO] Summary:
2019-03-19 06:07:52,439 [INFO] Batch 250000, worst loss 0.060594 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:07:52,440 [INFO] Regularization: 3052.387207 * 0.0000010000 = 0.0030523872
2019-03-19 06:07:52,441 [INFO] Sum of grad norms: 0.039705
2019-03-19 06:07:52,441 [INFO] ---------------------------------
2019-03-19 06:07:57,448 [INFO] ---------------------------------
2019-03-19 06:07:57,449 [INFO] Evaluation:
2019-03-19 06:07:57,450 [INFO] Batch 250000, worst loss 0.057391 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:07:57,451 [INFO] ---------------------------------
2019-03-19 06:07:57,452 [INFO] Finished training, saved to file classifier/1552933539/1552972077_7_classifier_final.pth
2019-03-19 06:07:57,628 [INFO] ---------------------------------
2019-03-19 06:07:57,630 [INFO] Training model #8: (1, 64, 201) @ 1
2019-03-19 06:08:13,476 [INFO] ---------------------------------
2019-03-19 06:08:13,477 [INFO] Summary:
2019-03-19 06:08:13,478 [INFO] Batch 1000, worst loss 16.389553 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 06:08:13,478 [INFO] Regularization: 9383.958984 * 0.0000010000 = 0.0093839588
2019-03-19 06:08:13,479 [INFO] Sum of grad norms: 0.669981
2019-03-19 06:08:13,480 [INFO] ---------------------------------
2019-03-19 06:08:29,569 [INFO] ---------------------------------
2019-03-19 06:08:29,569 [INFO] Summary:
2019-03-19 06:08:29,570 [INFO] Batch 2000, worst loss 0.164589 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 06:08:29,571 [INFO] Regularization: 8074.116699 * 0.0000010000 = 0.0080741169
2019-03-19 06:08:29,571 [INFO] Sum of grad norms: 3.185657
2019-03-19 06:08:29,572 [INFO] ---------------------------------
2019-03-19 06:08:48,126 [INFO] ---------------------------------
2019-03-19 06:08:48,127 [INFO] Summary:
2019-03-19 06:08:48,127 [INFO] Batch 3000, worst loss 0.101002 (incl. reg.) of 1000 batches, learning rate 0.001646 @cl.-depth 1
2019-03-19 06:08:48,128 [INFO] Regularization: 7364.253418 * 0.0000010000 = 0.0073642535
2019-03-19 06:08:48,128 [INFO] Sum of grad norms: 1.764541
2019-03-19 06:08:48,129 [INFO] ---------------------------------
2019-03-19 06:09:07,079 [INFO] ---------------------------------
2019-03-19 06:09:07,080 [INFO] Summary:
2019-03-19 06:09:07,080 [INFO] Batch 4000, worst loss 0.079489 (incl. reg.) of 1000 batches, learning rate 0.001010 @cl.-depth 1
2019-03-19 06:09:07,081 [INFO] Regularization: 6821.452637 * 0.0000010000 = 0.0068214526
2019-03-19 06:09:07,081 [INFO] Sum of grad norms: 0.401093
2019-03-19 06:09:07,082 [INFO] ---------------------------------
2019-03-19 06:09:25,843 [INFO] ---------------------------------
2019-03-19 06:09:25,844 [INFO] Summary:
2019-03-19 06:09:25,845 [INFO] Batch 5000, worst loss 0.074737 (incl. reg.) of 1000 batches, learning rate 0.000795 @cl.-depth 1
2019-03-19 06:09:25,845 [INFO] Regularization: 6457.536621 * 0.0000010000 = 0.0064575365
2019-03-19 06:09:25,846 [INFO] Sum of grad norms: 0.207938
2019-03-19 06:09:25,846 [INFO] ---------------------------------
2019-03-19 06:09:44,574 [INFO] ---------------------------------
2019-03-19 06:09:44,575 [INFO] Summary:
2019-03-19 06:09:44,575 [INFO] Batch 6000, worst loss 0.073053 (incl. reg.) of 1000 batches, learning rate 0.000747 @cl.-depth 1
2019-03-19 06:09:44,576 [INFO] Regularization: 6147.130859 * 0.0000010000 = 0.0061471309
2019-03-19 06:09:44,576 [INFO] Sum of grad norms: 0.416507
2019-03-19 06:09:44,577 [INFO] ---------------------------------
2019-03-19 06:10:03,675 [INFO] ---------------------------------
2019-03-19 06:10:03,676 [INFO] Summary:
2019-03-19 06:10:03,676 [INFO] Batch 7000, worst loss 0.072535 (incl. reg.) of 1000 batches, learning rate 0.000731 @cl.-depth 1
2019-03-19 06:10:03,677 [INFO] Regularization: 5883.856445 * 0.0000010000 = 0.0058838562
2019-03-19 06:10:03,677 [INFO] Sum of grad norms: 0.154988
2019-03-19 06:10:03,678 [INFO] ---------------------------------
2019-03-19 06:10:22,391 [INFO] ---------------------------------
2019-03-19 06:10:22,392 [INFO] Summary:
2019-03-19 06:10:22,393 [INFO] Batch 8000, worst loss 0.071357 (incl. reg.) of 1000 batches, learning rate 0.000725 @cl.-depth 1
2019-03-19 06:10:22,393 [INFO] Regularization: 5683.503418 * 0.0000010000 = 0.0056835036
2019-03-19 06:10:22,394 [INFO] Sum of grad norms: 0.213000
2019-03-19 06:10:22,395 [INFO] ---------------------------------
2019-03-19 06:10:41,342 [INFO] ---------------------------------
2019-03-19 06:10:41,343 [INFO] Summary:
2019-03-19 06:10:41,344 [INFO] Batch 9000, worst loss 0.072457 (incl. reg.) of 1000 batches, learning rate 0.000714 @cl.-depth 1
2019-03-19 06:10:41,344 [INFO] Regularization: 5508.258301 * 0.0000010000 = 0.0055082585
2019-03-19 06:10:41,345 [INFO] Sum of grad norms: 0.671252
2019-03-19 06:10:41,346 [INFO] ---------------------------------
2019-03-19 06:11:00,709 [INFO] ---------------------------------
2019-03-19 06:11:00,710 [INFO] Summary:
2019-03-19 06:11:00,711 [INFO] Batch 10000, worst loss 0.070712 (incl. reg.) of 1000 batches, learning rate 0.000714 @cl.-depth 1
2019-03-19 06:11:00,711 [INFO] Regularization: 5366.314453 * 0.0000010000 = 0.0053663147
2019-03-19 06:11:00,712 [INFO] Sum of grad norms: 1.350732
2019-03-19 06:11:00,713 [INFO] ---------------------------------
2019-03-19 06:11:05,666 [INFO] ---------------------------------
2019-03-19 06:11:05,667 [INFO] Evaluation:
2019-03-19 06:11:05,668 [INFO] Batch 10000, worst loss 0.066562 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:11:05,669 [INFO] ---------------------------------
2019-03-19 06:11:24,255 [INFO] ---------------------------------
2019-03-19 06:11:24,256 [INFO] Summary:
2019-03-19 06:11:24,256 [INFO] Batch 11000, worst loss 0.069280 (incl. reg.) of 1000 batches, learning rate 0.000707 @cl.-depth 1
2019-03-19 06:11:24,257 [INFO] Regularization: 5247.074707 * 0.0000010000 = 0.0052470746
2019-03-19 06:11:24,257 [INFO] Sum of grad norms: 0.326386
2019-03-19 06:11:24,258 [INFO] ---------------------------------
2019-03-19 06:11:43,430 [INFO] ---------------------------------
2019-03-19 06:11:43,431 [INFO] Summary:
2019-03-19 06:11:43,432 [INFO] Batch 12000, worst loss 0.068472 (incl. reg.) of 1000 batches, learning rate 0.000693 @cl.-depth 1
2019-03-19 06:11:43,432 [INFO] Regularization: 5157.955566 * 0.0000010000 = 0.0051579555
2019-03-19 06:11:43,433 [INFO] Sum of grad norms: 0.384647
2019-03-19 06:11:43,433 [INFO] ---------------------------------
2019-03-19 06:12:02,163 [INFO] ---------------------------------
2019-03-19 06:12:02,164 [INFO] Summary:
2019-03-19 06:12:02,165 [INFO] Batch 13000, worst loss 0.072166 (incl. reg.) of 1000 batches, learning rate 0.000685 @cl.-depth 1
2019-03-19 06:12:02,165 [INFO] Regularization: 5070.638672 * 0.0000010000 = 0.0050706388
2019-03-19 06:12:02,166 [INFO] Sum of grad norms: 1.012030
2019-03-19 06:12:02,166 [INFO] ---------------------------------
2019-03-19 06:12:21,095 [INFO] ---------------------------------
2019-03-19 06:12:21,096 [INFO] Summary:
2019-03-19 06:12:21,097 [INFO] Batch 14000, worst loss 0.069193 (incl. reg.) of 1000 batches, learning rate 0.000685 @cl.-depth 1
2019-03-19 06:12:21,097 [INFO] Regularization: 5000.636719 * 0.0000010000 = 0.0050006369
2019-03-19 06:12:21,098 [INFO] Sum of grad norms: 0.767698
2019-03-19 06:12:21,098 [INFO] ---------------------------------
2019-03-19 06:12:40,209 [INFO] ---------------------------------
2019-03-19 06:12:40,210 [INFO] Summary:
2019-03-19 06:12:40,211 [INFO] Batch 15000, worst loss 0.069621 (incl. reg.) of 1000 batches, learning rate 0.000685 @cl.-depth 1
2019-03-19 06:12:40,211 [INFO] Regularization: 4935.770508 * 0.0000010000 = 0.0049357703
2019-03-19 06:12:40,212 [INFO] Sum of grad norms: 0.309826
2019-03-19 06:12:40,213 [INFO] ---------------------------------
2019-03-19 06:12:59,359 [INFO] ---------------------------------
2019-03-19 06:12:59,360 [INFO] Summary:
2019-03-19 06:12:59,361 [INFO] Batch 16000, worst loss 0.068309 (incl. reg.) of 1000 batches, learning rate 0.000685 @cl.-depth 1
2019-03-19 06:12:59,362 [INFO] Regularization: 4853.779785 * 0.0000010000 = 0.0048537799
2019-03-19 06:12:59,362 [INFO] Sum of grad norms: 0.794747
2019-03-19 06:12:59,363 [INFO] ---------------------------------
2019-03-19 06:13:18,535 [INFO] ---------------------------------
2019-03-19 06:13:18,536 [INFO] Summary:
2019-03-19 06:13:18,536 [INFO] Batch 17000, worst loss 0.070558 (incl. reg.) of 1000 batches, learning rate 0.000683 @cl.-depth 1
2019-03-19 06:13:18,537 [INFO] Regularization: 4773.599609 * 0.0000010000 = 0.0047735996
2019-03-19 06:13:18,537 [INFO] Sum of grad norms: 1.835141
2019-03-19 06:13:18,538 [INFO] ---------------------------------
2019-03-19 06:13:37,604 [INFO] ---------------------------------
2019-03-19 06:13:37,605 [INFO] Summary:
2019-03-19 06:13:37,606 [INFO] Batch 18000, worst loss 0.067981 (incl. reg.) of 1000 batches, learning rate 0.000683 @cl.-depth 1
2019-03-19 06:13:37,606 [INFO] Regularization: 4719.146973 * 0.0000010000 = 0.0047191470
2019-03-19 06:13:37,607 [INFO] Sum of grad norms: 0.098780
2019-03-19 06:13:37,608 [INFO] ---------------------------------
2019-03-19 06:13:57,072 [INFO] ---------------------------------
2019-03-19 06:13:57,073 [INFO] Summary:
2019-03-19 06:13:57,074 [INFO] Batch 19000, worst loss 0.069232 (incl. reg.) of 1000 batches, learning rate 0.000680 @cl.-depth 1
2019-03-19 06:13:57,075 [INFO] Regularization: 4649.866699 * 0.0000010000 = 0.0046498668
2019-03-19 06:13:57,076 [INFO] Sum of grad norms: 0.387851
2019-03-19 06:13:57,076 [INFO] ---------------------------------
2019-03-19 06:14:15,875 [INFO] ---------------------------------
2019-03-19 06:14:15,876 [INFO] Summary:
2019-03-19 06:14:15,877 [INFO] Batch 20000, worst loss 0.066774 (incl. reg.) of 1000 batches, learning rate 0.000680 @cl.-depth 1
2019-03-19 06:14:15,877 [INFO] Regularization: 4571.899902 * 0.0000010000 = 0.0045718998
2019-03-19 06:14:15,878 [INFO] Sum of grad norms: 1.523099
2019-03-19 06:14:15,878 [INFO] ---------------------------------
2019-03-19 06:14:20,775 [INFO] ---------------------------------
2019-03-19 06:14:20,776 [INFO] Evaluation:
2019-03-19 06:14:20,778 [INFO] Batch 20000, worst loss 0.061484 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:14:20,779 [INFO] ---------------------------------
2019-03-19 06:14:39,515 [INFO] ---------------------------------
2019-03-19 06:14:39,516 [INFO] Summary:
2019-03-19 06:14:39,516 [INFO] Batch 21000, worst loss 0.066482 (incl. reg.) of 1000 batches, learning rate 0.000668 @cl.-depth 1
2019-03-19 06:14:39,517 [INFO] Regularization: 4515.472168 * 0.0000010000 = 0.0045154723
2019-03-19 06:14:39,517 [INFO] Sum of grad norms: 0.083886
2019-03-19 06:14:39,518 [INFO] ---------------------------------
2019-03-19 06:14:58,454 [INFO] ---------------------------------
2019-03-19 06:14:58,455 [INFO] Summary:
2019-03-19 06:14:58,456 [INFO] Batch 22000, worst loss 0.066600 (incl. reg.) of 1000 batches, learning rate 0.000665 @cl.-depth 1
2019-03-19 06:14:58,456 [INFO] Regularization: 4463.125488 * 0.0000010000 = 0.0044631255
2019-03-19 06:14:58,457 [INFO] Sum of grad norms: 0.229793
2019-03-19 06:14:58,457 [INFO] ---------------------------------
2019-03-19 06:15:17,419 [INFO] ---------------------------------
2019-03-19 06:15:17,420 [INFO] Summary:
2019-03-19 06:15:17,422 [INFO] Batch 23000, worst loss 0.065719 (incl. reg.) of 1000 batches, learning rate 0.000665 @cl.-depth 1
2019-03-19 06:15:17,422 [INFO] Regularization: 4387.977051 * 0.0000010000 = 0.0043879771
2019-03-19 06:15:17,423 [INFO] Sum of grad norms: 0.962254
2019-03-19 06:15:17,423 [INFO] ---------------------------------
2019-03-19 06:15:36,334 [INFO] ---------------------------------
2019-03-19 06:15:36,335 [INFO] Summary:
2019-03-19 06:15:36,335 [INFO] Batch 24000, worst loss 0.067433 (incl. reg.) of 1000 batches, learning rate 0.000657 @cl.-depth 1
2019-03-19 06:15:36,336 [INFO] Regularization: 4337.792480 * 0.0000010000 = 0.0043377923
2019-03-19 06:15:36,336 [INFO] Sum of grad norms: 0.286774
2019-03-19 06:15:36,337 [INFO] ---------------------------------
2019-03-19 06:15:55,716 [INFO] ---------------------------------
2019-03-19 06:15:55,717 [INFO] Summary:
2019-03-19 06:15:55,717 [INFO] Batch 25000, worst loss 0.065498 (incl. reg.) of 1000 batches, learning rate 0.000657 @cl.-depth 1
2019-03-19 06:15:55,718 [INFO] Regularization: 4292.284668 * 0.0000010000 = 0.0042922846
2019-03-19 06:15:55,719 [INFO] Sum of grad norms: 0.656095
2019-03-19 06:15:55,719 [INFO] ---------------------------------
2019-03-19 06:16:14,440 [INFO] ---------------------------------
2019-03-19 06:16:14,441 [INFO] Summary:
2019-03-19 06:16:14,441 [INFO] Batch 26000, worst loss 0.067012 (incl. reg.) of 1000 batches, learning rate 0.000655 @cl.-depth 1
2019-03-19 06:16:14,442 [INFO] Regularization: 4232.685059 * 0.0000010000 = 0.0042326851
2019-03-19 06:16:14,443 [INFO] Sum of grad norms: 0.426333
2019-03-19 06:16:14,443 [INFO] ---------------------------------
2019-03-19 06:16:33,054 [INFO] ---------------------------------
2019-03-19 06:16:33,055 [INFO] Summary:
2019-03-19 06:16:33,056 [INFO] Batch 27000, worst loss 0.064940 (incl. reg.) of 1000 batches, learning rate 0.000655 @cl.-depth 1
2019-03-19 06:16:33,057 [INFO] Regularization: 4174.311035 * 0.0000010000 = 0.0041743112
2019-03-19 06:16:33,057 [INFO] Sum of grad norms: 0.327678
2019-03-19 06:16:33,058 [INFO] ---------------------------------
2019-03-19 06:16:51,579 [INFO] ---------------------------------
2019-03-19 06:16:51,580 [INFO] Summary:
2019-03-19 06:16:51,581 [INFO] Batch 28000, worst loss 0.065093 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-19 06:16:51,581 [INFO] Regularization: 4123.622070 * 0.0000010000 = 0.0041236221
2019-03-19 06:16:51,582 [INFO] Sum of grad norms: 1.092824
2019-03-19 06:16:51,583 [INFO] ---------------------------------
2019-03-19 06:17:10,890 [INFO] ---------------------------------
2019-03-19 06:17:10,890 [INFO] Summary:
2019-03-19 06:17:10,891 [INFO] Batch 29000, worst loss 0.065062 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-19 06:17:10,892 [INFO] Regularization: 4067.900879 * 0.0000010000 = 0.0040679011
2019-03-19 06:17:10,892 [INFO] Sum of grad norms: 0.157977
2019-03-19 06:17:10,893 [INFO] ---------------------------------
2019-03-19 06:17:29,685 [INFO] ---------------------------------
2019-03-19 06:17:29,686 [INFO] Summary:
2019-03-19 06:17:29,686 [INFO] Batch 30000, worst loss 0.065619 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-19 06:17:29,687 [INFO] Regularization: 4020.832031 * 0.0000010000 = 0.0040208320
2019-03-19 06:17:29,688 [INFO] Sum of grad norms: 0.640932
2019-03-19 06:17:29,688 [INFO] ---------------------------------
2019-03-19 06:17:34,647 [INFO] ---------------------------------
2019-03-19 06:17:34,649 [INFO] Evaluation:
2019-03-19 06:17:34,649 [INFO] Batch 30000, worst loss 0.059778 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:17:34,650 [INFO] ---------------------------------
2019-03-19 06:17:53,907 [INFO] ---------------------------------
2019-03-19 06:17:53,908 [INFO] Summary:
2019-03-19 06:17:53,908 [INFO] Batch 31000, worst loss 0.064688 (incl. reg.) of 1000 batches, learning rate 0.000649 @cl.-depth 1
2019-03-19 06:17:53,909 [INFO] Regularization: 3968.622314 * 0.0000010000 = 0.0039686225
2019-03-19 06:17:53,910 [INFO] Sum of grad norms: 0.382451
2019-03-19 06:17:53,910 [INFO] ---------------------------------
2019-03-19 06:18:12,934 [INFO] ---------------------------------
2019-03-19 06:18:12,935 [INFO] Summary:
2019-03-19 06:18:12,936 [INFO] Batch 32000, worst loss 0.064773 (incl. reg.) of 1000 batches, learning rate 0.000647 @cl.-depth 1
2019-03-19 06:18:12,937 [INFO] Regularization: 3917.331055 * 0.0000010000 = 0.0039173309
2019-03-19 06:18:12,937 [INFO] Sum of grad norms: 0.228990
2019-03-19 06:18:12,938 [INFO] ---------------------------------
2019-03-19 06:18:31,932 [INFO] ---------------------------------
2019-03-19 06:18:31,933 [INFO] Summary:
2019-03-19 06:18:31,933 [INFO] Batch 33000, worst loss 0.063976 (incl. reg.) of 1000 batches, learning rate 0.000647 @cl.-depth 1
2019-03-19 06:18:31,934 [INFO] Regularization: 3868.605713 * 0.0000010000 = 0.0038686057
2019-03-19 06:18:31,935 [INFO] Sum of grad norms: 0.145436
2019-03-19 06:18:31,935 [INFO] ---------------------------------
2019-03-19 06:18:50,990 [INFO] ---------------------------------
2019-03-19 06:18:50,991 [INFO] Summary:
2019-03-19 06:18:50,991 [INFO] Batch 34000, worst loss 0.064593 (incl. reg.) of 1000 batches, learning rate 0.000640 @cl.-depth 1
2019-03-19 06:18:50,992 [INFO] Regularization: 3818.996338 * 0.0000010000 = 0.0038189963
2019-03-19 06:18:50,992 [INFO] Sum of grad norms: 0.282399
2019-03-19 06:18:50,993 [INFO] ---------------------------------
2019-03-19 06:19:10,274 [INFO] ---------------------------------
2019-03-19 06:19:10,275 [INFO] Summary:
2019-03-19 06:19:10,276 [INFO] Batch 35000, worst loss 0.063538 (incl. reg.) of 1000 batches, learning rate 0.000640 @cl.-depth 1
2019-03-19 06:19:10,276 [INFO] Regularization: 3769.681885 * 0.0000010000 = 0.0037696818
2019-03-19 06:19:10,277 [INFO] Sum of grad norms: 0.351347
2019-03-19 06:19:10,277 [INFO] ---------------------------------
2019-03-19 06:19:29,508 [INFO] ---------------------------------
2019-03-19 06:19:29,509 [INFO] Summary:
2019-03-19 06:19:29,509 [INFO] Batch 36000, worst loss 0.063448 (incl. reg.) of 1000 batches, learning rate 0.000635 @cl.-depth 1
2019-03-19 06:19:29,510 [INFO] Regularization: 3724.040039 * 0.0000010000 = 0.0037240400
2019-03-19 06:19:29,510 [INFO] Sum of grad norms: 0.157570
2019-03-19 06:19:29,511 [INFO] ---------------------------------
2019-03-19 06:19:48,505 [INFO] ---------------------------------
2019-03-19 06:19:48,506 [INFO] Summary:
2019-03-19 06:19:48,507 [INFO] Batch 37000, worst loss 0.063238 (incl. reg.) of 1000 batches, learning rate 0.000634 @cl.-depth 1
2019-03-19 06:19:48,507 [INFO] Regularization: 3680.010254 * 0.0000010000 = 0.0036800103
2019-03-19 06:19:48,508 [INFO] Sum of grad norms: 0.064513
2019-03-19 06:19:48,508 [INFO] ---------------------------------
2019-03-19 06:20:07,484 [INFO] ---------------------------------
2019-03-19 06:20:07,484 [INFO] Summary:
2019-03-19 06:20:07,485 [INFO] Batch 38000, worst loss 0.064463 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-19 06:20:07,486 [INFO] Regularization: 3639.547607 * 0.0000010000 = 0.0036395476
2019-03-19 06:20:07,486 [INFO] Sum of grad norms: 0.190731
2019-03-19 06:20:07,487 [INFO] ---------------------------------
2019-03-19 06:20:26,403 [INFO] ---------------------------------
2019-03-19 06:20:26,404 [INFO] Summary:
2019-03-19 06:20:26,404 [INFO] Batch 39000, worst loss 0.063785 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-19 06:20:26,405 [INFO] Regularization: 3594.785645 * 0.0000010000 = 0.0035947857
2019-03-19 06:20:26,405 [INFO] Sum of grad norms: 0.448322
2019-03-19 06:20:26,406 [INFO] ---------------------------------
2019-03-19 06:20:45,528 [INFO] ---------------------------------
2019-03-19 06:20:45,529 [INFO] Summary:
2019-03-19 06:20:45,530 [INFO] Batch 40000, worst loss 0.063062 (incl. reg.) of 1000 batches, learning rate 0.000632 @cl.-depth 1
2019-03-19 06:20:45,530 [INFO] Regularization: 3546.880859 * 0.0000010000 = 0.0035468808
2019-03-19 06:20:45,531 [INFO] Sum of grad norms: 0.063451
2019-03-19 06:20:45,531 [INFO] ---------------------------------
2019-03-19 06:20:50,454 [INFO] ---------------------------------
2019-03-19 06:20:50,455 [INFO] Evaluation:
2019-03-19 06:20:50,456 [INFO] Batch 40000, worst loss 0.058736 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:20:50,456 [INFO] ---------------------------------
2019-03-19 06:21:09,193 [INFO] ---------------------------------
2019-03-19 06:21:09,194 [INFO] Summary:
2019-03-19 06:21:09,195 [INFO] Batch 41000, worst loss 0.063441 (incl. reg.) of 1000 batches, learning rate 0.000315 @cl.-depth 1
2019-03-19 06:21:09,196 [INFO] Regularization: 3507.555420 * 0.0000010000 = 0.0035075555
2019-03-19 06:21:09,196 [INFO] Sum of grad norms: 1.252626
2019-03-19 06:21:09,197 [INFO] ---------------------------------
2019-03-19 06:21:27,820 [INFO] ---------------------------------
2019-03-19 06:21:27,821 [INFO] Summary:
2019-03-19 06:21:27,822 [INFO] Batch 42000, worst loss 0.062216 (incl. reg.) of 1000 batches, learning rate 0.000315 @cl.-depth 1
2019-03-19 06:21:27,822 [INFO] Regularization: 3473.898682 * 0.0000010000 = 0.0034738986
2019-03-19 06:21:27,823 [INFO] Sum of grad norms: 0.146724
2019-03-19 06:21:27,823 [INFO] ---------------------------------
2019-03-19 06:21:47,181 [INFO] ---------------------------------
2019-03-19 06:21:47,182 [INFO] Summary:
2019-03-19 06:21:47,183 [INFO] Batch 43000, worst loss 0.061889 (incl. reg.) of 1000 batches, learning rate 0.000315 @cl.-depth 1
2019-03-19 06:21:47,183 [INFO] Regularization: 3447.376221 * 0.0000010000 = 0.0034473762
2019-03-19 06:21:47,184 [INFO] Sum of grad norms: 0.284130
2019-03-19 06:21:47,185 [INFO] ---------------------------------
2019-03-19 06:22:06,528 [INFO] ---------------------------------
2019-03-19 06:22:06,529 [INFO] Summary:
2019-03-19 06:22:06,529 [INFO] Batch 44000, worst loss 0.062046 (incl. reg.) of 1000 batches, learning rate 0.000315 @cl.-depth 1
2019-03-19 06:22:06,530 [INFO] Regularization: 3420.718750 * 0.0000010000 = 0.0034207187
2019-03-19 06:22:06,530 [INFO] Sum of grad norms: 0.466936
2019-03-19 06:22:06,531 [INFO] ---------------------------------
2019-03-19 06:22:25,474 [INFO] ---------------------------------
2019-03-19 06:22:25,475 [INFO] Summary:
2019-03-19 06:22:25,476 [INFO] Batch 45000, worst loss 0.061918 (incl. reg.) of 1000 batches, learning rate 0.000315 @cl.-depth 1
2019-03-19 06:22:25,476 [INFO] Regularization: 3396.371338 * 0.0000010000 = 0.0033963714
2019-03-19 06:22:25,477 [INFO] Sum of grad norms: 0.300355
2019-03-19 06:22:25,478 [INFO] ---------------------------------
2019-03-19 06:22:44,265 [INFO] ---------------------------------
2019-03-19 06:22:44,266 [INFO] Summary:
2019-03-19 06:22:44,267 [INFO] Batch 46000, worst loss 0.061793 (incl. reg.) of 1000 batches, learning rate 0.000315 @cl.-depth 1
2019-03-19 06:22:44,267 [INFO] Regularization: 3372.461426 * 0.0000010000 = 0.0033724613
2019-03-19 06:22:44,268 [INFO] Sum of grad norms: 0.117720
2019-03-19 06:22:44,268 [INFO] ---------------------------------
2019-03-19 06:23:02,932 [INFO] ---------------------------------
2019-03-19 06:23:02,933 [INFO] Summary:
2019-03-19 06:23:02,933 [INFO] Batch 47000, worst loss 0.061702 (incl. reg.) of 1000 batches, learning rate 0.000315 @cl.-depth 1
2019-03-19 06:23:02,934 [INFO] Regularization: 3348.154541 * 0.0000010000 = 0.0033481545
2019-03-19 06:23:02,934 [INFO] Sum of grad norms: 0.205660
2019-03-19 06:23:02,935 [INFO] ---------------------------------
2019-03-19 06:23:22,181 [INFO] ---------------------------------
2019-03-19 06:23:22,182 [INFO] Summary:
2019-03-19 06:23:22,182 [INFO] Batch 48000, worst loss 0.061617 (incl. reg.) of 1000 batches, learning rate 0.000315 @cl.-depth 1
2019-03-19 06:23:22,183 [INFO] Regularization: 3324.374512 * 0.0000010000 = 0.0033243746
2019-03-19 06:23:22,183 [INFO] Sum of grad norms: 0.108275
2019-03-19 06:23:22,184 [INFO] ---------------------------------
2019-03-19 06:23:40,978 [INFO] ---------------------------------
2019-03-19 06:23:40,979 [INFO] Summary:
2019-03-19 06:23:40,979 [INFO] Batch 49000, worst loss 0.061522 (incl. reg.) of 1000 batches, learning rate 0.000315 @cl.-depth 1
2019-03-19 06:23:40,980 [INFO] Regularization: 3300.567139 * 0.0000010000 = 0.0033005672
2019-03-19 06:23:40,980 [INFO] Sum of grad norms: 0.050978
2019-03-19 06:23:40,981 [INFO] ---------------------------------
2019-03-19 06:23:59,372 [INFO] ---------------------------------
2019-03-19 06:23:59,373 [INFO] Summary:
2019-03-19 06:23:59,373 [INFO] Batch 50000, worst loss 0.061594 (incl. reg.) of 1000 batches, learning rate 0.000315 @cl.-depth 1
2019-03-19 06:23:59,374 [INFO] Regularization: 3277.976318 * 0.0000010000 = 0.0032779763
2019-03-19 06:23:59,374 [INFO] Sum of grad norms: 0.038901
2019-03-19 06:23:59,375 [INFO] ---------------------------------
2019-03-19 06:24:04,350 [INFO] ---------------------------------
2019-03-19 06:24:04,351 [INFO] Evaluation:
2019-03-19 06:24:04,351 [INFO] Batch 50000, worst loss 0.058265 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:24:04,352 [INFO] ---------------------------------
2019-03-19 06:24:23,153 [INFO] ---------------------------------
2019-03-19 06:24:23,154 [INFO] Summary:
2019-03-19 06:24:23,154 [INFO] Batch 51000, worst loss 0.061413 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 06:24:23,155 [INFO] Regularization: 3254.739990 * 0.0000010000 = 0.0032547400
2019-03-19 06:24:23,155 [INFO] Sum of grad norms: 0.220608
2019-03-19 06:24:23,156 [INFO] ---------------------------------
2019-03-19 06:24:41,704 [INFO] ---------------------------------
2019-03-19 06:24:41,705 [INFO] Summary:
2019-03-19 06:24:41,706 [INFO] Batch 52000, worst loss 0.061328 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 06:24:41,706 [INFO] Regularization: 3238.345459 * 0.0000010000 = 0.0032383455
2019-03-19 06:24:41,707 [INFO] Sum of grad norms: 0.114623
2019-03-19 06:24:41,707 [INFO] ---------------------------------
2019-03-19 06:25:00,722 [INFO] ---------------------------------
2019-03-19 06:25:00,723 [INFO] Summary:
2019-03-19 06:25:00,723 [INFO] Batch 53000, worst loss 0.061322 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 06:25:00,724 [INFO] Regularization: 3224.871338 * 0.0000010000 = 0.0032248714
2019-03-19 06:25:00,724 [INFO] Sum of grad norms: 0.101849
2019-03-19 06:25:00,725 [INFO] ---------------------------------
2019-03-19 06:25:19,472 [INFO] ---------------------------------
2019-03-19 06:25:19,473 [INFO] Summary:
2019-03-19 06:25:19,473 [INFO] Batch 54000, worst loss 0.061364 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 06:25:19,474 [INFO] Regularization: 3212.323242 * 0.0000010000 = 0.0032123232
2019-03-19 06:25:19,475 [INFO] Sum of grad norms: 0.066045
2019-03-19 06:25:19,475 [INFO] ---------------------------------
2019-03-19 06:25:38,600 [INFO] ---------------------------------
2019-03-19 06:25:38,600 [INFO] Summary:
2019-03-19 06:25:38,601 [INFO] Batch 55000, worst loss 0.061247 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 06:25:38,602 [INFO] Regularization: 3200.348633 * 0.0000010000 = 0.0032003487
2019-03-19 06:25:38,602 [INFO] Sum of grad norms: 0.095328
2019-03-19 06:25:38,603 [INFO] ---------------------------------
2019-03-19 06:25:57,423 [INFO] ---------------------------------
2019-03-19 06:25:57,424 [INFO] Summary:
2019-03-19 06:25:57,425 [INFO] Batch 56000, worst loss 0.061148 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 06:25:57,425 [INFO] Regularization: 3186.841797 * 0.0000010000 = 0.0031868417
2019-03-19 06:25:57,426 [INFO] Sum of grad norms: 0.077550
2019-03-19 06:25:57,426 [INFO] ---------------------------------
2019-03-19 06:26:16,073 [INFO] ---------------------------------
2019-03-19 06:26:16,074 [INFO] Summary:
2019-03-19 06:26:16,075 [INFO] Batch 57000, worst loss 0.061327 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 06:26:16,075 [INFO] Regularization: 3173.871338 * 0.0000010000 = 0.0031738714
2019-03-19 06:26:16,075 [INFO] Sum of grad norms: 0.148490
2019-03-19 06:26:16,076 [INFO] ---------------------------------
2019-03-19 06:26:35,265 [INFO] ---------------------------------
2019-03-19 06:26:35,266 [INFO] Summary:
2019-03-19 06:26:35,266 [INFO] Batch 58000, worst loss 0.061117 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 06:26:35,267 [INFO] Regularization: 3162.634033 * 0.0000010000 = 0.0031626341
2019-03-19 06:26:35,268 [INFO] Sum of grad norms: 0.135757
2019-03-19 06:26:35,268 [INFO] ---------------------------------
2019-03-19 06:26:54,183 [INFO] ---------------------------------
2019-03-19 06:26:54,183 [INFO] Summary:
2019-03-19 06:26:54,184 [INFO] Batch 59000, worst loss 0.061253 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 06:26:54,184 [INFO] Regularization: 3149.413086 * 0.0000010000 = 0.0031494130
2019-03-19 06:26:54,185 [INFO] Sum of grad norms: 0.038694
2019-03-19 06:26:54,186 [INFO] ---------------------------------
2019-03-19 06:27:12,817 [INFO] ---------------------------------
2019-03-19 06:27:12,818 [INFO] Summary:
2019-03-19 06:27:12,819 [INFO] Batch 60000, worst loss 0.061516 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 06:27:12,820 [INFO] Regularization: 3136.865967 * 0.0000010000 = 0.0031368660
2019-03-19 06:27:12,821 [INFO] Sum of grad norms: 0.093637
2019-03-19 06:27:12,822 [INFO] ---------------------------------
2019-03-19 06:27:17,656 [INFO] ---------------------------------
2019-03-19 06:27:17,657 [INFO] Evaluation:
2019-03-19 06:27:17,658 [INFO] Batch 60000, worst loss 0.057812 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:27:17,659 [INFO] ---------------------------------
2019-03-19 06:27:36,484 [INFO] ---------------------------------
2019-03-19 06:27:36,485 [INFO] Summary:
2019-03-19 06:27:36,486 [INFO] Batch 61000, worst loss 0.061299 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 06:27:36,486 [INFO] Regularization: 3124.984619 * 0.0000010000 = 0.0031249847
2019-03-19 06:27:36,487 [INFO] Sum of grad norms: 0.037430
2019-03-19 06:27:36,488 [INFO] ---------------------------------
2019-03-19 06:27:55,534 [INFO] ---------------------------------
2019-03-19 06:27:55,535 [INFO] Summary:
2019-03-19 06:27:55,535 [INFO] Batch 62000, worst loss 0.061171 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 06:27:55,536 [INFO] Regularization: 3117.105957 * 0.0000010000 = 0.0031171059
2019-03-19 06:27:55,537 [INFO] Sum of grad norms: 0.032060
2019-03-19 06:27:55,537 [INFO] ---------------------------------
2019-03-19 06:28:14,215 [INFO] ---------------------------------
2019-03-19 06:28:14,216 [INFO] Summary:
2019-03-19 06:28:14,216 [INFO] Batch 63000, worst loss 0.061056 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 06:28:14,217 [INFO] Regularization: 3110.535156 * 0.0000010000 = 0.0031105352
2019-03-19 06:28:14,218 [INFO] Sum of grad norms: 0.077271
2019-03-19 06:28:14,219 [INFO] ---------------------------------
2019-03-19 06:28:32,506 [INFO] ---------------------------------
2019-03-19 06:28:32,507 [INFO] Summary:
2019-03-19 06:28:32,508 [INFO] Batch 64000, worst loss 0.060970 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 06:28:32,509 [INFO] Regularization: 3104.112061 * 0.0000010000 = 0.0031041121
2019-03-19 06:28:32,509 [INFO] Sum of grad norms: 0.061136
2019-03-19 06:28:32,510 [INFO] ---------------------------------
2019-03-19 06:28:51,116 [INFO] ---------------------------------
2019-03-19 06:28:51,117 [INFO] Summary:
2019-03-19 06:28:51,118 [INFO] Batch 65000, worst loss 0.061030 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 06:28:51,119 [INFO] Regularization: 3097.542480 * 0.0000010000 = 0.0030975426
2019-03-19 06:28:51,119 [INFO] Sum of grad norms: 0.042917
2019-03-19 06:28:51,120 [INFO] ---------------------------------
2019-03-19 06:29:09,741 [INFO] ---------------------------------
2019-03-19 06:29:09,742 [INFO] Summary:
2019-03-19 06:29:09,743 [INFO] Batch 66000, worst loss 0.061035 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 06:29:09,744 [INFO] Regularization: 3091.483887 * 0.0000010000 = 0.0030914838
2019-03-19 06:29:09,745 [INFO] Sum of grad norms: 0.117376
2019-03-19 06:29:09,745 [INFO] ---------------------------------
2019-03-19 06:29:28,509 [INFO] ---------------------------------
2019-03-19 06:29:28,510 [INFO] Summary:
2019-03-19 06:29:28,510 [INFO] Batch 67000, worst loss 0.060927 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 06:29:28,511 [INFO] Regularization: 3085.498779 * 0.0000010000 = 0.0030854987
2019-03-19 06:29:28,511 [INFO] Sum of grad norms: 0.025297
2019-03-19 06:29:28,512 [INFO] ---------------------------------
2019-03-19 06:29:47,443 [INFO] ---------------------------------
2019-03-19 06:29:47,444 [INFO] Summary:
2019-03-19 06:29:47,445 [INFO] Batch 68000, worst loss 0.061056 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 06:29:47,445 [INFO] Regularization: 3078.838867 * 0.0000010000 = 0.0030788388
2019-03-19 06:29:47,446 [INFO] Sum of grad norms: 0.115956
2019-03-19 06:29:47,446 [INFO] ---------------------------------
2019-03-19 06:30:06,046 [INFO] ---------------------------------
2019-03-19 06:30:06,047 [INFO] Summary:
2019-03-19 06:30:06,047 [INFO] Batch 69000, worst loss 0.061061 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 06:30:06,048 [INFO] Regularization: 3071.960693 * 0.0000010000 = 0.0030719608
2019-03-19 06:30:06,048 [INFO] Sum of grad norms: 0.071097
2019-03-19 06:30:06,049 [INFO] ---------------------------------
2019-03-19 06:30:24,608 [INFO] ---------------------------------
2019-03-19 06:30:24,609 [INFO] Summary:
2019-03-19 06:30:24,610 [INFO] Batch 70000, worst loss 0.060943 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 06:30:24,610 [INFO] Regularization: 3066.420410 * 0.0000010000 = 0.0030664203
2019-03-19 06:30:24,611 [INFO] Sum of grad norms: 0.088258
2019-03-19 06:30:24,611 [INFO] ---------------------------------
2019-03-19 06:30:29,587 [INFO] ---------------------------------
2019-03-19 06:30:29,589 [INFO] Evaluation:
2019-03-19 06:30:29,589 [INFO] Batch 70000, worst loss 0.057885 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:30:29,590 [INFO] ---------------------------------
2019-03-19 06:30:48,422 [INFO] ---------------------------------
2019-03-19 06:30:48,423 [INFO] Summary:
2019-03-19 06:30:48,424 [INFO] Batch 71000, worst loss 0.060919 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 06:30:48,424 [INFO] Regularization: 3059.623047 * 0.0000010000 = 0.0030596231
2019-03-19 06:30:48,425 [INFO] Sum of grad norms: 0.032240
2019-03-19 06:30:48,425 [INFO] ---------------------------------
2019-03-19 06:31:07,079 [INFO] ---------------------------------
2019-03-19 06:31:07,080 [INFO] Summary:
2019-03-19 06:31:07,081 [INFO] Batch 72000, worst loss 0.060920 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 06:31:07,082 [INFO] Regularization: 3055.676758 * 0.0000010000 = 0.0030556768
2019-03-19 06:31:07,083 [INFO] Sum of grad norms: 0.037546
2019-03-19 06:31:07,083 [INFO] ---------------------------------
2019-03-19 06:31:26,022 [INFO] ---------------------------------
2019-03-19 06:31:26,023 [INFO] Summary:
2019-03-19 06:31:26,024 [INFO] Batch 73000, worst loss 0.060913 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 06:31:26,024 [INFO] Regularization: 3051.959961 * 0.0000010000 = 0.0030519599
2019-03-19 06:31:26,025 [INFO] Sum of grad norms: 0.048372
2019-03-19 06:31:26,026 [INFO] ---------------------------------
2019-03-19 06:31:45,134 [INFO] ---------------------------------
2019-03-19 06:31:45,135 [INFO] Summary:
2019-03-19 06:31:45,135 [INFO] Batch 74000, worst loss 0.061047 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 06:31:45,136 [INFO] Regularization: 3048.748047 * 0.0000010000 = 0.0030487480
2019-03-19 06:31:45,136 [INFO] Sum of grad norms: 0.104709
2019-03-19 06:31:45,137 [INFO] ---------------------------------
2019-03-19 06:32:04,000 [INFO] ---------------------------------
2019-03-19 06:32:04,001 [INFO] Summary:
2019-03-19 06:32:04,002 [INFO] Batch 75000, worst loss 0.061018 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 06:32:04,002 [INFO] Regularization: 3045.221191 * 0.0000010000 = 0.0030452211
2019-03-19 06:32:04,003 [INFO] Sum of grad norms: 0.074982
2019-03-19 06:32:04,003 [INFO] ---------------------------------
2019-03-19 06:32:22,917 [INFO] ---------------------------------
2019-03-19 06:32:22,918 [INFO] Summary:
2019-03-19 06:32:22,918 [INFO] Batch 76000, worst loss 0.060790 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 06:32:22,919 [INFO] Regularization: 3041.808594 * 0.0000010000 = 0.0030418085
2019-03-19 06:32:22,919 [INFO] Sum of grad norms: 0.079739
2019-03-19 06:32:22,920 [INFO] ---------------------------------
2019-03-19 06:32:41,778 [INFO] ---------------------------------
2019-03-19 06:32:41,779 [INFO] Summary:
2019-03-19 06:32:41,780 [INFO] Batch 77000, worst loss 0.060995 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 06:32:41,780 [INFO] Regularization: 3038.395264 * 0.0000010000 = 0.0030383952
2019-03-19 06:32:41,781 [INFO] Sum of grad norms: 0.031522
2019-03-19 06:32:41,781 [INFO] ---------------------------------
2019-03-19 06:33:00,439 [INFO] ---------------------------------
2019-03-19 06:33:00,440 [INFO] Summary:
2019-03-19 06:33:00,440 [INFO] Batch 78000, worst loss 0.061123 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 06:33:00,441 [INFO] Regularization: 3035.098145 * 0.0000010000 = 0.0030350981
2019-03-19 06:33:00,441 [INFO] Sum of grad norms: 0.149133
2019-03-19 06:33:00,442 [INFO] ---------------------------------
2019-03-19 06:33:18,909 [INFO] ---------------------------------
2019-03-19 06:33:18,910 [INFO] Summary:
2019-03-19 06:33:18,910 [INFO] Batch 79000, worst loss 0.061123 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 06:33:18,911 [INFO] Regularization: 3031.780273 * 0.0000010000 = 0.0030317802
2019-03-19 06:33:18,911 [INFO] Sum of grad norms: 0.044154
2019-03-19 06:33:18,912 [INFO] ---------------------------------
2019-03-19 06:33:37,720 [INFO] ---------------------------------
2019-03-19 06:33:37,721 [INFO] Summary:
2019-03-19 06:33:37,722 [INFO] Batch 80000, worst loss 0.060860 (incl. reg.) of 1000 batches, learning rate 0.000039 @cl.-depth 1
2019-03-19 06:33:37,723 [INFO] Regularization: 3028.240234 * 0.0000010000 = 0.0030282403
2019-03-19 06:33:37,723 [INFO] Sum of grad norms: 0.040685
2019-03-19 06:33:37,724 [INFO] ---------------------------------
2019-03-19 06:33:42,659 [INFO] ---------------------------------
2019-03-19 06:33:42,660 [INFO] Evaluation:
2019-03-19 06:33:42,660 [INFO] Batch 80000, worst loss 0.057757 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:33:42,661 [INFO] ---------------------------------
2019-03-19 06:34:01,380 [INFO] ---------------------------------
2019-03-19 06:34:01,381 [INFO] Summary:
2019-03-19 06:34:01,382 [INFO] Batch 81000, worst loss 0.060728 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 06:34:01,382 [INFO] Regularization: 3024.895752 * 0.0000010000 = 0.0030248957
2019-03-19 06:34:01,383 [INFO] Sum of grad norms: 0.045502
2019-03-19 06:34:01,383 [INFO] ---------------------------------
2019-03-19 06:34:20,279 [INFO] ---------------------------------
2019-03-19 06:34:20,280 [INFO] Summary:
2019-03-19 06:34:20,280 [INFO] Batch 82000, worst loss 0.060932 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 06:34:20,281 [INFO] Regularization: 3022.536865 * 0.0000010000 = 0.0030225369
2019-03-19 06:34:20,281 [INFO] Sum of grad norms: 0.033914
2019-03-19 06:34:20,282 [INFO] ---------------------------------
2019-03-19 06:34:39,001 [INFO] ---------------------------------
2019-03-19 06:34:39,002 [INFO] Summary:
2019-03-19 06:34:39,003 [INFO] Batch 83000, worst loss 0.060854 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 06:34:39,003 [INFO] Regularization: 3020.791260 * 0.0000010000 = 0.0030207913
2019-03-19 06:34:39,003 [INFO] Sum of grad norms: 0.032845
2019-03-19 06:34:39,004 [INFO] ---------------------------------
2019-03-19 06:34:57,881 [INFO] ---------------------------------
2019-03-19 06:34:57,882 [INFO] Summary:
2019-03-19 06:34:57,882 [INFO] Batch 84000, worst loss 0.060752 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 06:34:57,883 [INFO] Regularization: 3019.064941 * 0.0000010000 = 0.0030190649
2019-03-19 06:34:57,884 [INFO] Sum of grad norms: 0.024456
2019-03-19 06:34:57,884 [INFO] ---------------------------------
2019-03-19 06:35:16,593 [INFO] ---------------------------------
2019-03-19 06:35:16,594 [INFO] Summary:
2019-03-19 06:35:16,595 [INFO] Batch 85000, worst loss 0.060914 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 06:35:16,595 [INFO] Regularization: 3017.129150 * 0.0000010000 = 0.0030171291
2019-03-19 06:35:16,596 [INFO] Sum of grad norms: 0.042132
2019-03-19 06:35:16,597 [INFO] ---------------------------------
2019-03-19 06:35:35,371 [INFO] ---------------------------------
2019-03-19 06:35:35,372 [INFO] Summary:
2019-03-19 06:35:35,372 [INFO] Batch 86000, worst loss 0.060899 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 06:35:35,373 [INFO] Regularization: 3015.337158 * 0.0000010000 = 0.0030153370
2019-03-19 06:35:35,373 [INFO] Sum of grad norms: 0.023888
2019-03-19 06:35:35,374 [INFO] ---------------------------------
2019-03-19 06:35:54,463 [INFO] ---------------------------------
2019-03-19 06:35:54,464 [INFO] Summary:
2019-03-19 06:35:54,465 [INFO] Batch 87000, worst loss 0.060648 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 06:35:54,465 [INFO] Regularization: 3013.551270 * 0.0000010000 = 0.0030135512
2019-03-19 06:35:54,466 [INFO] Sum of grad norms: 0.021509
2019-03-19 06:35:54,466 [INFO] ---------------------------------
2019-03-19 06:36:13,205 [INFO] ---------------------------------
2019-03-19 06:36:13,206 [INFO] Summary:
2019-03-19 06:36:13,207 [INFO] Batch 88000, worst loss 0.060797 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 06:36:13,207 [INFO] Regularization: 3011.827637 * 0.0000010000 = 0.0030118276
2019-03-19 06:36:13,208 [INFO] Sum of grad norms: 0.057995
2019-03-19 06:36:13,209 [INFO] ---------------------------------
2019-03-19 06:36:31,994 [INFO] ---------------------------------
2019-03-19 06:36:31,995 [INFO] Summary:
2019-03-19 06:36:31,996 [INFO] Batch 89000, worst loss 0.060644 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 06:36:31,996 [INFO] Regularization: 3010.060059 * 0.0000010000 = 0.0030100599
2019-03-19 06:36:31,997 [INFO] Sum of grad norms: 0.052807
2019-03-19 06:36:31,998 [INFO] ---------------------------------
2019-03-19 06:36:50,857 [INFO] ---------------------------------
2019-03-19 06:36:50,859 [INFO] Summary:
2019-03-19 06:36:50,859 [INFO] Batch 90000, worst loss 0.060799 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 06:36:50,860 [INFO] Regularization: 3008.565186 * 0.0000010000 = 0.0030085652
2019-03-19 06:36:50,860 [INFO] Sum of grad norms: 0.016651
2019-03-19 06:36:50,861 [INFO] ---------------------------------
2019-03-19 06:36:55,790 [INFO] ---------------------------------
2019-03-19 06:36:55,790 [INFO] Evaluation:
2019-03-19 06:36:55,791 [INFO] Batch 90000, worst loss 0.057811 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:36:55,792 [INFO] ---------------------------------
2019-03-19 06:37:14,827 [INFO] ---------------------------------
2019-03-19 06:37:14,828 [INFO] Summary:
2019-03-19 06:37:14,829 [INFO] Batch 91000, worst loss 0.060926 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 06:37:14,829 [INFO] Regularization: 3006.975098 * 0.0000010000 = 0.0030069752
2019-03-19 06:37:14,830 [INFO] Sum of grad norms: 0.026592
2019-03-19 06:37:14,830 [INFO] ---------------------------------
2019-03-19 06:37:33,333 [INFO] ---------------------------------
2019-03-19 06:37:33,334 [INFO] Summary:
2019-03-19 06:37:33,335 [INFO] Batch 92000, worst loss 0.060931 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 06:37:33,336 [INFO] Regularization: 3005.713379 * 0.0000010000 = 0.0030057135
2019-03-19 06:37:33,336 [INFO] Sum of grad norms: 0.021019
2019-03-19 06:37:33,337 [INFO] ---------------------------------
2019-03-19 06:37:52,683 [INFO] ---------------------------------
2019-03-19 06:37:52,684 [INFO] Summary:
2019-03-19 06:37:52,685 [INFO] Batch 93000, worst loss 0.060700 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 06:37:52,686 [INFO] Regularization: 3004.746826 * 0.0000010000 = 0.0030047467
2019-03-19 06:37:52,686 [INFO] Sum of grad norms: 0.032547
2019-03-19 06:37:52,687 [INFO] ---------------------------------
2019-03-19 06:38:11,519 [INFO] ---------------------------------
2019-03-19 06:38:11,520 [INFO] Summary:
2019-03-19 06:38:11,521 [INFO] Batch 94000, worst loss 0.060838 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 06:38:11,521 [INFO] Regularization: 3003.818848 * 0.0000010000 = 0.0030038189
2019-03-19 06:38:11,522 [INFO] Sum of grad norms: 0.076425
2019-03-19 06:38:11,522 [INFO] ---------------------------------
2019-03-19 06:38:30,472 [INFO] ---------------------------------
2019-03-19 06:38:30,473 [INFO] Summary:
2019-03-19 06:38:30,473 [INFO] Batch 95000, worst loss 0.060835 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 06:38:30,474 [INFO] Regularization: 3002.798584 * 0.0000010000 = 0.0030027987
2019-03-19 06:38:30,474 [INFO] Sum of grad norms: 0.030551
2019-03-19 06:38:30,475 [INFO] ---------------------------------
2019-03-19 06:38:49,487 [INFO] ---------------------------------
2019-03-19 06:38:49,487 [INFO] Summary:
2019-03-19 06:38:49,488 [INFO] Batch 96000, worst loss 0.060830 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 06:38:49,488 [INFO] Regularization: 3001.835693 * 0.0000010000 = 0.0030018357
2019-03-19 06:38:49,489 [INFO] Sum of grad norms: 0.098235
2019-03-19 06:38:49,490 [INFO] ---------------------------------
2019-03-19 06:39:08,416 [INFO] ---------------------------------
2019-03-19 06:39:08,417 [INFO] Summary:
2019-03-19 06:39:08,417 [INFO] Batch 97000, worst loss 0.060849 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 06:39:08,418 [INFO] Regularization: 3001.041748 * 0.0000010000 = 0.0030010417
2019-03-19 06:39:08,418 [INFO] Sum of grad norms: 0.041542
2019-03-19 06:39:08,419 [INFO] ---------------------------------
2019-03-19 06:39:27,591 [INFO] ---------------------------------
2019-03-19 06:39:27,592 [INFO] Summary:
2019-03-19 06:39:27,593 [INFO] Batch 98000, worst loss 0.060856 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 06:39:27,593 [INFO] Regularization: 3000.117920 * 0.0000010000 = 0.0030001178
2019-03-19 06:39:27,594 [INFO] Sum of grad norms: 0.038347
2019-03-19 06:39:27,595 [INFO] ---------------------------------
2019-03-19 06:39:46,390 [INFO] ---------------------------------
2019-03-19 06:39:46,391 [INFO] Summary:
2019-03-19 06:39:46,392 [INFO] Batch 99000, worst loss 0.060972 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 06:39:46,392 [INFO] Regularization: 2999.068115 * 0.0000010000 = 0.0029990680
2019-03-19 06:39:46,393 [INFO] Sum of grad norms: 0.063566
2019-03-19 06:39:46,393 [INFO] ---------------------------------
2019-03-19 06:40:05,142 [INFO] ---------------------------------
2019-03-19 06:40:05,143 [INFO] Summary:
2019-03-19 06:40:05,143 [INFO] Batch 100000, worst loss 0.060962 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 06:40:05,144 [INFO] Regularization: 2998.279785 * 0.0000010000 = 0.0029982799
2019-03-19 06:40:05,144 [INFO] Sum of grad norms: 0.045290
2019-03-19 06:40:05,145 [INFO] ---------------------------------
2019-03-19 06:40:10,111 [INFO] ---------------------------------
2019-03-19 06:40:10,112 [INFO] Evaluation:
2019-03-19 06:40:10,113 [INFO] Batch 100000, worst loss 0.057794 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:40:10,115 [INFO] ---------------------------------
2019-03-19 06:40:28,406 [INFO] ---------------------------------
2019-03-19 06:40:28,407 [INFO] Summary:
2019-03-19 06:40:28,407 [INFO] Batch 101000, worst loss 0.060902 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 06:40:28,408 [INFO] Regularization: 2997.402588 * 0.0000010000 = 0.0029974026
2019-03-19 06:40:28,409 [INFO] Sum of grad norms: 0.027494
2019-03-19 06:40:28,409 [INFO] ---------------------------------
2019-03-19 06:40:47,095 [INFO] ---------------------------------
2019-03-19 06:40:47,096 [INFO] Summary:
2019-03-19 06:40:47,096 [INFO] Batch 102000, worst loss 0.060892 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 06:40:47,097 [INFO] Regularization: 2996.829834 * 0.0000010000 = 0.0029968298
2019-03-19 06:40:47,097 [INFO] Sum of grad norms: 0.023185
2019-03-19 06:40:47,098 [INFO] ---------------------------------
2019-03-19 06:41:06,093 [INFO] ---------------------------------
2019-03-19 06:41:06,094 [INFO] Summary:
2019-03-19 06:41:06,095 [INFO] Batch 103000, worst loss 0.060817 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 06:41:06,097 [INFO] Regularization: 2996.338867 * 0.0000010000 = 0.0029963388
2019-03-19 06:41:06,098 [INFO] Sum of grad norms: 0.038631
2019-03-19 06:41:06,099 [INFO] ---------------------------------
2019-03-19 06:41:24,836 [INFO] ---------------------------------
2019-03-19 06:41:24,837 [INFO] Summary:
2019-03-19 06:41:24,838 [INFO] Batch 104000, worst loss 0.060820 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 06:41:24,838 [INFO] Regularization: 2995.898682 * 0.0000010000 = 0.0029958987
2019-03-19 06:41:24,839 [INFO] Sum of grad norms: 0.042249
2019-03-19 06:41:24,839 [INFO] ---------------------------------
2019-03-19 06:41:43,732 [INFO] ---------------------------------
2019-03-19 06:41:43,733 [INFO] Summary:
2019-03-19 06:41:43,733 [INFO] Batch 105000, worst loss 0.060806 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 06:41:43,734 [INFO] Regularization: 2995.437012 * 0.0000010000 = 0.0029954370
2019-03-19 06:41:43,735 [INFO] Sum of grad norms: 0.035319
2019-03-19 06:41:43,735 [INFO] ---------------------------------
2019-03-19 06:42:02,594 [INFO] ---------------------------------
2019-03-19 06:42:02,595 [INFO] Summary:
2019-03-19 06:42:02,596 [INFO] Batch 106000, worst loss 0.060805 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 06:42:02,596 [INFO] Regularization: 2995.024902 * 0.0000010000 = 0.0029950249
2019-03-19 06:42:02,597 [INFO] Sum of grad norms: 0.038920
2019-03-19 06:42:02,598 [INFO] ---------------------------------
2019-03-19 06:42:21,482 [INFO] ---------------------------------
2019-03-19 06:42:21,483 [INFO] Summary:
2019-03-19 06:42:21,483 [INFO] Batch 107000, worst loss 0.060848 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 06:42:21,484 [INFO] Regularization: 2994.635010 * 0.0000010000 = 0.0029946349
2019-03-19 06:42:21,485 [INFO] Sum of grad norms: 0.052989
2019-03-19 06:42:21,485 [INFO] ---------------------------------
2019-03-19 06:42:40,133 [INFO] ---------------------------------
2019-03-19 06:42:40,134 [INFO] Summary:
2019-03-19 06:42:40,135 [INFO] Batch 108000, worst loss 0.060782 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 06:42:40,135 [INFO] Regularization: 2994.219727 * 0.0000010000 = 0.0029942198
2019-03-19 06:42:40,136 [INFO] Sum of grad norms: 0.072723
2019-03-19 06:42:40,137 [INFO] ---------------------------------
2019-03-19 06:42:58,807 [INFO] ---------------------------------
2019-03-19 06:42:58,808 [INFO] Summary:
2019-03-19 06:42:58,808 [INFO] Batch 109000, worst loss 0.060871 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 06:42:58,809 [INFO] Regularization: 2993.715576 * 0.0000010000 = 0.0029937155
2019-03-19 06:42:58,809 [INFO] Sum of grad norms: 0.018687
2019-03-19 06:42:58,810 [INFO] ---------------------------------
2019-03-19 06:43:17,659 [INFO] ---------------------------------
2019-03-19 06:43:17,660 [INFO] Summary:
2019-03-19 06:43:17,660 [INFO] Batch 110000, worst loss 0.060750 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 06:43:17,661 [INFO] Regularization: 2993.317871 * 0.0000010000 = 0.0029933178
2019-03-19 06:43:17,661 [INFO] Sum of grad norms: 0.093369
2019-03-19 06:43:17,662 [INFO] ---------------------------------
2019-03-19 06:43:22,564 [INFO] ---------------------------------
2019-03-19 06:43:22,565 [INFO] Evaluation:
2019-03-19 06:43:22,566 [INFO] Batch 110000, worst loss 0.057755 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:43:22,567 [INFO] ---------------------------------
2019-03-19 06:43:41,226 [INFO] ---------------------------------
2019-03-19 06:43:41,227 [INFO] Summary:
2019-03-19 06:43:41,228 [INFO] Batch 111000, worst loss 0.060731 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 06:43:41,228 [INFO] Regularization: 2992.946045 * 0.0000010000 = 0.0029929460
2019-03-19 06:43:41,229 [INFO] Sum of grad norms: 0.037914
2019-03-19 06:43:41,229 [INFO] ---------------------------------
2019-03-19 06:44:00,090 [INFO] ---------------------------------
2019-03-19 06:44:00,090 [INFO] Summary:
2019-03-19 06:44:00,091 [INFO] Batch 112000, worst loss 0.060960 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 06:44:00,091 [INFO] Regularization: 2992.658691 * 0.0000010000 = 0.0029926586
2019-03-19 06:44:00,092 [INFO] Sum of grad norms: 0.024273
2019-03-19 06:44:00,092 [INFO] ---------------------------------
2019-03-19 06:44:19,025 [INFO] ---------------------------------
2019-03-19 06:44:19,026 [INFO] Summary:
2019-03-19 06:44:19,027 [INFO] Batch 113000, worst loss 0.061015 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 06:44:19,027 [INFO] Regularization: 2992.386719 * 0.0000010000 = 0.0029923867
2019-03-19 06:44:19,027 [INFO] Sum of grad norms: 0.029259
2019-03-19 06:44:19,028 [INFO] ---------------------------------
2019-03-19 06:44:37,785 [INFO] ---------------------------------
2019-03-19 06:44:37,786 [INFO] Summary:
2019-03-19 06:44:37,786 [INFO] Batch 114000, worst loss 0.061012 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 06:44:37,787 [INFO] Regularization: 2992.114502 * 0.0000010000 = 0.0029921145
2019-03-19 06:44:37,788 [INFO] Sum of grad norms: 0.034047
2019-03-19 06:44:37,788 [INFO] ---------------------------------
2019-03-19 06:44:56,448 [INFO] ---------------------------------
2019-03-19 06:44:56,449 [INFO] Summary:
2019-03-19 06:44:56,450 [INFO] Batch 115000, worst loss 0.060823 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 06:44:56,450 [INFO] Regularization: 2991.922852 * 0.0000010000 = 0.0029919229
2019-03-19 06:44:56,451 [INFO] Sum of grad norms: 0.050766
2019-03-19 06:44:56,451 [INFO] ---------------------------------
2019-03-19 06:45:15,169 [INFO] ---------------------------------
2019-03-19 06:45:15,169 [INFO] Summary:
2019-03-19 06:45:15,170 [INFO] Batch 116000, worst loss 0.060830 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 06:45:15,171 [INFO] Regularization: 2991.686523 * 0.0000010000 = 0.0029916866
2019-03-19 06:45:15,171 [INFO] Sum of grad norms: 0.034944
2019-03-19 06:45:15,172 [INFO] ---------------------------------
2019-03-19 06:45:33,705 [INFO] ---------------------------------
2019-03-19 06:45:33,706 [INFO] Summary:
2019-03-19 06:45:33,706 [INFO] Batch 117000, worst loss 0.060740 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 06:45:33,707 [INFO] Regularization: 2991.459473 * 0.0000010000 = 0.0029914596
2019-03-19 06:45:33,707 [INFO] Sum of grad norms: 0.044475
2019-03-19 06:45:33,708 [INFO] ---------------------------------
2019-03-19 06:45:52,593 [INFO] ---------------------------------
2019-03-19 06:45:52,594 [INFO] Summary:
2019-03-19 06:45:52,595 [INFO] Batch 118000, worst loss 0.060908 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 06:45:52,595 [INFO] Regularization: 2991.231445 * 0.0000010000 = 0.0029912314
2019-03-19 06:45:52,596 [INFO] Sum of grad norms: 0.096787
2019-03-19 06:45:52,596 [INFO] ---------------------------------
2019-03-19 06:46:11,727 [INFO] ---------------------------------
2019-03-19 06:46:11,728 [INFO] Summary:
2019-03-19 06:46:11,728 [INFO] Batch 119000, worst loss 0.060906 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 06:46:11,729 [INFO] Regularization: 2991.035400 * 0.0000010000 = 0.0029910353
2019-03-19 06:46:11,729 [INFO] Sum of grad norms: 0.080038
2019-03-19 06:46:11,730 [INFO] ---------------------------------
2019-03-19 06:46:30,461 [INFO] ---------------------------------
2019-03-19 06:46:30,462 [INFO] Summary:
2019-03-19 06:46:30,462 [INFO] Batch 120000, worst loss 0.060743 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 06:46:30,463 [INFO] Regularization: 2990.782715 * 0.0000010000 = 0.0029907827
2019-03-19 06:46:30,463 [INFO] Sum of grad norms: 0.027449
2019-03-19 06:46:30,464 [INFO] ---------------------------------
2019-03-19 06:46:35,374 [INFO] ---------------------------------
2019-03-19 06:46:35,375 [INFO] Evaluation:
2019-03-19 06:46:35,376 [INFO] Batch 120000, worst loss 0.057679 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:46:35,377 [INFO] ---------------------------------
2019-03-19 06:46:53,912 [INFO] ---------------------------------
2019-03-19 06:46:53,913 [INFO] Summary:
2019-03-19 06:46:53,914 [INFO] Batch 121000, worst loss 0.060671 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:46:53,915 [INFO] Regularization: 2990.523926 * 0.0000010000 = 0.0029905238
2019-03-19 06:46:53,915 [INFO] Sum of grad norms: 0.036895
2019-03-19 06:46:53,916 [INFO] ---------------------------------
2019-03-19 06:47:12,703 [INFO] ---------------------------------
2019-03-19 06:47:12,704 [INFO] Summary:
2019-03-19 06:47:12,705 [INFO] Batch 122000, worst loss 0.060625 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:47:12,705 [INFO] Regularization: 2990.357178 * 0.0000010000 = 0.0029903571
2019-03-19 06:47:12,706 [INFO] Sum of grad norms: 0.021027
2019-03-19 06:47:12,706 [INFO] ---------------------------------
2019-03-19 06:47:31,295 [INFO] ---------------------------------
2019-03-19 06:47:31,296 [INFO] Summary:
2019-03-19 06:47:31,296 [INFO] Batch 123000, worst loss 0.060717 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:47:31,297 [INFO] Regularization: 2990.233887 * 0.0000010000 = 0.0029902339
2019-03-19 06:47:31,297 [INFO] Sum of grad norms: 0.052638
2019-03-19 06:47:31,298 [INFO] ---------------------------------
2019-03-19 06:47:50,063 [INFO] ---------------------------------
2019-03-19 06:47:50,064 [INFO] Summary:
2019-03-19 06:47:50,065 [INFO] Batch 124000, worst loss 0.060716 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:47:50,065 [INFO] Regularization: 2990.085693 * 0.0000010000 = 0.0029900856
2019-03-19 06:47:50,066 [INFO] Sum of grad norms: 0.030547
2019-03-19 06:47:50,067 [INFO] ---------------------------------
2019-03-19 06:48:08,861 [INFO] ---------------------------------
2019-03-19 06:48:08,861 [INFO] Summary:
2019-03-19 06:48:08,862 [INFO] Batch 125000, worst loss 0.060638 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:48:08,863 [INFO] Regularization: 2989.985107 * 0.0000010000 = 0.0029899850
2019-03-19 06:48:08,863 [INFO] Sum of grad norms: 0.042221
2019-03-19 06:48:08,864 [INFO] ---------------------------------
2019-03-19 06:48:27,659 [INFO] ---------------------------------
2019-03-19 06:48:27,660 [INFO] Summary:
2019-03-19 06:48:27,661 [INFO] Batch 126000, worst loss 0.060799 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:48:27,661 [INFO] Regularization: 2989.875244 * 0.0000010000 = 0.0029898752
2019-03-19 06:48:27,662 [INFO] Sum of grad norms: 0.033758
2019-03-19 06:48:27,662 [INFO] ---------------------------------
2019-03-19 06:48:46,542 [INFO] ---------------------------------
2019-03-19 06:48:46,543 [INFO] Summary:
2019-03-19 06:48:46,544 [INFO] Batch 127000, worst loss 0.060798 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:48:46,544 [INFO] Regularization: 2989.760254 * 0.0000010000 = 0.0029897601
2019-03-19 06:48:46,545 [INFO] Sum of grad norms: 0.023095
2019-03-19 06:48:46,545 [INFO] ---------------------------------
2019-03-19 06:49:05,322 [INFO] ---------------------------------
2019-03-19 06:49:05,323 [INFO] Summary:
2019-03-19 06:49:05,323 [INFO] Batch 128000, worst loss 0.060694 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:49:05,324 [INFO] Regularization: 2989.630859 * 0.0000010000 = 0.0029896309
2019-03-19 06:49:05,324 [INFO] Sum of grad norms: 0.083822
2019-03-19 06:49:05,325 [INFO] ---------------------------------
2019-03-19 06:49:23,787 [INFO] ---------------------------------
2019-03-19 06:49:23,788 [INFO] Summary:
2019-03-19 06:49:23,789 [INFO] Batch 129000, worst loss 0.060756 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:49:23,789 [INFO] Regularization: 2989.491943 * 0.0000010000 = 0.0029894919
2019-03-19 06:49:23,790 [INFO] Sum of grad norms: 0.064236
2019-03-19 06:49:23,790 [INFO] ---------------------------------
2019-03-19 06:49:42,281 [INFO] ---------------------------------
2019-03-19 06:49:42,282 [INFO] Summary:
2019-03-19 06:49:42,282 [INFO] Batch 130000, worst loss 0.060755 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:49:42,283 [INFO] Regularization: 2989.393066 * 0.0000010000 = 0.0029893930
2019-03-19 06:49:42,283 [INFO] Sum of grad norms: 0.055633
2019-03-19 06:49:42,284 [INFO] ---------------------------------
2019-03-19 06:49:47,153 [INFO] ---------------------------------
2019-03-19 06:49:47,153 [INFO] Evaluation:
2019-03-19 06:49:47,155 [INFO] Batch 130000, worst loss 0.057751 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:49:47,156 [INFO] ---------------------------------
2019-03-19 06:50:05,975 [INFO] ---------------------------------
2019-03-19 06:50:05,975 [INFO] Summary:
2019-03-19 06:50:05,976 [INFO] Batch 131000, worst loss 0.060752 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:50:05,976 [INFO] Regularization: 2989.292969 * 0.0000010000 = 0.0029892931
2019-03-19 06:50:05,977 [INFO] Sum of grad norms: 0.026297
2019-03-19 06:50:05,977 [INFO] ---------------------------------
2019-03-19 06:50:24,908 [INFO] ---------------------------------
2019-03-19 06:50:24,909 [INFO] Summary:
2019-03-19 06:50:24,909 [INFO] Batch 132000, worst loss 0.060758 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:50:24,910 [INFO] Regularization: 2989.213135 * 0.0000010000 = 0.0029892132
2019-03-19 06:50:24,910 [INFO] Sum of grad norms: 0.059995
2019-03-19 06:50:24,911 [INFO] ---------------------------------
2019-03-19 06:50:43,676 [INFO] ---------------------------------
2019-03-19 06:50:43,677 [INFO] Summary:
2019-03-19 06:50:43,677 [INFO] Batch 133000, worst loss 0.061028 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:50:43,678 [INFO] Regularization: 2989.163330 * 0.0000010000 = 0.0029891634
2019-03-19 06:50:43,678 [INFO] Sum of grad norms: 0.062658
2019-03-19 06:50:43,679 [INFO] ---------------------------------
2019-03-19 06:51:02,471 [INFO] ---------------------------------
2019-03-19 06:51:02,472 [INFO] Summary:
2019-03-19 06:51:02,473 [INFO] Batch 134000, worst loss 0.061031 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:51:02,473 [INFO] Regularization: 2989.106689 * 0.0000010000 = 0.0029891066
2019-03-19 06:51:02,473 [INFO] Sum of grad norms: 0.090248
2019-03-19 06:51:02,474 [INFO] ---------------------------------
2019-03-19 06:51:21,462 [INFO] ---------------------------------
2019-03-19 06:51:21,463 [INFO] Summary:
2019-03-19 06:51:21,464 [INFO] Batch 135000, worst loss 0.060989 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:51:21,464 [INFO] Regularization: 2989.039062 * 0.0000010000 = 0.0029890391
2019-03-19 06:51:21,465 [INFO] Sum of grad norms: 0.052337
2019-03-19 06:51:21,465 [INFO] ---------------------------------
2019-03-19 06:51:40,191 [INFO] ---------------------------------
2019-03-19 06:51:40,192 [INFO] Summary:
2019-03-19 06:51:40,193 [INFO] Batch 136000, worst loss 0.060989 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:51:40,193 [INFO] Regularization: 2988.971191 * 0.0000010000 = 0.0029889711
2019-03-19 06:51:40,194 [INFO] Sum of grad norms: 0.049287
2019-03-19 06:51:40,194 [INFO] ---------------------------------
2019-03-19 06:51:59,056 [INFO] ---------------------------------
2019-03-19 06:51:59,057 [INFO] Summary:
2019-03-19 06:51:59,057 [INFO] Batch 137000, worst loss 0.060941 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:51:59,058 [INFO] Regularization: 2988.923096 * 0.0000010000 = 0.0029889231
2019-03-19 06:51:59,058 [INFO] Sum of grad norms: 0.023811
2019-03-19 06:51:59,059 [INFO] ---------------------------------
2019-03-19 06:52:17,844 [INFO] ---------------------------------
2019-03-19 06:52:17,845 [INFO] Summary:
2019-03-19 06:52:17,846 [INFO] Batch 138000, worst loss 0.060939 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:52:17,846 [INFO] Regularization: 2988.861328 * 0.0000010000 = 0.0029888614
2019-03-19 06:52:17,847 [INFO] Sum of grad norms: 0.049955
2019-03-19 06:52:17,847 [INFO] ---------------------------------
2019-03-19 06:52:36,738 [INFO] ---------------------------------
2019-03-19 06:52:36,739 [INFO] Summary:
2019-03-19 06:52:36,740 [INFO] Batch 139000, worst loss 0.060888 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:52:36,740 [INFO] Regularization: 2988.800781 * 0.0000010000 = 0.0029888009
2019-03-19 06:52:36,741 [INFO] Sum of grad norms: 0.029675
2019-03-19 06:52:36,741 [INFO] ---------------------------------
2019-03-19 06:52:55,928 [INFO] ---------------------------------
2019-03-19 06:52:55,929 [INFO] Summary:
2019-03-19 06:52:55,930 [INFO] Batch 140000, worst loss 0.060687 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 06:52:55,931 [INFO] Regularization: 2988.742920 * 0.0000010000 = 0.0029887429
2019-03-19 06:52:55,931 [INFO] Sum of grad norms: 0.031572
2019-03-19 06:52:55,932 [INFO] ---------------------------------
2019-03-19 06:53:00,903 [INFO] ---------------------------------
2019-03-19 06:53:00,904 [INFO] Evaluation:
2019-03-19 06:53:00,906 [INFO] Batch 140000, worst loss 0.057717 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:53:00,906 [INFO] ---------------------------------
2019-03-19 06:53:19,655 [INFO] ---------------------------------
2019-03-19 06:53:19,656 [INFO] Summary:
2019-03-19 06:53:19,657 [INFO] Batch 141000, worst loss 0.060799 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:53:19,657 [INFO] Regularization: 2988.692139 * 0.0000010000 = 0.0029886921
2019-03-19 06:53:19,658 [INFO] Sum of grad norms: 0.037917
2019-03-19 06:53:19,659 [INFO] ---------------------------------
2019-03-19 06:53:38,135 [INFO] ---------------------------------
2019-03-19 06:53:38,136 [INFO] Summary:
2019-03-19 06:53:38,136 [INFO] Batch 142000, worst loss 0.060823 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:53:38,137 [INFO] Regularization: 2988.653076 * 0.0000010000 = 0.0029886530
2019-03-19 06:53:38,138 [INFO] Sum of grad norms: 0.019942
2019-03-19 06:53:38,138 [INFO] ---------------------------------
2019-03-19 06:53:56,441 [INFO] ---------------------------------
2019-03-19 06:53:56,442 [INFO] Summary:
2019-03-19 06:53:56,442 [INFO] Batch 143000, worst loss 0.060822 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:53:56,443 [INFO] Regularization: 2988.619873 * 0.0000010000 = 0.0029886200
2019-03-19 06:53:56,443 [INFO] Sum of grad norms: 0.037450
2019-03-19 06:53:56,444 [INFO] ---------------------------------
2019-03-19 06:54:15,448 [INFO] ---------------------------------
2019-03-19 06:54:15,449 [INFO] Summary:
2019-03-19 06:54:15,450 [INFO] Batch 144000, worst loss 0.060846 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:54:15,451 [INFO] Regularization: 2988.584961 * 0.0000010000 = 0.0029885850
2019-03-19 06:54:15,451 [INFO] Sum of grad norms: 0.027065
2019-03-19 06:54:15,452 [INFO] ---------------------------------
2019-03-19 06:54:34,406 [INFO] ---------------------------------
2019-03-19 06:54:34,407 [INFO] Summary:
2019-03-19 06:54:34,407 [INFO] Batch 145000, worst loss 0.060650 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:54:34,408 [INFO] Regularization: 2988.549072 * 0.0000010000 = 0.0029885489
2019-03-19 06:54:34,408 [INFO] Sum of grad norms: 0.030875
2019-03-19 06:54:34,409 [INFO] ---------------------------------
2019-03-19 06:54:53,447 [INFO] ---------------------------------
2019-03-19 06:54:53,448 [INFO] Summary:
2019-03-19 06:54:53,449 [INFO] Batch 146000, worst loss 0.060750 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:54:53,449 [INFO] Regularization: 2988.517090 * 0.0000010000 = 0.0029885171
2019-03-19 06:54:53,450 [INFO] Sum of grad norms: 0.055669
2019-03-19 06:54:53,451 [INFO] ---------------------------------
2019-03-19 06:55:12,437 [INFO] ---------------------------------
2019-03-19 06:55:12,438 [INFO] Summary:
2019-03-19 06:55:12,439 [INFO] Batch 147000, worst loss 0.060733 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:55:12,439 [INFO] Regularization: 2988.484619 * 0.0000010000 = 0.0029884847
2019-03-19 06:55:12,440 [INFO] Sum of grad norms: 0.023173
2019-03-19 06:55:12,441 [INFO] ---------------------------------
2019-03-19 06:55:31,806 [INFO] ---------------------------------
2019-03-19 06:55:31,807 [INFO] Summary:
2019-03-19 06:55:31,808 [INFO] Batch 148000, worst loss 0.060692 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:55:31,808 [INFO] Regularization: 2988.449951 * 0.0000010000 = 0.0029884500
2019-03-19 06:55:31,809 [INFO] Sum of grad norms: 0.037947
2019-03-19 06:55:31,810 [INFO] ---------------------------------
2019-03-19 06:55:50,679 [INFO] ---------------------------------
2019-03-19 06:55:50,680 [INFO] Summary:
2019-03-19 06:55:50,681 [INFO] Batch 149000, worst loss 0.060756 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:55:50,682 [INFO] Regularization: 2988.427002 * 0.0000010000 = 0.0029884269
2019-03-19 06:55:50,683 [INFO] Sum of grad norms: 0.032253
2019-03-19 06:55:50,684 [INFO] ---------------------------------
2019-03-19 06:56:09,553 [INFO] ---------------------------------
2019-03-19 06:56:09,554 [INFO] Summary:
2019-03-19 06:56:09,555 [INFO] Batch 150000, worst loss 0.060756 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:56:09,555 [INFO] Regularization: 2988.398193 * 0.0000010000 = 0.0029883981
2019-03-19 06:56:09,556 [INFO] Sum of grad norms: 0.033505
2019-03-19 06:56:09,556 [INFO] ---------------------------------
2019-03-19 06:56:14,533 [INFO] ---------------------------------
2019-03-19 06:56:14,535 [INFO] Evaluation:
2019-03-19 06:56:14,536 [INFO] Batch 150000, worst loss 0.057817 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:56:14,536 [INFO] ---------------------------------
2019-03-19 06:56:33,283 [INFO] ---------------------------------
2019-03-19 06:56:33,283 [INFO] Summary:
2019-03-19 06:56:33,284 [INFO] Batch 151000, worst loss 0.060806 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:56:33,284 [INFO] Regularization: 2988.376221 * 0.0000010000 = 0.0029883762
2019-03-19 06:56:33,285 [INFO] Sum of grad norms: 0.044024
2019-03-19 06:56:33,286 [INFO] ---------------------------------
2019-03-19 06:56:52,342 [INFO] ---------------------------------
2019-03-19 06:56:52,342 [INFO] Summary:
2019-03-19 06:56:52,343 [INFO] Batch 152000, worst loss 0.060714 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:56:52,343 [INFO] Regularization: 2988.358398 * 0.0000010000 = 0.0029883585
2019-03-19 06:56:52,344 [INFO] Sum of grad norms: 0.113004
2019-03-19 06:56:52,345 [INFO] ---------------------------------
2019-03-19 06:57:10,967 [INFO] ---------------------------------
2019-03-19 06:57:10,968 [INFO] Summary:
2019-03-19 06:57:10,968 [INFO] Batch 153000, worst loss 0.060837 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:57:10,969 [INFO] Regularization: 2988.342285 * 0.0000010000 = 0.0029883422
2019-03-19 06:57:10,969 [INFO] Sum of grad norms: 0.023906
2019-03-19 06:57:10,970 [INFO] ---------------------------------
2019-03-19 06:57:29,883 [INFO] ---------------------------------
2019-03-19 06:57:29,884 [INFO] Summary:
2019-03-19 06:57:29,884 [INFO] Batch 154000, worst loss 0.060837 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:57:29,885 [INFO] Regularization: 2988.328125 * 0.0000010000 = 0.0029883282
2019-03-19 06:57:29,885 [INFO] Sum of grad norms: 0.035802
2019-03-19 06:57:29,886 [INFO] ---------------------------------
2019-03-19 06:57:48,783 [INFO] ---------------------------------
2019-03-19 06:57:48,784 [INFO] Summary:
2019-03-19 06:57:48,785 [INFO] Batch 155000, worst loss 0.060783 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:57:48,785 [INFO] Regularization: 2988.314453 * 0.0000010000 = 0.0029883145
2019-03-19 06:57:48,786 [INFO] Sum of grad norms: 0.023262
2019-03-19 06:57:48,786 [INFO] ---------------------------------
2019-03-19 06:58:07,640 [INFO] ---------------------------------
2019-03-19 06:58:07,641 [INFO] Summary:
2019-03-19 06:58:07,642 [INFO] Batch 156000, worst loss 0.060727 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:58:07,642 [INFO] Regularization: 2988.303955 * 0.0000010000 = 0.0029883040
2019-03-19 06:58:07,643 [INFO] Sum of grad norms: 0.021843
2019-03-19 06:58:07,643 [INFO] ---------------------------------
2019-03-19 06:58:26,435 [INFO] ---------------------------------
2019-03-19 06:58:26,436 [INFO] Summary:
2019-03-19 06:58:26,437 [INFO] Batch 157000, worst loss 0.060692 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:58:26,437 [INFO] Regularization: 2988.293701 * 0.0000010000 = 0.0029882938
2019-03-19 06:58:26,438 [INFO] Sum of grad norms: 0.039180
2019-03-19 06:58:26,438 [INFO] ---------------------------------
2019-03-19 06:58:45,708 [INFO] ---------------------------------
2019-03-19 06:58:45,709 [INFO] Summary:
2019-03-19 06:58:45,710 [INFO] Batch 158000, worst loss 0.060735 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:58:45,710 [INFO] Regularization: 2988.275879 * 0.0000010000 = 0.0029882758
2019-03-19 06:58:45,711 [INFO] Sum of grad norms: 0.022681
2019-03-19 06:58:45,711 [INFO] ---------------------------------
2019-03-19 06:59:04,168 [INFO] ---------------------------------
2019-03-19 06:59:04,169 [INFO] Summary:
2019-03-19 06:59:04,169 [INFO] Batch 159000, worst loss 0.060806 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:59:04,170 [INFO] Regularization: 2988.261230 * 0.0000010000 = 0.0029882612
2019-03-19 06:59:04,171 [INFO] Sum of grad norms: 0.026449
2019-03-19 06:59:04,172 [INFO] ---------------------------------
2019-03-19 06:59:23,071 [INFO] ---------------------------------
2019-03-19 06:59:23,071 [INFO] Summary:
2019-03-19 06:59:23,072 [INFO] Batch 160000, worst loss 0.060806 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:59:23,073 [INFO] Regularization: 2988.248047 * 0.0000010000 = 0.0029882481
2019-03-19 06:59:23,073 [INFO] Sum of grad norms: 0.049443
2019-03-19 06:59:23,074 [INFO] ---------------------------------
2019-03-19 06:59:28,011 [INFO] ---------------------------------
2019-03-19 06:59:28,011 [INFO] Evaluation:
2019-03-19 06:59:28,014 [INFO] Batch 160000, worst loss 0.057725 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 06:59:28,016 [INFO] ---------------------------------
2019-03-19 06:59:46,774 [INFO] ---------------------------------
2019-03-19 06:59:46,775 [INFO] Summary:
2019-03-19 06:59:46,776 [INFO] Batch 161000, worst loss 0.060898 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 06:59:46,777 [INFO] Regularization: 2988.234375 * 0.0000010000 = 0.0029882344
2019-03-19 06:59:46,777 [INFO] Sum of grad norms: 0.026284
2019-03-19 06:59:46,778 [INFO] ---------------------------------
2019-03-19 07:00:05,642 [INFO] ---------------------------------
2019-03-19 07:00:05,643 [INFO] Summary:
2019-03-19 07:00:05,644 [INFO] Batch 162000, worst loss 0.060897 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:00:05,644 [INFO] Regularization: 2988.222168 * 0.0000010000 = 0.0029882221
2019-03-19 07:00:05,645 [INFO] Sum of grad norms: 0.021389
2019-03-19 07:00:05,645 [INFO] ---------------------------------
2019-03-19 07:00:24,220 [INFO] ---------------------------------
2019-03-19 07:00:24,221 [INFO] Summary:
2019-03-19 07:00:24,221 [INFO] Batch 163000, worst loss 0.060935 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:00:24,222 [INFO] Regularization: 2988.213867 * 0.0000010000 = 0.0029882139
2019-03-19 07:00:24,222 [INFO] Sum of grad norms: 0.105280
2019-03-19 07:00:24,223 [INFO] ---------------------------------
2019-03-19 07:00:43,099 [INFO] ---------------------------------
2019-03-19 07:00:43,100 [INFO] Summary:
2019-03-19 07:00:43,101 [INFO] Batch 164000, worst loss 0.060935 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:00:43,102 [INFO] Regularization: 2988.209229 * 0.0000010000 = 0.0029882092
2019-03-19 07:00:43,103 [INFO] Sum of grad norms: 0.030667
2019-03-19 07:00:43,104 [INFO] ---------------------------------
2019-03-19 07:01:01,636 [INFO] ---------------------------------
2019-03-19 07:01:01,637 [INFO] Summary:
2019-03-19 07:01:01,638 [INFO] Batch 165000, worst loss 0.060837 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:01:01,639 [INFO] Regularization: 2988.200195 * 0.0000010000 = 0.0029882002
2019-03-19 07:01:01,639 [INFO] Sum of grad norms: 0.049394
2019-03-19 07:01:01,640 [INFO] ---------------------------------
2019-03-19 07:01:20,193 [INFO] ---------------------------------
2019-03-19 07:01:20,194 [INFO] Summary:
2019-03-19 07:01:20,195 [INFO] Batch 166000, worst loss 0.060837 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:01:20,196 [INFO] Regularization: 2988.193848 * 0.0000010000 = 0.0029881939
2019-03-19 07:01:20,197 [INFO] Sum of grad norms: 0.049250
2019-03-19 07:01:20,198 [INFO] ---------------------------------
2019-03-19 07:01:38,868 [INFO] ---------------------------------
2019-03-19 07:01:38,869 [INFO] Summary:
2019-03-19 07:01:38,870 [INFO] Batch 167000, worst loss 0.060768 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:01:38,871 [INFO] Regularization: 2988.189453 * 0.0000010000 = 0.0029881895
2019-03-19 07:01:38,871 [INFO] Sum of grad norms: 0.051503
2019-03-19 07:01:38,872 [INFO] ---------------------------------
2019-03-19 07:01:57,808 [INFO] ---------------------------------
2019-03-19 07:01:57,809 [INFO] Summary:
2019-03-19 07:01:57,809 [INFO] Batch 168000, worst loss 0.060870 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:01:57,810 [INFO] Regularization: 2988.183105 * 0.0000010000 = 0.0029881832
2019-03-19 07:01:57,810 [INFO] Sum of grad norms: 0.027035
2019-03-19 07:01:57,811 [INFO] ---------------------------------
2019-03-19 07:02:16,874 [INFO] ---------------------------------
2019-03-19 07:02:16,875 [INFO] Summary:
2019-03-19 07:02:16,875 [INFO] Batch 169000, worst loss 0.060870 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:02:16,876 [INFO] Regularization: 2988.175781 * 0.0000010000 = 0.0029881757
2019-03-19 07:02:16,876 [INFO] Sum of grad norms: 0.022972
2019-03-19 07:02:16,877 [INFO] ---------------------------------
2019-03-19 07:02:35,921 [INFO] ---------------------------------
2019-03-19 07:02:35,922 [INFO] Summary:
2019-03-19 07:02:35,923 [INFO] Batch 170000, worst loss 0.060796 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:02:35,923 [INFO] Regularization: 2988.168945 * 0.0000010000 = 0.0029881690
2019-03-19 07:02:35,924 [INFO] Sum of grad norms: 0.038660
2019-03-19 07:02:35,924 [INFO] ---------------------------------
2019-03-19 07:02:40,928 [INFO] ---------------------------------
2019-03-19 07:02:40,928 [INFO] Evaluation:
2019-03-19 07:02:40,929 [INFO] Batch 170000, worst loss 0.057808 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:02:40,930 [INFO] ---------------------------------
2019-03-19 07:02:59,756 [INFO] ---------------------------------
2019-03-19 07:02:59,757 [INFO] Summary:
2019-03-19 07:02:59,758 [INFO] Batch 171000, worst loss 0.060816 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:02:59,759 [INFO] Regularization: 2988.161865 * 0.0000010000 = 0.0029881618
2019-03-19 07:02:59,759 [INFO] Sum of grad norms: 0.062156
2019-03-19 07:02:59,760 [INFO] ---------------------------------
2019-03-19 07:03:18,592 [INFO] ---------------------------------
2019-03-19 07:03:18,593 [INFO] Summary:
2019-03-19 07:03:18,594 [INFO] Batch 172000, worst loss 0.060815 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:03:18,595 [INFO] Regularization: 2988.157715 * 0.0000010000 = 0.0029881578
2019-03-19 07:03:18,595 [INFO] Sum of grad norms: 0.021683
2019-03-19 07:03:18,596 [INFO] ---------------------------------
2019-03-19 07:03:37,613 [INFO] ---------------------------------
2019-03-19 07:03:37,615 [INFO] Summary:
2019-03-19 07:03:37,615 [INFO] Batch 173000, worst loss 0.060785 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:03:37,616 [INFO] Regularization: 2988.154785 * 0.0000010000 = 0.0029881548
2019-03-19 07:03:37,616 [INFO] Sum of grad norms: 0.040447
2019-03-19 07:03:37,617 [INFO] ---------------------------------
2019-03-19 07:03:56,565 [INFO] ---------------------------------
2019-03-19 07:03:56,566 [INFO] Summary:
2019-03-19 07:03:56,566 [INFO] Batch 174000, worst loss 0.060928 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:03:56,567 [INFO] Regularization: 2988.152344 * 0.0000010000 = 0.0029881524
2019-03-19 07:03:56,567 [INFO] Sum of grad norms: 0.029593
2019-03-19 07:03:56,568 [INFO] ---------------------------------
2019-03-19 07:04:15,406 [INFO] ---------------------------------
2019-03-19 07:04:15,407 [INFO] Summary:
2019-03-19 07:04:15,408 [INFO] Batch 175000, worst loss 0.060928 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:04:15,408 [INFO] Regularization: 2988.149170 * 0.0000010000 = 0.0029881492
2019-03-19 07:04:15,408 [INFO] Sum of grad norms: 0.076378
2019-03-19 07:04:15,409 [INFO] ---------------------------------
2019-03-19 07:04:33,872 [INFO] ---------------------------------
2019-03-19 07:04:33,873 [INFO] Summary:
2019-03-19 07:04:33,873 [INFO] Batch 176000, worst loss 0.060790 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:04:33,874 [INFO] Regularization: 2988.146973 * 0.0000010000 = 0.0029881469
2019-03-19 07:04:33,874 [INFO] Sum of grad norms: 0.029528
2019-03-19 07:04:33,875 [INFO] ---------------------------------
2019-03-19 07:04:52,513 [INFO] ---------------------------------
2019-03-19 07:04:52,514 [INFO] Summary:
2019-03-19 07:04:52,515 [INFO] Batch 177000, worst loss 0.060790 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:04:52,515 [INFO] Regularization: 2988.145020 * 0.0000010000 = 0.0029881450
2019-03-19 07:04:52,516 [INFO] Sum of grad norms: 0.102858
2019-03-19 07:04:52,516 [INFO] ---------------------------------
2019-03-19 07:05:10,957 [INFO] ---------------------------------
2019-03-19 07:05:10,958 [INFO] Summary:
2019-03-19 07:05:10,959 [INFO] Batch 178000, worst loss 0.060759 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:05:10,960 [INFO] Regularization: 2988.140381 * 0.0000010000 = 0.0029881403
2019-03-19 07:05:10,960 [INFO] Sum of grad norms: 0.029675
2019-03-19 07:05:10,961 [INFO] ---------------------------------
2019-03-19 07:05:29,564 [INFO] ---------------------------------
2019-03-19 07:05:29,565 [INFO] Summary:
2019-03-19 07:05:29,566 [INFO] Batch 179000, worst loss 0.060760 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:05:29,566 [INFO] Regularization: 2988.138184 * 0.0000010000 = 0.0029881382
2019-03-19 07:05:29,567 [INFO] Sum of grad norms: 0.023497
2019-03-19 07:05:29,568 [INFO] ---------------------------------
2019-03-19 07:05:48,100 [INFO] ---------------------------------
2019-03-19 07:05:48,101 [INFO] Summary:
2019-03-19 07:05:48,102 [INFO] Batch 180000, worst loss 0.060759 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:05:48,102 [INFO] Regularization: 2988.135498 * 0.0000010000 = 0.0029881354
2019-03-19 07:05:48,103 [INFO] Sum of grad norms: 0.075296
2019-03-19 07:05:48,104 [INFO] ---------------------------------
2019-03-19 07:05:52,995 [INFO] ---------------------------------
2019-03-19 07:05:52,996 [INFO] Evaluation:
2019-03-19 07:05:52,998 [INFO] Batch 180000, worst loss 0.057755 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:05:52,999 [INFO] ---------------------------------
2019-03-19 07:06:11,665 [INFO] ---------------------------------
2019-03-19 07:06:11,666 [INFO] Summary:
2019-03-19 07:06:11,667 [INFO] Batch 181000, worst loss 0.060834 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:06:11,667 [INFO] Regularization: 2988.132812 * 0.0000010000 = 0.0029881329
2019-03-19 07:06:11,668 [INFO] Sum of grad norms: 0.034477
2019-03-19 07:06:11,669 [INFO] ---------------------------------
2019-03-19 07:06:30,613 [INFO] ---------------------------------
2019-03-19 07:06:30,614 [INFO] Summary:
2019-03-19 07:06:30,614 [INFO] Batch 182000, worst loss 0.060834 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:06:30,615 [INFO] Regularization: 2988.131836 * 0.0000010000 = 0.0029881317
2019-03-19 07:06:30,615 [INFO] Sum of grad norms: 0.041258
2019-03-19 07:06:30,616 [INFO] ---------------------------------
2019-03-19 07:06:49,211 [INFO] ---------------------------------
2019-03-19 07:06:49,212 [INFO] Summary:
2019-03-19 07:06:49,213 [INFO] Batch 183000, worst loss 0.060715 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:06:49,214 [INFO] Regularization: 2988.130859 * 0.0000010000 = 0.0029881308
2019-03-19 07:06:49,214 [INFO] Sum of grad norms: 0.062143
2019-03-19 07:06:49,215 [INFO] ---------------------------------
2019-03-19 07:07:07,811 [INFO] ---------------------------------
2019-03-19 07:07:07,812 [INFO] Summary:
2019-03-19 07:07:07,813 [INFO] Batch 184000, worst loss 0.060802 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:07:07,814 [INFO] Regularization: 2988.129639 * 0.0000010000 = 0.0029881296
2019-03-19 07:07:07,814 [INFO] Sum of grad norms: 0.043070
2019-03-19 07:07:07,815 [INFO] ---------------------------------
2019-03-19 07:07:26,489 [INFO] ---------------------------------
2019-03-19 07:07:26,490 [INFO] Summary:
2019-03-19 07:07:26,491 [INFO] Batch 185000, worst loss 0.060802 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:07:26,491 [INFO] Regularization: 2988.129395 * 0.0000010000 = 0.0029881294
2019-03-19 07:07:26,492 [INFO] Sum of grad norms: 0.057370
2019-03-19 07:07:26,493 [INFO] ---------------------------------
2019-03-19 07:07:45,230 [INFO] ---------------------------------
2019-03-19 07:07:45,231 [INFO] Summary:
2019-03-19 07:07:45,232 [INFO] Batch 186000, worst loss 0.060823 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:07:45,232 [INFO] Regularization: 2988.128906 * 0.0000010000 = 0.0029881289
2019-03-19 07:07:45,233 [INFO] Sum of grad norms: 0.046247
2019-03-19 07:07:45,233 [INFO] ---------------------------------
2019-03-19 07:08:04,052 [INFO] ---------------------------------
2019-03-19 07:08:04,053 [INFO] Summary:
2019-03-19 07:08:04,053 [INFO] Batch 187000, worst loss 0.060823 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:08:04,054 [INFO] Regularization: 2988.128418 * 0.0000010000 = 0.0029881285
2019-03-19 07:08:04,054 [INFO] Sum of grad norms: 0.025639
2019-03-19 07:08:04,055 [INFO] ---------------------------------
2019-03-19 07:08:22,885 [INFO] ---------------------------------
2019-03-19 07:08:22,886 [INFO] Summary:
2019-03-19 07:08:22,886 [INFO] Batch 188000, worst loss 0.060882 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:08:22,887 [INFO] Regularization: 2988.127686 * 0.0000010000 = 0.0029881278
2019-03-19 07:08:22,887 [INFO] Sum of grad norms: 0.028587
2019-03-19 07:08:22,888 [INFO] ---------------------------------
2019-03-19 07:08:41,229 [INFO] ---------------------------------
2019-03-19 07:08:41,230 [INFO] Summary:
2019-03-19 07:08:41,231 [INFO] Batch 189000, worst loss 0.060882 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:08:41,231 [INFO] Regularization: 2988.126953 * 0.0000010000 = 0.0029881271
2019-03-19 07:08:41,232 [INFO] Sum of grad norms: 0.023440
2019-03-19 07:08:41,232 [INFO] ---------------------------------
2019-03-19 07:08:59,995 [INFO] ---------------------------------
2019-03-19 07:08:59,996 [INFO] Summary:
2019-03-19 07:08:59,997 [INFO] Batch 190000, worst loss 0.060818 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:08:59,997 [INFO] Regularization: 2988.125977 * 0.0000010000 = 0.0029881259
2019-03-19 07:08:59,998 [INFO] Sum of grad norms: 0.029988
2019-03-19 07:08:59,998 [INFO] ---------------------------------
2019-03-19 07:09:04,890 [INFO] ---------------------------------
2019-03-19 07:09:04,891 [INFO] Evaluation:
2019-03-19 07:09:04,892 [INFO] Batch 190000, worst loss 0.057830 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:09:04,894 [INFO] ---------------------------------
2019-03-19 07:09:23,618 [INFO] ---------------------------------
2019-03-19 07:09:23,619 [INFO] Summary:
2019-03-19 07:09:23,619 [INFO] Batch 191000, worst loss 0.060685 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:09:23,620 [INFO] Regularization: 2988.125488 * 0.0000010000 = 0.0029881254
2019-03-19 07:09:23,620 [INFO] Sum of grad norms: 0.028069
2019-03-19 07:09:23,621 [INFO] ---------------------------------
2019-03-19 07:09:42,169 [INFO] ---------------------------------
2019-03-19 07:09:42,170 [INFO] Summary:
2019-03-19 07:09:42,171 [INFO] Batch 192000, worst loss 0.061047 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:09:42,171 [INFO] Regularization: 2988.125000 * 0.0000010000 = 0.0029881250
2019-03-19 07:09:42,172 [INFO] Sum of grad norms: 0.025333
2019-03-19 07:09:42,172 [INFO] ---------------------------------
2019-03-19 07:10:01,203 [INFO] ---------------------------------
2019-03-19 07:10:01,204 [INFO] Summary:
2019-03-19 07:10:01,205 [INFO] Batch 193000, worst loss 0.061047 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:10:01,205 [INFO] Regularization: 2988.124512 * 0.0000010000 = 0.0029881245
2019-03-19 07:10:01,206 [INFO] Sum of grad norms: 0.062100
2019-03-19 07:10:01,206 [INFO] ---------------------------------
2019-03-19 07:10:19,914 [INFO] ---------------------------------
2019-03-19 07:10:19,915 [INFO] Summary:
2019-03-19 07:10:19,916 [INFO] Batch 194000, worst loss 0.060702 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:10:19,916 [INFO] Regularization: 2988.124512 * 0.0000010000 = 0.0029881245
2019-03-19 07:10:19,917 [INFO] Sum of grad norms: 0.027892
2019-03-19 07:10:19,917 [INFO] ---------------------------------
2019-03-19 07:10:38,791 [INFO] ---------------------------------
2019-03-19 07:10:38,792 [INFO] Summary:
2019-03-19 07:10:38,793 [INFO] Batch 195000, worst loss 0.060889 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:10:38,793 [INFO] Regularization: 2988.124023 * 0.0000010000 = 0.0029881240
2019-03-19 07:10:38,794 [INFO] Sum of grad norms: 0.025935
2019-03-19 07:10:38,794 [INFO] ---------------------------------
2019-03-19 07:10:57,394 [INFO] ---------------------------------
2019-03-19 07:10:57,395 [INFO] Summary:
2019-03-19 07:10:57,396 [INFO] Batch 196000, worst loss 0.060889 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:10:57,396 [INFO] Regularization: 2988.123535 * 0.0000010000 = 0.0029881236
2019-03-19 07:10:57,397 [INFO] Sum of grad norms: 0.046037
2019-03-19 07:10:57,397 [INFO] ---------------------------------
2019-03-19 07:11:16,050 [INFO] ---------------------------------
2019-03-19 07:11:16,051 [INFO] Summary:
2019-03-19 07:11:16,051 [INFO] Batch 197000, worst loss 0.060709 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:11:16,052 [INFO] Regularization: 2988.123291 * 0.0000010000 = 0.0029881233
2019-03-19 07:11:16,052 [INFO] Sum of grad norms: 0.055689
2019-03-19 07:11:16,053 [INFO] ---------------------------------
2019-03-19 07:11:34,592 [INFO] ---------------------------------
2019-03-19 07:11:34,593 [INFO] Summary:
2019-03-19 07:11:34,594 [INFO] Batch 198000, worst loss 0.060679 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:11:34,594 [INFO] Regularization: 2988.123047 * 0.0000010000 = 0.0029881231
2019-03-19 07:11:34,595 [INFO] Sum of grad norms: 0.028819
2019-03-19 07:11:34,595 [INFO] ---------------------------------
2019-03-19 07:11:53,583 [INFO] ---------------------------------
2019-03-19 07:11:53,584 [INFO] Summary:
2019-03-19 07:11:53,585 [INFO] Batch 199000, worst loss 0.060875 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:11:53,585 [INFO] Regularization: 2988.123047 * 0.0000010000 = 0.0029881231
2019-03-19 07:11:53,586 [INFO] Sum of grad norms: 0.082429
2019-03-19 07:11:53,586 [INFO] ---------------------------------
2019-03-19 07:12:12,100 [INFO] ---------------------------------
2019-03-19 07:12:12,101 [INFO] Summary:
2019-03-19 07:12:12,101 [INFO] Batch 200000, worst loss 0.060875 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:12:12,102 [INFO] Regularization: 2988.122559 * 0.0000010000 = 0.0029881226
2019-03-19 07:12:12,102 [INFO] Sum of grad norms: 0.033102
2019-03-19 07:12:12,103 [INFO] ---------------------------------
2019-03-19 07:12:17,037 [INFO] ---------------------------------
2019-03-19 07:12:17,038 [INFO] Evaluation:
2019-03-19 07:12:17,038 [INFO] Batch 200000, worst loss 0.057885 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:12:17,040 [INFO] ---------------------------------
2019-03-19 07:12:35,901 [INFO] ---------------------------------
2019-03-19 07:12:35,902 [INFO] Summary:
2019-03-19 07:12:35,903 [INFO] Batch 201000, worst loss 0.060873 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:12:35,903 [INFO] Regularization: 2988.122070 * 0.0000010000 = 0.0029881222
2019-03-19 07:12:35,904 [INFO] Sum of grad norms: 0.035053
2019-03-19 07:12:35,905 [INFO] ---------------------------------
2019-03-19 07:12:54,315 [INFO] ---------------------------------
2019-03-19 07:12:54,317 [INFO] Summary:
2019-03-19 07:12:54,317 [INFO] Batch 202000, worst loss 0.060767 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:12:54,318 [INFO] Regularization: 2988.122070 * 0.0000010000 = 0.0029881222
2019-03-19 07:12:54,318 [INFO] Sum of grad norms: 0.041780
2019-03-19 07:12:54,319 [INFO] ---------------------------------
2019-03-19 07:13:13,509 [INFO] ---------------------------------
2019-03-19 07:13:13,510 [INFO] Summary:
2019-03-19 07:13:13,511 [INFO] Batch 203000, worst loss 0.060856 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:13:13,511 [INFO] Regularization: 2988.121826 * 0.0000010000 = 0.0029881217
2019-03-19 07:13:13,512 [INFO] Sum of grad norms: 0.035292
2019-03-19 07:13:13,513 [INFO] ---------------------------------
2019-03-19 07:13:31,994 [INFO] ---------------------------------
2019-03-19 07:13:31,995 [INFO] Summary:
2019-03-19 07:13:31,996 [INFO] Batch 204000, worst loss 0.060856 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:13:31,996 [INFO] Regularization: 2988.121582 * 0.0000010000 = 0.0029881215
2019-03-19 07:13:31,997 [INFO] Sum of grad norms: 0.093117
2019-03-19 07:13:31,997 [INFO] ---------------------------------
2019-03-19 07:13:50,642 [INFO] ---------------------------------
2019-03-19 07:13:50,643 [INFO] Summary:
2019-03-19 07:13:50,644 [INFO] Batch 205000, worst loss 0.060767 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:13:50,644 [INFO] Regularization: 2988.121582 * 0.0000010000 = 0.0029881215
2019-03-19 07:13:50,645 [INFO] Sum of grad norms: 0.026000
2019-03-19 07:13:50,645 [INFO] ---------------------------------
2019-03-19 07:14:09,368 [INFO] ---------------------------------
2019-03-19 07:14:09,369 [INFO] Summary:
2019-03-19 07:14:09,370 [INFO] Batch 206000, worst loss 0.060661 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:14:09,370 [INFO] Regularization: 2988.121582 * 0.0000010000 = 0.0029881215
2019-03-19 07:14:09,371 [INFO] Sum of grad norms: 0.036504
2019-03-19 07:14:09,371 [INFO] ---------------------------------
2019-03-19 07:14:28,399 [INFO] ---------------------------------
2019-03-19 07:14:28,400 [INFO] Summary:
2019-03-19 07:14:28,401 [INFO] Batch 207000, worst loss 0.060819 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:14:28,401 [INFO] Regularization: 2988.121582 * 0.0000010000 = 0.0029881215
2019-03-19 07:14:28,402 [INFO] Sum of grad norms: 0.034051
2019-03-19 07:14:28,402 [INFO] ---------------------------------
2019-03-19 07:14:47,022 [INFO] ---------------------------------
2019-03-19 07:14:47,023 [INFO] Summary:
2019-03-19 07:14:47,023 [INFO] Batch 208000, worst loss 0.060819 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:14:47,024 [INFO] Regularization: 2988.121582 * 0.0000010000 = 0.0029881215
2019-03-19 07:14:47,024 [INFO] Sum of grad norms: 0.037618
2019-03-19 07:14:47,025 [INFO] ---------------------------------
2019-03-19 07:15:05,697 [INFO] ---------------------------------
2019-03-19 07:15:05,698 [INFO] Summary:
2019-03-19 07:15:05,699 [INFO] Batch 209000, worst loss 0.060854 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:15:05,699 [INFO] Regularization: 2988.121582 * 0.0000010000 = 0.0029881215
2019-03-19 07:15:05,700 [INFO] Sum of grad norms: 0.061328
2019-03-19 07:15:05,700 [INFO] ---------------------------------
2019-03-19 07:15:24,315 [INFO] ---------------------------------
2019-03-19 07:15:24,316 [INFO] Summary:
2019-03-19 07:15:24,317 [INFO] Batch 210000, worst loss 0.060854 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:15:24,317 [INFO] Regularization: 2988.121094 * 0.0000010000 = 0.0029881210
2019-03-19 07:15:24,318 [INFO] Sum of grad norms: 0.028708
2019-03-19 07:15:24,319 [INFO] ---------------------------------
2019-03-19 07:15:29,218 [INFO] ---------------------------------
2019-03-19 07:15:29,218 [INFO] Evaluation:
2019-03-19 07:15:29,219 [INFO] Batch 210000, worst loss 0.057892 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:15:29,220 [INFO] ---------------------------------
2019-03-19 07:15:47,789 [INFO] ---------------------------------
2019-03-19 07:15:47,790 [INFO] Summary:
2019-03-19 07:15:47,791 [INFO] Batch 211000, worst loss 0.061028 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:15:47,791 [INFO] Regularization: 2988.121094 * 0.0000010000 = 0.0029881210
2019-03-19 07:15:47,792 [INFO] Sum of grad norms: 0.058989
2019-03-19 07:15:47,792 [INFO] ---------------------------------
2019-03-19 07:16:06,489 [INFO] ---------------------------------
2019-03-19 07:16:06,490 [INFO] Summary:
2019-03-19 07:16:06,491 [INFO] Batch 212000, worst loss 0.061028 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:16:06,491 [INFO] Regularization: 2988.121094 * 0.0000010000 = 0.0029881210
2019-03-19 07:16:06,492 [INFO] Sum of grad norms: 0.030957
2019-03-19 07:16:06,492 [INFO] ---------------------------------
2019-03-19 07:16:25,552 [INFO] ---------------------------------
2019-03-19 07:16:25,553 [INFO] Summary:
2019-03-19 07:16:25,554 [INFO] Batch 213000, worst loss 0.061028 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:16:25,554 [INFO] Regularization: 2988.121094 * 0.0000010000 = 0.0029881210
2019-03-19 07:16:25,555 [INFO] Sum of grad norms: 0.020336
2019-03-19 07:16:25,555 [INFO] ---------------------------------
2019-03-19 07:16:44,438 [INFO] ---------------------------------
2019-03-19 07:16:44,439 [INFO] Summary:
2019-03-19 07:16:44,440 [INFO] Batch 214000, worst loss 0.060836 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:16:44,440 [INFO] Regularization: 2988.120850 * 0.0000010000 = 0.0029881208
2019-03-19 07:16:44,441 [INFO] Sum of grad norms: 0.043788
2019-03-19 07:16:44,441 [INFO] ---------------------------------
2019-03-19 07:17:03,328 [INFO] ---------------------------------
2019-03-19 07:17:03,329 [INFO] Summary:
2019-03-19 07:17:03,329 [INFO] Batch 215000, worst loss 0.060723 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:17:03,330 [INFO] Regularization: 2988.120850 * 0.0000010000 = 0.0029881208
2019-03-19 07:17:03,330 [INFO] Sum of grad norms: 0.031417
2019-03-19 07:17:03,331 [INFO] ---------------------------------
2019-03-19 07:17:22,469 [INFO] ---------------------------------
2019-03-19 07:17:22,470 [INFO] Summary:
2019-03-19 07:17:22,471 [INFO] Batch 216000, worst loss 0.060723 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:17:22,471 [INFO] Regularization: 2988.120850 * 0.0000010000 = 0.0029881208
2019-03-19 07:17:22,472 [INFO] Sum of grad norms: 0.093496
2019-03-19 07:17:22,472 [INFO] ---------------------------------
2019-03-19 07:17:41,374 [INFO] ---------------------------------
2019-03-19 07:17:41,376 [INFO] Summary:
2019-03-19 07:17:41,376 [INFO] Batch 217000, worst loss 0.060738 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:17:41,377 [INFO] Regularization: 2988.120850 * 0.0000010000 = 0.0029881208
2019-03-19 07:17:41,377 [INFO] Sum of grad norms: 0.014175
2019-03-19 07:17:41,378 [INFO] ---------------------------------
2019-03-19 07:18:00,077 [INFO] ---------------------------------
2019-03-19 07:18:00,078 [INFO] Summary:
2019-03-19 07:18:00,079 [INFO] Batch 218000, worst loss 0.060738 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:18:00,079 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:18:00,080 [INFO] Sum of grad norms: 0.035935
2019-03-19 07:18:00,081 [INFO] ---------------------------------
2019-03-19 07:18:18,743 [INFO] ---------------------------------
2019-03-19 07:18:18,744 [INFO] Summary:
2019-03-19 07:18:18,745 [INFO] Batch 219000, worst loss 0.060914 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:18:18,746 [INFO] Regularization: 2988.120850 * 0.0000010000 = 0.0029881208
2019-03-19 07:18:18,746 [INFO] Sum of grad norms: 0.039215
2019-03-19 07:18:18,747 [INFO] ---------------------------------
2019-03-19 07:18:37,551 [INFO] ---------------------------------
2019-03-19 07:18:37,552 [INFO] Summary:
2019-03-19 07:18:37,553 [INFO] Batch 220000, worst loss 0.061051 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:18:37,554 [INFO] Regularization: 2988.120850 * 0.0000010000 = 0.0029881208
2019-03-19 07:18:37,554 [INFO] Sum of grad norms: 0.033077
2019-03-19 07:18:37,555 [INFO] ---------------------------------
2019-03-19 07:18:42,454 [INFO] ---------------------------------
2019-03-19 07:18:42,455 [INFO] Evaluation:
2019-03-19 07:18:42,456 [INFO] Batch 220000, worst loss 0.058063 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:18:42,459 [INFO] ---------------------------------
2019-03-19 07:19:01,547 [INFO] ---------------------------------
2019-03-19 07:19:01,548 [INFO] Summary:
2019-03-19 07:19:01,549 [INFO] Batch 221000, worst loss 0.060877 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:19:01,549 [INFO] Regularization: 2988.120850 * 0.0000010000 = 0.0029881208
2019-03-19 07:19:01,550 [INFO] Sum of grad norms: 0.035545
2019-03-19 07:19:01,550 [INFO] ---------------------------------
2019-03-19 07:19:20,638 [INFO] ---------------------------------
2019-03-19 07:19:20,639 [INFO] Summary:
2019-03-19 07:19:20,640 [INFO] Batch 222000, worst loss 0.060804 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:19:20,641 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:19:20,642 [INFO] Sum of grad norms: 0.028667
2019-03-19 07:19:20,643 [INFO] ---------------------------------
2019-03-19 07:19:39,694 [INFO] ---------------------------------
2019-03-19 07:19:39,695 [INFO] Summary:
2019-03-19 07:19:39,695 [INFO] Batch 223000, worst loss 0.060803 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:19:39,696 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:19:39,696 [INFO] Sum of grad norms: 0.051109
2019-03-19 07:19:39,697 [INFO] ---------------------------------
2019-03-19 07:19:58,646 [INFO] ---------------------------------
2019-03-19 07:19:58,647 [INFO] Summary:
2019-03-19 07:19:58,647 [INFO] Batch 224000, worst loss 0.060918 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:19:58,648 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:19:58,648 [INFO] Sum of grad norms: 0.029013
2019-03-19 07:19:58,649 [INFO] ---------------------------------
2019-03-19 07:20:17,547 [INFO] ---------------------------------
2019-03-19 07:20:17,548 [INFO] Summary:
2019-03-19 07:20:17,549 [INFO] Batch 225000, worst loss 0.060918 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:20:17,549 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:20:17,550 [INFO] Sum of grad norms: 0.084537
2019-03-19 07:20:17,551 [INFO] ---------------------------------
2019-03-19 07:20:36,534 [INFO] ---------------------------------
2019-03-19 07:20:36,535 [INFO] Summary:
2019-03-19 07:20:36,536 [INFO] Batch 226000, worst loss 0.060791 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:20:36,536 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:20:36,537 [INFO] Sum of grad norms: 0.050023
2019-03-19 07:20:36,538 [INFO] ---------------------------------
2019-03-19 07:20:55,211 [INFO] ---------------------------------
2019-03-19 07:20:55,212 [INFO] Summary:
2019-03-19 07:20:55,213 [INFO] Batch 227000, worst loss 0.060839 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:20:55,213 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:20:55,214 [INFO] Sum of grad norms: 0.036261
2019-03-19 07:20:55,215 [INFO] ---------------------------------
2019-03-19 07:21:13,817 [INFO] ---------------------------------
2019-03-19 07:21:13,818 [INFO] Summary:
2019-03-19 07:21:13,819 [INFO] Batch 228000, worst loss 0.060839 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:21:13,819 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:21:13,820 [INFO] Sum of grad norms: 0.031000
2019-03-19 07:21:13,821 [INFO] ---------------------------------
2019-03-19 07:21:32,295 [INFO] ---------------------------------
2019-03-19 07:21:32,296 [INFO] Summary:
2019-03-19 07:21:32,296 [INFO] Batch 229000, worst loss 0.060839 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:21:32,297 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:21:32,297 [INFO] Sum of grad norms: 0.034051
2019-03-19 07:21:32,298 [INFO] ---------------------------------
2019-03-19 07:21:50,885 [INFO] ---------------------------------
2019-03-19 07:21:50,887 [INFO] Summary:
2019-03-19 07:21:50,887 [INFO] Batch 230000, worst loss 0.060885 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:21:50,888 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:21:50,888 [INFO] Sum of grad norms: 0.022234
2019-03-19 07:21:50,889 [INFO] ---------------------------------
2019-03-19 07:21:55,824 [INFO] ---------------------------------
2019-03-19 07:21:55,825 [INFO] Evaluation:
2019-03-19 07:21:55,825 [INFO] Batch 230000, worst loss 0.057897 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:21:55,827 [INFO] ---------------------------------
2019-03-19 07:22:14,317 [INFO] ---------------------------------
2019-03-19 07:22:14,318 [INFO] Summary:
2019-03-19 07:22:14,318 [INFO] Batch 231000, worst loss 0.060787 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:22:14,319 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:22:14,319 [INFO] Sum of grad norms: 0.027083
2019-03-19 07:22:14,320 [INFO] ---------------------------------
2019-03-19 07:22:33,024 [INFO] ---------------------------------
2019-03-19 07:22:33,025 [INFO] Summary:
2019-03-19 07:22:33,025 [INFO] Batch 232000, worst loss 0.060787 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:22:33,026 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:22:33,026 [INFO] Sum of grad norms: 0.042215
2019-03-19 07:22:33,027 [INFO] ---------------------------------
2019-03-19 07:22:51,806 [INFO] ---------------------------------
2019-03-19 07:22:51,806 [INFO] Summary:
2019-03-19 07:22:51,807 [INFO] Batch 233000, worst loss 0.060750 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:22:51,808 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:22:51,808 [INFO] Sum of grad norms: 0.068896
2019-03-19 07:22:51,809 [INFO] ---------------------------------
2019-03-19 07:23:10,653 [INFO] ---------------------------------
2019-03-19 07:23:10,654 [INFO] Summary:
2019-03-19 07:23:10,655 [INFO] Batch 234000, worst loss 0.060813 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:23:10,655 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:23:10,656 [INFO] Sum of grad norms: 0.046763
2019-03-19 07:23:10,657 [INFO] ---------------------------------
2019-03-19 07:23:29,607 [INFO] ---------------------------------
2019-03-19 07:23:29,608 [INFO] Summary:
2019-03-19 07:23:29,609 [INFO] Batch 235000, worst loss 0.060813 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:23:29,609 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:23:29,610 [INFO] Sum of grad norms: 0.059909
2019-03-19 07:23:29,610 [INFO] ---------------------------------
2019-03-19 07:23:48,293 [INFO] ---------------------------------
2019-03-19 07:23:48,294 [INFO] Summary:
2019-03-19 07:23:48,295 [INFO] Batch 236000, worst loss 0.060783 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:23:48,296 [INFO] Regularization: 2988.120605 * 0.0000010000 = 0.0029881205
2019-03-19 07:23:48,296 [INFO] Sum of grad norms: 0.052959
2019-03-19 07:23:48,297 [INFO] ---------------------------------
2019-03-19 07:24:07,431 [INFO] ---------------------------------
2019-03-19 07:24:07,432 [INFO] Summary:
2019-03-19 07:24:07,433 [INFO] Batch 237000, worst loss 0.061056 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:24:07,433 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:24:07,434 [INFO] Sum of grad norms: 0.036598
2019-03-19 07:24:07,434 [INFO] ---------------------------------
2019-03-19 07:24:25,974 [INFO] ---------------------------------
2019-03-19 07:24:25,975 [INFO] Summary:
2019-03-19 07:24:25,975 [INFO] Batch 238000, worst loss 0.061056 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:24:25,976 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:24:25,977 [INFO] Sum of grad norms: 0.038791
2019-03-19 07:24:25,977 [INFO] ---------------------------------
2019-03-19 07:24:45,105 [INFO] ---------------------------------
2019-03-19 07:24:45,106 [INFO] Summary:
2019-03-19 07:24:45,107 [INFO] Batch 239000, worst loss 0.061056 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:24:45,107 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:24:45,108 [INFO] Sum of grad norms: 0.031995
2019-03-19 07:24:45,109 [INFO] ---------------------------------
2019-03-19 07:25:03,875 [INFO] ---------------------------------
2019-03-19 07:25:03,876 [INFO] Summary:
2019-03-19 07:25:03,877 [INFO] Batch 240000, worst loss 0.060810 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:25:03,877 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:25:03,878 [INFO] Sum of grad norms: 0.019835
2019-03-19 07:25:03,879 [INFO] ---------------------------------
2019-03-19 07:25:08,765 [INFO] ---------------------------------
2019-03-19 07:25:08,766 [INFO] Evaluation:
2019-03-19 07:25:08,767 [INFO] Batch 240000, worst loss 0.057822 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:25:08,771 [INFO] ---------------------------------
2019-03-19 07:25:27,759 [INFO] ---------------------------------
2019-03-19 07:25:27,760 [INFO] Summary:
2019-03-19 07:25:27,760 [INFO] Batch 241000, worst loss 0.060733 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:25:27,761 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:25:27,761 [INFO] Sum of grad norms: 0.025874
2019-03-19 07:25:27,762 [INFO] ---------------------------------
2019-03-19 07:25:46,683 [INFO] ---------------------------------
2019-03-19 07:25:46,684 [INFO] Summary:
2019-03-19 07:25:46,684 [INFO] Batch 242000, worst loss 0.060813 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:25:46,685 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:25:46,685 [INFO] Sum of grad norms: 0.024708
2019-03-19 07:25:46,686 [INFO] ---------------------------------
2019-03-19 07:26:05,442 [INFO] ---------------------------------
2019-03-19 07:26:05,443 [INFO] Summary:
2019-03-19 07:26:05,444 [INFO] Batch 243000, worst loss 0.060813 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:26:05,444 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:26:05,445 [INFO] Sum of grad norms: 0.038356
2019-03-19 07:26:05,445 [INFO] ---------------------------------
2019-03-19 07:26:24,238 [INFO] ---------------------------------
2019-03-19 07:26:24,239 [INFO] Summary:
2019-03-19 07:26:24,240 [INFO] Batch 244000, worst loss 0.060797 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:26:24,240 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:26:24,241 [INFO] Sum of grad norms: 0.033306
2019-03-19 07:26:24,241 [INFO] ---------------------------------
2019-03-19 07:26:42,918 [INFO] ---------------------------------
2019-03-19 07:26:42,919 [INFO] Summary:
2019-03-19 07:26:42,919 [INFO] Batch 245000, worst loss 0.060712 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:26:42,920 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:26:42,920 [INFO] Sum of grad norms: 0.112194
2019-03-19 07:26:42,921 [INFO] ---------------------------------
2019-03-19 07:27:01,518 [INFO] ---------------------------------
2019-03-19 07:27:01,519 [INFO] Summary:
2019-03-19 07:27:01,519 [INFO] Batch 246000, worst loss 0.060763 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:27:01,520 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:27:01,520 [INFO] Sum of grad norms: 0.024373
2019-03-19 07:27:01,521 [INFO] ---------------------------------
2019-03-19 07:27:20,238 [INFO] ---------------------------------
2019-03-19 07:27:20,239 [INFO] Summary:
2019-03-19 07:27:20,240 [INFO] Batch 247000, worst loss 0.060755 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:27:20,240 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:27:20,241 [INFO] Sum of grad norms: 0.028195
2019-03-19 07:27:20,241 [INFO] ---------------------------------
2019-03-19 07:27:39,271 [INFO] ---------------------------------
2019-03-19 07:27:39,272 [INFO] Summary:
2019-03-19 07:27:39,273 [INFO] Batch 248000, worst loss 0.060763 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:27:39,274 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:27:39,274 [INFO] Sum of grad norms: 0.032772
2019-03-19 07:27:39,275 [INFO] ---------------------------------
2019-03-19 07:27:57,967 [INFO] ---------------------------------
2019-03-19 07:27:57,968 [INFO] Summary:
2019-03-19 07:27:57,969 [INFO] Batch 249000, worst loss 0.060786 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:27:57,969 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:27:57,970 [INFO] Sum of grad norms: 0.035566
2019-03-19 07:27:57,971 [INFO] ---------------------------------
2019-03-19 07:28:16,931 [INFO] ---------------------------------
2019-03-19 07:28:16,932 [INFO] Summary:
2019-03-19 07:28:16,933 [INFO] Batch 250000, worst loss 0.060786 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 07:28:16,933 [INFO] Regularization: 2988.120361 * 0.0000010000 = 0.0029881203
2019-03-19 07:28:16,934 [INFO] Sum of grad norms: 0.032227
2019-03-19 07:28:16,935 [INFO] ---------------------------------
2019-03-19 07:28:21,882 [INFO] ---------------------------------
2019-03-19 07:28:21,883 [INFO] Evaluation:
2019-03-19 07:28:21,886 [INFO] Batch 250000, worst loss 0.057798 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:28:21,886 [INFO] ---------------------------------
2019-03-19 07:28:21,887 [INFO] Finished training, saved to file classifier/1552933539/1552976901_8_classifier_final.pth
2019-03-19 07:28:22,051 [INFO] ---------------------------------
2019-03-19 07:28:22,053 [INFO] Training model #9: (1, 64, 201) @ 1
2019-03-19 07:28:37,971 [INFO] ---------------------------------
2019-03-19 07:28:37,972 [INFO] Summary:
2019-03-19 07:28:37,973 [INFO] Batch 1000, worst loss 18.018419 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 07:28:37,973 [INFO] Regularization: 9478.531250 * 0.0000010000 = 0.0094785308
2019-03-19 07:28:37,974 [INFO] Sum of grad norms: 1.074880
2019-03-19 07:28:37,974 [INFO] ---------------------------------
2019-03-19 07:28:53,971 [INFO] ---------------------------------
2019-03-19 07:28:53,972 [INFO] Summary:
2019-03-19 07:28:53,973 [INFO] Batch 2000, worst loss 0.198520 (incl. reg.) of 1000 batches, learning rate 0.005000 @cl.-depth 1
2019-03-19 07:28:53,973 [INFO] Regularization: 8019.241699 * 0.0000010000 = 0.0080192415
2019-03-19 07:28:53,974 [INFO] Sum of grad norms: 3.989281
2019-03-19 07:28:53,975 [INFO] ---------------------------------
2019-03-19 07:29:12,825 [INFO] ---------------------------------
2019-03-19 07:29:12,826 [INFO] Summary:
2019-03-19 07:29:12,827 [INFO] Batch 3000, worst loss 0.091169 (incl. reg.) of 1000 batches, learning rate 0.001985 @cl.-depth 1
2019-03-19 07:29:12,827 [INFO] Regularization: 7065.909180 * 0.0000010000 = 0.0070659090
2019-03-19 07:29:12,828 [INFO] Sum of grad norms: 1.514479
2019-03-19 07:29:12,829 [INFO] ---------------------------------
2019-03-19 07:29:31,740 [INFO] ---------------------------------
2019-03-19 07:29:31,741 [INFO] Summary:
2019-03-19 07:29:31,741 [INFO] Batch 4000, worst loss 0.078917 (incl. reg.) of 1000 batches, learning rate 0.000912 @cl.-depth 1
2019-03-19 07:29:31,742 [INFO] Regularization: 6548.943848 * 0.0000010000 = 0.0065489439
2019-03-19 07:29:31,742 [INFO] Sum of grad norms: 0.518541
2019-03-19 07:29:31,743 [INFO] ---------------------------------
2019-03-19 07:29:50,440 [INFO] ---------------------------------
2019-03-19 07:29:50,441 [INFO] Summary:
2019-03-19 07:29:50,441 [INFO] Batch 5000, worst loss 0.074628 (incl. reg.) of 1000 batches, learning rate 0.000789 @cl.-depth 1
2019-03-19 07:29:50,442 [INFO] Regularization: 6087.366699 * 0.0000010000 = 0.0060873665
2019-03-19 07:29:50,442 [INFO] Sum of grad norms: 0.208822
2019-03-19 07:29:50,443 [INFO] ---------------------------------
2019-03-19 07:30:09,318 [INFO] ---------------------------------
2019-03-19 07:30:09,319 [INFO] Summary:
2019-03-19 07:30:09,319 [INFO] Batch 6000, worst loss 0.073207 (incl. reg.) of 1000 batches, learning rate 0.000746 @cl.-depth 1
2019-03-19 07:30:09,320 [INFO] Regularization: 5777.356934 * 0.0000010000 = 0.0057773571
2019-03-19 07:30:09,321 [INFO] Sum of grad norms: 0.709487
2019-03-19 07:30:09,321 [INFO] ---------------------------------
2019-03-19 07:30:27,961 [INFO] ---------------------------------
2019-03-19 07:30:27,962 [INFO] Summary:
2019-03-19 07:30:27,963 [INFO] Batch 7000, worst loss 0.072980 (incl. reg.) of 1000 batches, learning rate 0.000732 @cl.-depth 1
2019-03-19 07:30:27,963 [INFO] Regularization: 5567.071289 * 0.0000010000 = 0.0055670715
2019-03-19 07:30:27,964 [INFO] Sum of grad norms: 0.912169
2019-03-19 07:30:27,964 [INFO] ---------------------------------
2019-03-19 07:30:47,384 [INFO] ---------------------------------
2019-03-19 07:30:47,385 [INFO] Summary:
2019-03-19 07:30:47,385 [INFO] Batch 8000, worst loss 0.072336 (incl. reg.) of 1000 batches, learning rate 0.000730 @cl.-depth 1
2019-03-19 07:30:47,386 [INFO] Regularization: 5402.735352 * 0.0000010000 = 0.0054027354
2019-03-19 07:30:47,386 [INFO] Sum of grad norms: 0.228396
2019-03-19 07:30:47,387 [INFO] ---------------------------------
2019-03-19 07:31:06,326 [INFO] ---------------------------------
2019-03-19 07:31:06,327 [INFO] Summary:
2019-03-19 07:31:06,328 [INFO] Batch 9000, worst loss 0.070160 (incl. reg.) of 1000 batches, learning rate 0.000723 @cl.-depth 1
2019-03-19 07:31:06,329 [INFO] Regularization: 5254.730469 * 0.0000010000 = 0.0052547306
2019-03-19 07:31:06,329 [INFO] Sum of grad norms: 0.646625
2019-03-19 07:31:06,330 [INFO] ---------------------------------
2019-03-19 07:31:25,201 [INFO] ---------------------------------
2019-03-19 07:31:25,202 [INFO] Summary:
2019-03-19 07:31:25,203 [INFO] Batch 10000, worst loss 0.071302 (incl. reg.) of 1000 batches, learning rate 0.000702 @cl.-depth 1
2019-03-19 07:31:25,204 [INFO] Regularization: 5107.061035 * 0.0000010000 = 0.0051070610
2019-03-19 07:31:25,204 [INFO] Sum of grad norms: 0.694751
2019-03-19 07:31:25,205 [INFO] ---------------------------------
2019-03-19 07:31:30,166 [INFO] ---------------------------------
2019-03-19 07:31:30,167 [INFO] Evaluation:
2019-03-19 07:31:30,168 [INFO] Batch 10000, worst loss 0.062978 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:31:30,169 [INFO] ---------------------------------
2019-03-19 07:31:49,114 [INFO] ---------------------------------
2019-03-19 07:31:49,115 [INFO] Summary:
2019-03-19 07:31:49,116 [INFO] Batch 11000, worst loss 0.068797 (incl. reg.) of 1000 batches, learning rate 0.000702 @cl.-depth 1
2019-03-19 07:31:49,116 [INFO] Regularization: 4972.674805 * 0.0000010000 = 0.0049726749
2019-03-19 07:31:49,117 [INFO] Sum of grad norms: 0.597787
2019-03-19 07:31:49,118 [INFO] ---------------------------------
2019-03-19 07:32:08,256 [INFO] ---------------------------------
2019-03-19 07:32:08,257 [INFO] Summary:
2019-03-19 07:32:08,257 [INFO] Batch 12000, worst loss 0.070251 (incl. reg.) of 1000 batches, learning rate 0.000688 @cl.-depth 1
2019-03-19 07:32:08,258 [INFO] Regularization: 4866.662598 * 0.0000010000 = 0.0048666624
2019-03-19 07:32:08,258 [INFO] Sum of grad norms: 0.255842
2019-03-19 07:32:08,259 [INFO] ---------------------------------
2019-03-19 07:32:27,360 [INFO] ---------------------------------
2019-03-19 07:32:27,361 [INFO] Summary:
2019-03-19 07:32:27,362 [INFO] Batch 13000, worst loss 0.068401 (incl. reg.) of 1000 batches, learning rate 0.000688 @cl.-depth 1
2019-03-19 07:32:27,363 [INFO] Regularization: 4791.011719 * 0.0000010000 = 0.0047910116
2019-03-19 07:32:27,363 [INFO] Sum of grad norms: 0.608361
2019-03-19 07:32:27,364 [INFO] ---------------------------------
2019-03-19 07:32:46,054 [INFO] ---------------------------------
2019-03-19 07:32:46,055 [INFO] Summary:
2019-03-19 07:32:46,056 [INFO] Batch 14000, worst loss 0.068231 (incl. reg.) of 1000 batches, learning rate 0.000684 @cl.-depth 1
2019-03-19 07:32:46,057 [INFO] Regularization: 4709.231934 * 0.0000010000 = 0.0047092321
2019-03-19 07:32:46,057 [INFO] Sum of grad norms: 0.135107
2019-03-19 07:32:46,058 [INFO] ---------------------------------
2019-03-19 07:33:05,191 [INFO] ---------------------------------
2019-03-19 07:33:05,192 [INFO] Summary:
2019-03-19 07:33:05,193 [INFO] Batch 15000, worst loss 0.068487 (incl. reg.) of 1000 batches, learning rate 0.000682 @cl.-depth 1
2019-03-19 07:33:05,194 [INFO] Regularization: 4645.298828 * 0.0000010000 = 0.0046452987
2019-03-19 07:33:05,194 [INFO] Sum of grad norms: 0.541915
2019-03-19 07:33:05,195 [INFO] ---------------------------------
2019-03-19 07:33:24,136 [INFO] ---------------------------------
2019-03-19 07:33:24,137 [INFO] Summary:
2019-03-19 07:33:24,137 [INFO] Batch 16000, worst loss 0.068186 (incl. reg.) of 1000 batches, learning rate 0.000682 @cl.-depth 1
2019-03-19 07:33:24,138 [INFO] Regularization: 4574.739258 * 0.0000010000 = 0.0045747394
2019-03-19 07:33:24,138 [INFO] Sum of grad norms: 0.846721
2019-03-19 07:33:24,139 [INFO] ---------------------------------
2019-03-19 07:33:43,308 [INFO] ---------------------------------
2019-03-19 07:33:43,309 [INFO] Summary:
2019-03-19 07:33:43,310 [INFO] Batch 17000, worst loss 0.066616 (incl. reg.) of 1000 batches, learning rate 0.000682 @cl.-depth 1
2019-03-19 07:33:43,311 [INFO] Regularization: 4505.009277 * 0.0000010000 = 0.0045050094
2019-03-19 07:33:43,311 [INFO] Sum of grad norms: 0.482034
2019-03-19 07:33:43,312 [INFO] ---------------------------------
2019-03-19 07:34:02,259 [INFO] ---------------------------------
2019-03-19 07:34:02,260 [INFO] Summary:
2019-03-19 07:34:02,261 [INFO] Batch 18000, worst loss 0.067418 (incl. reg.) of 1000 batches, learning rate 0.000666 @cl.-depth 1
2019-03-19 07:34:02,261 [INFO] Regularization: 4452.097656 * 0.0000010000 = 0.0044520977
2019-03-19 07:34:02,262 [INFO] Sum of grad norms: 0.170540
2019-03-19 07:34:02,263 [INFO] ---------------------------------
2019-03-19 07:34:20,924 [INFO] ---------------------------------
2019-03-19 07:34:20,925 [INFO] Summary:
2019-03-19 07:34:20,926 [INFO] Batch 19000, worst loss 0.069226 (incl. reg.) of 1000 batches, learning rate 0.000666 @cl.-depth 1
2019-03-19 07:34:20,927 [INFO] Regularization: 4381.735840 * 0.0000010000 = 0.0043817358
2019-03-19 07:34:20,927 [INFO] Sum of grad norms: 0.173931
2019-03-19 07:34:20,928 [INFO] ---------------------------------
2019-03-19 07:34:40,182 [INFO] ---------------------------------
2019-03-19 07:34:40,183 [INFO] Summary:
2019-03-19 07:34:40,183 [INFO] Batch 20000, worst loss 0.068468 (incl. reg.) of 1000 batches, learning rate 0.000666 @cl.-depth 1
2019-03-19 07:34:40,184 [INFO] Regularization: 4321.343750 * 0.0000010000 = 0.0043213437
2019-03-19 07:34:40,184 [INFO] Sum of grad norms: 0.323350
2019-03-19 07:34:40,185 [INFO] ---------------------------------
2019-03-19 07:34:45,058 [INFO] ---------------------------------
2019-03-19 07:34:45,059 [INFO] Evaluation:
2019-03-19 07:34:45,060 [INFO] Batch 20000, worst loss 0.060372 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:34:45,063 [INFO] ---------------------------------
2019-03-19 07:35:04,048 [INFO] ---------------------------------
2019-03-19 07:35:04,049 [INFO] Summary:
2019-03-19 07:35:04,049 [INFO] Batch 21000, worst loss 0.070830 (incl. reg.) of 1000 batches, learning rate 0.000666 @cl.-depth 1
2019-03-19 07:35:04,050 [INFO] Regularization: 4264.528809 * 0.0000010000 = 0.0042645289
2019-03-19 07:35:04,050 [INFO] Sum of grad norms: 0.358758
2019-03-19 07:35:04,051 [INFO] ---------------------------------
2019-03-19 07:35:23,156 [INFO] ---------------------------------
2019-03-19 07:35:23,157 [INFO] Summary:
2019-03-19 07:35:23,158 [INFO] Batch 22000, worst loss 0.068302 (incl. reg.) of 1000 batches, learning rate 0.000666 @cl.-depth 1
2019-03-19 07:35:23,158 [INFO] Regularization: 4208.289551 * 0.0000010000 = 0.0042082896
2019-03-19 07:35:23,159 [INFO] Sum of grad norms: 0.096221
2019-03-19 07:35:23,159 [INFO] ---------------------------------
2019-03-19 07:35:41,749 [INFO] ---------------------------------
2019-03-19 07:35:41,751 [INFO] Summary:
2019-03-19 07:35:41,751 [INFO] Batch 23000, worst loss 0.066856 (incl. reg.) of 1000 batches, learning rate 0.000666 @cl.-depth 1
2019-03-19 07:35:41,752 [INFO] Regularization: 4144.640625 * 0.0000010000 = 0.0041446406
2019-03-19 07:35:41,752 [INFO] Sum of grad norms: 0.332965
2019-03-19 07:35:41,753 [INFO] ---------------------------------
2019-03-19 07:36:00,276 [INFO] ---------------------------------
2019-03-19 07:36:00,277 [INFO] Summary:
2019-03-19 07:36:00,278 [INFO] Batch 24000, worst loss 0.066435 (incl. reg.) of 1000 batches, learning rate 0.000666 @cl.-depth 1
2019-03-19 07:36:00,279 [INFO] Regularization: 4086.304688 * 0.0000010000 = 0.0040863045
2019-03-19 07:36:00,280 [INFO] Sum of grad norms: 0.387317
2019-03-19 07:36:00,281 [INFO] ---------------------------------
2019-03-19 07:36:19,290 [INFO] ---------------------------------
2019-03-19 07:36:19,291 [INFO] Summary:
2019-03-19 07:36:19,292 [INFO] Batch 25000, worst loss 0.065065 (incl. reg.) of 1000 batches, learning rate 0.000664 @cl.-depth 1
2019-03-19 07:36:19,292 [INFO] Regularization: 4042.494141 * 0.0000010000 = 0.0040424941
2019-03-19 07:36:19,293 [INFO] Sum of grad norms: 0.085459
2019-03-19 07:36:19,293 [INFO] ---------------------------------
2019-03-19 07:36:38,133 [INFO] ---------------------------------
2019-03-19 07:36:38,134 [INFO] Summary:
2019-03-19 07:36:38,135 [INFO] Batch 26000, worst loss 0.065340 (incl. reg.) of 1000 batches, learning rate 0.000651 @cl.-depth 1
2019-03-19 07:36:38,136 [INFO] Regularization: 3988.502197 * 0.0000010000 = 0.0039885021
2019-03-19 07:36:38,136 [INFO] Sum of grad norms: 0.474281
2019-03-19 07:36:38,137 [INFO] ---------------------------------
2019-03-19 07:36:56,972 [INFO] ---------------------------------
2019-03-19 07:36:56,973 [INFO] Summary:
2019-03-19 07:36:56,974 [INFO] Batch 27000, worst loss 0.064628 (incl. reg.) of 1000 batches, learning rate 0.000651 @cl.-depth 1
2019-03-19 07:36:56,974 [INFO] Regularization: 3935.068848 * 0.0000010000 = 0.0039350688
2019-03-19 07:36:56,975 [INFO] Sum of grad norms: 0.176870
2019-03-19 07:36:56,976 [INFO] ---------------------------------
2019-03-19 07:37:15,758 [INFO] ---------------------------------
2019-03-19 07:37:15,759 [INFO] Summary:
2019-03-19 07:37:15,759 [INFO] Batch 28000, worst loss 0.064777 (incl. reg.) of 1000 batches, learning rate 0.000646 @cl.-depth 1
2019-03-19 07:37:15,760 [INFO] Regularization: 3890.774902 * 0.0000010000 = 0.0038907749
2019-03-19 07:37:15,760 [INFO] Sum of grad norms: 0.184582
2019-03-19 07:37:15,761 [INFO] ---------------------------------
2019-03-19 07:37:34,724 [INFO] ---------------------------------
2019-03-19 07:37:34,725 [INFO] Summary:
2019-03-19 07:37:34,725 [INFO] Batch 29000, worst loss 0.064188 (incl. reg.) of 1000 batches, learning rate 0.000646 @cl.-depth 1
2019-03-19 07:37:34,726 [INFO] Regularization: 3838.467041 * 0.0000010000 = 0.0038384669
2019-03-19 07:37:34,726 [INFO] Sum of grad norms: 0.943941
2019-03-19 07:37:34,727 [INFO] ---------------------------------
2019-03-19 07:37:53,703 [INFO] ---------------------------------
2019-03-19 07:37:53,704 [INFO] Summary:
2019-03-19 07:37:53,705 [INFO] Batch 30000, worst loss 0.064641 (incl. reg.) of 1000 batches, learning rate 0.000642 @cl.-depth 1
2019-03-19 07:37:53,705 [INFO] Regularization: 3798.419434 * 0.0000010000 = 0.0037984194
2019-03-19 07:37:53,706 [INFO] Sum of grad norms: 0.067767
2019-03-19 07:37:53,706 [INFO] ---------------------------------
2019-03-19 07:37:58,636 [INFO] ---------------------------------
2019-03-19 07:37:58,637 [INFO] Evaluation:
2019-03-19 07:37:58,638 [INFO] Batch 30000, worst loss 0.058933 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:37:58,639 [INFO] ---------------------------------
2019-03-19 07:38:18,100 [INFO] ---------------------------------
2019-03-19 07:38:18,101 [INFO] Summary:
2019-03-19 07:38:18,102 [INFO] Batch 31000, worst loss 0.064134 (incl. reg.) of 1000 batches, learning rate 0.000642 @cl.-depth 1
2019-03-19 07:38:18,103 [INFO] Regularization: 3753.363281 * 0.0000010000 = 0.0037533634
2019-03-19 07:38:18,103 [INFO] Sum of grad norms: 0.598826
2019-03-19 07:38:18,104 [INFO] ---------------------------------
2019-03-19 07:38:37,040 [INFO] ---------------------------------
2019-03-19 07:38:37,041 [INFO] Summary:
2019-03-19 07:38:37,042 [INFO] Batch 32000, worst loss 0.063866 (incl. reg.) of 1000 batches, learning rate 0.000641 @cl.-depth 1
2019-03-19 07:38:37,042 [INFO] Regularization: 3712.655029 * 0.0000010000 = 0.0037126550
2019-03-19 07:38:37,043 [INFO] Sum of grad norms: 0.125707
2019-03-19 07:38:37,044 [INFO] ---------------------------------
2019-03-19 07:38:55,660 [INFO] ---------------------------------
2019-03-19 07:38:55,661 [INFO] Summary:
2019-03-19 07:38:55,662 [INFO] Batch 33000, worst loss 0.063934 (incl. reg.) of 1000 batches, learning rate 0.000639 @cl.-depth 1
2019-03-19 07:38:55,662 [INFO] Regularization: 3668.348633 * 0.0000010000 = 0.0036683485
2019-03-19 07:38:55,663 [INFO] Sum of grad norms: 0.205525
2019-03-19 07:38:55,664 [INFO] ---------------------------------
2019-03-19 07:39:14,331 [INFO] ---------------------------------
2019-03-19 07:39:14,332 [INFO] Summary:
2019-03-19 07:39:14,333 [INFO] Batch 34000, worst loss 0.064072 (incl. reg.) of 1000 batches, learning rate 0.000639 @cl.-depth 1
2019-03-19 07:39:14,334 [INFO] Regularization: 3628.456299 * 0.0000010000 = 0.0036284563
2019-03-19 07:39:14,334 [INFO] Sum of grad norms: 0.042870
2019-03-19 07:39:14,335 [INFO] ---------------------------------
2019-03-19 07:39:33,331 [INFO] ---------------------------------
2019-03-19 07:39:33,332 [INFO] Summary:
2019-03-19 07:39:33,333 [INFO] Batch 35000, worst loss 0.066347 (incl. reg.) of 1000 batches, learning rate 0.000639 @cl.-depth 1
2019-03-19 07:39:33,333 [INFO] Regularization: 3596.177979 * 0.0000010000 = 0.0035961780
2019-03-19 07:39:33,334 [INFO] Sum of grad norms: 0.068413
2019-03-19 07:39:33,334 [INFO] ---------------------------------
2019-03-19 07:39:52,229 [INFO] ---------------------------------
2019-03-19 07:39:52,230 [INFO] Summary:
2019-03-19 07:39:52,231 [INFO] Batch 36000, worst loss 0.063686 (incl. reg.) of 1000 batches, learning rate 0.000639 @cl.-depth 1
2019-03-19 07:39:52,231 [INFO] Regularization: 3553.283691 * 0.0000010000 = 0.0035532836
2019-03-19 07:39:52,232 [INFO] Sum of grad norms: 0.529920
2019-03-19 07:39:52,232 [INFO] ---------------------------------
2019-03-19 07:40:11,546 [INFO] ---------------------------------
2019-03-19 07:40:11,547 [INFO] Summary:
2019-03-19 07:40:11,548 [INFO] Batch 37000, worst loss 0.064851 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-19 07:40:11,549 [INFO] Regularization: 3511.326416 * 0.0000010000 = 0.0035113264
2019-03-19 07:40:11,549 [INFO] Sum of grad norms: 0.179345
2019-03-19 07:40:11,550 [INFO] ---------------------------------
2019-03-19 07:40:30,566 [INFO] ---------------------------------
2019-03-19 07:40:30,566 [INFO] Summary:
2019-03-19 07:40:30,567 [INFO] Batch 38000, worst loss 0.063327 (incl. reg.) of 1000 batches, learning rate 0.000637 @cl.-depth 1
2019-03-19 07:40:30,568 [INFO] Regularization: 3471.467041 * 0.0000010000 = 0.0034714669
2019-03-19 07:40:30,568 [INFO] Sum of grad norms: 0.117615
2019-03-19 07:40:30,569 [INFO] ---------------------------------
2019-03-19 07:40:49,401 [INFO] ---------------------------------
2019-03-19 07:40:49,401 [INFO] Summary:
2019-03-19 07:40:49,402 [INFO] Batch 39000, worst loss 0.063371 (incl. reg.) of 1000 batches, learning rate 0.000633 @cl.-depth 1
2019-03-19 07:40:49,402 [INFO] Regularization: 3436.733154 * 0.0000010000 = 0.0034367330
2019-03-19 07:40:49,403 [INFO] Sum of grad norms: 1.124415
2019-03-19 07:40:49,403 [INFO] ---------------------------------
2019-03-19 07:41:08,382 [INFO] ---------------------------------
2019-03-19 07:41:08,383 [INFO] Summary:
2019-03-19 07:41:08,383 [INFO] Batch 40000, worst loss 0.065615 (incl. reg.) of 1000 batches, learning rate 0.000633 @cl.-depth 1
2019-03-19 07:41:08,384 [INFO] Regularization: 3405.647217 * 0.0000010000 = 0.0034056471
2019-03-19 07:41:08,385 [INFO] Sum of grad norms: 0.555549
2019-03-19 07:41:08,385 [INFO] ---------------------------------
2019-03-19 07:41:13,362 [INFO] ---------------------------------
2019-03-19 07:41:13,363 [INFO] Evaluation:
2019-03-19 07:41:13,364 [INFO] Batch 40000, worst loss 0.058340 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:41:13,365 [INFO] ---------------------------------
2019-03-19 07:41:32,478 [INFO] ---------------------------------
2019-03-19 07:41:32,479 [INFO] Summary:
2019-03-19 07:41:32,480 [INFO] Batch 41000, worst loss 0.062836 (incl. reg.) of 1000 batches, learning rate 0.000317 @cl.-depth 1
2019-03-19 07:41:32,480 [INFO] Regularization: 3369.641846 * 0.0000010000 = 0.0033696417
2019-03-19 07:41:32,480 [INFO] Sum of grad norms: 0.161463
2019-03-19 07:41:32,481 [INFO] ---------------------------------
2019-03-19 07:41:51,342 [INFO] ---------------------------------
2019-03-19 07:41:51,343 [INFO] Summary:
2019-03-19 07:41:51,344 [INFO] Batch 42000, worst loss 0.061553 (incl. reg.) of 1000 batches, learning rate 0.000317 @cl.-depth 1
2019-03-19 07:41:51,344 [INFO] Regularization: 3337.851318 * 0.0000010000 = 0.0033378513
2019-03-19 07:41:51,345 [INFO] Sum of grad norms: 0.267481
2019-03-19 07:41:51,345 [INFO] ---------------------------------
2019-03-19 07:42:09,901 [INFO] ---------------------------------
2019-03-19 07:42:09,901 [INFO] Summary:
2019-03-19 07:42:09,902 [INFO] Batch 43000, worst loss 0.061575 (incl. reg.) of 1000 batches, learning rate 0.000317 @cl.-depth 1
2019-03-19 07:42:09,902 [INFO] Regularization: 3314.296631 * 0.0000010000 = 0.0033142967
2019-03-19 07:42:09,903 [INFO] Sum of grad norms: 0.106658
2019-03-19 07:42:09,904 [INFO] ---------------------------------
2019-03-19 07:42:28,954 [INFO] ---------------------------------
2019-03-19 07:42:28,955 [INFO] Summary:
2019-03-19 07:42:28,956 [INFO] Batch 44000, worst loss 0.061418 (incl. reg.) of 1000 batches, learning rate 0.000317 @cl.-depth 1
2019-03-19 07:42:28,956 [INFO] Regularization: 3293.404541 * 0.0000010000 = 0.0032934046
2019-03-19 07:42:28,957 [INFO] Sum of grad norms: 0.044014
2019-03-19 07:42:28,958 [INFO] ---------------------------------
2019-03-19 07:42:47,770 [INFO] ---------------------------------
2019-03-19 07:42:47,771 [INFO] Summary:
2019-03-19 07:42:47,771 [INFO] Batch 45000, worst loss 0.061508 (incl. reg.) of 1000 batches, learning rate 0.000317 @cl.-depth 1
2019-03-19 07:42:47,772 [INFO] Regularization: 3272.370605 * 0.0000010000 = 0.0032723707
2019-03-19 07:42:47,772 [INFO] Sum of grad norms: 0.513886
2019-03-19 07:42:47,773 [INFO] ---------------------------------
2019-03-19 07:43:06,503 [INFO] ---------------------------------
2019-03-19 07:43:06,504 [INFO] Summary:
2019-03-19 07:43:06,505 [INFO] Batch 46000, worst loss 0.061573 (incl. reg.) of 1000 batches, learning rate 0.000317 @cl.-depth 1
2019-03-19 07:43:06,505 [INFO] Regularization: 3251.654541 * 0.0000010000 = 0.0032516546
2019-03-19 07:43:06,506 [INFO] Sum of grad norms: 0.089085
2019-03-19 07:43:06,506 [INFO] ---------------------------------
2019-03-19 07:43:25,331 [INFO] ---------------------------------
2019-03-19 07:43:25,332 [INFO] Summary:
2019-03-19 07:43:25,333 [INFO] Batch 47000, worst loss 0.061370 (incl. reg.) of 1000 batches, learning rate 0.000317 @cl.-depth 1
2019-03-19 07:43:25,333 [INFO] Regularization: 3229.600342 * 0.0000010000 = 0.0032296004
2019-03-19 07:43:25,334 [INFO] Sum of grad norms: 0.136353
2019-03-19 07:43:25,334 [INFO] ---------------------------------
2019-03-19 07:43:44,068 [INFO] ---------------------------------
2019-03-19 07:43:44,069 [INFO] Summary:
2019-03-19 07:43:44,070 [INFO] Batch 48000, worst loss 0.061459 (incl. reg.) of 1000 batches, learning rate 0.000317 @cl.-depth 1
2019-03-19 07:43:44,071 [INFO] Regularization: 3208.945801 * 0.0000010000 = 0.0032089457
2019-03-19 07:43:44,071 [INFO] Sum of grad norms: 0.108974
2019-03-19 07:43:44,072 [INFO] ---------------------------------
2019-03-19 07:44:02,761 [INFO] ---------------------------------
2019-03-19 07:44:02,762 [INFO] Summary:
2019-03-19 07:44:02,762 [INFO] Batch 49000, worst loss 0.061275 (incl. reg.) of 1000 batches, learning rate 0.000317 @cl.-depth 1
2019-03-19 07:44:02,763 [INFO] Regularization: 3186.114258 * 0.0000010000 = 0.0031861144
2019-03-19 07:44:02,763 [INFO] Sum of grad norms: 0.034500
2019-03-19 07:44:02,764 [INFO] ---------------------------------
2019-03-19 07:44:21,562 [INFO] ---------------------------------
2019-03-19 07:44:21,563 [INFO] Summary:
2019-03-19 07:44:21,563 [INFO] Batch 50000, worst loss 0.061202 (incl. reg.) of 1000 batches, learning rate 0.000317 @cl.-depth 1
2019-03-19 07:44:21,564 [INFO] Regularization: 3166.676758 * 0.0000010000 = 0.0031666767
2019-03-19 07:44:21,564 [INFO] Sum of grad norms: 0.078222
2019-03-19 07:44:21,565 [INFO] ---------------------------------
2019-03-19 07:44:26,523 [INFO] ---------------------------------
2019-03-19 07:44:26,523 [INFO] Evaluation:
2019-03-19 07:44:26,524 [INFO] Batch 50000, worst loss 0.058073 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:44:26,525 [INFO] ---------------------------------
2019-03-19 07:44:45,029 [INFO] ---------------------------------
2019-03-19 07:44:45,030 [INFO] Summary:
2019-03-19 07:44:45,031 [INFO] Batch 51000, worst loss 0.061260 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 07:44:45,031 [INFO] Regularization: 3146.142090 * 0.0000010000 = 0.0031461420
2019-03-19 07:44:45,032 [INFO] Sum of grad norms: 0.229692
2019-03-19 07:44:45,033 [INFO] ---------------------------------
2019-03-19 07:45:03,686 [INFO] ---------------------------------
2019-03-19 07:45:03,687 [INFO] Summary:
2019-03-19 07:45:03,688 [INFO] Batch 52000, worst loss 0.061016 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 07:45:03,688 [INFO] Regularization: 3130.473877 * 0.0000010000 = 0.0031304739
2019-03-19 07:45:03,689 [INFO] Sum of grad norms: 0.038943
2019-03-19 07:45:03,689 [INFO] ---------------------------------
2019-03-19 07:45:22,685 [INFO] ---------------------------------
2019-03-19 07:45:22,686 [INFO] Summary:
2019-03-19 07:45:22,686 [INFO] Batch 53000, worst loss 0.061022 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 07:45:22,687 [INFO] Regularization: 3118.627197 * 0.0000010000 = 0.0031186272
2019-03-19 07:45:22,687 [INFO] Sum of grad norms: 0.145857
2019-03-19 07:45:22,688 [INFO] ---------------------------------
2019-03-19 07:45:41,594 [INFO] ---------------------------------
2019-03-19 07:45:41,596 [INFO] Summary:
2019-03-19 07:45:41,596 [INFO] Batch 54000, worst loss 0.061088 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 07:45:41,597 [INFO] Regularization: 3107.921387 * 0.0000010000 = 0.0031079215
2019-03-19 07:45:41,598 [INFO] Sum of grad norms: 0.037161
2019-03-19 07:45:41,599 [INFO] ---------------------------------
2019-03-19 07:46:00,290 [INFO] ---------------------------------
2019-03-19 07:46:00,292 [INFO] Summary:
2019-03-19 07:46:00,292 [INFO] Batch 55000, worst loss 0.060872 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 07:46:00,293 [INFO] Regularization: 3097.034424 * 0.0000010000 = 0.0030970345
2019-03-19 07:46:00,293 [INFO] Sum of grad norms: 0.092059
2019-03-19 07:46:00,294 [INFO] ---------------------------------
2019-03-19 07:46:18,957 [INFO] ---------------------------------
2019-03-19 07:46:18,958 [INFO] Summary:
2019-03-19 07:46:18,958 [INFO] Batch 56000, worst loss 0.060891 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 07:46:18,959 [INFO] Regularization: 3085.291016 * 0.0000010000 = 0.0030852910
2019-03-19 07:46:18,960 [INFO] Sum of grad norms: 0.311752
2019-03-19 07:46:18,960 [INFO] ---------------------------------
2019-03-19 07:46:37,671 [INFO] ---------------------------------
2019-03-19 07:46:37,672 [INFO] Summary:
2019-03-19 07:46:37,673 [INFO] Batch 57000, worst loss 0.061007 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 07:46:37,673 [INFO] Regularization: 3074.820312 * 0.0000010000 = 0.0030748204
2019-03-19 07:46:37,674 [INFO] Sum of grad norms: 0.031363
2019-03-19 07:46:37,674 [INFO] ---------------------------------
2019-03-19 07:46:56,491 [INFO] ---------------------------------
2019-03-19 07:46:56,492 [INFO] Summary:
2019-03-19 07:46:56,492 [INFO] Batch 58000, worst loss 0.060980 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 07:46:56,493 [INFO] Regularization: 3063.450439 * 0.0000010000 = 0.0030634503
2019-03-19 07:46:56,493 [INFO] Sum of grad norms: 0.074602
2019-03-19 07:46:56,494 [INFO] ---------------------------------
2019-03-19 07:47:15,394 [INFO] ---------------------------------
2019-03-19 07:47:15,395 [INFO] Summary:
2019-03-19 07:47:15,396 [INFO] Batch 59000, worst loss 0.060838 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 07:47:15,396 [INFO] Regularization: 3052.175537 * 0.0000010000 = 0.0030521755
2019-03-19 07:47:15,397 [INFO] Sum of grad norms: 0.096375
2019-03-19 07:47:15,398 [INFO] ---------------------------------
2019-03-19 07:47:34,418 [INFO] ---------------------------------
2019-03-19 07:47:34,419 [INFO] Summary:
2019-03-19 07:47:34,419 [INFO] Batch 60000, worst loss 0.061194 (incl. reg.) of 1000 batches, learning rate 0.000158 @cl.-depth 1
2019-03-19 07:47:34,420 [INFO] Regularization: 3041.426514 * 0.0000010000 = 0.0030414264
2019-03-19 07:47:34,421 [INFO] Sum of grad norms: 0.209820
2019-03-19 07:47:34,421 [INFO] ---------------------------------
2019-03-19 07:47:39,339 [INFO] ---------------------------------
2019-03-19 07:47:39,341 [INFO] Evaluation:
2019-03-19 07:47:39,342 [INFO] Batch 60000, worst loss 0.057808 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:47:39,343 [INFO] ---------------------------------
2019-03-19 07:47:57,892 [INFO] ---------------------------------
2019-03-19 07:47:57,893 [INFO] Summary:
2019-03-19 07:47:57,893 [INFO] Batch 61000, worst loss 0.060897 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 07:47:57,894 [INFO] Regularization: 3030.337158 * 0.0000010000 = 0.0030303372
2019-03-19 07:47:57,894 [INFO] Sum of grad norms: 0.105222
2019-03-19 07:47:57,895 [INFO] ---------------------------------
2019-03-19 07:48:16,804 [INFO] ---------------------------------
2019-03-19 07:48:16,805 [INFO] Summary:
2019-03-19 07:48:16,805 [INFO] Batch 62000, worst loss 0.060858 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 07:48:16,806 [INFO] Regularization: 3023.377930 * 0.0000010000 = 0.0030233779
2019-03-19 07:48:16,807 [INFO] Sum of grad norms: 0.037613
2019-03-19 07:48:16,807 [INFO] ---------------------------------
2019-03-19 07:48:35,452 [INFO] ---------------------------------
2019-03-19 07:48:35,453 [INFO] Summary:
2019-03-19 07:48:35,454 [INFO] Batch 63000, worst loss 0.060816 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 07:48:35,454 [INFO] Regularization: 3017.727051 * 0.0000010000 = 0.0030177271
2019-03-19 07:48:35,455 [INFO] Sum of grad norms: 0.035108
2019-03-19 07:48:35,456 [INFO] ---------------------------------
2019-03-19 07:48:54,289 [INFO] ---------------------------------
2019-03-19 07:48:54,290 [INFO] Summary:
2019-03-19 07:48:54,290 [INFO] Batch 64000, worst loss 0.060738 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 07:48:54,291 [INFO] Regularization: 3011.692383 * 0.0000010000 = 0.0030116923
2019-03-19 07:48:54,291 [INFO] Sum of grad norms: 0.115445
2019-03-19 07:48:54,292 [INFO] ---------------------------------
2019-03-19 07:49:13,034 [INFO] ---------------------------------
2019-03-19 07:49:13,035 [INFO] Summary:
2019-03-19 07:49:13,036 [INFO] Batch 65000, worst loss 0.060787 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 07:49:13,036 [INFO] Regularization: 3005.888428 * 0.0000010000 = 0.0030058883
2019-03-19 07:49:13,037 [INFO] Sum of grad norms: 0.031608
2019-03-19 07:49:13,038 [INFO] ---------------------------------
2019-03-19 07:49:32,103 [INFO] ---------------------------------
2019-03-19 07:49:32,104 [INFO] Summary:
2019-03-19 07:49:32,104 [INFO] Batch 66000, worst loss 0.060798 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 07:49:32,105 [INFO] Regularization: 3000.468018 * 0.0000010000 = 0.0030004680
2019-03-19 07:49:32,105 [INFO] Sum of grad norms: 0.119394
2019-03-19 07:49:32,106 [INFO] ---------------------------------
2019-03-19 07:49:51,048 [INFO] ---------------------------------
2019-03-19 07:49:51,049 [INFO] Summary:
2019-03-19 07:49:51,050 [INFO] Batch 67000, worst loss 0.060682 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 07:49:51,050 [INFO] Regularization: 2994.812988 * 0.0000010000 = 0.0029948130
2019-03-19 07:49:51,051 [INFO] Sum of grad norms: 0.072518
2019-03-19 07:49:51,052 [INFO] ---------------------------------
2019-03-19 07:50:09,853 [INFO] ---------------------------------
2019-03-19 07:50:09,854 [INFO] Summary:
2019-03-19 07:50:09,855 [INFO] Batch 68000, worst loss 0.060771 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 07:50:09,855 [INFO] Regularization: 2988.664307 * 0.0000010000 = 0.0029886642
2019-03-19 07:50:09,856 [INFO] Sum of grad norms: 0.070221
2019-03-19 07:50:09,857 [INFO] ---------------------------------
2019-03-19 07:50:28,626 [INFO] ---------------------------------
2019-03-19 07:50:28,627 [INFO] Summary:
2019-03-19 07:50:28,628 [INFO] Batch 69000, worst loss 0.060823 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 07:50:28,628 [INFO] Regularization: 2982.839355 * 0.0000010000 = 0.0029828392
2019-03-19 07:50:28,629 [INFO] Sum of grad norms: 0.104980
2019-03-19 07:50:28,629 [INFO] ---------------------------------
2019-03-19 07:50:47,076 [INFO] ---------------------------------
2019-03-19 07:50:47,077 [INFO] Summary:
2019-03-19 07:50:47,078 [INFO] Batch 70000, worst loss 0.060691 (incl. reg.) of 1000 batches, learning rate 0.000079 @cl.-depth 1
2019-03-19 07:50:47,078 [INFO] Regularization: 2977.297852 * 0.0000010000 = 0.0029772979
2019-03-19 07:50:47,079 [INFO] Sum of grad norms: 0.120046
2019-03-19 07:50:47,080 [INFO] ---------------------------------
2019-03-19 07:50:52,055 [INFO] ---------------------------------
2019-03-19 07:50:52,056 [INFO] Evaluation:
2019-03-19 07:50:52,057 [INFO] Batch 70000, worst loss 0.057792 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:50:52,058 [INFO] ---------------------------------
2019-03-19 07:51:10,877 [INFO] ---------------------------------
2019-03-19 07:51:10,878 [INFO] Summary:
2019-03-19 07:51:10,879 [INFO] Batch 71000, worst loss 0.060643 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 07:51:10,879 [INFO] Regularization: 2971.032471 * 0.0000010000 = 0.0029710324
2019-03-19 07:51:10,880 [INFO] Sum of grad norms: 0.034358
2019-03-19 07:51:10,880 [INFO] ---------------------------------
2019-03-19 07:51:29,668 [INFO] ---------------------------------
2019-03-19 07:51:29,669 [INFO] Summary:
2019-03-19 07:51:29,670 [INFO] Batch 72000, worst loss 0.060682 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 07:51:29,670 [INFO] Regularization: 2967.067871 * 0.0000010000 = 0.0029670678
2019-03-19 07:51:29,671 [INFO] Sum of grad norms: 0.040585
2019-03-19 07:51:29,672 [INFO] ---------------------------------
2019-03-19 07:51:48,767 [INFO] ---------------------------------
2019-03-19 07:51:48,768 [INFO] Summary:
2019-03-19 07:51:48,769 [INFO] Batch 73000, worst loss 0.060595 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 07:51:48,769 [INFO] Regularization: 2964.092773 * 0.0000010000 = 0.0029640927
2019-03-19 07:51:48,771 [INFO] Sum of grad norms: 0.028037
2019-03-19 07:51:48,772 [INFO] ---------------------------------
2019-03-19 07:52:07,551 [INFO] ---------------------------------
2019-03-19 07:52:07,552 [INFO] Summary:
2019-03-19 07:52:07,553 [INFO] Batch 74000, worst loss 0.060839 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 07:52:07,553 [INFO] Regularization: 2960.889404 * 0.0000010000 = 0.0029608894
2019-03-19 07:52:07,554 [INFO] Sum of grad norms: 0.074314
2019-03-19 07:52:07,554 [INFO] ---------------------------------
2019-03-19 07:52:26,703 [INFO] ---------------------------------
2019-03-19 07:52:26,704 [INFO] Summary:
2019-03-19 07:52:26,705 [INFO] Batch 75000, worst loss 0.060832 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 07:52:26,705 [INFO] Regularization: 2957.184814 * 0.0000010000 = 0.0029571848
2019-03-19 07:52:26,706 [INFO] Sum of grad norms: 0.024914
2019-03-19 07:52:26,706 [INFO] ---------------------------------
2019-03-19 07:52:45,534 [INFO] ---------------------------------
2019-03-19 07:52:45,535 [INFO] Summary:
2019-03-19 07:52:45,536 [INFO] Batch 76000, worst loss 0.060667 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 07:52:45,536 [INFO] Regularization: 2954.257568 * 0.0000010000 = 0.0029542577
2019-03-19 07:52:45,537 [INFO] Sum of grad norms: 0.026245
2019-03-19 07:52:45,537 [INFO] ---------------------------------
2019-03-19 07:53:04,463 [INFO] ---------------------------------
2019-03-19 07:53:04,463 [INFO] Summary:
2019-03-19 07:53:04,464 [INFO] Batch 77000, worst loss 0.060782 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 07:53:04,465 [INFO] Regularization: 2950.778076 * 0.0000010000 = 0.0029507780
2019-03-19 07:53:04,465 [INFO] Sum of grad norms: 0.040304
2019-03-19 07:53:04,466 [INFO] ---------------------------------
2019-03-19 07:53:23,424 [INFO] ---------------------------------
2019-03-19 07:53:23,425 [INFO] Summary:
2019-03-19 07:53:23,425 [INFO] Batch 78000, worst loss 0.060738 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 07:53:23,426 [INFO] Regularization: 2947.630127 * 0.0000010000 = 0.0029476301
2019-03-19 07:53:23,427 [INFO] Sum of grad norms: 0.026331
2019-03-19 07:53:23,427 [INFO] ---------------------------------
2019-03-19 07:53:41,932 [INFO] ---------------------------------
2019-03-19 07:53:41,933 [INFO] Summary:
2019-03-19 07:53:41,933 [INFO] Batch 79000, worst loss 0.060738 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 07:53:41,934 [INFO] Regularization: 2944.404297 * 0.0000010000 = 0.0029444043
2019-03-19 07:53:41,935 [INFO] Sum of grad norms: 0.067792
2019-03-19 07:53:41,935 [INFO] ---------------------------------
2019-03-19 07:54:00,501 [INFO] ---------------------------------
2019-03-19 07:54:00,501 [INFO] Summary:
2019-03-19 07:54:00,502 [INFO] Batch 80000, worst loss 0.060646 (incl. reg.) of 1000 batches, learning rate 0.000040 @cl.-depth 1
2019-03-19 07:54:00,502 [INFO] Regularization: 2941.136719 * 0.0000010000 = 0.0029411367
2019-03-19 07:54:00,503 [INFO] Sum of grad norms: 0.034399
2019-03-19 07:54:00,503 [INFO] ---------------------------------
2019-03-19 07:54:05,422 [INFO] ---------------------------------
2019-03-19 07:54:05,423 [INFO] Evaluation:
2019-03-19 07:54:05,423 [INFO] Batch 80000, worst loss 0.057600 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:54:05,424 [INFO] ---------------------------------
2019-03-19 07:54:24,018 [INFO] ---------------------------------
2019-03-19 07:54:24,019 [INFO] Summary:
2019-03-19 07:54:24,020 [INFO] Batch 81000, worst loss 0.060417 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 07:54:24,020 [INFO] Regularization: 2938.087646 * 0.0000010000 = 0.0029380876
2019-03-19 07:54:24,021 [INFO] Sum of grad norms: 0.038695
2019-03-19 07:54:24,022 [INFO] ---------------------------------
2019-03-19 07:54:42,802 [INFO] ---------------------------------
2019-03-19 07:54:42,803 [INFO] Summary:
2019-03-19 07:54:42,804 [INFO] Batch 82000, worst loss 0.060794 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 07:54:42,804 [INFO] Regularization: 2936.077393 * 0.0000010000 = 0.0029360773
2019-03-19 07:54:42,805 [INFO] Sum of grad norms: 0.028056
2019-03-19 07:54:42,806 [INFO] ---------------------------------
2019-03-19 07:55:01,224 [INFO] ---------------------------------
2019-03-19 07:55:01,225 [INFO] Summary:
2019-03-19 07:55:01,225 [INFO] Batch 83000, worst loss 0.060525 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 07:55:01,226 [INFO] Regularization: 2934.525146 * 0.0000010000 = 0.0029345253
2019-03-19 07:55:01,226 [INFO] Sum of grad norms: 0.015650
2019-03-19 07:55:01,227 [INFO] ---------------------------------
2019-03-19 07:55:19,763 [INFO] ---------------------------------
2019-03-19 07:55:19,764 [INFO] Summary:
2019-03-19 07:55:19,764 [INFO] Batch 84000, worst loss 0.060671 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 07:55:19,765 [INFO] Regularization: 2932.840088 * 0.0000010000 = 0.0029328400
2019-03-19 07:55:19,765 [INFO] Sum of grad norms: 0.017930
2019-03-19 07:55:19,766 [INFO] ---------------------------------
2019-03-19 07:55:38,005 [INFO] ---------------------------------
2019-03-19 07:55:38,006 [INFO] Summary:
2019-03-19 07:55:38,006 [INFO] Batch 85000, worst loss 0.060583 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 07:55:38,007 [INFO] Regularization: 2931.187744 * 0.0000010000 = 0.0029311876
2019-03-19 07:55:38,007 [INFO] Sum of grad norms: 0.042755
2019-03-19 07:55:38,008 [INFO] ---------------------------------
2019-03-19 07:55:56,954 [INFO] ---------------------------------
2019-03-19 07:55:56,955 [INFO] Summary:
2019-03-19 07:55:56,956 [INFO] Batch 86000, worst loss 0.060589 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 07:55:56,956 [INFO] Regularization: 2929.429688 * 0.0000010000 = 0.0029294298
2019-03-19 07:55:56,957 [INFO] Sum of grad norms: 0.111727
2019-03-19 07:55:56,957 [INFO] ---------------------------------
2019-03-19 07:56:15,719 [INFO] ---------------------------------
2019-03-19 07:56:15,720 [INFO] Summary:
2019-03-19 07:56:15,720 [INFO] Batch 87000, worst loss 0.060527 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 07:56:15,721 [INFO] Regularization: 2927.888184 * 0.0000010000 = 0.0029278882
2019-03-19 07:56:15,721 [INFO] Sum of grad norms: 0.100179
2019-03-19 07:56:15,722 [INFO] ---------------------------------
2019-03-19 07:56:34,600 [INFO] ---------------------------------
2019-03-19 07:56:34,601 [INFO] Summary:
2019-03-19 07:56:34,602 [INFO] Batch 88000, worst loss 0.060512 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 07:56:34,603 [INFO] Regularization: 2926.127686 * 0.0000010000 = 0.0029261278
2019-03-19 07:56:34,604 [INFO] Sum of grad norms: 0.028432
2019-03-19 07:56:34,604 [INFO] ---------------------------------
2019-03-19 07:56:53,318 [INFO] ---------------------------------
2019-03-19 07:56:53,319 [INFO] Summary:
2019-03-19 07:56:53,319 [INFO] Batch 89000, worst loss 0.060534 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 07:56:53,320 [INFO] Regularization: 2924.478027 * 0.0000010000 = 0.0029244779
2019-03-19 07:56:53,320 [INFO] Sum of grad norms: 0.071599
2019-03-19 07:56:53,321 [INFO] ---------------------------------
2019-03-19 07:57:12,028 [INFO] ---------------------------------
2019-03-19 07:57:12,029 [INFO] Summary:
2019-03-19 07:57:12,030 [INFO] Batch 90000, worst loss 0.060526 (incl. reg.) of 1000 batches, learning rate 0.000020 @cl.-depth 1
2019-03-19 07:57:12,030 [INFO] Regularization: 2922.903076 * 0.0000010000 = 0.0029229030
2019-03-19 07:57:12,031 [INFO] Sum of grad norms: 0.137781
2019-03-19 07:57:12,031 [INFO] ---------------------------------
2019-03-19 07:57:16,935 [INFO] ---------------------------------
2019-03-19 07:57:16,936 [INFO] Evaluation:
2019-03-19 07:57:16,937 [INFO] Batch 90000, worst loss 0.057689 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 07:57:16,938 [INFO] ---------------------------------
2019-03-19 07:57:35,550 [INFO] ---------------------------------
2019-03-19 07:57:35,551 [INFO] Summary:
2019-03-19 07:57:35,551 [INFO] Batch 91000, worst loss 0.060607 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 07:57:35,552 [INFO] Regularization: 2921.223633 * 0.0000010000 = 0.0029212236
2019-03-19 07:57:35,552 [INFO] Sum of grad norms: 0.039799
2019-03-19 07:57:35,553 [INFO] ---------------------------------
2019-03-19 07:57:54,383 [INFO] ---------------------------------
2019-03-19 07:57:54,384 [INFO] Summary:
2019-03-19 07:57:54,384 [INFO] Batch 92000, worst loss 0.060579 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 07:57:54,385 [INFO] Regularization: 2920.208496 * 0.0000010000 = 0.0029202085
2019-03-19 07:57:54,385 [INFO] Sum of grad norms: 0.099157
2019-03-19 07:57:54,386 [INFO] ---------------------------------
2019-03-19 07:58:13,148 [INFO] ---------------------------------
2019-03-19 07:58:13,149 [INFO] Summary:
2019-03-19 07:58:13,150 [INFO] Batch 93000, worst loss 0.060446 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 07:58:13,151 [INFO] Regularization: 2919.341309 * 0.0000010000 = 0.0029193412
2019-03-19 07:58:13,151 [INFO] Sum of grad norms: 0.060125
2019-03-19 07:58:13,152 [INFO] ---------------------------------
2019-03-19 07:58:32,049 [INFO] ---------------------------------
2019-03-19 07:58:32,050 [INFO] Summary:
2019-03-19 07:58:32,051 [INFO] Batch 94000, worst loss 0.060711 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 07:58:32,051 [INFO] Regularization: 2918.594482 * 0.0000010000 = 0.0029185945
2019-03-19 07:58:32,052 [INFO] Sum of grad norms: 0.027454
2019-03-19 07:58:32,052 [INFO] ---------------------------------
2019-03-19 07:58:50,942 [INFO] ---------------------------------
2019-03-19 07:58:50,943 [INFO] Summary:
2019-03-19 07:58:50,943 [INFO] Batch 95000, worst loss 0.060529 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 07:58:50,944 [INFO] Regularization: 2917.631592 * 0.0000010000 = 0.0029176315
2019-03-19 07:58:50,944 [INFO] Sum of grad norms: 0.019779
2019-03-19 07:58:50,945 [INFO] ---------------------------------
2019-03-19 07:59:09,792 [INFO] ---------------------------------
2019-03-19 07:59:09,793 [INFO] Summary:
2019-03-19 07:59:09,794 [INFO] Batch 96000, worst loss 0.060504 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 07:59:09,794 [INFO] Regularization: 2916.604980 * 0.0000010000 = 0.0029166050
2019-03-19 07:59:09,795 [INFO] Sum of grad norms: 0.072032
2019-03-19 07:59:09,795 [INFO] ---------------------------------
2019-03-19 07:59:28,601 [INFO] ---------------------------------
2019-03-19 07:59:28,602 [INFO] Summary:
2019-03-19 07:59:28,603 [INFO] Batch 97000, worst loss 0.060476 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 07:59:28,603 [INFO] Regularization: 2915.888184 * 0.0000010000 = 0.0029158881
2019-03-19 07:59:28,604 [INFO] Sum of grad norms: 0.054421
2019-03-19 07:59:28,604 [INFO] ---------------------------------
2019-03-19 07:59:47,360 [INFO] ---------------------------------
2019-03-19 07:59:47,361 [INFO] Summary:
2019-03-19 07:59:47,362 [INFO] Batch 98000, worst loss 0.060572 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 07:59:47,362 [INFO] Regularization: 2914.920166 * 0.0000010000 = 0.0029149202
2019-03-19 07:59:47,363 [INFO] Sum of grad norms: 0.025238
2019-03-19 07:59:47,364 [INFO] ---------------------------------
2019-03-19 08:00:06,121 [INFO] ---------------------------------
2019-03-19 08:00:06,122 [INFO] Summary:
2019-03-19 08:00:06,123 [INFO] Batch 99000, worst loss 0.060662 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 08:00:06,124 [INFO] Regularization: 2914.076904 * 0.0000010000 = 0.0029140769
2019-03-19 08:00:06,124 [INFO] Sum of grad norms: 0.020505
2019-03-19 08:00:06,125 [INFO] ---------------------------------
2019-03-19 08:00:24,848 [INFO] ---------------------------------
2019-03-19 08:00:24,849 [INFO] Summary:
2019-03-19 08:00:24,850 [INFO] Batch 100000, worst loss 0.060655 (incl. reg.) of 1000 batches, learning rate 0.000010 @cl.-depth 1
2019-03-19 08:00:24,850 [INFO] Regularization: 2913.389648 * 0.0000010000 = 0.0029133896
2019-03-19 08:00:24,851 [INFO] Sum of grad norms: 0.029781
2019-03-19 08:00:24,851 [INFO] ---------------------------------
2019-03-19 08:00:29,832 [INFO] ---------------------------------
2019-03-19 08:00:29,833 [INFO] Evaluation:
2019-03-19 08:00:29,834 [INFO] Batch 100000, worst loss 0.057724 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:00:29,834 [INFO] ---------------------------------
2019-03-19 08:00:48,414 [INFO] ---------------------------------
2019-03-19 08:00:48,415 [INFO] Summary:
2019-03-19 08:00:48,416 [INFO] Batch 101000, worst loss 0.060632 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 08:00:48,417 [INFO] Regularization: 2912.448486 * 0.0000010000 = 0.0029124485
2019-03-19 08:00:48,417 [INFO] Sum of grad norms: 0.028336
2019-03-19 08:00:48,418 [INFO] ---------------------------------
2019-03-19 08:01:06,875 [INFO] ---------------------------------
2019-03-19 08:01:06,876 [INFO] Summary:
2019-03-19 08:01:06,877 [INFO] Batch 102000, worst loss 0.060635 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 08:01:06,877 [INFO] Regularization: 2911.910156 * 0.0000010000 = 0.0029119102
2019-03-19 08:01:06,878 [INFO] Sum of grad norms: 0.020288
2019-03-19 08:01:06,878 [INFO] ---------------------------------
2019-03-19 08:01:25,609 [INFO] ---------------------------------
2019-03-19 08:01:25,610 [INFO] Summary:
2019-03-19 08:01:25,611 [INFO] Batch 103000, worst loss 0.060697 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 08:01:25,611 [INFO] Regularization: 2911.454346 * 0.0000010000 = 0.0029114543
2019-03-19 08:01:25,612 [INFO] Sum of grad norms: 0.016230
2019-03-19 08:01:25,612 [INFO] ---------------------------------
2019-03-19 08:01:44,199 [INFO] ---------------------------------
2019-03-19 08:01:44,200 [INFO] Summary:
2019-03-19 08:01:44,201 [INFO] Batch 104000, worst loss 0.060695 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 08:01:44,201 [INFO] Regularization: 2911.045410 * 0.0000010000 = 0.0029110454
2019-03-19 08:01:44,202 [INFO] Sum of grad norms: 0.057421
2019-03-19 08:01:44,202 [INFO] ---------------------------------
2019-03-19 08:02:02,646 [INFO] ---------------------------------
2019-03-19 08:02:02,647 [INFO] Summary:
2019-03-19 08:02:02,648 [INFO] Batch 105000, worst loss 0.060540 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 08:02:02,648 [INFO] Regularization: 2910.667725 * 0.0000010000 = 0.0029106678
2019-03-19 08:02:02,649 [INFO] Sum of grad norms: 0.078637
2019-03-19 08:02:02,650 [INFO] ---------------------------------
2019-03-19 08:02:21,326 [INFO] ---------------------------------
2019-03-19 08:02:21,327 [INFO] Summary:
2019-03-19 08:02:21,327 [INFO] Batch 106000, worst loss 0.060536 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 08:02:21,328 [INFO] Regularization: 2910.213623 * 0.0000010000 = 0.0029102135
2019-03-19 08:02:21,329 [INFO] Sum of grad norms: 0.037013
2019-03-19 08:02:21,329 [INFO] ---------------------------------
2019-03-19 08:02:40,104 [INFO] ---------------------------------
2019-03-19 08:02:40,105 [INFO] Summary:
2019-03-19 08:02:40,106 [INFO] Batch 107000, worst loss 0.060537 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 08:02:40,106 [INFO] Regularization: 2909.780762 * 0.0000010000 = 0.0029097807
2019-03-19 08:02:40,107 [INFO] Sum of grad norms: 0.014587
2019-03-19 08:02:40,108 [INFO] ---------------------------------
2019-03-19 08:02:58,709 [INFO] ---------------------------------
2019-03-19 08:02:58,710 [INFO] Summary:
2019-03-19 08:02:58,711 [INFO] Batch 108000, worst loss 0.060443 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 08:02:58,711 [INFO] Regularization: 2909.366943 * 0.0000010000 = 0.0029093670
2019-03-19 08:02:58,712 [INFO] Sum of grad norms: 0.033559
2019-03-19 08:02:58,712 [INFO] ---------------------------------
2019-03-19 08:03:17,551 [INFO] ---------------------------------
2019-03-19 08:03:17,552 [INFO] Summary:
2019-03-19 08:03:17,553 [INFO] Batch 109000, worst loss 0.060568 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 08:03:17,554 [INFO] Regularization: 2908.901123 * 0.0000010000 = 0.0029089011
2019-03-19 08:03:17,555 [INFO] Sum of grad norms: 0.036651
2019-03-19 08:03:17,555 [INFO] ---------------------------------
2019-03-19 08:03:36,087 [INFO] ---------------------------------
2019-03-19 08:03:36,088 [INFO] Summary:
2019-03-19 08:03:36,089 [INFO] Batch 110000, worst loss 0.060632 (incl. reg.) of 1000 batches, learning rate 0.000005 @cl.-depth 1
2019-03-19 08:03:36,089 [INFO] Regularization: 2908.568359 * 0.0000010000 = 0.0029085684
2019-03-19 08:03:36,090 [INFO] Sum of grad norms: 0.022488
2019-03-19 08:03:36,090 [INFO] ---------------------------------
2019-03-19 08:03:41,014 [INFO] ---------------------------------
2019-03-19 08:03:41,015 [INFO] Evaluation:
2019-03-19 08:03:41,016 [INFO] Batch 110000, worst loss 0.057722 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:03:41,016 [INFO] ---------------------------------
2019-03-19 08:03:59,815 [INFO] ---------------------------------
2019-03-19 08:03:59,816 [INFO] Summary:
2019-03-19 08:03:59,817 [INFO] Batch 111000, worst loss 0.060550 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 08:03:59,817 [INFO] Regularization: 2908.121338 * 0.0000010000 = 0.0029081213
2019-03-19 08:03:59,818 [INFO] Sum of grad norms: 0.031140
2019-03-19 08:03:59,818 [INFO] ---------------------------------
2019-03-19 08:04:18,485 [INFO] ---------------------------------
2019-03-19 08:04:18,485 [INFO] Summary:
2019-03-19 08:04:18,486 [INFO] Batch 112000, worst loss 0.060661 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 08:04:18,486 [INFO] Regularization: 2907.813232 * 0.0000010000 = 0.0029078133
2019-03-19 08:04:18,487 [INFO] Sum of grad norms: 0.040815
2019-03-19 08:04:18,487 [INFO] ---------------------------------
2019-03-19 08:04:37,467 [INFO] ---------------------------------
2019-03-19 08:04:37,467 [INFO] Summary:
2019-03-19 08:04:37,468 [INFO] Batch 113000, worst loss 0.060844 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 08:04:37,469 [INFO] Regularization: 2907.567139 * 0.0000010000 = 0.0029075672
2019-03-19 08:04:37,469 [INFO] Sum of grad norms: 0.105218
2019-03-19 08:04:37,470 [INFO] ---------------------------------
2019-03-19 08:04:55,727 [INFO] ---------------------------------
2019-03-19 08:04:55,728 [INFO] Summary:
2019-03-19 08:04:55,728 [INFO] Batch 114000, worst loss 0.060842 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 08:04:55,729 [INFO] Regularization: 2907.360596 * 0.0000010000 = 0.0029073607
2019-03-19 08:04:55,730 [INFO] Sum of grad norms: 0.104431
2019-03-19 08:04:55,730 [INFO] ---------------------------------
2019-03-19 08:05:14,347 [INFO] ---------------------------------
2019-03-19 08:05:14,348 [INFO] Summary:
2019-03-19 08:05:14,348 [INFO] Batch 115000, worst loss 0.060564 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 08:05:14,349 [INFO] Regularization: 2907.185303 * 0.0000010000 = 0.0029071853
2019-03-19 08:05:14,349 [INFO] Sum of grad norms: 0.029037
2019-03-19 08:05:14,350 [INFO] ---------------------------------
2019-03-19 08:05:33,108 [INFO] ---------------------------------
2019-03-19 08:05:33,109 [INFO] Summary:
2019-03-19 08:05:33,110 [INFO] Batch 116000, worst loss 0.060514 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 08:05:33,110 [INFO] Regularization: 2906.950684 * 0.0000010000 = 0.0029069507
2019-03-19 08:05:33,111 [INFO] Sum of grad norms: 0.028879
2019-03-19 08:05:33,111 [INFO] ---------------------------------
2019-03-19 08:05:51,521 [INFO] ---------------------------------
2019-03-19 08:05:51,522 [INFO] Summary:
2019-03-19 08:05:51,523 [INFO] Batch 117000, worst loss 0.060570 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 08:05:51,523 [INFO] Regularization: 2906.717285 * 0.0000010000 = 0.0029067174
2019-03-19 08:05:51,524 [INFO] Sum of grad norms: 0.028812
2019-03-19 08:05:51,525 [INFO] ---------------------------------
2019-03-19 08:06:10,404 [INFO] ---------------------------------
2019-03-19 08:06:10,405 [INFO] Summary:
2019-03-19 08:06:10,406 [INFO] Batch 118000, worst loss 0.060572 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 08:06:10,407 [INFO] Regularization: 2906.514648 * 0.0000010000 = 0.0029065146
2019-03-19 08:06:10,407 [INFO] Sum of grad norms: 0.030356
2019-03-19 08:06:10,408 [INFO] ---------------------------------
2019-03-19 08:06:29,283 [INFO] ---------------------------------
2019-03-19 08:06:29,284 [INFO] Summary:
2019-03-19 08:06:29,284 [INFO] Batch 119000, worst loss 0.060509 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 08:06:29,285 [INFO] Regularization: 2906.310303 * 0.0000010000 = 0.0029063104
2019-03-19 08:06:29,285 [INFO] Sum of grad norms: 0.028789
2019-03-19 08:06:29,286 [INFO] ---------------------------------
2019-03-19 08:06:47,809 [INFO] ---------------------------------
2019-03-19 08:06:47,810 [INFO] Summary:
2019-03-19 08:06:47,811 [INFO] Batch 120000, worst loss 0.060500 (incl. reg.) of 1000 batches, learning rate 0.000002 @cl.-depth 1
2019-03-19 08:06:47,811 [INFO] Regularization: 2906.076904 * 0.0000010000 = 0.0029060768
2019-03-19 08:06:47,812 [INFO] Sum of grad norms: 0.033359
2019-03-19 08:06:47,813 [INFO] ---------------------------------
2019-03-19 08:06:52,715 [INFO] ---------------------------------
2019-03-19 08:06:52,716 [INFO] Evaluation:
2019-03-19 08:06:52,718 [INFO] Batch 120000, worst loss 0.057595 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:06:52,719 [INFO] ---------------------------------
2019-03-19 08:07:11,714 [INFO] ---------------------------------
2019-03-19 08:07:11,715 [INFO] Summary:
2019-03-19 08:07:11,716 [INFO] Batch 121000, worst loss 0.060420 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:07:11,716 [INFO] Regularization: 2905.825195 * 0.0000010000 = 0.0029058252
2019-03-19 08:07:11,717 [INFO] Sum of grad norms: 0.058619
2019-03-19 08:07:11,717 [INFO] ---------------------------------
2019-03-19 08:07:30,675 [INFO] ---------------------------------
2019-03-19 08:07:30,676 [INFO] Summary:
2019-03-19 08:07:30,677 [INFO] Batch 122000, worst loss 0.060374 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:07:30,677 [INFO] Regularization: 2905.654053 * 0.0000010000 = 0.0029056540
2019-03-19 08:07:30,678 [INFO] Sum of grad norms: 0.029143
2019-03-19 08:07:30,678 [INFO] ---------------------------------
2019-03-19 08:07:49,326 [INFO] ---------------------------------
2019-03-19 08:07:49,327 [INFO] Summary:
2019-03-19 08:07:49,327 [INFO] Batch 123000, worst loss 0.060652 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:07:49,328 [INFO] Regularization: 2905.538574 * 0.0000010000 = 0.0029055385
2019-03-19 08:07:49,328 [INFO] Sum of grad norms: 0.028822
2019-03-19 08:07:49,329 [INFO] ---------------------------------
2019-03-19 08:08:08,162 [INFO] ---------------------------------
2019-03-19 08:08:08,163 [INFO] Summary:
2019-03-19 08:08:08,164 [INFO] Batch 124000, worst loss 0.060653 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:08:08,164 [INFO] Regularization: 2905.448242 * 0.0000010000 = 0.0029054482
2019-03-19 08:08:08,165 [INFO] Sum of grad norms: 0.061919
2019-03-19 08:08:08,165 [INFO] ---------------------------------
2019-03-19 08:08:26,714 [INFO] ---------------------------------
2019-03-19 08:08:26,715 [INFO] Summary:
2019-03-19 08:08:26,715 [INFO] Batch 125000, worst loss 0.060492 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:08:26,716 [INFO] Regularization: 2905.333740 * 0.0000010000 = 0.0029053336
2019-03-19 08:08:26,716 [INFO] Sum of grad norms: 0.036006
2019-03-19 08:08:26,717 [INFO] ---------------------------------
2019-03-19 08:08:45,344 [INFO] ---------------------------------
2019-03-19 08:08:45,345 [INFO] Summary:
2019-03-19 08:08:45,346 [INFO] Batch 126000, worst loss 0.060560 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:08:45,346 [INFO] Regularization: 2905.239258 * 0.0000010000 = 0.0029052394
2019-03-19 08:08:45,347 [INFO] Sum of grad norms: 0.027174
2019-03-19 08:08:45,347 [INFO] ---------------------------------
2019-03-19 08:09:04,510 [INFO] ---------------------------------
2019-03-19 08:09:04,511 [INFO] Summary:
2019-03-19 08:09:04,512 [INFO] Batch 127000, worst loss 0.060537 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:09:04,512 [INFO] Regularization: 2905.121826 * 0.0000010000 = 0.0029051218
2019-03-19 08:09:04,513 [INFO] Sum of grad norms: 0.045803
2019-03-19 08:09:04,514 [INFO] ---------------------------------
2019-03-19 08:09:23,431 [INFO] ---------------------------------
2019-03-19 08:09:23,432 [INFO] Summary:
2019-03-19 08:09:23,432 [INFO] Batch 128000, worst loss 0.060597 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:09:23,433 [INFO] Regularization: 2905.022705 * 0.0000010000 = 0.0029050226
2019-03-19 08:09:23,433 [INFO] Sum of grad norms: 0.028118
2019-03-19 08:09:23,434 [INFO] ---------------------------------
2019-03-19 08:09:42,187 [INFO] ---------------------------------
2019-03-19 08:09:42,188 [INFO] Summary:
2019-03-19 08:09:42,189 [INFO] Batch 129000, worst loss 0.060529 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:09:42,190 [INFO] Regularization: 2904.904053 * 0.0000010000 = 0.0029049041
2019-03-19 08:09:42,190 [INFO] Sum of grad norms: 0.025758
2019-03-19 08:09:42,191 [INFO] ---------------------------------
2019-03-19 08:10:00,799 [INFO] ---------------------------------
2019-03-19 08:10:00,800 [INFO] Summary:
2019-03-19 08:10:00,801 [INFO] Batch 130000, worst loss 0.060535 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:10:00,802 [INFO] Regularization: 2904.797852 * 0.0000010000 = 0.0029047979
2019-03-19 08:10:00,803 [INFO] Sum of grad norms: 0.019734
2019-03-19 08:10:00,804 [INFO] ---------------------------------
2019-03-19 08:10:05,776 [INFO] ---------------------------------
2019-03-19 08:10:05,777 [INFO] Evaluation:
2019-03-19 08:10:05,778 [INFO] Batch 130000, worst loss 0.057630 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:10:05,779 [INFO] ---------------------------------
2019-03-19 08:10:24,645 [INFO] ---------------------------------
2019-03-19 08:10:24,646 [INFO] Summary:
2019-03-19 08:10:24,647 [INFO] Batch 131000, worst loss 0.060604 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:10:24,647 [INFO] Regularization: 2904.690674 * 0.0000010000 = 0.0029046906
2019-03-19 08:10:24,648 [INFO] Sum of grad norms: 0.020242
2019-03-19 08:10:24,648 [INFO] ---------------------------------
2019-03-19 08:10:43,264 [INFO] ---------------------------------
2019-03-19 08:10:43,265 [INFO] Summary:
2019-03-19 08:10:43,266 [INFO] Batch 132000, worst loss 0.060573 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:10:43,266 [INFO] Regularization: 2904.614258 * 0.0000010000 = 0.0029046142
2019-03-19 08:10:43,267 [INFO] Sum of grad norms: 0.047031
2019-03-19 08:10:43,268 [INFO] ---------------------------------
2019-03-19 08:11:02,054 [INFO] ---------------------------------
2019-03-19 08:11:02,055 [INFO] Summary:
2019-03-19 08:11:02,055 [INFO] Batch 133000, worst loss 0.060678 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:11:02,056 [INFO] Regularization: 2904.559082 * 0.0000010000 = 0.0029045590
2019-03-19 08:11:02,056 [INFO] Sum of grad norms: 0.026727
2019-03-19 08:11:02,057 [INFO] ---------------------------------
2019-03-19 08:11:20,723 [INFO] ---------------------------------
2019-03-19 08:11:20,724 [INFO] Summary:
2019-03-19 08:11:20,725 [INFO] Batch 134000, worst loss 0.060621 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:11:20,725 [INFO] Regularization: 2904.511719 * 0.0000010000 = 0.0029045118
2019-03-19 08:11:20,726 [INFO] Sum of grad norms: 0.042140
2019-03-19 08:11:20,727 [INFO] ---------------------------------
2019-03-19 08:11:39,517 [INFO] ---------------------------------
2019-03-19 08:11:39,518 [INFO] Summary:
2019-03-19 08:11:39,518 [INFO] Batch 135000, worst loss 0.060373 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:11:39,519 [INFO] Regularization: 2904.432373 * 0.0000010000 = 0.0029044324
2019-03-19 08:11:39,519 [INFO] Sum of grad norms: 0.034480
2019-03-19 08:11:39,520 [INFO] ---------------------------------
2019-03-19 08:11:58,674 [INFO] ---------------------------------
2019-03-19 08:11:58,675 [INFO] Summary:
2019-03-19 08:11:58,675 [INFO] Batch 136000, worst loss 0.060550 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:11:58,676 [INFO] Regularization: 2904.369629 * 0.0000010000 = 0.0029043697
2019-03-19 08:11:58,676 [INFO] Sum of grad norms: 0.021629
2019-03-19 08:11:58,677 [INFO] ---------------------------------
2019-03-19 08:12:17,169 [INFO] ---------------------------------
2019-03-19 08:12:17,170 [INFO] Summary:
2019-03-19 08:12:17,170 [INFO] Batch 137000, worst loss 0.060601 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:12:17,171 [INFO] Regularization: 2904.329834 * 0.0000010000 = 0.0029043299
2019-03-19 08:12:17,171 [INFO] Sum of grad norms: 0.034345
2019-03-19 08:12:17,172 [INFO] ---------------------------------
2019-03-19 08:12:36,084 [INFO] ---------------------------------
2019-03-19 08:12:36,085 [INFO] Summary:
2019-03-19 08:12:36,086 [INFO] Batch 138000, worst loss 0.060599 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:12:36,087 [INFO] Regularization: 2904.268799 * 0.0000010000 = 0.0029042687
2019-03-19 08:12:36,087 [INFO] Sum of grad norms: 0.050672
2019-03-19 08:12:36,088 [INFO] ---------------------------------
2019-03-19 08:12:55,105 [INFO] ---------------------------------
2019-03-19 08:12:55,106 [INFO] Summary:
2019-03-19 08:12:55,107 [INFO] Batch 139000, worst loss 0.060554 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:12:55,107 [INFO] Regularization: 2904.229492 * 0.0000010000 = 0.0029042296
2019-03-19 08:12:55,108 [INFO] Sum of grad norms: 0.078445
2019-03-19 08:12:55,108 [INFO] ---------------------------------
2019-03-19 08:13:14,304 [INFO] ---------------------------------
2019-03-19 08:13:14,305 [INFO] Summary:
2019-03-19 08:13:14,305 [INFO] Batch 140000, worst loss 0.060536 (incl. reg.) of 1000 batches, learning rate 0.000001 @cl.-depth 1
2019-03-19 08:13:14,306 [INFO] Regularization: 2904.179688 * 0.0000010000 = 0.0029041797
2019-03-19 08:13:14,306 [INFO] Sum of grad norms: 0.024962
2019-03-19 08:13:14,307 [INFO] ---------------------------------
2019-03-19 08:13:19,203 [INFO] ---------------------------------
2019-03-19 08:13:19,204 [INFO] Evaluation:
2019-03-19 08:13:19,205 [INFO] Batch 140000, worst loss 0.057630 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:13:19,207 [INFO] ---------------------------------
2019-03-19 08:13:38,161 [INFO] ---------------------------------
2019-03-19 08:13:38,162 [INFO] Summary:
2019-03-19 08:13:38,162 [INFO] Batch 141000, worst loss 0.060561 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:13:38,163 [INFO] Regularization: 2904.129639 * 0.0000010000 = 0.0029041297
2019-03-19 08:13:38,163 [INFO] Sum of grad norms: 0.027554
2019-03-19 08:13:38,164 [INFO] ---------------------------------
2019-03-19 08:13:57,271 [INFO] ---------------------------------
2019-03-19 08:13:57,272 [INFO] Summary:
2019-03-19 08:13:57,273 [INFO] Batch 142000, worst loss 0.060559 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:13:57,274 [INFO] Regularization: 2904.093506 * 0.0000010000 = 0.0029040936
2019-03-19 08:13:57,274 [INFO] Sum of grad norms: 0.031708
2019-03-19 08:13:57,275 [INFO] ---------------------------------
2019-03-19 08:14:15,990 [INFO] ---------------------------------
2019-03-19 08:14:15,991 [INFO] Summary:
2019-03-19 08:14:15,992 [INFO] Batch 143000, worst loss 0.060491 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:14:15,992 [INFO] Regularization: 2904.057373 * 0.0000010000 = 0.0029040573
2019-03-19 08:14:15,993 [INFO] Sum of grad norms: 0.045316
2019-03-19 08:14:15,993 [INFO] ---------------------------------
2019-03-19 08:14:35,049 [INFO] ---------------------------------
2019-03-19 08:14:35,050 [INFO] Summary:
2019-03-19 08:14:35,050 [INFO] Batch 144000, worst loss 0.060484 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:14:35,051 [INFO] Regularization: 2904.012451 * 0.0000010000 = 0.0029040123
2019-03-19 08:14:35,052 [INFO] Sum of grad norms: 0.056422
2019-03-19 08:14:35,052 [INFO] ---------------------------------
2019-03-19 08:14:53,778 [INFO] ---------------------------------
2019-03-19 08:14:53,779 [INFO] Summary:
2019-03-19 08:14:53,779 [INFO] Batch 145000, worst loss 0.060482 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:14:53,780 [INFO] Regularization: 2903.981689 * 0.0000010000 = 0.0029039816
2019-03-19 08:14:53,781 [INFO] Sum of grad norms: 0.030154
2019-03-19 08:14:53,781 [INFO] ---------------------------------
2019-03-19 08:15:12,569 [INFO] ---------------------------------
2019-03-19 08:15:12,570 [INFO] Summary:
2019-03-19 08:15:12,571 [INFO] Batch 146000, worst loss 0.060532 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:15:12,572 [INFO] Regularization: 2903.955322 * 0.0000010000 = 0.0029039553
2019-03-19 08:15:12,572 [INFO] Sum of grad norms: 0.039447
2019-03-19 08:15:12,573 [INFO] ---------------------------------
2019-03-19 08:15:31,090 [INFO] ---------------------------------
2019-03-19 08:15:31,091 [INFO] Summary:
2019-03-19 08:15:31,092 [INFO] Batch 147000, worst loss 0.060514 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:15:31,092 [INFO] Regularization: 2903.917480 * 0.0000010000 = 0.0029039176
2019-03-19 08:15:31,093 [INFO] Sum of grad norms: 0.047562
2019-03-19 08:15:31,094 [INFO] ---------------------------------
2019-03-19 08:15:50,232 [INFO] ---------------------------------
2019-03-19 08:15:50,233 [INFO] Summary:
2019-03-19 08:15:50,234 [INFO] Batch 148000, worst loss 0.060537 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:15:50,234 [INFO] Regularization: 2903.892822 * 0.0000010000 = 0.0029038929
2019-03-19 08:15:50,235 [INFO] Sum of grad norms: 0.057177
2019-03-19 08:15:50,236 [INFO] ---------------------------------
2019-03-19 08:16:09,259 [INFO] ---------------------------------
2019-03-19 08:16:09,260 [INFO] Summary:
2019-03-19 08:16:09,261 [INFO] Batch 149000, worst loss 0.060703 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:16:09,261 [INFO] Regularization: 2903.859863 * 0.0000010000 = 0.0029038598
2019-03-19 08:16:09,262 [INFO] Sum of grad norms: 0.034985
2019-03-19 08:16:09,262 [INFO] ---------------------------------
2019-03-19 08:16:28,007 [INFO] ---------------------------------
2019-03-19 08:16:28,008 [INFO] Summary:
2019-03-19 08:16:28,008 [INFO] Batch 150000, worst loss 0.060703 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:16:28,009 [INFO] Regularization: 2903.831055 * 0.0000010000 = 0.0029038310
2019-03-19 08:16:28,009 [INFO] Sum of grad norms: 0.019643
2019-03-19 08:16:28,010 [INFO] ---------------------------------
2019-03-19 08:16:33,023 [INFO] ---------------------------------
2019-03-19 08:16:33,024 [INFO] Evaluation:
2019-03-19 08:16:33,025 [INFO] Batch 150000, worst loss 0.057630 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:16:33,025 [INFO] ---------------------------------
2019-03-19 08:16:52,116 [INFO] ---------------------------------
2019-03-19 08:16:52,117 [INFO] Summary:
2019-03-19 08:16:52,118 [INFO] Batch 151000, worst loss 0.060479 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:16:52,118 [INFO] Regularization: 2903.796631 * 0.0000010000 = 0.0029037967
2019-03-19 08:16:52,119 [INFO] Sum of grad norms: 0.036036
2019-03-19 08:16:52,119 [INFO] ---------------------------------
2019-03-19 08:17:11,162 [INFO] ---------------------------------
2019-03-19 08:17:11,163 [INFO] Summary:
2019-03-19 08:17:11,163 [INFO] Batch 152000, worst loss 0.060531 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:17:11,164 [INFO] Regularization: 2903.784180 * 0.0000010000 = 0.0029037842
2019-03-19 08:17:11,164 [INFO] Sum of grad norms: 0.067568
2019-03-19 08:17:11,165 [INFO] ---------------------------------
2019-03-19 08:17:30,074 [INFO] ---------------------------------
2019-03-19 08:17:30,075 [INFO] Summary:
2019-03-19 08:17:30,076 [INFO] Batch 153000, worst loss 0.060599 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:17:30,076 [INFO] Regularization: 2903.769043 * 0.0000010000 = 0.0029037690
2019-03-19 08:17:30,077 [INFO] Sum of grad norms: 0.059074
2019-03-19 08:17:30,077 [INFO] ---------------------------------
2019-03-19 08:17:48,802 [INFO] ---------------------------------
2019-03-19 08:17:48,803 [INFO] Summary:
2019-03-19 08:17:48,803 [INFO] Batch 154000, worst loss 0.060665 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:17:48,804 [INFO] Regularization: 2903.754883 * 0.0000010000 = 0.0029037548
2019-03-19 08:17:48,805 [INFO] Sum of grad norms: 0.043299
2019-03-19 08:17:48,805 [INFO] ---------------------------------
2019-03-19 08:18:07,528 [INFO] ---------------------------------
2019-03-19 08:18:07,529 [INFO] Summary:
2019-03-19 08:18:07,530 [INFO] Batch 155000, worst loss 0.060632 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:18:07,531 [INFO] Regularization: 2903.737305 * 0.0000010000 = 0.0029037374
2019-03-19 08:18:07,531 [INFO] Sum of grad norms: 0.033245
2019-03-19 08:18:07,532 [INFO] ---------------------------------
2019-03-19 08:18:26,146 [INFO] ---------------------------------
2019-03-19 08:18:26,147 [INFO] Summary:
2019-03-19 08:18:26,147 [INFO] Batch 156000, worst loss 0.060633 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:18:26,148 [INFO] Regularization: 2903.725586 * 0.0000010000 = 0.0029037255
2019-03-19 08:18:26,148 [INFO] Sum of grad norms: 0.020330
2019-03-19 08:18:26,149 [INFO] ---------------------------------
2019-03-19 08:18:44,945 [INFO] ---------------------------------
2019-03-19 08:18:44,946 [INFO] Summary:
2019-03-19 08:18:44,946 [INFO] Batch 157000, worst loss 0.060488 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:18:44,947 [INFO] Regularization: 2903.714600 * 0.0000010000 = 0.0029037145
2019-03-19 08:18:44,947 [INFO] Sum of grad norms: 0.061434
2019-03-19 08:18:44,948 [INFO] ---------------------------------
2019-03-19 08:19:03,765 [INFO] ---------------------------------
2019-03-19 08:19:03,765 [INFO] Summary:
2019-03-19 08:19:03,766 [INFO] Batch 158000, worst loss 0.060652 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:19:03,767 [INFO] Regularization: 2903.699219 * 0.0000010000 = 0.0029036992
2019-03-19 08:19:03,767 [INFO] Sum of grad norms: 0.011978
2019-03-19 08:19:03,768 [INFO] ---------------------------------
2019-03-19 08:19:22,158 [INFO] ---------------------------------
2019-03-19 08:19:22,159 [INFO] Summary:
2019-03-19 08:19:22,160 [INFO] Batch 159000, worst loss 0.060653 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:19:22,161 [INFO] Regularization: 2903.683350 * 0.0000010000 = 0.0029036833
2019-03-19 08:19:22,161 [INFO] Sum of grad norms: 0.061346
2019-03-19 08:19:22,162 [INFO] ---------------------------------
2019-03-19 08:19:40,994 [INFO] ---------------------------------
2019-03-19 08:19:40,994 [INFO] Summary:
2019-03-19 08:19:40,995 [INFO] Batch 160000, worst loss 0.060506 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:19:40,996 [INFO] Regularization: 2903.666748 * 0.0000010000 = 0.0029036668
2019-03-19 08:19:40,996 [INFO] Sum of grad norms: 0.030068
2019-03-19 08:19:40,997 [INFO] ---------------------------------
2019-03-19 08:19:45,892 [INFO] ---------------------------------
2019-03-19 08:19:45,893 [INFO] Evaluation:
2019-03-19 08:19:45,894 [INFO] Batch 160000, worst loss 0.057501 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:19:45,895 [INFO] ---------------------------------
2019-03-19 08:20:05,117 [INFO] ---------------------------------
2019-03-19 08:20:05,118 [INFO] Summary:
2019-03-19 08:20:05,119 [INFO] Batch 161000, worst loss 0.060626 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:20:05,120 [INFO] Regularization: 2903.652100 * 0.0000010000 = 0.0029036521
2019-03-19 08:20:05,121 [INFO] Sum of grad norms: 0.038885
2019-03-19 08:20:05,121 [INFO] ---------------------------------
2019-03-19 08:20:23,806 [INFO] ---------------------------------
2019-03-19 08:20:23,807 [INFO] Summary:
2019-03-19 08:20:23,808 [INFO] Batch 162000, worst loss 0.060626 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:20:23,809 [INFO] Regularization: 2903.638672 * 0.0000010000 = 0.0029036386
2019-03-19 08:20:23,809 [INFO] Sum of grad norms: 0.038049
2019-03-19 08:20:23,810 [INFO] ---------------------------------
2019-03-19 08:20:42,350 [INFO] ---------------------------------
2019-03-19 08:20:42,351 [INFO] Summary:
2019-03-19 08:20:42,352 [INFO] Batch 163000, worst loss 0.060539 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:20:42,352 [INFO] Regularization: 2903.629639 * 0.0000010000 = 0.0029036296
2019-03-19 08:20:42,353 [INFO] Sum of grad norms: 0.056622
2019-03-19 08:20:42,353 [INFO] ---------------------------------
2019-03-19 08:21:00,763 [INFO] ---------------------------------
2019-03-19 08:21:00,764 [INFO] Summary:
2019-03-19 08:21:00,764 [INFO] Batch 164000, worst loss 0.060825 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:21:00,765 [INFO] Regularization: 2903.624023 * 0.0000010000 = 0.0029036240
2019-03-19 08:21:00,765 [INFO] Sum of grad norms: 0.027811
2019-03-19 08:21:00,766 [INFO] ---------------------------------
2019-03-19 08:21:19,525 [INFO] ---------------------------------
2019-03-19 08:21:19,526 [INFO] Summary:
2019-03-19 08:21:19,526 [INFO] Batch 165000, worst loss 0.060534 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:21:19,527 [INFO] Regularization: 2903.616211 * 0.0000010000 = 0.0029036163
2019-03-19 08:21:19,527 [INFO] Sum of grad norms: 0.022402
2019-03-19 08:21:19,528 [INFO] ---------------------------------
2019-03-19 08:21:38,125 [INFO] ---------------------------------
2019-03-19 08:21:38,125 [INFO] Summary:
2019-03-19 08:21:38,126 [INFO] Batch 166000, worst loss 0.060559 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:21:38,127 [INFO] Regularization: 2903.610596 * 0.0000010000 = 0.0029036107
2019-03-19 08:21:38,127 [INFO] Sum of grad norms: 0.023884
2019-03-19 08:21:38,128 [INFO] ---------------------------------
2019-03-19 08:21:56,884 [INFO] ---------------------------------
2019-03-19 08:21:56,885 [INFO] Summary:
2019-03-19 08:21:56,886 [INFO] Batch 167000, worst loss 0.060558 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:21:56,886 [INFO] Regularization: 2903.605957 * 0.0000010000 = 0.0029036060
2019-03-19 08:21:56,887 [INFO] Sum of grad norms: 0.034572
2019-03-19 08:21:56,888 [INFO] ---------------------------------
2019-03-19 08:22:15,411 [INFO] ---------------------------------
2019-03-19 08:22:15,412 [INFO] Summary:
2019-03-19 08:22:15,413 [INFO] Batch 168000, worst loss 0.060565 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:22:15,413 [INFO] Regularization: 2903.599609 * 0.0000010000 = 0.0029035995
2019-03-19 08:22:15,414 [INFO] Sum of grad norms: 0.032365
2019-03-19 08:22:15,414 [INFO] ---------------------------------
2019-03-19 08:22:34,498 [INFO] ---------------------------------
2019-03-19 08:22:34,499 [INFO] Summary:
2019-03-19 08:22:34,500 [INFO] Batch 169000, worst loss 0.060565 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:22:34,500 [INFO] Regularization: 2903.592773 * 0.0000010000 = 0.0029035928
2019-03-19 08:22:34,501 [INFO] Sum of grad norms: 0.032079
2019-03-19 08:22:34,501 [INFO] ---------------------------------
2019-03-19 08:22:53,044 [INFO] ---------------------------------
2019-03-19 08:22:53,045 [INFO] Summary:
2019-03-19 08:22:53,045 [INFO] Batch 170000, worst loss 0.060548 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:22:53,046 [INFO] Regularization: 2903.586914 * 0.0000010000 = 0.0029035870
2019-03-19 08:22:53,046 [INFO] Sum of grad norms: 0.064614
2019-03-19 08:22:53,047 [INFO] ---------------------------------
2019-03-19 08:22:58,028 [INFO] ---------------------------------
2019-03-19 08:22:58,029 [INFO] Evaluation:
2019-03-19 08:22:58,030 [INFO] Batch 170000, worst loss 0.057617 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:22:58,030 [INFO] ---------------------------------
2019-03-19 08:23:16,948 [INFO] ---------------------------------
2019-03-19 08:23:16,949 [INFO] Summary:
2019-03-19 08:23:16,950 [INFO] Batch 171000, worst loss 0.060478 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:23:16,950 [INFO] Regularization: 2903.583984 * 0.0000010000 = 0.0029035839
2019-03-19 08:23:16,951 [INFO] Sum of grad norms: 0.019178
2019-03-19 08:23:16,952 [INFO] ---------------------------------
2019-03-19 08:23:35,687 [INFO] ---------------------------------
2019-03-19 08:23:35,688 [INFO] Summary:
2019-03-19 08:23:35,688 [INFO] Batch 172000, worst loss 0.060440 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:23:35,689 [INFO] Regularization: 2903.579590 * 0.0000010000 = 0.0029035795
2019-03-19 08:23:35,689 [INFO] Sum of grad norms: 0.032183
2019-03-19 08:23:35,690 [INFO] ---------------------------------
2019-03-19 08:23:54,013 [INFO] ---------------------------------
2019-03-19 08:23:54,014 [INFO] Summary:
2019-03-19 08:23:54,014 [INFO] Batch 173000, worst loss 0.060474 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:23:54,015 [INFO] Regularization: 2903.576660 * 0.0000010000 = 0.0029035767
2019-03-19 08:23:54,015 [INFO] Sum of grad norms: 0.024383
2019-03-19 08:23:54,016 [INFO] ---------------------------------
2019-03-19 08:24:12,731 [INFO] ---------------------------------
2019-03-19 08:24:12,732 [INFO] Summary:
2019-03-19 08:24:12,732 [INFO] Batch 174000, worst loss 0.060721 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:24:12,733 [INFO] Regularization: 2903.575439 * 0.0000010000 = 0.0029035755
2019-03-19 08:24:12,733 [INFO] Sum of grad norms: 0.032761
2019-03-19 08:24:12,734 [INFO] ---------------------------------
2019-03-19 08:24:31,512 [INFO] ---------------------------------
2019-03-19 08:24:31,513 [INFO] Summary:
2019-03-19 08:24:31,513 [INFO] Batch 175000, worst loss 0.060413 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:24:31,514 [INFO] Regularization: 2903.571777 * 0.0000010000 = 0.0029035718
2019-03-19 08:24:31,514 [INFO] Sum of grad norms: 0.020938
2019-03-19 08:24:31,515 [INFO] ---------------------------------
2019-03-19 08:24:50,335 [INFO] ---------------------------------
2019-03-19 08:24:50,336 [INFO] Summary:
2019-03-19 08:24:50,337 [INFO] Batch 176000, worst loss 0.060502 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:24:50,337 [INFO] Regularization: 2903.569824 * 0.0000010000 = 0.0029035697
2019-03-19 08:24:50,338 [INFO] Sum of grad norms: 0.026496
2019-03-19 08:24:50,338 [INFO] ---------------------------------
2019-03-19 08:25:09,025 [INFO] ---------------------------------
2019-03-19 08:25:09,026 [INFO] Summary:
2019-03-19 08:25:09,026 [INFO] Batch 177000, worst loss 0.060479 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:25:09,027 [INFO] Regularization: 2903.568359 * 0.0000010000 = 0.0029035683
2019-03-19 08:25:09,028 [INFO] Sum of grad norms: 0.034934
2019-03-19 08:25:09,028 [INFO] ---------------------------------
2019-03-19 08:25:27,773 [INFO] ---------------------------------
2019-03-19 08:25:27,774 [INFO] Summary:
2019-03-19 08:25:27,775 [INFO] Batch 178000, worst loss 0.060479 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:25:27,775 [INFO] Regularization: 2903.564453 * 0.0000010000 = 0.0029035644
2019-03-19 08:25:27,776 [INFO] Sum of grad norms: 0.027654
2019-03-19 08:25:27,776 [INFO] ---------------------------------
2019-03-19 08:25:46,479 [INFO] ---------------------------------
2019-03-19 08:25:46,480 [INFO] Summary:
2019-03-19 08:25:46,480 [INFO] Batch 179000, worst loss 0.060502 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:25:46,481 [INFO] Regularization: 2903.559570 * 0.0000010000 = 0.0029035595
2019-03-19 08:25:46,481 [INFO] Sum of grad norms: 0.036946
2019-03-19 08:25:46,482 [INFO] ---------------------------------
2019-03-19 08:26:05,677 [INFO] ---------------------------------
2019-03-19 08:26:05,678 [INFO] Summary:
2019-03-19 08:26:05,679 [INFO] Batch 180000, worst loss 0.060504 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:26:05,680 [INFO] Regularization: 2903.557617 * 0.0000010000 = 0.0029035576
2019-03-19 08:26:05,680 [INFO] Sum of grad norms: 0.019679
2019-03-19 08:26:05,681 [INFO] ---------------------------------
2019-03-19 08:26:10,575 [INFO] ---------------------------------
2019-03-19 08:26:10,576 [INFO] Evaluation:
2019-03-19 08:26:10,577 [INFO] Batch 180000, worst loss 0.057586 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:26:10,578 [INFO] ---------------------------------
2019-03-19 08:26:29,404 [INFO] ---------------------------------
2019-03-19 08:26:29,405 [INFO] Summary:
2019-03-19 08:26:29,406 [INFO] Batch 181000, worst loss 0.060495 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:26:29,406 [INFO] Regularization: 2903.555420 * 0.0000010000 = 0.0029035555
2019-03-19 08:26:29,407 [INFO] Sum of grad norms: 0.017021
2019-03-19 08:26:29,408 [INFO] ---------------------------------
2019-03-19 08:26:48,124 [INFO] ---------------------------------
2019-03-19 08:26:48,125 [INFO] Summary:
2019-03-19 08:26:48,125 [INFO] Batch 182000, worst loss 0.060520 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:26:48,126 [INFO] Regularization: 2903.554688 * 0.0000010000 = 0.0029035546
2019-03-19 08:26:48,126 [INFO] Sum of grad norms: 0.055265
2019-03-19 08:26:48,127 [INFO] ---------------------------------
2019-03-19 08:27:06,736 [INFO] ---------------------------------
2019-03-19 08:27:06,737 [INFO] Summary:
2019-03-19 08:27:06,738 [INFO] Batch 183000, worst loss 0.060309 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:27:06,738 [INFO] Regularization: 2903.553467 * 0.0000010000 = 0.0029035534
2019-03-19 08:27:06,739 [INFO] Sum of grad norms: 0.054348
2019-03-19 08:27:06,739 [INFO] ---------------------------------
2019-03-19 08:27:25,366 [INFO] ---------------------------------
2019-03-19 08:27:25,367 [INFO] Summary:
2019-03-19 08:27:25,367 [INFO] Batch 184000, worst loss 0.060522 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:27:25,368 [INFO] Regularization: 2903.553467 * 0.0000010000 = 0.0029035534
2019-03-19 08:27:25,368 [INFO] Sum of grad norms: 0.038382
2019-03-19 08:27:25,369 [INFO] ---------------------------------
2019-03-19 08:27:44,127 [INFO] ---------------------------------
2019-03-19 08:27:44,128 [INFO] Summary:
2019-03-19 08:27:44,129 [INFO] Batch 185000, worst loss 0.060522 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:27:44,129 [INFO] Regularization: 2903.553223 * 0.0000010000 = 0.0029035532
2019-03-19 08:27:44,130 [INFO] Sum of grad norms: 0.069820
2019-03-19 08:27:44,130 [INFO] ---------------------------------
2019-03-19 08:28:02,910 [INFO] ---------------------------------
2019-03-19 08:28:02,911 [INFO] Summary:
2019-03-19 08:28:02,912 [INFO] Batch 186000, worst loss 0.060725 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:28:02,912 [INFO] Regularization: 2903.551758 * 0.0000010000 = 0.0029035518
2019-03-19 08:28:02,913 [INFO] Sum of grad norms: 0.023502
2019-03-19 08:28:02,913 [INFO] ---------------------------------
2019-03-19 08:28:21,592 [INFO] ---------------------------------
2019-03-19 08:28:21,593 [INFO] Summary:
2019-03-19 08:28:21,594 [INFO] Batch 187000, worst loss 0.060470 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:28:21,594 [INFO] Regularization: 2903.551270 * 0.0000010000 = 0.0029035513
2019-03-19 08:28:21,595 [INFO] Sum of grad norms: 0.032319
2019-03-19 08:28:21,596 [INFO] ---------------------------------
2019-03-19 08:28:40,330 [INFO] ---------------------------------
2019-03-19 08:28:40,331 [INFO] Summary:
2019-03-19 08:28:40,332 [INFO] Batch 188000, worst loss 0.060536 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:28:40,332 [INFO] Regularization: 2903.551270 * 0.0000010000 = 0.0029035513
2019-03-19 08:28:40,333 [INFO] Sum of grad norms: 0.047123
2019-03-19 08:28:40,334 [INFO] ---------------------------------
2019-03-19 08:28:59,034 [INFO] ---------------------------------
2019-03-19 08:28:59,035 [INFO] Summary:
2019-03-19 08:28:59,035 [INFO] Batch 189000, worst loss 0.060630 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:28:59,036 [INFO] Regularization: 2903.549561 * 0.0000010000 = 0.0029035495
2019-03-19 08:28:59,036 [INFO] Sum of grad norms: 0.031270
2019-03-19 08:28:59,037 [INFO] ---------------------------------
2019-03-19 08:29:17,911 [INFO] ---------------------------------
2019-03-19 08:29:17,912 [INFO] Summary:
2019-03-19 08:29:17,913 [INFO] Batch 190000, worst loss 0.060631 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:29:17,914 [INFO] Regularization: 2903.548340 * 0.0000010000 = 0.0029035483
2019-03-19 08:29:17,914 [INFO] Sum of grad norms: 0.034004
2019-03-19 08:29:17,915 [INFO] ---------------------------------
2019-03-19 08:29:22,935 [INFO] ---------------------------------
2019-03-19 08:29:22,936 [INFO] Evaluation:
2019-03-19 08:29:22,936 [INFO] Batch 190000, worst loss 0.057728 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:29:22,937 [INFO] ---------------------------------
2019-03-19 08:29:41,853 [INFO] ---------------------------------
2019-03-19 08:29:41,854 [INFO] Summary:
2019-03-19 08:29:41,855 [INFO] Batch 191000, worst loss 0.060699 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:29:41,855 [INFO] Regularization: 2903.548828 * 0.0000010000 = 0.0029035488
2019-03-19 08:29:41,856 [INFO] Sum of grad norms: 0.032061
2019-03-19 08:29:41,857 [INFO] ---------------------------------
2019-03-19 08:30:00,590 [INFO] ---------------------------------
2019-03-19 08:30:00,591 [INFO] Summary:
2019-03-19 08:30:00,591 [INFO] Batch 192000, worst loss 0.060699 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:30:00,592 [INFO] Regularization: 2903.548340 * 0.0000010000 = 0.0029035483
2019-03-19 08:30:00,592 [INFO] Sum of grad norms: 0.041006
2019-03-19 08:30:00,593 [INFO] ---------------------------------
2019-03-19 08:30:19,262 [INFO] ---------------------------------
2019-03-19 08:30:19,263 [INFO] Summary:
2019-03-19 08:30:19,263 [INFO] Batch 193000, worst loss 0.060537 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:30:19,264 [INFO] Regularization: 2903.548096 * 0.0000010000 = 0.0029035481
2019-03-19 08:30:19,265 [INFO] Sum of grad norms: 0.022196
2019-03-19 08:30:19,265 [INFO] ---------------------------------
2019-03-19 08:30:37,793 [INFO] ---------------------------------
2019-03-19 08:30:37,794 [INFO] Summary:
2019-03-19 08:30:37,794 [INFO] Batch 194000, worst loss 0.060622 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:30:37,795 [INFO] Regularization: 2903.547852 * 0.0000010000 = 0.0029035478
2019-03-19 08:30:37,795 [INFO] Sum of grad norms: 0.050892
2019-03-19 08:30:37,796 [INFO] ---------------------------------
2019-03-19 08:30:56,416 [INFO] ---------------------------------
2019-03-19 08:30:56,417 [INFO] Summary:
2019-03-19 08:30:56,417 [INFO] Batch 195000, worst loss 0.060595 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:30:56,418 [INFO] Regularization: 2903.547852 * 0.0000010000 = 0.0029035478
2019-03-19 08:30:56,418 [INFO] Sum of grad norms: 0.022672
2019-03-19 08:30:56,419 [INFO] ---------------------------------
2019-03-19 08:31:14,969 [INFO] ---------------------------------
2019-03-19 08:31:14,970 [INFO] Summary:
2019-03-19 08:31:14,971 [INFO] Batch 196000, worst loss 0.060534 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:31:14,971 [INFO] Regularization: 2903.547363 * 0.0000010000 = 0.0029035474
2019-03-19 08:31:14,972 [INFO] Sum of grad norms: 0.020590
2019-03-19 08:31:14,973 [INFO] ---------------------------------
2019-03-19 08:31:33,944 [INFO] ---------------------------------
2019-03-19 08:31:33,945 [INFO] Summary:
2019-03-19 08:31:33,946 [INFO] Batch 197000, worst loss 0.060474 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:31:33,947 [INFO] Regularization: 2903.546875 * 0.0000010000 = 0.0029035469
2019-03-19 08:31:33,948 [INFO] Sum of grad norms: 0.030291
2019-03-19 08:31:33,948 [INFO] ---------------------------------
2019-03-19 08:31:52,969 [INFO] ---------------------------------
2019-03-19 08:31:52,970 [INFO] Summary:
2019-03-19 08:31:52,970 [INFO] Batch 198000, worst loss 0.060593 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:31:52,971 [INFO] Regularization: 2903.546631 * 0.0000010000 = 0.0029035467
2019-03-19 08:31:52,971 [INFO] Sum of grad norms: 0.026686
2019-03-19 08:31:52,972 [INFO] ---------------------------------
2019-03-19 08:32:11,800 [INFO] ---------------------------------
2019-03-19 08:32:11,801 [INFO] Summary:
2019-03-19 08:32:11,801 [INFO] Batch 199000, worst loss 0.060679 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:32:11,802 [INFO] Regularization: 2903.546387 * 0.0000010000 = 0.0029035464
2019-03-19 08:32:11,803 [INFO] Sum of grad norms: 0.014528
2019-03-19 08:32:11,804 [INFO] ---------------------------------
2019-03-19 08:32:30,808 [INFO] ---------------------------------
2019-03-19 08:32:30,809 [INFO] Summary:
2019-03-19 08:32:30,810 [INFO] Batch 200000, worst loss 0.060757 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:32:30,810 [INFO] Regularization: 2903.545898 * 0.0000010000 = 0.0029035460
2019-03-19 08:32:30,811 [INFO] Sum of grad norms: 0.035622
2019-03-19 08:32:30,811 [INFO] ---------------------------------
2019-03-19 08:32:35,786 [INFO] ---------------------------------
2019-03-19 08:32:35,787 [INFO] Evaluation:
2019-03-19 08:32:35,789 [INFO] Batch 200000, worst loss 0.057683 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:32:35,789 [INFO] ---------------------------------
2019-03-19 08:32:54,444 [INFO] ---------------------------------
2019-03-19 08:32:54,445 [INFO] Summary:
2019-03-19 08:32:54,445 [INFO] Batch 201000, worst loss 0.060608 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:32:54,446 [INFO] Regularization: 2903.545410 * 0.0000010000 = 0.0029035455
2019-03-19 08:32:54,446 [INFO] Sum of grad norms: 0.033999
2019-03-19 08:32:54,447 [INFO] ---------------------------------
2019-03-19 08:33:13,314 [INFO] ---------------------------------
2019-03-19 08:33:13,315 [INFO] Summary:
2019-03-19 08:33:13,316 [INFO] Batch 202000, worst loss 0.060610 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:33:13,316 [INFO] Regularization: 2903.545410 * 0.0000010000 = 0.0029035455
2019-03-19 08:33:13,317 [INFO] Sum of grad norms: 0.022432
2019-03-19 08:33:13,317 [INFO] ---------------------------------
2019-03-19 08:33:32,180 [INFO] ---------------------------------
2019-03-19 08:33:32,181 [INFO] Summary:
2019-03-19 08:33:32,181 [INFO] Batch 203000, worst loss 0.060486 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:33:32,182 [INFO] Regularization: 2903.545410 * 0.0000010000 = 0.0029035455
2019-03-19 08:33:32,182 [INFO] Sum of grad norms: 0.027307
2019-03-19 08:33:32,183 [INFO] ---------------------------------
2019-03-19 08:33:51,073 [INFO] ---------------------------------
2019-03-19 08:33:51,074 [INFO] Summary:
2019-03-19 08:33:51,075 [INFO] Batch 204000, worst loss 0.060612 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:33:51,075 [INFO] Regularization: 2903.545166 * 0.0000010000 = 0.0029035450
2019-03-19 08:33:51,076 [INFO] Sum of grad norms: 0.027427
2019-03-19 08:33:51,076 [INFO] ---------------------------------
2019-03-19 08:34:10,006 [INFO] ---------------------------------
2019-03-19 08:34:10,007 [INFO] Summary:
2019-03-19 08:34:10,008 [INFO] Batch 205000, worst loss 0.060612 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:34:10,008 [INFO] Regularization: 2903.545166 * 0.0000010000 = 0.0029035450
2019-03-19 08:34:10,009 [INFO] Sum of grad norms: 0.033915
2019-03-19 08:34:10,010 [INFO] ---------------------------------
2019-03-19 08:34:28,924 [INFO] ---------------------------------
2019-03-19 08:34:28,925 [INFO] Summary:
2019-03-19 08:34:28,926 [INFO] Batch 206000, worst loss 0.060516 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:34:28,926 [INFO] Regularization: 2903.544922 * 0.0000010000 = 0.0029035448
2019-03-19 08:34:28,927 [INFO] Sum of grad norms: 0.039069
2019-03-19 08:34:28,927 [INFO] ---------------------------------
2019-03-19 08:34:47,761 [INFO] ---------------------------------
2019-03-19 08:34:47,762 [INFO] Summary:
2019-03-19 08:34:47,763 [INFO] Batch 207000, worst loss 0.060607 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:34:47,763 [INFO] Regularization: 2903.544922 * 0.0000010000 = 0.0029035448
2019-03-19 08:34:47,764 [INFO] Sum of grad norms: 0.039578
2019-03-19 08:34:47,764 [INFO] ---------------------------------
2019-03-19 08:35:06,213 [INFO] ---------------------------------
2019-03-19 08:35:06,214 [INFO] Summary:
2019-03-19 08:35:06,215 [INFO] Batch 208000, worst loss 0.060696 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:35:06,215 [INFO] Regularization: 2903.544678 * 0.0000010000 = 0.0029035446
2019-03-19 08:35:06,216 [INFO] Sum of grad norms: 0.027705
2019-03-19 08:35:06,216 [INFO] ---------------------------------
2019-03-19 08:35:25,032 [INFO] ---------------------------------
2019-03-19 08:35:25,033 [INFO] Summary:
2019-03-19 08:35:25,034 [INFO] Batch 209000, worst loss 0.060447 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:35:25,035 [INFO] Regularization: 2903.544678 * 0.0000010000 = 0.0029035446
2019-03-19 08:35:25,035 [INFO] Sum of grad norms: 0.018917
2019-03-19 08:35:25,037 [INFO] ---------------------------------
2019-03-19 08:35:43,367 [INFO] ---------------------------------
2019-03-19 08:35:43,368 [INFO] Summary:
2019-03-19 08:35:43,369 [INFO] Batch 210000, worst loss 0.060474 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:35:43,369 [INFO] Regularization: 2903.544922 * 0.0000010000 = 0.0029035448
2019-03-19 08:35:43,370 [INFO] Sum of grad norms: 0.017803
2019-03-19 08:35:43,370 [INFO] ---------------------------------
2019-03-19 08:35:48,298 [INFO] ---------------------------------
2019-03-19 08:35:48,299 [INFO] Evaluation:
2019-03-19 08:35:48,299 [INFO] Batch 210000, worst loss 0.057470 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:35:48,302 [INFO] ---------------------------------
2019-03-19 08:36:07,216 [INFO] ---------------------------------
2019-03-19 08:36:07,217 [INFO] Summary:
2019-03-19 08:36:07,217 [INFO] Batch 211000, worst loss 0.060590 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:36:07,218 [INFO] Regularization: 2903.544678 * 0.0000010000 = 0.0029035446
2019-03-19 08:36:07,218 [INFO] Sum of grad norms: 0.059843
2019-03-19 08:36:07,219 [INFO] ---------------------------------
2019-03-19 08:36:26,360 [INFO] ---------------------------------
2019-03-19 08:36:26,361 [INFO] Summary:
2019-03-19 08:36:26,362 [INFO] Batch 212000, worst loss 0.060768 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:36:26,363 [INFO] Regularization: 2903.544678 * 0.0000010000 = 0.0029035446
2019-03-19 08:36:26,363 [INFO] Sum of grad norms: 0.027069
2019-03-19 08:36:26,364 [INFO] ---------------------------------
2019-03-19 08:36:45,292 [INFO] ---------------------------------
2019-03-19 08:36:45,293 [INFO] Summary:
2019-03-19 08:36:45,294 [INFO] Batch 213000, worst loss 0.060679 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:36:45,294 [INFO] Regularization: 2903.544678 * 0.0000010000 = 0.0029035446
2019-03-19 08:36:45,295 [INFO] Sum of grad norms: 0.043115
2019-03-19 08:36:45,295 [INFO] ---------------------------------
2019-03-19 08:37:04,412 [INFO] ---------------------------------
2019-03-19 08:37:04,413 [INFO] Summary:
2019-03-19 08:37:04,413 [INFO] Batch 214000, worst loss 0.060679 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:37:04,414 [INFO] Regularization: 2903.544922 * 0.0000010000 = 0.0029035448
2019-03-19 08:37:04,414 [INFO] Sum of grad norms: 0.023842
2019-03-19 08:37:04,415 [INFO] ---------------------------------
2019-03-19 08:37:23,087 [INFO] ---------------------------------
2019-03-19 08:37:23,088 [INFO] Summary:
2019-03-19 08:37:23,089 [INFO] Batch 215000, worst loss 0.060471 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:37:23,089 [INFO] Regularization: 2903.544922 * 0.0000010000 = 0.0029035448
2019-03-19 08:37:23,090 [INFO] Sum of grad norms: 0.021792
2019-03-19 08:37:23,090 [INFO] ---------------------------------
2019-03-19 08:37:41,634 [INFO] ---------------------------------
2019-03-19 08:37:41,635 [INFO] Summary:
2019-03-19 08:37:41,635 [INFO] Batch 216000, worst loss 0.060589 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:37:41,636 [INFO] Regularization: 2903.544434 * 0.0000010000 = 0.0029035443
2019-03-19 08:37:41,636 [INFO] Sum of grad norms: 0.025742
2019-03-19 08:37:41,637 [INFO] ---------------------------------
2019-03-19 08:38:00,427 [INFO] ---------------------------------
2019-03-19 08:38:00,428 [INFO] Summary:
2019-03-19 08:38:00,429 [INFO] Batch 217000, worst loss 0.060546 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:38:00,429 [INFO] Regularization: 2903.544434 * 0.0000010000 = 0.0029035443
2019-03-19 08:38:00,430 [INFO] Sum of grad norms: 0.022751
2019-03-19 08:38:00,430 [INFO] ---------------------------------
2019-03-19 08:38:18,901 [INFO] ---------------------------------
2019-03-19 08:38:18,902 [INFO] Summary:
2019-03-19 08:38:18,903 [INFO] Batch 218000, worst loss 0.060461 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:38:18,904 [INFO] Regularization: 2903.544434 * 0.0000010000 = 0.0029035443
2019-03-19 08:38:18,904 [INFO] Sum of grad norms: 0.041759
2019-03-19 08:38:18,905 [INFO] ---------------------------------
2019-03-19 08:38:38,068 [INFO] ---------------------------------
2019-03-19 08:38:38,068 [INFO] Summary:
2019-03-19 08:38:38,069 [INFO] Batch 219000, worst loss 0.060655 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:38:38,069 [INFO] Regularization: 2903.544434 * 0.0000010000 = 0.0029035443
2019-03-19 08:38:38,070 [INFO] Sum of grad norms: 0.067423
2019-03-19 08:38:38,070 [INFO] ---------------------------------
2019-03-19 08:38:56,878 [INFO] ---------------------------------
2019-03-19 08:38:56,879 [INFO] Summary:
2019-03-19 08:38:56,880 [INFO] Batch 220000, worst loss 0.060756 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:38:56,880 [INFO] Regularization: 2903.544434 * 0.0000010000 = 0.0029035443
2019-03-19 08:38:56,881 [INFO] Sum of grad norms: 0.027608
2019-03-19 08:38:56,882 [INFO] ---------------------------------
2019-03-19 08:39:01,822 [INFO] ---------------------------------
2019-03-19 08:39:01,822 [INFO] Evaluation:
2019-03-19 08:39:01,823 [INFO] Batch 220000, worst loss 0.057761 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:39:01,824 [INFO] ---------------------------------
2019-03-19 08:39:20,546 [INFO] ---------------------------------
2019-03-19 08:39:20,546 [INFO] Summary:
2019-03-19 08:39:20,547 [INFO] Batch 221000, worst loss 0.060664 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:39:20,548 [INFO] Regularization: 2903.544434 * 0.0000010000 = 0.0029035443
2019-03-19 08:39:20,548 [INFO] Sum of grad norms: 0.034015
2019-03-19 08:39:20,549 [INFO] ---------------------------------
2019-03-19 08:39:39,447 [INFO] ---------------------------------
2019-03-19 08:39:39,448 [INFO] Summary:
2019-03-19 08:39:39,449 [INFO] Batch 222000, worst loss 0.060464 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:39:39,449 [INFO] Regularization: 2903.544434 * 0.0000010000 = 0.0029035443
2019-03-19 08:39:39,450 [INFO] Sum of grad norms: 0.023845
2019-03-19 08:39:39,450 [INFO] ---------------------------------
2019-03-19 08:39:57,939 [INFO] ---------------------------------
2019-03-19 08:39:57,939 [INFO] Summary:
2019-03-19 08:39:57,940 [INFO] Batch 223000, worst loss 0.060572 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:39:57,941 [INFO] Regularization: 2903.544434 * 0.0000010000 = 0.0029035443
2019-03-19 08:39:57,942 [INFO] Sum of grad norms: 0.047587
2019-03-19 08:39:57,943 [INFO] ---------------------------------
2019-03-19 08:40:16,686 [INFO] ---------------------------------
2019-03-19 08:40:16,687 [INFO] Summary:
2019-03-19 08:40:16,688 [INFO] Batch 224000, worst loss 0.060572 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:40:16,689 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:40:16,690 [INFO] Sum of grad norms: 0.036671
2019-03-19 08:40:16,691 [INFO] ---------------------------------
2019-03-19 08:40:35,263 [INFO] ---------------------------------
2019-03-19 08:40:35,264 [INFO] Summary:
2019-03-19 08:40:35,264 [INFO] Batch 225000, worst loss 0.060558 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:40:35,265 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:40:35,265 [INFO] Sum of grad norms: 0.022483
2019-03-19 08:40:35,266 [INFO] ---------------------------------
2019-03-19 08:40:53,630 [INFO] ---------------------------------
2019-03-19 08:40:53,632 [INFO] Summary:
2019-03-19 08:40:53,632 [INFO] Batch 226000, worst loss 0.060695 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:40:53,633 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:40:53,633 [INFO] Sum of grad norms: 0.026599
2019-03-19 08:40:53,634 [INFO] ---------------------------------
2019-03-19 08:41:12,419 [INFO] ---------------------------------
2019-03-19 08:41:12,420 [INFO] Summary:
2019-03-19 08:41:12,420 [INFO] Batch 227000, worst loss 0.060695 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:41:12,421 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:41:12,421 [INFO] Sum of grad norms: 0.020190
2019-03-19 08:41:12,422 [INFO] ---------------------------------
2019-03-19 08:41:31,438 [INFO] ---------------------------------
2019-03-19 08:41:31,439 [INFO] Summary:
2019-03-19 08:41:31,439 [INFO] Batch 228000, worst loss 0.060750 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:41:31,440 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:41:31,441 [INFO] Sum of grad norms: 0.034715
2019-03-19 08:41:31,441 [INFO] ---------------------------------
2019-03-19 08:41:49,827 [INFO] ---------------------------------
2019-03-19 08:41:49,828 [INFO] Summary:
2019-03-19 08:41:49,828 [INFO] Batch 229000, worst loss 0.060602 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:41:49,829 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:41:49,829 [INFO] Sum of grad norms: 0.050093
2019-03-19 08:41:49,830 [INFO] ---------------------------------
2019-03-19 08:42:08,764 [INFO] ---------------------------------
2019-03-19 08:42:08,765 [INFO] Summary:
2019-03-19 08:42:08,766 [INFO] Batch 230000, worst loss 0.060581 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:42:08,767 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:42:08,767 [INFO] Sum of grad norms: 0.040681
2019-03-19 08:42:08,768 [INFO] ---------------------------------
2019-03-19 08:42:13,740 [INFO] ---------------------------------
2019-03-19 08:42:13,740 [INFO] Evaluation:
2019-03-19 08:42:13,744 [INFO] Batch 230000, worst loss 0.057567 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:42:13,747 [INFO] ---------------------------------
2019-03-19 08:42:32,539 [INFO] ---------------------------------
2019-03-19 08:42:32,540 [INFO] Summary:
2019-03-19 08:42:32,541 [INFO] Batch 231000, worst loss 0.060514 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:42:32,541 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:42:32,542 [INFO] Sum of grad norms: 0.024667
2019-03-19 08:42:32,542 [INFO] ---------------------------------
2019-03-19 08:42:51,090 [INFO] ---------------------------------
2019-03-19 08:42:51,091 [INFO] Summary:
2019-03-19 08:42:51,092 [INFO] Batch 232000, worst loss 0.060518 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:42:51,092 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:42:51,093 [INFO] Sum of grad norms: 0.018198
2019-03-19 08:42:51,094 [INFO] ---------------------------------
2019-03-19 08:43:09,739 [INFO] ---------------------------------
2019-03-19 08:43:09,740 [INFO] Summary:
2019-03-19 08:43:09,741 [INFO] Batch 233000, worst loss 0.060636 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:43:09,742 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:43:09,742 [INFO] Sum of grad norms: 0.044272
2019-03-19 08:43:09,743 [INFO] ---------------------------------
2019-03-19 08:43:28,529 [INFO] ---------------------------------
2019-03-19 08:43:28,530 [INFO] Summary:
2019-03-19 08:43:28,530 [INFO] Batch 234000, worst loss 0.060681 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:43:28,531 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:43:28,531 [INFO] Sum of grad norms: 0.028920
2019-03-19 08:43:28,532 [INFO] ---------------------------------
2019-03-19 08:43:47,075 [INFO] ---------------------------------
2019-03-19 08:43:47,076 [INFO] Summary:
2019-03-19 08:43:47,077 [INFO] Batch 235000, worst loss 0.060681 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:43:47,077 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:43:47,078 [INFO] Sum of grad norms: 0.037174
2019-03-19 08:43:47,079 [INFO] ---------------------------------
2019-03-19 08:44:05,807 [INFO] ---------------------------------
2019-03-19 08:44:05,808 [INFO] Summary:
2019-03-19 08:44:05,809 [INFO] Batch 236000, worst loss 0.060619 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:44:05,809 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:44:05,810 [INFO] Sum of grad norms: 0.042028
2019-03-19 08:44:05,810 [INFO] ---------------------------------
2019-03-19 08:44:24,833 [INFO] ---------------------------------
2019-03-19 08:44:24,834 [INFO] Summary:
2019-03-19 08:44:24,835 [INFO] Batch 237000, worst loss 0.060762 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:44:24,835 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:44:24,836 [INFO] Sum of grad norms: 0.025224
2019-03-19 08:44:24,836 [INFO] ---------------------------------
2019-03-19 08:44:43,687 [INFO] ---------------------------------
2019-03-19 08:44:43,688 [INFO] Summary:
2019-03-19 08:44:43,689 [INFO] Batch 238000, worst loss 0.060762 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:44:43,689 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:44:43,690 [INFO] Sum of grad norms: 0.055720
2019-03-19 08:44:43,691 [INFO] ---------------------------------
2019-03-19 08:45:02,116 [INFO] ---------------------------------
2019-03-19 08:45:02,117 [INFO] Summary:
2019-03-19 08:45:02,118 [INFO] Batch 239000, worst loss 0.060601 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:45:02,118 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:45:02,119 [INFO] Sum of grad norms: 0.030340
2019-03-19 08:45:02,120 [INFO] ---------------------------------
2019-03-19 08:45:21,135 [INFO] ---------------------------------
2019-03-19 08:45:21,136 [INFO] Summary:
2019-03-19 08:45:21,137 [INFO] Batch 240000, worst loss 0.060493 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:45:21,137 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:45:21,138 [INFO] Sum of grad norms: 0.051139
2019-03-19 08:45:21,139 [INFO] ---------------------------------
2019-03-19 08:45:26,010 [INFO] ---------------------------------
2019-03-19 08:45:26,012 [INFO] Evaluation:
2019-03-19 08:45:26,012 [INFO] Batch 240000, worst loss 0.057745 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:45:26,013 [INFO] ---------------------------------
2019-03-19 08:45:44,807 [INFO] ---------------------------------
2019-03-19 08:45:44,808 [INFO] Summary:
2019-03-19 08:45:44,808 [INFO] Batch 241000, worst loss 0.060649 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:45:44,809 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:45:44,810 [INFO] Sum of grad norms: 0.019669
2019-03-19 08:45:44,810 [INFO] ---------------------------------
2019-03-19 08:46:03,481 [INFO] ---------------------------------
2019-03-19 08:46:03,482 [INFO] Summary:
2019-03-19 08:46:03,483 [INFO] Batch 242000, worst loss 0.060630 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:46:03,483 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:46:03,484 [INFO] Sum of grad norms: 0.026395
2019-03-19 08:46:03,484 [INFO] ---------------------------------
2019-03-19 08:46:22,289 [INFO] ---------------------------------
2019-03-19 08:46:22,290 [INFO] Summary:
2019-03-19 08:46:22,291 [INFO] Batch 243000, worst loss 0.060630 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:46:22,292 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:46:22,292 [INFO] Sum of grad norms: 0.025119
2019-03-19 08:46:22,293 [INFO] ---------------------------------
2019-03-19 08:46:40,995 [INFO] ---------------------------------
2019-03-19 08:46:40,995 [INFO] Summary:
2019-03-19 08:46:40,996 [INFO] Batch 244000, worst loss 0.060393 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:46:40,997 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:46:40,997 [INFO] Sum of grad norms: 0.045406
2019-03-19 08:46:40,998 [INFO] ---------------------------------
2019-03-19 08:46:59,933 [INFO] ---------------------------------
2019-03-19 08:46:59,934 [INFO] Summary:
2019-03-19 08:46:59,935 [INFO] Batch 245000, worst loss 0.060498 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:46:59,935 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:46:59,936 [INFO] Sum of grad norms: 0.020560
2019-03-19 08:46:59,937 [INFO] ---------------------------------
2019-03-19 08:47:18,522 [INFO] ---------------------------------
2019-03-19 08:47:18,523 [INFO] Summary:
2019-03-19 08:47:18,524 [INFO] Batch 246000, worst loss 0.060550 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:47:18,524 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:47:18,525 [INFO] Sum of grad norms: 0.033714
2019-03-19 08:47:18,525 [INFO] ---------------------------------
2019-03-19 08:47:37,155 [INFO] ---------------------------------
2019-03-19 08:47:37,156 [INFO] Summary:
2019-03-19 08:47:37,156 [INFO] Batch 247000, worst loss 0.060550 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:47:37,157 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:47:37,157 [INFO] Sum of grad norms: 0.024664
2019-03-19 08:47:37,158 [INFO] ---------------------------------
2019-03-19 08:47:55,427 [INFO] ---------------------------------
2019-03-19 08:47:55,428 [INFO] Summary:
2019-03-19 08:47:55,428 [INFO] Batch 248000, worst loss 0.060486 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:47:55,429 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:47:55,430 [INFO] Sum of grad norms: 0.019155
2019-03-19 08:47:55,430 [INFO] ---------------------------------
2019-03-19 08:48:13,934 [INFO] ---------------------------------
2019-03-19 08:48:13,935 [INFO] Summary:
2019-03-19 08:48:13,936 [INFO] Batch 249000, worst loss 0.060486 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:48:13,937 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:48:13,937 [INFO] Sum of grad norms: 0.025832
2019-03-19 08:48:13,938 [INFO] ---------------------------------
2019-03-19 08:48:32,731 [INFO] ---------------------------------
2019-03-19 08:48:32,731 [INFO] Summary:
2019-03-19 08:48:32,732 [INFO] Batch 250000, worst loss 0.060557 (incl. reg.) of 1000 batches, learning rate 0.000000 @cl.-depth 1
2019-03-19 08:48:32,732 [INFO] Regularization: 2903.544189 * 0.0000010000 = 0.0029035441
2019-03-19 08:48:32,733 [INFO] Sum of grad norms: 0.040533
2019-03-19 08:48:32,733 [INFO] ---------------------------------
2019-03-19 08:48:37,702 [INFO] ---------------------------------
2019-03-19 08:48:37,704 [INFO] Evaluation:
2019-03-19 08:48:37,704 [INFO] Batch 250000, worst loss 0.057528 (without reg.) of 1000 batches @cl.-depth 1
2019-03-19 08:48:37,705 [INFO] ---------------------------------
2019-03-19 08:48:37,705 [INFO] Finished training, saved to file classifier/1552933539/1552981717_9_classifier_final.pth
