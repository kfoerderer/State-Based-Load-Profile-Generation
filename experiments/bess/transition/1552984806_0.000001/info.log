2019-03-19 09:40:06,363 [INFO] batch_size: 32
2019-03-19 09:40:06,364 [INFO] learning_rate_initialization: 0.001000, learning_rate_loss_factor: 1.000000, learning_rate_decay_after: 50000, learning_rate_decay_at: 10000, learning_rate_decay_factor: 0.500000
2019-03-19 09:40:06,368 [INFO] weigths: tensor([20.,  1.,  1., 20., 80., 10., 10.,  1.,  1.], device='cuda:0')
2019-03-19 09:40:06,370 [INFO] regularization factor: 0.0000000100000000
2019-03-19 09:40:06,371 [INFO] unfolding_after: 1000, unfolding_at: 1000, unfolding_delta: 1, unfolding_share: 0.000000
2019-03-19 09:40:06,525 [INFO] ---------------------------------
2019-03-19 09:40:06,527 [INFO] Training model #0: (203, 64, 1) @ 2
2019-03-19 09:40:28,161 [INFO] ---------------------------------
2019-03-19 09:40:28,162 [INFO] Summary:
2019-03-19 09:40:28,162 [INFO] Batch 1000, worst loss 16.179588 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:40:28,163 [INFO] Regularization: 5715.272461 * 0.0000000100 = 0.0000571527 loss
2019-03-19 09:40:28,163 [INFO] unfolding 0, single step 1001
2019-03-19 09:40:28,164 [INFO] Sum of grad norms of most recent batch: 7.199772
2019-03-19 09:40:28,164 [INFO] ---------------------------------
2019-03-19 09:40:49,836 [INFO] ---------------------------------
2019-03-19 09:40:49,837 [INFO] Summary:
2019-03-19 09:40:49,837 [INFO] Batch 2000, worst loss 0.042063 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:40:49,838 [INFO] Regularization: 3047.364746 * 0.0000000100 = 0.0000304736 loss
2019-03-19 09:40:49,838 [INFO] unfolding 0, single step 2001
2019-03-19 09:40:49,839 [INFO] Sum of grad norms of most recent batch: 4.033135
2019-03-19 09:40:49,839 [INFO] ---------------------------------
2019-03-19 09:40:55,662 [INFO] ---------------------------------
2019-03-19 09:40:55,662 [INFO] Evaluation:
2019-03-19 09:40:55,663 [INFO] Batch 2000, worst loss 0.013358 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 09:40:55,664 [INFO] New best loss 0.013358, saved to file transition/1552984806/1552984855_0_transition_2000.pth
2019-03-19 09:40:55,676 [INFO] Target
2019-03-19 09:40:55,677 [INFO] [[0.4698]
 [0.382 ]
 [0.2652]
 [0.6372]
 [0.4574]
 [0.1036]
 [0.3412]
 [0.5132]
 [0.2082]
 [0.6706]
 [0.6228]
 [0.7688]
 [0.5351]
 [0.8235]
 [0.4245]
 [0.1485]
 [0.1652]
 [0.3141]
 [0.5928]
 [0.6599]
 [0.7722]
 [0.6116]
 [0.2836]
 [0.2386]
 [0.38  ]
 [0.0057]
 [0.4759]
 [0.5144]
 [0.1114]
 [0.0535]
 [0.61  ]
 [0.7526]]
2019-03-19 09:40:55,680 [INFO] Estimator output
2019-03-19 09:40:55,680 [INFO] [[0.474099]
 [0.376108]
 [0.262359]
 [0.634972]
 [0.455714]
 [0.1     ]
 [0.337606]
 [0.509434]
 [0.209228]
 [0.666654]
 [0.623369]
 [0.769492]
 [0.532598]
 [0.82513 ]
 [0.423457]
 [0.141895]
 [0.162296]
 [0.31575 ]
 [0.596806]
 [0.659405]
 [0.775932]
 [0.614653]
 [0.280115]
 [0.235088]
 [0.389669]
 [0.004216]
 [0.474647]
 [0.519855]
 [0.112059]
 [0.055334]
 [0.619816]
 [0.758685]]
2019-03-19 09:40:55,682 [INFO] ---------------------------------
2019-03-19 09:41:17,226 [INFO] ---------------------------------
2019-03-19 09:41:17,227 [INFO] Summary:
2019-03-19 09:41:17,228 [INFO] Batch 3000, worst loss 0.077419 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:41:17,229 [INFO] Regularization: 2806.902344 * 0.0000000100 = 0.0000280690 loss
2019-03-19 09:41:17,229 [INFO] unfolding 0, single step 3001
2019-03-19 09:41:17,229 [INFO] Sum of grad norms of most recent batch: 4.101258
2019-03-19 09:41:17,230 [INFO] ---------------------------------
2019-03-19 09:41:38,740 [INFO] ---------------------------------
2019-03-19 09:41:38,741 [INFO] Summary:
2019-03-19 09:41:38,741 [INFO] Batch 4000, worst loss 0.028585 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:41:38,742 [INFO] Regularization: 2623.498779 * 0.0000000100 = 0.0000262350 loss
2019-03-19 09:41:38,742 [INFO] unfolding 0, single step 4001
2019-03-19 09:41:38,743 [INFO] Sum of grad norms of most recent batch: 1.338378
2019-03-19 09:41:38,743 [INFO] ---------------------------------
2019-03-19 09:42:00,313 [INFO] ---------------------------------
2019-03-19 09:42:00,314 [INFO] Summary:
2019-03-19 09:42:00,314 [INFO] Batch 5000, worst loss 0.046391 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:42:00,315 [INFO] Regularization: 2437.356201 * 0.0000000100 = 0.0000243736 loss
2019-03-19 09:42:00,315 [INFO] unfolding 0, single step 5001
2019-03-19 09:42:00,316 [INFO] Sum of grad norms of most recent batch: 0.819008
2019-03-19 09:42:00,316 [INFO] ---------------------------------
2019-03-19 09:42:21,532 [INFO] ---------------------------------
2019-03-19 09:42:21,533 [INFO] Summary:
2019-03-19 09:42:21,534 [INFO] Batch 6000, worst loss 0.024202 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:42:21,534 [INFO] Regularization: 2246.143555 * 0.0000000100 = 0.0000224614 loss
2019-03-19 09:42:21,535 [INFO] unfolding 0, single step 6001
2019-03-19 09:42:21,535 [INFO] Sum of grad norms of most recent batch: 1.609934
2019-03-19 09:42:21,536 [INFO] ---------------------------------
2019-03-19 09:42:42,984 [INFO] ---------------------------------
2019-03-19 09:42:42,985 [INFO] Summary:
2019-03-19 09:42:42,986 [INFO] Batch 7000, worst loss 0.029182 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:42:42,986 [INFO] Regularization: 2067.870605 * 0.0000000100 = 0.0000206787 loss
2019-03-19 09:42:42,987 [INFO] unfolding 0, single step 7001
2019-03-19 09:42:42,987 [INFO] Sum of grad norms of most recent batch: 1.425667
2019-03-19 09:42:42,988 [INFO] ---------------------------------
2019-03-19 09:43:04,732 [INFO] ---------------------------------
2019-03-19 09:43:04,733 [INFO] Summary:
2019-03-19 09:43:04,734 [INFO] Batch 8000, worst loss 0.036324 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:43:04,734 [INFO] Regularization: 1884.713623 * 0.0000000100 = 0.0000188471 loss
2019-03-19 09:43:04,735 [INFO] unfolding 0, single step 8001
2019-03-19 09:43:04,735 [INFO] Sum of grad norms of most recent batch: 2.846223
2019-03-19 09:43:04,736 [INFO] ---------------------------------
2019-03-19 09:43:26,320 [INFO] ---------------------------------
2019-03-19 09:43:26,321 [INFO] Summary:
2019-03-19 09:43:26,322 [INFO] Batch 9000, worst loss 0.019907 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:43:26,322 [INFO] Regularization: 1746.675781 * 0.0000000100 = 0.0000174668 loss
2019-03-19 09:43:26,323 [INFO] unfolding 0, single step 9001
2019-03-19 09:43:26,323 [INFO] Sum of grad norms of most recent batch: 0.755054
2019-03-19 09:43:26,324 [INFO] ---------------------------------
2019-03-19 09:43:47,633 [INFO] ---------------------------------
2019-03-19 09:43:47,634 [INFO] Summary:
2019-03-19 09:43:47,634 [INFO] Batch 10000, worst loss 0.022667 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:43:47,635 [INFO] Regularization: 1621.580688 * 0.0000000100 = 0.0000162158 loss
2019-03-19 09:43:47,635 [INFO] unfolding 0, single step 10001
2019-03-19 09:43:47,636 [INFO] Sum of grad norms of most recent batch: 3.322663
2019-03-19 09:43:47,637 [INFO] ---------------------------------
2019-03-19 09:43:53,422 [INFO] ---------------------------------
2019-03-19 09:43:53,423 [INFO] Evaluation:
2019-03-19 09:43:53,424 [INFO] Batch 10000, worst loss 0.003291 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 09:43:53,425 [INFO] New best loss 0.003291, saved to file transition/1552984806/1552985033_0_transition_10000.pth
2019-03-19 09:43:53,437 [INFO] Target
2019-03-19 09:43:53,438 [INFO] [[0.9681]
 [0.9616]
 [0.2422]
 [0.5874]
 [0.948 ]
 [0.3746]
 [0.0178]
 [0.9021]
 [0.5285]
 [0.6822]
 [0.4316]
 [0.758 ]
 [0.6998]
 [0.2147]
 [0.4474]
 [0.4938]
 [0.3268]
 [0.3381]
 [0.7518]
 [0.176 ]
 [0.7139]
 [0.06  ]
 [0.1458]
 [0.8552]
 [0.0395]
 [0.4998]
 [0.238 ]
 [0.7541]
 [0.6194]
 [0.1941]
 [0.5576]
 [0.3094]]
2019-03-19 09:43:53,440 [INFO] Estimator output
2019-03-19 09:43:53,441 [INFO] [[0.968559]
 [0.967222]
 [0.242043]
 [0.589587]
 [0.94639 ]
 [0.372692]
 [0.018587]
 [0.900483]
 [0.527431]
 [0.680782]
 [0.43714 ]
 [0.75636 ]
 [0.696216]
 [0.216588]
 [0.445605]
 [0.492607]
 [0.327703]
 [0.338229]
 [0.752564]
 [0.174815]
 [0.711695]
 [0.060734]
 [0.144191]
 [0.854294]
 [0.039494]
 [0.496481]
 [0.237473]
 [0.75486 ]
 [0.619529]
 [0.194775]
 [0.558176]
 [0.309489]]
2019-03-19 09:43:53,443 [INFO] ---------------------------------
2019-03-19 09:44:14,560 [INFO] ---------------------------------
2019-03-19 09:44:14,561 [INFO] Summary:
2019-03-19 09:44:14,562 [INFO] Batch 11000, worst loss 0.024789 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:44:14,562 [INFO] Regularization: 1535.280640 * 0.0000000100 = 0.0000153528 loss
2019-03-19 09:44:14,563 [INFO] unfolding 0, single step 11001
2019-03-19 09:44:14,563 [INFO] Sum of grad norms of most recent batch: 0.841202
2019-03-19 09:44:14,564 [INFO] ---------------------------------
2019-03-19 09:44:36,271 [INFO] ---------------------------------
2019-03-19 09:44:36,272 [INFO] Summary:
2019-03-19 09:44:36,273 [INFO] Batch 12000, worst loss 0.018361 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:44:36,273 [INFO] Regularization: 1445.229736 * 0.0000000100 = 0.0000144523 loss
2019-03-19 09:44:36,274 [INFO] unfolding 0, single step 12001
2019-03-19 09:44:36,275 [INFO] Sum of grad norms of most recent batch: 1.900597
2019-03-19 09:44:36,275 [INFO] ---------------------------------
2019-03-19 09:44:57,924 [INFO] ---------------------------------
2019-03-19 09:44:57,925 [INFO] Summary:
2019-03-19 09:44:57,926 [INFO] Batch 13000, worst loss 0.024538 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:44:57,926 [INFO] Regularization: 1357.876709 * 0.0000000100 = 0.0000135788 loss
2019-03-19 09:44:57,927 [INFO] unfolding 0, single step 13001
2019-03-19 09:44:57,927 [INFO] Sum of grad norms of most recent batch: 2.523396
2019-03-19 09:44:57,928 [INFO] ---------------------------------
2019-03-19 09:45:19,316 [INFO] ---------------------------------
2019-03-19 09:45:19,317 [INFO] Summary:
2019-03-19 09:45:19,318 [INFO] Batch 14000, worst loss 0.026459 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:45:19,318 [INFO] Regularization: 1295.162354 * 0.0000000100 = 0.0000129516 loss
2019-03-19 09:45:19,319 [INFO] unfolding 0, single step 14001
2019-03-19 09:45:19,319 [INFO] Sum of grad norms of most recent batch: 1.570349
2019-03-19 09:45:19,320 [INFO] ---------------------------------
2019-03-19 09:45:41,327 [INFO] ---------------------------------
2019-03-19 09:45:41,328 [INFO] Summary:
2019-03-19 09:45:41,330 [INFO] Batch 15000, worst loss 0.013714 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:45:41,330 [INFO] Regularization: 1221.033081 * 0.0000000100 = 0.0000122103 loss
2019-03-19 09:45:41,330 [INFO] unfolding 0, single step 15001
2019-03-19 09:45:41,331 [INFO] Sum of grad norms of most recent batch: 2.099933
2019-03-19 09:45:41,332 [INFO] ---------------------------------
2019-03-19 09:46:02,575 [INFO] ---------------------------------
2019-03-19 09:46:02,575 [INFO] Summary:
2019-03-19 09:46:02,576 [INFO] Batch 16000, worst loss 0.018506 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:46:02,577 [INFO] Regularization: 1173.039307 * 0.0000000100 = 0.0000117304 loss
2019-03-19 09:46:02,577 [INFO] unfolding 0, single step 16001
2019-03-19 09:46:02,578 [INFO] Sum of grad norms of most recent batch: 4.325932
2019-03-19 09:46:02,578 [INFO] ---------------------------------
2019-03-19 09:46:24,311 [INFO] ---------------------------------
2019-03-19 09:46:24,312 [INFO] Summary:
2019-03-19 09:46:24,312 [INFO] Batch 17000, worst loss 0.022138 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:46:24,313 [INFO] Regularization: 1139.350220 * 0.0000000100 = 0.0000113935 loss
2019-03-19 09:46:24,313 [INFO] unfolding 0, single step 17001
2019-03-19 09:46:24,314 [INFO] Sum of grad norms of most recent batch: 10.297942
2019-03-19 09:46:24,314 [INFO] ---------------------------------
2019-03-19 09:46:45,945 [INFO] ---------------------------------
2019-03-19 09:46:45,946 [INFO] Summary:
2019-03-19 09:46:45,947 [INFO] Batch 18000, worst loss 0.021986 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:46:45,948 [INFO] Regularization: 1109.603638 * 0.0000000100 = 0.0000110960 loss
2019-03-19 09:46:45,948 [INFO] unfolding 0, single step 18001
2019-03-19 09:46:45,950 [INFO] Sum of grad norms of most recent batch: 1.152068
2019-03-19 09:46:45,951 [INFO] ---------------------------------
2019-03-19 09:47:07,579 [INFO] ---------------------------------
2019-03-19 09:47:07,580 [INFO] Summary:
2019-03-19 09:47:07,580 [INFO] Batch 19000, worst loss 0.012761 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:47:07,581 [INFO] Regularization: 1094.215088 * 0.0000000100 = 0.0000109422 loss
2019-03-19 09:47:07,581 [INFO] unfolding 0, single step 19001
2019-03-19 09:47:07,582 [INFO] Sum of grad norms of most recent batch: 1.833097
2019-03-19 09:47:07,583 [INFO] ---------------------------------
2019-03-19 09:47:29,338 [INFO] ---------------------------------
2019-03-19 09:47:29,339 [INFO] Summary:
2019-03-19 09:47:29,339 [INFO] Batch 20000, worst loss 0.015157 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:47:29,340 [INFO] Regularization: 1056.321411 * 0.0000000100 = 0.0000105632 loss
2019-03-19 09:47:29,340 [INFO] unfolding 0, single step 20001
2019-03-19 09:47:29,341 [INFO] Sum of grad norms of most recent batch: 1.045557
2019-03-19 09:47:29,342 [INFO] ---------------------------------
2019-03-19 09:47:35,187 [INFO] ---------------------------------
2019-03-19 09:47:35,188 [INFO] Evaluation:
2019-03-19 09:47:35,190 [INFO] Batch 20000, worst loss 0.002795 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 09:47:35,192 [INFO] New best loss 0.002795, saved to file transition/1552984806/1552985255_0_transition_20000.pth
2019-03-19 09:47:35,205 [INFO] Target
2019-03-19 09:47:35,206 [INFO] [[0.7191]
 [0.8649]
 [0.3468]
 [0.3046]
 [0.7047]
 [0.9082]
 [0.2793]
 [0.6186]
 [0.6334]
 [0.4304]
 [0.203 ]
 [0.3634]
 [0.0834]
 [0.0535]
 [0.0265]
 [0.063 ]
 [0.6452]
 [0.387 ]
 [0.3051]
 [0.062 ]
 [0.1475]
 [0.8754]
 [0.7186]
 [0.8528]
 [0.3356]
 [0.5767]
 [0.5586]
 [0.5847]
 [0.5928]
 [0.2264]
 [0.1773]
 [0.7426]]
2019-03-19 09:47:35,208 [INFO] Estimator output
2019-03-19 09:47:35,209 [INFO] [[0.717562]
 [0.862108]
 [0.346917]
 [0.305473]
 [0.701565]
 [0.906928]
 [0.278898]
 [0.618371]
 [0.633515]
 [0.433683]
 [0.203888]
 [0.363907]
 [0.083791]
 [0.053118]
 [0.024532]
 [0.063959]
 [0.645414]
 [0.386999]
 [0.304722]
 [0.061738]
 [0.150746]
 [0.880621]
 [0.716973]
 [0.853241]
 [0.336328]
 [0.572288]
 [0.558647]
 [0.585424]
 [0.590901]
 [0.226568]
 [0.177728]
 [0.740608]]
2019-03-19 09:47:35,211 [INFO] ---------------------------------
2019-03-19 09:47:56,586 [INFO] ---------------------------------
2019-03-19 09:47:56,587 [INFO] Summary:
2019-03-19 09:47:56,587 [INFO] Batch 21000, worst loss 0.014042 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:47:56,588 [INFO] Regularization: 1092.440674 * 0.0000000100 = 0.0000109244 loss
2019-03-19 09:47:56,588 [INFO] unfolding 0, single step 21001
2019-03-19 09:47:56,589 [INFO] Sum of grad norms of most recent batch: 0.758710
2019-03-19 09:47:56,590 [INFO] ---------------------------------
2019-03-19 09:48:17,946 [INFO] ---------------------------------
2019-03-19 09:48:17,947 [INFO] Summary:
2019-03-19 09:48:17,948 [INFO] Batch 22000, worst loss 0.014483 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:48:17,948 [INFO] Regularization: 1064.975098 * 0.0000000100 = 0.0000106498 loss
2019-03-19 09:48:17,948 [INFO] unfolding 0, single step 22001
2019-03-19 09:48:17,949 [INFO] Sum of grad norms of most recent batch: 0.896570
2019-03-19 09:48:17,949 [INFO] ---------------------------------
2019-03-19 09:48:39,624 [INFO] ---------------------------------
2019-03-19 09:48:39,624 [INFO] Summary:
2019-03-19 09:48:39,625 [INFO] Batch 23000, worst loss 0.014566 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:48:39,627 [INFO] Regularization: 1056.924927 * 0.0000000100 = 0.0000105692 loss
2019-03-19 09:48:39,627 [INFO] unfolding 0, single step 23001
2019-03-19 09:48:39,628 [INFO] Sum of grad norms of most recent batch: 2.367843
2019-03-19 09:48:39,629 [INFO] ---------------------------------
2019-03-19 09:49:00,668 [INFO] ---------------------------------
2019-03-19 09:49:00,669 [INFO] Summary:
2019-03-19 09:49:00,670 [INFO] Batch 24000, worst loss 0.011144 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:49:00,671 [INFO] Regularization: 1047.242798 * 0.0000000100 = 0.0000104724 loss
2019-03-19 09:49:00,671 [INFO] unfolding 0, single step 24001
2019-03-19 09:49:00,672 [INFO] Sum of grad norms of most recent batch: 0.738779
2019-03-19 09:49:00,673 [INFO] ---------------------------------
2019-03-19 09:49:22,118 [INFO] ---------------------------------
2019-03-19 09:49:22,119 [INFO] Summary:
2019-03-19 09:49:22,120 [INFO] Batch 25000, worst loss 0.013052 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:49:22,120 [INFO] Regularization: 1030.992920 * 0.0000000100 = 0.0000103099 loss
2019-03-19 09:49:22,121 [INFO] unfolding 0, single step 25001
2019-03-19 09:49:22,121 [INFO] Sum of grad norms of most recent batch: 1.378161
2019-03-19 09:49:22,122 [INFO] ---------------------------------
2019-03-19 09:49:43,630 [INFO] ---------------------------------
2019-03-19 09:49:43,631 [INFO] Summary:
2019-03-19 09:49:43,632 [INFO] Batch 26000, worst loss 0.021135 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:49:43,632 [INFO] Regularization: 1027.308960 * 0.0000000100 = 0.0000102731 loss
2019-03-19 09:49:43,633 [INFO] unfolding 0, single step 26001
2019-03-19 09:49:43,633 [INFO] Sum of grad norms of most recent batch: 3.157062
2019-03-19 09:49:43,634 [INFO] ---------------------------------
2019-03-19 09:50:05,045 [INFO] ---------------------------------
2019-03-19 09:50:05,047 [INFO] Summary:
2019-03-19 09:50:05,047 [INFO] Batch 27000, worst loss 0.022052 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:50:05,048 [INFO] Regularization: 1010.005554 * 0.0000000100 = 0.0000101001 loss
2019-03-19 09:50:05,048 [INFO] unfolding 0, single step 27001
2019-03-19 09:50:05,049 [INFO] Sum of grad norms of most recent batch: 3.693418
2019-03-19 09:50:05,049 [INFO] ---------------------------------
2019-03-19 09:50:26,532 [INFO] ---------------------------------
2019-03-19 09:50:26,534 [INFO] Summary:
2019-03-19 09:50:26,534 [INFO] Batch 28000, worst loss 0.011040 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:50:26,535 [INFO] Regularization: 1011.530762 * 0.0000000100 = 0.0000101153 loss
2019-03-19 09:50:26,536 [INFO] unfolding 0, single step 28001
2019-03-19 09:50:26,538 [INFO] Sum of grad norms of most recent batch: 3.641351
2019-03-19 09:50:26,538 [INFO] ---------------------------------
2019-03-19 09:50:48,051 [INFO] ---------------------------------
2019-03-19 09:50:48,052 [INFO] Summary:
2019-03-19 09:50:48,053 [INFO] Batch 29000, worst loss 0.013560 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:50:48,053 [INFO] Regularization: 1016.519104 * 0.0000000100 = 0.0000101652 loss
2019-03-19 09:50:48,054 [INFO] unfolding 0, single step 29001
2019-03-19 09:50:48,054 [INFO] Sum of grad norms of most recent batch: 0.816953
2019-03-19 09:50:48,055 [INFO] ---------------------------------
2019-03-19 09:51:09,641 [INFO] ---------------------------------
2019-03-19 09:51:09,642 [INFO] Summary:
2019-03-19 09:51:09,642 [INFO] Batch 30000, worst loss 0.018008 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:51:09,643 [INFO] Regularization: 1000.684448 * 0.0000000100 = 0.0000100068 loss
2019-03-19 09:51:09,643 [INFO] unfolding 0, single step 30001
2019-03-19 09:51:09,644 [INFO] Sum of grad norms of most recent batch: 0.840714
2019-03-19 09:51:09,644 [INFO] ---------------------------------
2019-03-19 09:51:15,412 [INFO] ---------------------------------
2019-03-19 09:51:15,413 [INFO] Evaluation:
2019-03-19 09:51:15,415 [INFO] Batch 30000, worst loss 0.005209 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 09:51:15,417 [INFO] ---------------------------------
2019-03-19 09:51:36,953 [INFO] ---------------------------------
2019-03-19 09:51:36,954 [INFO] Summary:
2019-03-19 09:51:36,955 [INFO] Batch 31000, worst loss 0.012239 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:51:36,955 [INFO] Regularization: 977.093994 * 0.0000000100 = 0.0000097709 loss
2019-03-19 09:51:36,956 [INFO] unfolding 0, single step 31001
2019-03-19 09:51:36,956 [INFO] Sum of grad norms of most recent batch: 1.928134
2019-03-19 09:51:36,957 [INFO] ---------------------------------
2019-03-19 09:51:58,560 [INFO] ---------------------------------
2019-03-19 09:51:58,561 [INFO] Summary:
2019-03-19 09:51:58,562 [INFO] Batch 32000, worst loss 0.010545 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:51:58,562 [INFO] Regularization: 981.511292 * 0.0000000100 = 0.0000098151 loss
2019-03-19 09:51:58,562 [INFO] unfolding 0, single step 32001
2019-03-19 09:51:58,563 [INFO] Sum of grad norms of most recent batch: 0.971889
2019-03-19 09:51:58,564 [INFO] ---------------------------------
2019-03-19 09:52:20,105 [INFO] ---------------------------------
2019-03-19 09:52:20,107 [INFO] Summary:
2019-03-19 09:52:20,107 [INFO] Batch 33000, worst loss 0.011510 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:52:20,108 [INFO] Regularization: 973.430115 * 0.0000000100 = 0.0000097343 loss
2019-03-19 09:52:20,108 [INFO] unfolding 0, single step 33001
2019-03-19 09:52:20,109 [INFO] Sum of grad norms of most recent batch: 3.505357
2019-03-19 09:52:20,109 [INFO] ---------------------------------
2019-03-19 09:52:41,654 [INFO] ---------------------------------
2019-03-19 09:52:41,655 [INFO] Summary:
2019-03-19 09:52:41,656 [INFO] Batch 34000, worst loss 0.010800 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:52:41,656 [INFO] Regularization: 977.517578 * 0.0000000100 = 0.0000097752 loss
2019-03-19 09:52:41,657 [INFO] unfolding 0, single step 34001
2019-03-19 09:52:41,657 [INFO] Sum of grad norms of most recent batch: 1.053678
2019-03-19 09:52:41,658 [INFO] ---------------------------------
2019-03-19 09:53:03,404 [INFO] ---------------------------------
2019-03-19 09:53:03,405 [INFO] Summary:
2019-03-19 09:53:03,405 [INFO] Batch 35000, worst loss 0.009213 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:53:03,406 [INFO] Regularization: 942.848328 * 0.0000000100 = 0.0000094285 loss
2019-03-19 09:53:03,406 [INFO] unfolding 0, single step 35001
2019-03-19 09:53:03,407 [INFO] Sum of grad norms of most recent batch: 1.512059
2019-03-19 09:53:03,407 [INFO] ---------------------------------
2019-03-19 09:53:24,799 [INFO] ---------------------------------
2019-03-19 09:53:24,800 [INFO] Summary:
2019-03-19 09:53:24,801 [INFO] Batch 36000, worst loss 0.007911 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:53:24,801 [INFO] Regularization: 951.269531 * 0.0000000100 = 0.0000095127 loss
2019-03-19 09:53:24,802 [INFO] unfolding 0, single step 36001
2019-03-19 09:53:24,802 [INFO] Sum of grad norms of most recent batch: 0.754384
2019-03-19 09:53:24,803 [INFO] ---------------------------------
2019-03-19 09:53:46,108 [INFO] ---------------------------------
2019-03-19 09:53:46,109 [INFO] Summary:
2019-03-19 09:53:46,110 [INFO] Batch 37000, worst loss 0.011755 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:53:46,110 [INFO] Regularization: 918.240967 * 0.0000000100 = 0.0000091824 loss
2019-03-19 09:53:46,111 [INFO] unfolding 0, single step 37001
2019-03-19 09:53:46,111 [INFO] Sum of grad norms of most recent batch: 2.542893
2019-03-19 09:53:46,112 [INFO] ---------------------------------
2019-03-19 09:54:07,816 [INFO] ---------------------------------
2019-03-19 09:54:07,817 [INFO] Summary:
2019-03-19 09:54:07,818 [INFO] Batch 38000, worst loss 0.008594 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:54:07,818 [INFO] Regularization: 932.906006 * 0.0000000100 = 0.0000093291 loss
2019-03-19 09:54:07,818 [INFO] unfolding 0, single step 38001
2019-03-19 09:54:07,819 [INFO] Sum of grad norms of most recent batch: 1.476269
2019-03-19 09:54:07,820 [INFO] ---------------------------------
2019-03-19 09:54:29,292 [INFO] ---------------------------------
2019-03-19 09:54:29,292 [INFO] Summary:
2019-03-19 09:54:29,293 [INFO] Batch 39000, worst loss 0.014843 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:54:29,293 [INFO] Regularization: 927.328552 * 0.0000000100 = 0.0000092733 loss
2019-03-19 09:54:29,293 [INFO] unfolding 0, single step 39001
2019-03-19 09:54:29,294 [INFO] Sum of grad norms of most recent batch: 0.463524
2019-03-19 09:54:29,295 [INFO] ---------------------------------
2019-03-19 09:54:51,126 [INFO] ---------------------------------
2019-03-19 09:54:51,127 [INFO] Summary:
2019-03-19 09:54:51,128 [INFO] Batch 40000, worst loss 0.012636 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:54:51,128 [INFO] Regularization: 943.628113 * 0.0000000100 = 0.0000094363 loss
2019-03-19 09:54:51,129 [INFO] unfolding 0, single step 40001
2019-03-19 09:54:51,129 [INFO] Sum of grad norms of most recent batch: 1.990589
2019-03-19 09:54:51,130 [INFO] ---------------------------------
2019-03-19 09:54:56,978 [INFO] ---------------------------------
2019-03-19 09:54:56,979 [INFO] Evaluation:
2019-03-19 09:54:56,980 [INFO] Batch 40000, worst loss 0.007696 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 09:54:56,981 [INFO] ---------------------------------
2019-03-19 09:55:18,459 [INFO] ---------------------------------
2019-03-19 09:55:18,460 [INFO] Summary:
2019-03-19 09:55:18,461 [INFO] Batch 41000, worst loss 0.015310 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:55:18,462 [INFO] Regularization: 907.717529 * 0.0000000100 = 0.0000090772 loss
2019-03-19 09:55:18,462 [INFO] unfolding 0, single step 41001
2019-03-19 09:55:18,463 [INFO] Sum of grad norms of most recent batch: 1.635351
2019-03-19 09:55:18,463 [INFO] ---------------------------------
2019-03-19 09:55:39,887 [INFO] ---------------------------------
2019-03-19 09:55:39,888 [INFO] Summary:
2019-03-19 09:55:39,889 [INFO] Batch 42000, worst loss 0.008590 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:55:39,889 [INFO] Regularization: 881.680420 * 0.0000000100 = 0.0000088168 loss
2019-03-19 09:55:39,890 [INFO] unfolding 0, single step 42001
2019-03-19 09:55:39,890 [INFO] Sum of grad norms of most recent batch: 2.215178
2019-03-19 09:55:39,891 [INFO] ---------------------------------
2019-03-19 09:56:01,508 [INFO] ---------------------------------
2019-03-19 09:56:01,509 [INFO] Summary:
2019-03-19 09:56:01,510 [INFO] Batch 43000, worst loss 0.008688 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:56:01,511 [INFO] Regularization: 842.161682 * 0.0000000100 = 0.0000084216 loss
2019-03-19 09:56:01,511 [INFO] unfolding 0, single step 43001
2019-03-19 09:56:01,512 [INFO] Sum of grad norms of most recent batch: 1.363037
2019-03-19 09:56:01,513 [INFO] ---------------------------------
2019-03-19 09:56:22,906 [INFO] ---------------------------------
2019-03-19 09:56:22,907 [INFO] Summary:
2019-03-19 09:56:22,908 [INFO] Batch 44000, worst loss 0.016465 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:56:22,908 [INFO] Regularization: 918.725037 * 0.0000000100 = 0.0000091873 loss
2019-03-19 09:56:22,909 [INFO] unfolding 0, single step 44001
2019-03-19 09:56:22,909 [INFO] Sum of grad norms of most recent batch: 2.931879
2019-03-19 09:56:22,910 [INFO] ---------------------------------
2019-03-19 09:56:44,658 [INFO] ---------------------------------
2019-03-19 09:56:44,659 [INFO] Summary:
2019-03-19 09:56:44,660 [INFO] Batch 45000, worst loss 0.008265 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:56:44,661 [INFO] Regularization: 881.022949 * 0.0000000100 = 0.0000088102 loss
2019-03-19 09:56:44,661 [INFO] unfolding 0, single step 45001
2019-03-19 09:56:44,662 [INFO] Sum of grad norms of most recent batch: 2.676137
2019-03-19 09:56:44,662 [INFO] ---------------------------------
2019-03-19 09:57:06,175 [INFO] ---------------------------------
2019-03-19 09:57:06,176 [INFO] Summary:
2019-03-19 09:57:06,176 [INFO] Batch 46000, worst loss 0.012556 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:57:06,177 [INFO] Regularization: 860.426025 * 0.0000000100 = 0.0000086043 loss
2019-03-19 09:57:06,177 [INFO] unfolding 0, single step 46001
2019-03-19 09:57:06,177 [INFO] Sum of grad norms of most recent batch: 1.395110
2019-03-19 09:57:06,178 [INFO] ---------------------------------
2019-03-19 09:57:27,871 [INFO] ---------------------------------
2019-03-19 09:57:27,872 [INFO] Summary:
2019-03-19 09:57:27,873 [INFO] Batch 47000, worst loss 0.010877 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:57:27,873 [INFO] Regularization: 829.652771 * 0.0000000100 = 0.0000082965 loss
2019-03-19 09:57:27,873 [INFO] unfolding 0, single step 47001
2019-03-19 09:57:27,874 [INFO] Sum of grad norms of most recent batch: 1.075393
2019-03-19 09:57:27,875 [INFO] ---------------------------------
2019-03-19 09:57:49,432 [INFO] ---------------------------------
2019-03-19 09:57:49,433 [INFO] Summary:
2019-03-19 09:57:49,434 [INFO] Batch 48000, worst loss 0.008359 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:57:49,434 [INFO] Regularization: 820.411865 * 0.0000000100 = 0.0000082041 loss
2019-03-19 09:57:49,435 [INFO] unfolding 0, single step 48001
2019-03-19 09:57:49,435 [INFO] Sum of grad norms of most recent batch: 1.470314
2019-03-19 09:57:49,436 [INFO] ---------------------------------
2019-03-19 09:58:11,071 [INFO] ---------------------------------
2019-03-19 09:58:11,072 [INFO] Summary:
2019-03-19 09:58:11,073 [INFO] Batch 49000, worst loss 0.013957 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:58:11,073 [INFO] Regularization: 829.158020 * 0.0000000100 = 0.0000082916 loss
2019-03-19 09:58:11,074 [INFO] unfolding 0, single step 49001
2019-03-19 09:58:11,074 [INFO] Sum of grad norms of most recent batch: 2.667708
2019-03-19 09:58:11,075 [INFO] ---------------------------------
2019-03-19 09:58:32,763 [INFO] ---------------------------------
2019-03-19 09:58:32,764 [INFO] Summary:
2019-03-19 09:58:32,764 [INFO] Batch 50000, worst loss 0.009637 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:58:32,765 [INFO] Regularization: 792.853760 * 0.0000000100 = 0.0000079285 loss
2019-03-19 09:58:32,765 [INFO] unfolding 0, single step 50001
2019-03-19 09:58:32,766 [INFO] Sum of grad norms of most recent batch: 1.109338
2019-03-19 09:58:32,766 [INFO] ---------------------------------
2019-03-19 09:58:38,538 [INFO] ---------------------------------
2019-03-19 09:58:38,539 [INFO] Evaluation:
2019-03-19 09:58:38,539 [INFO] Batch 50000, worst loss 0.002624 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 09:58:38,541 [INFO] New best loss 0.002624, saved to file transition/1552984806/1552985918_0_transition_50000.pth
2019-03-19 09:58:38,553 [INFO] Target
2019-03-19 09:58:38,554 [INFO] [[0.3296]
 [0.3999]
 [0.7765]
 [0.5622]
 [0.2353]
 [0.9061]
 [0.0405]
 [0.801 ]
 [0.6752]
 [0.4651]
 [0.598 ]
 [0.0418]
 [0.0052]
 [0.087 ]
 [0.8094]
 [0.4435]
 [0.4746]
 [0.3626]
 [0.1309]
 [0.4505]
 [0.8416]
 [0.4968]
 [0.2918]
 [0.0502]
 [0.3694]
 [0.1633]
 [0.3218]
 [0.4928]
 [0.766 ]
 [0.6123]
 [0.7107]
 [0.6267]]
2019-03-19 09:58:38,555 [INFO] Estimator output
2019-03-19 09:58:38,556 [INFO] [[0.330898]
 [0.399934]
 [0.778266]
 [0.561857]
 [0.237578]
 [0.907091]
 [0.041035]
 [0.801037]
 [0.672366]
 [0.465813]
 [0.597997]
 [0.042793]
 [0.001907]
 [0.086667]
 [0.814335]
 [0.441693]
 [0.475706]
 [0.364345]
 [0.131156]
 [0.451315]
 [0.841302]
 [0.49856 ]
 [0.292392]
 [0.05104 ]
 [0.374033]
 [0.164458]
 [0.322987]
 [0.491949]
 [0.768111]
 [0.611771]
 [0.711829]
 [0.628254]]
2019-03-19 09:58:38,558 [INFO] ---------------------------------
2019-03-19 09:59:00,301 [INFO] ---------------------------------
2019-03-19 09:59:00,302 [INFO] Summary:
2019-03-19 09:59:00,302 [INFO] Batch 51000, worst loss 0.007243 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:59:00,303 [INFO] Regularization: 789.154968 * 0.0000000100 = 0.0000078916 loss
2019-03-19 09:59:00,303 [INFO] unfolding 0, single step 51001
2019-03-19 09:59:00,304 [INFO] Sum of grad norms of most recent batch: 0.767786
2019-03-19 09:59:00,304 [INFO] ---------------------------------
2019-03-19 09:59:22,002 [INFO] ---------------------------------
2019-03-19 09:59:22,003 [INFO] Summary:
2019-03-19 09:59:22,004 [INFO] Batch 52000, worst loss 0.009124 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:59:22,004 [INFO] Regularization: 797.460754 * 0.0000000100 = 0.0000079746 loss
2019-03-19 09:59:22,005 [INFO] unfolding 0, single step 52001
2019-03-19 09:59:22,005 [INFO] Sum of grad norms of most recent batch: 1.765747
2019-03-19 09:59:22,006 [INFO] ---------------------------------
2019-03-19 09:59:43,566 [INFO] ---------------------------------
2019-03-19 09:59:43,566 [INFO] Summary:
2019-03-19 09:59:43,567 [INFO] Batch 53000, worst loss 0.008991 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 09:59:43,568 [INFO] Regularization: 848.603271 * 0.0000000100 = 0.0000084860 loss
2019-03-19 09:59:43,568 [INFO] unfolding 0, single step 53001
2019-03-19 09:59:43,569 [INFO] Sum of grad norms of most recent batch: 1.194477
2019-03-19 09:59:43,569 [INFO] ---------------------------------
2019-03-19 10:00:05,018 [INFO] ---------------------------------
2019-03-19 10:00:05,019 [INFO] Summary:
2019-03-19 10:00:05,019 [INFO] Batch 54000, worst loss 0.014594 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:00:05,020 [INFO] Regularization: 878.645386 * 0.0000000100 = 0.0000087865 loss
2019-03-19 10:00:05,020 [INFO] unfolding 0, single step 54001
2019-03-19 10:00:05,021 [INFO] Sum of grad norms of most recent batch: 1.116059
2019-03-19 10:00:05,021 [INFO] ---------------------------------
2019-03-19 10:00:26,481 [INFO] ---------------------------------
2019-03-19 10:00:26,482 [INFO] Summary:
2019-03-19 10:00:26,483 [INFO] Batch 55000, worst loss 0.007760 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:00:26,483 [INFO] Regularization: 837.282410 * 0.0000000100 = 0.0000083728 loss
2019-03-19 10:00:26,484 [INFO] unfolding 0, single step 55001
2019-03-19 10:00:26,485 [INFO] Sum of grad norms of most recent batch: 0.213184
2019-03-19 10:00:26,485 [INFO] ---------------------------------
2019-03-19 10:00:47,771 [INFO] ---------------------------------
2019-03-19 10:00:47,772 [INFO] Summary:
2019-03-19 10:00:47,772 [INFO] Batch 56000, worst loss 0.011215 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:00:47,773 [INFO] Regularization: 858.971191 * 0.0000000100 = 0.0000085897 loss
2019-03-19 10:00:47,773 [INFO] unfolding 0, single step 56001
2019-03-19 10:00:47,774 [INFO] Sum of grad norms of most recent batch: 1.470316
2019-03-19 10:00:47,774 [INFO] ---------------------------------
2019-03-19 10:01:09,579 [INFO] ---------------------------------
2019-03-19 10:01:09,580 [INFO] Summary:
2019-03-19 10:01:09,581 [INFO] Batch 57000, worst loss 0.007598 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:01:09,581 [INFO] Regularization: 845.049805 * 0.0000000100 = 0.0000084505 loss
2019-03-19 10:01:09,582 [INFO] unfolding 0, single step 57001
2019-03-19 10:01:09,582 [INFO] Sum of grad norms of most recent batch: 1.419330
2019-03-19 10:01:09,583 [INFO] ---------------------------------
2019-03-19 10:01:30,982 [INFO] ---------------------------------
2019-03-19 10:01:30,983 [INFO] Summary:
2019-03-19 10:01:30,984 [INFO] Batch 58000, worst loss 0.007391 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:01:30,984 [INFO] Regularization: 819.067932 * 0.0000000100 = 0.0000081907 loss
2019-03-19 10:01:30,985 [INFO] unfolding 0, single step 58001
2019-03-19 10:01:30,986 [INFO] Sum of grad norms of most recent batch: 0.621178
2019-03-19 10:01:30,986 [INFO] ---------------------------------
2019-03-19 10:01:52,481 [INFO] ---------------------------------
2019-03-19 10:01:52,482 [INFO] Summary:
2019-03-19 10:01:52,482 [INFO] Batch 59000, worst loss 0.008635 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:01:52,483 [INFO] Regularization: 846.406006 * 0.0000000100 = 0.0000084641 loss
2019-03-19 10:01:52,483 [INFO] unfolding 0, single step 59001
2019-03-19 10:01:52,484 [INFO] Sum of grad norms of most recent batch: 1.463444
2019-03-19 10:01:52,484 [INFO] ---------------------------------
2019-03-19 10:02:13,864 [INFO] ---------------------------------
2019-03-19 10:02:13,865 [INFO] Summary:
2019-03-19 10:02:13,866 [INFO] Batch 60000, worst loss 0.011049 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:02:13,866 [INFO] Regularization: 840.321594 * 0.0000000100 = 0.0000084032 loss
2019-03-19 10:02:13,867 [INFO] unfolding 0, single step 60001
2019-03-19 10:02:13,867 [INFO] Sum of grad norms of most recent batch: 0.878989
2019-03-19 10:02:13,868 [INFO] ---------------------------------
2019-03-19 10:02:19,660 [INFO] ---------------------------------
2019-03-19 10:02:19,661 [INFO] Evaluation:
2019-03-19 10:02:19,662 [INFO] Batch 60000, worst loss 0.008978 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:02:19,663 [INFO] ---------------------------------
2019-03-19 10:02:41,263 [INFO] ---------------------------------
2019-03-19 10:02:41,264 [INFO] Summary:
2019-03-19 10:02:41,264 [INFO] Batch 61000, worst loss 0.013350 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:02:41,265 [INFO] Regularization: 849.180542 * 0.0000000100 = 0.0000084918 loss
2019-03-19 10:02:41,265 [INFO] unfolding 0, single step 61001
2019-03-19 10:02:41,266 [INFO] Sum of grad norms of most recent batch: 1.181111
2019-03-19 10:02:41,266 [INFO] ---------------------------------
2019-03-19 10:03:02,557 [INFO] ---------------------------------
2019-03-19 10:03:02,558 [INFO] Summary:
2019-03-19 10:03:02,558 [INFO] Batch 62000, worst loss 0.001807 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:03:02,559 [INFO] Regularization: 796.846191 * 0.0000000100 = 0.0000079685 loss
2019-03-19 10:03:02,559 [INFO] unfolding 0, single step 62001
2019-03-19 10:03:02,560 [INFO] Sum of grad norms of most recent batch: 0.226048
2019-03-19 10:03:02,560 [INFO] ---------------------------------
2019-03-19 10:03:08,266 [INFO] ---------------------------------
2019-03-19 10:03:08,267 [INFO] Evaluation:
2019-03-19 10:03:08,268 [INFO] Batch 62000, worst loss 0.001156 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:03:08,269 [INFO] New best loss 0.001156, saved to file transition/1552984806/1552986188_0_transition_62000.pth
2019-03-19 10:03:08,281 [INFO] Target
2019-03-19 10:03:08,282 [INFO] [[0.3398]
 [0.0747]
 [0.7623]
 [0.555 ]
 [0.2681]
 [0.1576]
 [0.6904]
 [0.7605]
 [0.4531]
 [0.6845]
 [0.0763]
 [0.2258]
 [0.7902]
 [0.2667]
 [0.9824]
 [0.543 ]
 [0.6488]
 [0.8445]
 [0.3359]
 [0.7387]
 [0.2858]
 [0.5105]
 [0.972 ]
 [0.3232]
 [0.7435]
 [0.6232]
 [0.2963]
 [0.3522]
 [0.7329]
 [0.8306]
 [0.4188]
 [0.9846]]
2019-03-19 10:03:08,283 [INFO] Estimator output
2019-03-19 10:03:08,283 [INFO] [[0.339827]
 [0.07562 ]
 [0.76284 ]
 [0.554784]
 [0.268452]
 [0.157688]
 [0.693311]
 [0.763705]
 [0.452957]
 [0.684972]
 [0.076204]
 [0.227092]
 [0.79045 ]
 [0.266652]
 [0.98268 ]
 [0.542985]
 [0.648888]
 [0.845029]
 [0.335554]
 [0.739819]
 [0.287113]
 [0.510297]
 [0.972351]
 [0.323022]
 [0.740951]
 [0.622685]
 [0.294767]
 [0.352009]
 [0.732823]
 [0.83373 ]
 [0.418876]
 [0.985155]]
2019-03-19 10:03:08,285 [INFO] ---------------------------------
2019-03-19 10:03:29,894 [INFO] ---------------------------------
2019-03-19 10:03:29,895 [INFO] Summary:
2019-03-19 10:03:29,896 [INFO] Batch 63000, worst loss 0.001204 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:03:29,896 [INFO] Regularization: 753.235596 * 0.0000000100 = 0.0000075324 loss
2019-03-19 10:03:29,897 [INFO] unfolding 0, single step 63001
2019-03-19 10:03:29,897 [INFO] Sum of grad norms of most recent batch: 0.657473
2019-03-19 10:03:29,898 [INFO] ---------------------------------
2019-03-19 10:03:51,476 [INFO] ---------------------------------
2019-03-19 10:03:51,477 [INFO] Summary:
2019-03-19 10:03:51,478 [INFO] Batch 64000, worst loss 0.001458 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:03:51,478 [INFO] Regularization: 710.325745 * 0.0000000100 = 0.0000071033 loss
2019-03-19 10:03:51,479 [INFO] unfolding 0, single step 64001
2019-03-19 10:03:51,480 [INFO] Sum of grad norms of most recent batch: 0.227732
2019-03-19 10:03:51,480 [INFO] ---------------------------------
2019-03-19 10:04:13,356 [INFO] ---------------------------------
2019-03-19 10:04:13,357 [INFO] Summary:
2019-03-19 10:04:13,358 [INFO] Batch 65000, worst loss 0.002080 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:04:13,358 [INFO] Regularization: 673.325989 * 0.0000000100 = 0.0000067333 loss
2019-03-19 10:04:13,358 [INFO] unfolding 0, single step 65001
2019-03-19 10:04:13,359 [INFO] Sum of grad norms of most recent batch: 0.179177
2019-03-19 10:04:13,360 [INFO] ---------------------------------
2019-03-19 10:04:34,688 [INFO] ---------------------------------
2019-03-19 10:04:34,689 [INFO] Summary:
2019-03-19 10:04:34,689 [INFO] Batch 66000, worst loss 0.001337 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:04:34,690 [INFO] Regularization: 647.626221 * 0.0000000100 = 0.0000064763 loss
2019-03-19 10:04:34,690 [INFO] unfolding 0, single step 66001
2019-03-19 10:04:34,691 [INFO] Sum of grad norms of most recent batch: 0.560097
2019-03-19 10:04:34,692 [INFO] ---------------------------------
2019-03-19 10:04:56,221 [INFO] ---------------------------------
2019-03-19 10:04:56,222 [INFO] Summary:
2019-03-19 10:04:56,223 [INFO] Batch 67000, worst loss 0.001266 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:04:56,223 [INFO] Regularization: 613.096497 * 0.0000000100 = 0.0000061310 loss
2019-03-19 10:04:56,223 [INFO] unfolding 0, single step 67001
2019-03-19 10:04:56,224 [INFO] Sum of grad norms of most recent batch: 0.839343
2019-03-19 10:04:56,225 [INFO] ---------------------------------
2019-03-19 10:05:17,507 [INFO] ---------------------------------
2019-03-19 10:05:17,508 [INFO] Summary:
2019-03-19 10:05:17,508 [INFO] Batch 68000, worst loss 0.001162 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:05:17,509 [INFO] Regularization: 615.566711 * 0.0000000100 = 0.0000061557 loss
2019-03-19 10:05:17,509 [INFO] unfolding 0, single step 68001
2019-03-19 10:05:17,510 [INFO] Sum of grad norms of most recent batch: 0.362498
2019-03-19 10:05:17,510 [INFO] ---------------------------------
2019-03-19 10:05:38,445 [INFO] ---------------------------------
2019-03-19 10:05:38,446 [INFO] Summary:
2019-03-19 10:05:38,447 [INFO] Batch 69000, worst loss 0.001291 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:05:38,447 [INFO] Regularization: 590.367615 * 0.0000000100 = 0.0000059037 loss
2019-03-19 10:05:38,448 [INFO] unfolding 0, single step 69001
2019-03-19 10:05:38,448 [INFO] Sum of grad norms of most recent batch: 1.650033
2019-03-19 10:05:38,449 [INFO] ---------------------------------
2019-03-19 10:05:59,888 [INFO] ---------------------------------
2019-03-19 10:05:59,889 [INFO] Summary:
2019-03-19 10:05:59,890 [INFO] Batch 70000, worst loss 0.000912 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:05:59,891 [INFO] Regularization: 578.219971 * 0.0000000100 = 0.0000057822 loss
2019-03-19 10:05:59,891 [INFO] unfolding 0, single step 70001
2019-03-19 10:05:59,891 [INFO] Sum of grad norms of most recent batch: 0.682036
2019-03-19 10:05:59,892 [INFO] ---------------------------------
2019-03-19 10:06:05,748 [INFO] ---------------------------------
2019-03-19 10:06:05,751 [INFO] Evaluation:
2019-03-19 10:06:05,752 [INFO] Batch 70000, worst loss 0.000362 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:06:05,754 [INFO] New best loss 0.000362, saved to file transition/1552984806/1552986365_0_transition_70000.pth
2019-03-19 10:06:05,766 [INFO] Target
2019-03-19 10:06:05,767 [INFO] [[0.6522]
 [0.7229]
 [0.5624]
 [0.6311]
 [0.1769]
 [0.364 ]
 [0.1258]
 [0.7639]
 [0.0684]
 [0.581 ]
 [0.8517]
 [0.7107]
 [0.7806]
 [0.9707]
 [0.444 ]
 [0.3599]
 [0.2702]
 [0.4351]
 [0.7622]
 [0.4386]
 [0.2163]
 [0.2469]
 [0.1341]
 [0.238 ]
 [0.4188]
 [0.6437]
 [0.2426]
 [0.8138]
 [0.2688]
 [0.0323]
 [0.154 ]
 [0.2897]]
2019-03-19 10:06:05,769 [INFO] Estimator output
2019-03-19 10:06:05,770 [INFO] [[0.652126]
 [0.722701]
 [0.562748]
 [0.630957]
 [0.176762]
 [0.365067]
 [0.126029]
 [0.764216]
 [0.067879]
 [0.581355]
 [0.851525]
 [0.708771]
 [0.780943]
 [0.96875 ]
 [0.443437]
 [0.360145]
 [0.270568]
 [0.436444]
 [0.762117]
 [0.438552]
 [0.216885]
 [0.247435]
 [0.133477]
 [0.23823 ]
 [0.419334]
 [0.643796]
 [0.242287]
 [0.813772]
 [0.269346]
 [0.032764]
 [0.154407]
 [0.289478]]
2019-03-19 10:06:05,772 [INFO] ---------------------------------
2019-03-19 10:06:27,385 [INFO] ---------------------------------
2019-03-19 10:06:27,386 [INFO] Summary:
2019-03-19 10:06:27,387 [INFO] Batch 71000, worst loss 0.001690 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 10:06:27,387 [INFO] Regularization: 564.681702 * 0.0000000100 = 0.0000056468 loss
2019-03-19 10:06:27,388 [INFO] unfolding 0, single step 71001
2019-03-19 10:06:27,388 [INFO] Sum of grad norms of most recent batch: 0.308863
2019-03-19 10:06:27,389 [INFO] ---------------------------------
2019-03-19 10:06:49,104 [INFO] ---------------------------------
2019-03-19 10:06:49,105 [INFO] Summary:
2019-03-19 10:06:49,107 [INFO] Batch 72000, worst loss 0.000186 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 10:06:49,107 [INFO] Regularization: 534.002197 * 0.0000000100 = 0.0000053400 loss
2019-03-19 10:06:49,108 [INFO] unfolding 0, single step 72001
2019-03-19 10:06:49,108 [INFO] Sum of grad norms of most recent batch: 0.172340
2019-03-19 10:06:49,109 [INFO] ---------------------------------
2019-03-19 10:06:54,855 [INFO] ---------------------------------
2019-03-19 10:06:54,856 [INFO] Evaluation:
2019-03-19 10:06:54,856 [INFO] Batch 72000, worst loss 0.000146 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:06:54,857 [INFO] New best loss 0.000146, saved to file transition/1552984806/1552986414_0_transition_72000.pth
2019-03-19 10:06:54,868 [INFO] Target
2019-03-19 10:06:54,869 [INFO] [[0.264 ]
 [0.328 ]
 [0.5533]
 [0.9352]
 [0.2273]
 [0.1735]
 [0.5223]
 [0.3376]
 [0.8335]
 [0.1884]
 [0.597 ]
 [0.4419]
 [0.2696]
 [0.3499]
 [0.5939]
 [0.0517]
 [0.9117]
 [0.7575]
 [0.405 ]
 [0.3952]
 [0.6834]
 [0.7088]
 [0.6552]
 [0.4546]
 [0.7084]
 [0.1184]
 [0.1846]
 [0.2818]
 [0.0167]
 [0.6063]
 [0.9004]
 [0.2434]]
2019-03-19 10:06:54,871 [INFO] Estimator output
2019-03-19 10:06:54,872 [INFO] [[0.263913]
 [0.328369]
 [0.553384]
 [0.934969]
 [0.22755 ]
 [0.173695]
 [0.522587]
 [0.33773 ]
 [0.833579]
 [0.189636]
 [0.597126]
 [0.441664]
 [0.269689]
 [0.350106]
 [0.59394 ]
 [0.051813]
 [0.911638]
 [0.757607]
 [0.404907]
 [0.395278]
 [0.683589]
 [0.708995]
 [0.655333]
 [0.455028]
 [0.709635]
 [0.119636]
 [0.185028]
 [0.281942]
 [0.016766]
 [0.606391]
 [0.900182]
 [0.24345 ]]
2019-03-19 10:06:54,874 [INFO] ---------------------------------
2019-03-19 10:07:15,862 [INFO] ---------------------------------
2019-03-19 10:07:15,863 [INFO] Summary:
2019-03-19 10:07:15,863 [INFO] Batch 73000, worst loss 0.000095 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000186 @est.-depth 2
2019-03-19 10:07:15,864 [INFO] Regularization: 509.173828 * 0.0000000100 = 0.0000050917 loss
2019-03-19 10:07:15,864 [INFO] unfolding 0, single step 73001
2019-03-19 10:07:15,865 [INFO] Sum of grad norms of most recent batch: 0.051485
2019-03-19 10:07:15,865 [INFO] ---------------------------------
2019-03-19 10:07:21,709 [INFO] ---------------------------------
2019-03-19 10:07:21,710 [INFO] Evaluation:
2019-03-19 10:07:21,714 [INFO] Batch 73000, worst loss 0.000022 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:07:21,715 [INFO] New best loss 0.000022, saved to file transition/1552984806/1552986441_0_transition_73000.pth
2019-03-19 10:07:21,727 [INFO] Target
2019-03-19 10:07:21,728 [INFO] [[0.4293]
 [0.3676]
 [0.278 ]
 [0.022 ]
 [0.7634]
 [0.5322]
 [0.213 ]
 [0.1079]
 [0.3887]
 [0.1729]
 [0.3633]
 [0.6639]
 [0.01  ]
 [0.6294]
 [0.715 ]
 [0.2682]
 [0.3212]
 [0.7704]
 [0.0446]
 [0.4182]
 [0.3251]
 [0.9203]
 [0.669 ]
 [0.5144]
 [0.1554]
 [0.0494]
 [0.1816]
 [0.3067]
 [0.8447]
 [0.3797]
 [0.6424]
 [0.2156]]
2019-03-19 10:07:21,730 [INFO] Estimator output
2019-03-19 10:07:21,731 [INFO] [[0.429243]
 [0.367747]
 [0.277633]
 [0.021919]
 [0.763632]
 [0.532013]
 [0.213028]
 [0.107773]
 [0.388658]
 [0.172912]
 [0.363266]
 [0.663848]
 [0.009785]
 [0.629329]
 [0.714935]
 [0.267823]
 [0.321172]
 [0.770399]
 [0.044517]
 [0.418261]
 [0.325262]
 [0.920126]
 [0.668999]
 [0.51456 ]
 [0.155343]
 [0.04893 ]
 [0.1814  ]
 [0.306703]
 [0.844672]
 [0.379637]
 [0.642395]
 [0.215713]]
2019-03-19 10:07:21,733 [INFO] ---------------------------------
2019-03-19 10:07:42,911 [INFO] ---------------------------------
2019-03-19 10:07:42,913 [INFO] Summary:
2019-03-19 10:07:42,913 [INFO] Batch 74000, worst loss 0.000019 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000095 @est.-depth 2
2019-03-19 10:07:42,914 [INFO] Regularization: 493.740845 * 0.0000000100 = 0.0000049374 loss
2019-03-19 10:07:42,914 [INFO] unfolding 0, single step 74001
2019-03-19 10:07:42,915 [INFO] reducing reg_loss_factor
2019-03-19 10:07:42,915 [INFO] Sum of grad norms of most recent batch: 0.015751
2019-03-19 10:07:42,916 [INFO] ---------------------------------
2019-03-19 10:07:48,765 [INFO] ---------------------------------
2019-03-19 10:07:48,767 [INFO] Evaluation:
2019-03-19 10:07:48,768 [INFO] Batch 74000, worst loss 0.000002 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:07:48,768 [INFO] New best loss 0.000002, saved to file transition/1552984806/1552986468_0_transition_74000.pth
2019-03-19 10:07:48,780 [INFO] Target
2019-03-19 10:07:48,781 [INFO] [[0.1476]
 [0.7948]
 [0.1951]
 [0.7381]
 [0.3882]
 [0.8021]
 [0.2715]
 [0.8622]
 [0.3335]
 [0.7565]
 [0.875 ]
 [0.312 ]
 [0.2091]
 [0.549 ]
 [0.0223]
 [0.2292]
 [0.8177]
 [0.3797]
 [0.4269]
 [0.3034]
 [0.6099]
 [0.9574]
 [0.5486]
 [0.3688]
 [0.7589]
 [0.3076]
 [0.544 ]
 [0.9144]
 [0.124 ]
 [0.4604]
 [0.755 ]
 [0.0477]]
2019-03-19 10:07:48,783 [INFO] Estimator output
2019-03-19 10:07:48,784 [INFO] [[0.147667]
 [0.794822]
 [0.195109]
 [0.738116]
 [0.388203]
 [0.80212 ]
 [0.271499]
 [0.862209]
 [0.333415]
 [0.756533]
 [0.875015]
 [0.311985]
 [0.209174]
 [0.54902 ]
 [0.022402]
 [0.229198]
 [0.817694]
 [0.379812]
 [0.426904]
 [0.303401]
 [0.609912]
 [0.95741 ]
 [0.548596]
 [0.368821]
 [0.758931]
 [0.307669]
 [0.544106]
 [0.914406]
 [0.124019]
 [0.460397]
 [0.755011]
 [0.047689]]
2019-03-19 10:07:48,786 [INFO] ---------------------------------
2019-03-19 10:08:09,644 [INFO] ---------------------------------
2019-03-19 10:08:09,645 [INFO] Summary:
2019-03-19 10:08:09,646 [INFO] Batch 75000, worst loss 0.000002 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000019 @est.-depth 2
2019-03-19 10:08:09,646 [INFO] Regularization: 492.598938 * 0.0000000010 = 0.0000004926 loss
2019-03-19 10:08:09,647 [INFO] unfolding 0, single step 75001
2019-03-19 10:08:09,647 [INFO] reducing reg_loss_factor
2019-03-19 10:08:09,648 [INFO] Sum of grad norms of most recent batch: 0.009673
2019-03-19 10:08:09,648 [INFO] ---------------------------------
2019-03-19 10:08:15,467 [INFO] ---------------------------------
2019-03-19 10:08:15,468 [INFO] Evaluation:
2019-03-19 10:08:15,468 [INFO] Batch 75000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:08:15,469 [INFO] New best loss 0.000001, saved to file transition/1552984806/1552986495_0_transition_75000.pth
2019-03-19 10:08:15,481 [INFO] Target
2019-03-19 10:08:15,482 [INFO] [[0.1364]
 [0.8707]
 [0.157 ]
 [0.7603]
 [0.5996]
 [0.4341]
 [0.7561]
 [0.6198]
 [0.1669]
 [0.6568]
 [0.9182]
 [0.7423]
 [0.944 ]
 [0.1324]
 [0.9867]
 [0.3641]
 [0.5099]
 [0.6446]
 [0.3723]
 [0.0819]
 [0.8629]
 [0.9247]
 [0.0677]
 [0.1064]
 [0.4784]
 [0.5456]
 [0.4283]
 [0.2128]
 [0.1627]
 [0.2028]
 [0.6128]
 [0.2081]]
2019-03-19 10:08:15,484 [INFO] Estimator output
2019-03-19 10:08:15,484 [INFO] [[0.136395]
 [0.870699]
 [0.156964]
 [0.760292]
 [0.599599]
 [0.434098]
 [0.756099]
 [0.619789]
 [0.166897]
 [0.656795]
 [0.918198]
 [0.742389]
 [0.944003]
 [0.132399]
 [0.986696]
 [0.364098]
 [0.509899]
 [0.644645]
 [0.372189]
 [0.081897]
 [0.8629  ]
 [0.924696]
 [0.067693]
 [0.106398]
 [0.478399]
 [0.545589]
 [0.428296]
 [0.212793]
 [0.1627  ]
 [0.202858]
 [0.612859]
 [0.208099]]
2019-03-19 10:08:15,486 [INFO] ---------------------------------
2019-03-19 10:08:36,067 [INFO] ---------------------------------
2019-03-19 10:08:36,068 [INFO] Summary:
2019-03-19 10:08:36,069 [INFO] Batch 76000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000002 @est.-depth 2
2019-03-19 10:08:36,069 [INFO] Regularization: 492.575684 * 0.0000000001 = 0.0000000493 loss
2019-03-19 10:08:36,070 [INFO] unfolding 0, single step 76001
2019-03-19 10:08:36,070 [INFO] Sum of grad norms of most recent batch: 0.000546
2019-03-19 10:08:36,071 [INFO] ---------------------------------
2019-03-19 10:08:57,141 [INFO] ---------------------------------
2019-03-19 10:08:57,141 [INFO] Summary:
2019-03-19 10:08:57,142 [INFO] Batch 77000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 10:08:57,142 [INFO] Regularization: 492.571960 * 0.0000000001 = 0.0000000493 loss
2019-03-19 10:08:57,143 [INFO] unfolding 0, single step 77001
2019-03-19 10:08:57,143 [INFO] Sum of grad norms of most recent batch: 0.011944
2019-03-19 10:08:57,144 [INFO] ---------------------------------
2019-03-19 10:09:02,977 [INFO] ---------------------------------
2019-03-19 10:09:02,978 [INFO] Evaluation:
2019-03-19 10:09:02,979 [INFO] Batch 77000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:09:02,979 [INFO] New best loss 0.000001, saved to file transition/1552984806/1552986542_0_transition_77000.pth
2019-03-19 10:09:02,991 [INFO] Target
2019-03-19 10:09:02,992 [INFO] [[0.9787]
 [0.264 ]
 [0.4311]
 [0.9706]
 [0.0114]
 [0.0131]
 [0.522 ]
 [0.0484]
 [0.6934]
 [0.3346]
 [0.2404]
 [0.9098]
 [0.1364]
 [0.1281]
 [0.6882]
 [0.3692]
 [0.611 ]
 [0.7564]
 [0.555 ]
 [0.5327]
 [0.8339]
 [0.7868]
 [0.9564]
 [0.7635]
 [0.3323]
 [0.0758]
 [0.334 ]
 [0.6862]
 [0.5105]
 [0.8563]
 [0.288 ]
 [0.0985]]
2019-03-19 10:09:02,994 [INFO] Estimator output
2019-03-19 10:09:02,994 [INFO] [[0.9787  ]
 [0.264   ]
 [0.4311  ]
 [0.970597]
 [0.011399]
 [0.013099]
 [0.521999]
 [0.048399]
 [0.693392]
 [0.334594]
 [0.240399]
 [0.909813]
 [0.136395]
 [0.1282  ]
 [0.6882  ]
 [0.369205]
 [0.610999]
 [0.75641 ]
 [0.555   ]
 [0.532794]
 [0.8339  ]
 [0.786799]
 [0.95641 ]
 [0.763499]
 [0.3323  ]
 [0.075805]
 [0.333999]
 [0.686199]
 [0.510597]
 [0.8563  ]
 [0.288013]
 [0.098499]]
2019-03-19 10:09:02,996 [INFO] ---------------------------------
2019-03-19 10:09:24,347 [INFO] ---------------------------------
2019-03-19 10:09:24,348 [INFO] Summary:
2019-03-19 10:09:24,348 [INFO] Batch 78000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 10:09:24,349 [INFO] Regularization: 492.566956 * 0.0000000001 = 0.0000000493 loss
2019-03-19 10:09:24,349 [INFO] unfolding 0, single step 78001
2019-03-19 10:09:24,350 [INFO] Sum of grad norms of most recent batch: 0.006273
2019-03-19 10:09:24,350 [INFO] ---------------------------------
2019-03-19 10:09:45,297 [INFO] ---------------------------------
2019-03-19 10:09:45,297 [INFO] Summary:
2019-03-19 10:09:45,298 [INFO] Batch 79000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 10:09:45,298 [INFO] Regularization: 492.558411 * 0.0000000001 = 0.0000000493 loss
2019-03-19 10:09:45,299 [INFO] unfolding 0, single step 79001
2019-03-19 10:09:45,299 [INFO] Sum of grad norms of most recent batch: 0.006548
2019-03-19 10:09:45,300 [INFO] ---------------------------------
2019-03-19 10:10:06,238 [INFO] ---------------------------------
2019-03-19 10:10:06,239 [INFO] Summary:
2019-03-19 10:10:06,239 [INFO] Batch 80000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 10:10:06,240 [INFO] Regularization: 492.544708 * 0.0000000001 = 0.0000000493 loss
2019-03-19 10:10:06,240 [INFO] unfolding 0, single step 80001
2019-03-19 10:10:06,241 [INFO] Sum of grad norms of most recent batch: 0.010931
2019-03-19 10:10:06,241 [INFO] ---------------------------------
2019-03-19 10:10:12,004 [INFO] ---------------------------------
2019-03-19 10:10:12,005 [INFO] Evaluation:
2019-03-19 10:10:12,006 [INFO] Batch 80000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:10:12,006 [INFO] New best loss 0.000001, saved to file transition/1552984806/1552986612_0_transition_80000.pth
2019-03-19 10:10:12,017 [INFO] Target
2019-03-19 10:10:12,018 [INFO] [[0.4863]
 [0.1811]
 [0.3134]
 [0.3162]
 [0.4464]
 [0.4332]
 [0.1363]
 [0.9982]
 [0.084 ]
 [0.8889]
 [0.9804]
 [0.897 ]
 [0.2188]
 [0.7024]
 [0.4141]
 [0.9064]
 [0.3787]
 [0.4631]
 [0.1433]
 [0.8228]
 [0.8694]
 [0.5054]
 [0.2917]
 [0.0976]
 [0.081 ]
 [0.5228]
 [0.6446]
 [0.1761]
 [0.574 ]
 [0.2092]
 [0.4986]
 [0.2915]]
2019-03-19 10:10:12,019 [INFO] Estimator output
2019-03-19 10:10:12,020 [INFO] [[0.486299]
 [0.181102]
 [0.313401]
 [0.316201]
 [0.446402]
 [0.4332  ]
 [0.136299]
 [0.9982  ]
 [0.084   ]
 [0.8889  ]
 [0.980398]
 [0.897   ]
 [0.2188  ]
 [0.7024  ]
 [0.414101]
 [0.906401]
 [0.3788  ]
 [0.463101]
 [0.143398]
 [0.822798]
 [0.869399]
 [0.5054  ]
 [0.2917  ]
 [0.097594]
 [0.081001]
 [0.522806]
 [0.644601]
 [0.176101]
 [0.573999]
 [0.209202]
 [0.498602]
 [0.291594]]
2019-03-19 10:10:12,021 [INFO] ---------------------------------
2019-03-19 10:10:32,942 [INFO] ---------------------------------
2019-03-19 10:10:32,943 [INFO] Summary:
2019-03-19 10:10:32,944 [INFO] Batch 81000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:10:32,944 [INFO] Regularization: 492.525726 * 0.0000000001 = 0.0000000493 loss
2019-03-19 10:10:32,945 [INFO] unfolding 0, single step 81001
2019-03-19 10:10:32,945 [INFO] Sum of grad norms of most recent batch: 0.002668
2019-03-19 10:10:32,946 [INFO] ---------------------------------
2019-03-19 10:10:54,028 [INFO] ---------------------------------
2019-03-19 10:10:54,029 [INFO] Summary:
2019-03-19 10:10:54,030 [INFO] Batch 82000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:10:54,030 [INFO] Regularization: 492.511597 * 0.0000000001 = 0.0000000493 loss
2019-03-19 10:10:54,031 [INFO] unfolding 0, single step 82001
2019-03-19 10:10:54,031 [INFO] Sum of grad norms of most recent batch: 0.001681
2019-03-19 10:10:54,032 [INFO] ---------------------------------
2019-03-19 10:11:15,035 [INFO] ---------------------------------
2019-03-19 10:11:15,036 [INFO] Summary:
2019-03-19 10:11:15,036 [INFO] Batch 83000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:11:15,037 [INFO] Regularization: 492.495728 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:11:15,037 [INFO] unfolding 0, single step 83001
2019-03-19 10:11:15,038 [INFO] Sum of grad norms of most recent batch: 0.002570
2019-03-19 10:11:15,038 [INFO] ---------------------------------
2019-03-19 10:11:35,907 [INFO] ---------------------------------
2019-03-19 10:11:35,908 [INFO] Summary:
2019-03-19 10:11:35,908 [INFO] Batch 84000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:11:35,909 [INFO] Regularization: 492.473480 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:11:35,909 [INFO] unfolding 0, single step 84001
2019-03-19 10:11:35,910 [INFO] Sum of grad norms of most recent batch: 0.001410
2019-03-19 10:11:35,910 [INFO] ---------------------------------
2019-03-19 10:11:56,757 [INFO] ---------------------------------
2019-03-19 10:11:56,758 [INFO] Summary:
2019-03-19 10:11:56,759 [INFO] Batch 85000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:11:56,759 [INFO] Regularization: 492.448456 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:11:56,760 [INFO] unfolding 0, single step 85001
2019-03-19 10:11:56,760 [INFO] Sum of grad norms of most recent batch: 0.000907
2019-03-19 10:11:56,761 [INFO] ---------------------------------
2019-03-19 10:12:17,662 [INFO] ---------------------------------
2019-03-19 10:12:17,663 [INFO] Summary:
2019-03-19 10:12:17,663 [INFO] Batch 86000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:12:17,664 [INFO] Regularization: 492.417725 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:12:17,664 [INFO] unfolding 0, single step 86001
2019-03-19 10:12:17,665 [INFO] Sum of grad norms of most recent batch: 0.003159
2019-03-19 10:12:17,665 [INFO] ---------------------------------
2019-03-19 10:12:38,777 [INFO] ---------------------------------
2019-03-19 10:12:38,778 [INFO] Summary:
2019-03-19 10:12:38,779 [INFO] Batch 87000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:12:38,779 [INFO] Regularization: 492.385986 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:12:38,779 [INFO] unfolding 0, single step 87001
2019-03-19 10:12:38,780 [INFO] Sum of grad norms of most recent batch: 0.011259
2019-03-19 10:12:38,780 [INFO] ---------------------------------
2019-03-19 10:12:44,641 [INFO] ---------------------------------
2019-03-19 10:12:44,642 [INFO] Evaluation:
2019-03-19 10:12:44,643 [INFO] Batch 87000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:12:44,644 [INFO] New best loss 0.000001, saved to file transition/1552984806/1552986764_0_transition_87000.pth
2019-03-19 10:12:44,659 [INFO] Target
2019-03-19 10:12:44,659 [INFO] [[0.9741]
 [0.8028]
 [0.0853]
 [0.5241]
 [0.6271]
 [0.6247]
 [0.917 ]
 [0.801 ]
 [0.7617]
 [0.2429]
 [0.96  ]
 [0.1446]
 [0.0706]
 [0.7128]
 [0.8212]
 [0.8693]
 [0.6327]
 [0.3226]
 [0.666 ]
 [0.0587]
 [0.5417]
 [0.7431]
 [0.4147]
 [0.9507]
 [0.7354]
 [0.4915]
 [0.5838]
 [0.8714]
 [0.9792]
 [0.9146]
 [0.7852]
 [0.4099]]
2019-03-19 10:12:44,661 [INFO] Estimator output
2019-03-19 10:12:44,661 [INFO] [[0.9741  ]
 [0.802792]
 [0.085203]
 [0.5241  ]
 [0.627101]
 [0.624701]
 [0.916992]
 [0.800998]
 [0.7617  ]
 [0.2429  ]
 [0.96    ]
 [0.1446  ]
 [0.070598]
 [0.7128  ]
 [0.821197]
 [0.8693  ]
 [0.632605]
 [0.322601]
 [0.666001]
 [0.0587  ]
 [0.541792]
 [0.743101]
 [0.414701]
 [0.9507  ]
 [0.7354  ]
 [0.491501]
 [0.583804]
 [0.8714  ]
 [0.979199]
 [0.914599]
 [0.785199]
 [0.409899]]
2019-03-19 10:12:44,663 [INFO] ---------------------------------
2019-03-19 10:13:05,603 [INFO] ---------------------------------
2019-03-19 10:13:05,604 [INFO] Summary:
2019-03-19 10:13:05,605 [INFO] Batch 88000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:13:05,605 [INFO] Regularization: 492.352875 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:13:05,606 [INFO] unfolding 0, single step 88001
2019-03-19 10:13:05,606 [INFO] Sum of grad norms of most recent batch: 0.002373
2019-03-19 10:13:05,607 [INFO] ---------------------------------
2019-03-19 10:13:26,749 [INFO] ---------------------------------
2019-03-19 10:13:26,750 [INFO] Summary:
2019-03-19 10:13:26,751 [INFO] Batch 89000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:13:26,751 [INFO] Regularization: 492.314850 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:13:26,751 [INFO] unfolding 0, single step 89001
2019-03-19 10:13:26,752 [INFO] Sum of grad norms of most recent batch: 0.001304
2019-03-19 10:13:26,752 [INFO] ---------------------------------
2019-03-19 10:13:47,602 [INFO] ---------------------------------
2019-03-19 10:13:47,603 [INFO] Summary:
2019-03-19 10:13:47,604 [INFO] Batch 90000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:13:47,604 [INFO] Regularization: 492.280640 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:13:47,604 [INFO] unfolding 0, single step 90001
2019-03-19 10:13:47,605 [INFO] Sum of grad norms of most recent batch: 0.001427
2019-03-19 10:13:47,605 [INFO] ---------------------------------
2019-03-19 10:13:53,411 [INFO] ---------------------------------
2019-03-19 10:13:53,412 [INFO] Evaluation:
2019-03-19 10:13:53,413 [INFO] Batch 90000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:13:53,413 [INFO] ---------------------------------
2019-03-19 10:14:14,523 [INFO] ---------------------------------
2019-03-19 10:14:14,524 [INFO] Summary:
2019-03-19 10:14:14,524 [INFO] Batch 91000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:14:14,525 [INFO] Regularization: 492.243286 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:14:14,525 [INFO] unfolding 0, single step 91001
2019-03-19 10:14:14,526 [INFO] Sum of grad norms of most recent batch: 0.001669
2019-03-19 10:14:14,526 [INFO] ---------------------------------
2019-03-19 10:14:35,411 [INFO] ---------------------------------
2019-03-19 10:14:35,412 [INFO] Summary:
2019-03-19 10:14:35,412 [INFO] Batch 92000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:14:35,413 [INFO] Regularization: 492.226471 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:14:35,413 [INFO] unfolding 0, single step 92001
2019-03-19 10:14:35,414 [INFO] Sum of grad norms of most recent batch: 0.006598
2019-03-19 10:14:35,414 [INFO] ---------------------------------
2019-03-19 10:14:41,198 [INFO] ---------------------------------
2019-03-19 10:14:41,199 [INFO] Evaluation:
2019-03-19 10:14:41,200 [INFO] Batch 92000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:14:41,205 [INFO] New best loss 0.000001, saved to file transition/1552984806/1552986881_0_transition_92000.pth
2019-03-19 10:14:41,217 [INFO] Target
2019-03-19 10:14:41,218 [INFO] [[0.7899]
 [0.9569]
 [0.9758]
 [0.7017]
 [0.1635]
 [0.3034]
 [0.72  ]
 [0.0387]
 [0.774 ]
 [0.6684]
 [0.0197]
 [0.7871]
 [0.6827]
 [0.1988]
 [0.8264]
 [0.208 ]
 [0.3437]
 [0.1021]
 [0.9922]
 [0.5716]
 [0.9741]
 [0.7534]
 [0.7144]
 [0.5087]
 [0.8324]
 [0.6288]
 [0.5516]
 [0.0982]
 [0.7298]
 [0.6684]
 [0.8697]
 [0.3914]]
2019-03-19 10:14:41,220 [INFO] Estimator output
2019-03-19 10:14:41,220 [INFO] [[0.789812]
 [0.9569  ]
 [0.975803]
 [0.7017  ]
 [0.163406]
 [0.303406]
 [0.72    ]
 [0.0387  ]
 [0.774   ]
 [0.6684  ]
 [0.0197  ]
 [0.7871  ]
 [0.6827  ]
 [0.198799]
 [0.826408]
 [0.207999]
 [0.3437  ]
 [0.102099]
 [0.992201]
 [0.571585]
 [0.9741  ]
 [0.753395]
 [0.714401]
 [0.508606]
 [0.8324  ]
 [0.628799]
 [0.551585]
 [0.0982  ]
 [0.729792]
 [0.6684  ]
 [0.8697  ]
 [0.3914  ]]
2019-03-19 10:14:41,222 [INFO] ---------------------------------
2019-03-19 10:15:02,244 [INFO] ---------------------------------
2019-03-19 10:15:02,245 [INFO] Summary:
2019-03-19 10:15:02,246 [INFO] Batch 93000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:15:02,246 [INFO] Regularization: 492.206329 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:15:02,247 [INFO] unfolding 0, single step 93001
2019-03-19 10:15:02,247 [INFO] Sum of grad norms of most recent batch: 0.001424
2019-03-19 10:15:02,248 [INFO] ---------------------------------
2019-03-19 10:15:23,290 [INFO] ---------------------------------
2019-03-19 10:15:23,291 [INFO] Summary:
2019-03-19 10:15:23,292 [INFO] Batch 94000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:15:23,293 [INFO] Regularization: 492.189453 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:15:23,294 [INFO] unfolding 0, single step 94001
2019-03-19 10:15:23,295 [INFO] Sum of grad norms of most recent batch: 0.004231
2019-03-19 10:15:23,296 [INFO] ---------------------------------
2019-03-19 10:15:43,808 [INFO] ---------------------------------
2019-03-19 10:15:43,809 [INFO] Summary:
2019-03-19 10:15:43,810 [INFO] Batch 95000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:15:43,810 [INFO] Regularization: 492.170441 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:15:43,811 [INFO] unfolding 0, single step 95001
2019-03-19 10:15:43,812 [INFO] Sum of grad norms of most recent batch: 0.003588
2019-03-19 10:15:43,812 [INFO] ---------------------------------
2019-03-19 10:16:04,875 [INFO] ---------------------------------
2019-03-19 10:16:04,876 [INFO] Summary:
2019-03-19 10:16:04,876 [INFO] Batch 96000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:16:04,877 [INFO] Regularization: 492.151611 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:16:04,877 [INFO] unfolding 0, single step 96001
2019-03-19 10:16:04,878 [INFO] Sum of grad norms of most recent batch: 0.001820
2019-03-19 10:16:04,878 [INFO] ---------------------------------
2019-03-19 10:16:25,699 [INFO] ---------------------------------
2019-03-19 10:16:25,700 [INFO] Summary:
2019-03-19 10:16:25,700 [INFO] Batch 97000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:16:25,701 [INFO] Regularization: 492.133575 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:16:25,701 [INFO] unfolding 0, single step 97001
2019-03-19 10:16:25,702 [INFO] Sum of grad norms of most recent batch: 0.001691
2019-03-19 10:16:25,702 [INFO] ---------------------------------
2019-03-19 10:16:46,679 [INFO] ---------------------------------
2019-03-19 10:16:46,680 [INFO] Summary:
2019-03-19 10:16:46,680 [INFO] Batch 98000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:16:46,681 [INFO] Regularization: 492.115906 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:16:46,681 [INFO] unfolding 0, single step 98001
2019-03-19 10:16:46,682 [INFO] Sum of grad norms of most recent batch: 0.003049
2019-03-19 10:16:46,682 [INFO] ---------------------------------
2019-03-19 10:17:07,721 [INFO] ---------------------------------
2019-03-19 10:17:07,722 [INFO] Summary:
2019-03-19 10:17:07,722 [INFO] Batch 99000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:17:07,723 [INFO] Regularization: 492.098022 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:17:07,723 [INFO] unfolding 0, single step 99001
2019-03-19 10:17:07,724 [INFO] Sum of grad norms of most recent batch: 0.002947
2019-03-19 10:17:07,724 [INFO] ---------------------------------
2019-03-19 10:17:28,412 [INFO] ---------------------------------
2019-03-19 10:17:28,413 [INFO] Summary:
2019-03-19 10:17:28,414 [INFO] Batch 100000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:17:28,415 [INFO] Regularization: 492.079620 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:17:28,415 [INFO] unfolding 0, single step 100001
2019-03-19 10:17:28,416 [INFO] Sum of grad norms of most recent batch: 0.002193
2019-03-19 10:17:28,416 [INFO] ---------------------------------
2019-03-19 10:17:34,240 [INFO] ---------------------------------
2019-03-19 10:17:34,242 [INFO] Evaluation:
2019-03-19 10:17:34,243 [INFO] Batch 100000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:17:34,244 [INFO] ---------------------------------
2019-03-19 10:17:55,189 [INFO] ---------------------------------
2019-03-19 10:17:55,190 [INFO] Summary:
2019-03-19 10:17:55,190 [INFO] Batch 101000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:17:55,191 [INFO] Regularization: 492.063843 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:17:55,191 [INFO] unfolding 0, single step 101001
2019-03-19 10:17:55,192 [INFO] Sum of grad norms of most recent batch: 0.007058
2019-03-19 10:17:55,192 [INFO] ---------------------------------
2019-03-19 10:18:16,115 [INFO] ---------------------------------
2019-03-19 10:18:16,116 [INFO] Summary:
2019-03-19 10:18:16,117 [INFO] Batch 102000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:18:16,117 [INFO] Regularization: 492.053680 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:18:16,118 [INFO] unfolding 0, single step 102001
2019-03-19 10:18:16,118 [INFO] Sum of grad norms of most recent batch: 0.002254
2019-03-19 10:18:16,119 [INFO] ---------------------------------
2019-03-19 10:18:36,776 [INFO] ---------------------------------
2019-03-19 10:18:36,777 [INFO] Summary:
2019-03-19 10:18:36,777 [INFO] Batch 103000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:18:36,778 [INFO] Regularization: 492.045502 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:18:36,778 [INFO] unfolding 0, single step 103001
2019-03-19 10:18:36,779 [INFO] Sum of grad norms of most recent batch: 0.003042
2019-03-19 10:18:36,779 [INFO] ---------------------------------
2019-03-19 10:18:58,072 [INFO] ---------------------------------
2019-03-19 10:18:58,073 [INFO] Summary:
2019-03-19 10:18:58,074 [INFO] Batch 104000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:18:58,074 [INFO] Regularization: 492.036285 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:18:58,075 [INFO] unfolding 0, single step 104001
2019-03-19 10:18:58,075 [INFO] Sum of grad norms of most recent batch: 0.009597
2019-03-19 10:18:58,076 [INFO] ---------------------------------
2019-03-19 10:19:18,620 [INFO] ---------------------------------
2019-03-19 10:19:18,621 [INFO] Summary:
2019-03-19 10:19:18,622 [INFO] Batch 105000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:19:18,622 [INFO] Regularization: 492.029053 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:19:18,623 [INFO] unfolding 0, single step 105001
2019-03-19 10:19:18,623 [INFO] Sum of grad norms of most recent batch: 0.001265
2019-03-19 10:19:18,624 [INFO] ---------------------------------
2019-03-19 10:19:39,444 [INFO] ---------------------------------
2019-03-19 10:19:39,445 [INFO] Summary:
2019-03-19 10:19:39,445 [INFO] Batch 106000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:19:39,446 [INFO] Regularization: 492.019531 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:19:39,446 [INFO] unfolding 0, single step 106001
2019-03-19 10:19:39,447 [INFO] Sum of grad norms of most recent batch: 0.011694
2019-03-19 10:19:39,447 [INFO] ---------------------------------
2019-03-19 10:20:00,563 [INFO] ---------------------------------
2019-03-19 10:20:00,564 [INFO] Summary:
2019-03-19 10:20:00,565 [INFO] Batch 107000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:20:00,565 [INFO] Regularization: 492.013306 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:20:00,565 [INFO] unfolding 0, single step 107001
2019-03-19 10:20:00,566 [INFO] Sum of grad norms of most recent batch: 0.001061
2019-03-19 10:20:00,566 [INFO] ---------------------------------
2019-03-19 10:20:21,935 [INFO] ---------------------------------
2019-03-19 10:20:21,936 [INFO] Summary:
2019-03-19 10:20:21,937 [INFO] Batch 108000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:20:21,937 [INFO] Regularization: 492.003754 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:20:21,938 [INFO] unfolding 0, single step 108001
2019-03-19 10:20:21,939 [INFO] Sum of grad norms of most recent batch: 0.001501
2019-03-19 10:20:21,939 [INFO] ---------------------------------
2019-03-19 10:20:43,027 [INFO] ---------------------------------
2019-03-19 10:20:43,028 [INFO] Summary:
2019-03-19 10:20:43,029 [INFO] Batch 109000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:20:43,029 [INFO] Regularization: 491.996185 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:20:43,030 [INFO] unfolding 0, single step 109001
2019-03-19 10:20:43,030 [INFO] Sum of grad norms of most recent batch: 0.007897
2019-03-19 10:20:43,031 [INFO] ---------------------------------
2019-03-19 10:21:03,876 [INFO] ---------------------------------
2019-03-19 10:21:03,877 [INFO] Summary:
2019-03-19 10:21:03,878 [INFO] Batch 110000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:21:03,878 [INFO] Regularization: 491.987122 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:21:03,879 [INFO] unfolding 0, single step 110001
2019-03-19 10:21:03,879 [INFO] Sum of grad norms of most recent batch: 0.000737
2019-03-19 10:21:03,880 [INFO] ---------------------------------
2019-03-19 10:21:09,785 [INFO] ---------------------------------
2019-03-19 10:21:09,786 [INFO] Evaluation:
2019-03-19 10:21:09,787 [INFO] Batch 110000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:21:09,788 [INFO] ---------------------------------
2019-03-19 10:21:30,696 [INFO] ---------------------------------
2019-03-19 10:21:30,697 [INFO] Summary:
2019-03-19 10:21:30,697 [INFO] Batch 111000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:21:30,698 [INFO] Regularization: 491.979523 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:21:30,699 [INFO] unfolding 0, single step 111001
2019-03-19 10:21:30,699 [INFO] Sum of grad norms of most recent batch: 0.002864
2019-03-19 10:21:30,700 [INFO] ---------------------------------
2019-03-19 10:21:51,460 [INFO] ---------------------------------
2019-03-19 10:21:51,461 [INFO] Summary:
2019-03-19 10:21:51,461 [INFO] Batch 112000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:21:51,462 [INFO] Regularization: 491.976471 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:21:51,463 [INFO] unfolding 0, single step 112001
2019-03-19 10:21:51,463 [INFO] Sum of grad norms of most recent batch: 0.009464
2019-03-19 10:21:51,464 [INFO] ---------------------------------
2019-03-19 10:22:12,239 [INFO] ---------------------------------
2019-03-19 10:22:12,240 [INFO] Summary:
2019-03-19 10:22:12,241 [INFO] Batch 113000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:22:12,242 [INFO] Regularization: 491.973267 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:22:12,242 [INFO] unfolding 0, single step 113001
2019-03-19 10:22:12,243 [INFO] Sum of grad norms of most recent batch: 0.001687
2019-03-19 10:22:12,244 [INFO] ---------------------------------
2019-03-19 10:22:33,315 [INFO] ---------------------------------
2019-03-19 10:22:33,316 [INFO] Summary:
2019-03-19 10:22:33,317 [INFO] Batch 114000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:22:33,317 [INFO] Regularization: 491.970093 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:22:33,318 [INFO] unfolding 0, single step 114001
2019-03-19 10:22:33,318 [INFO] Sum of grad norms of most recent batch: 0.001922
2019-03-19 10:22:33,319 [INFO] ---------------------------------
2019-03-19 10:22:53,984 [INFO] ---------------------------------
2019-03-19 10:22:53,985 [INFO] Summary:
2019-03-19 10:22:53,985 [INFO] Batch 115000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:22:53,986 [INFO] Regularization: 491.967316 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:22:53,986 [INFO] unfolding 0, single step 115001
2019-03-19 10:22:53,987 [INFO] Sum of grad norms of most recent batch: 0.008621
2019-03-19 10:22:53,987 [INFO] ---------------------------------
2019-03-19 10:23:15,030 [INFO] ---------------------------------
2019-03-19 10:23:15,030 [INFO] Summary:
2019-03-19 10:23:15,031 [INFO] Batch 116000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:23:15,031 [INFO] Regularization: 491.964508 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:23:15,032 [INFO] unfolding 0, single step 116001
2019-03-19 10:23:15,032 [INFO] Sum of grad norms of most recent batch: 0.006807
2019-03-19 10:23:15,033 [INFO] ---------------------------------
2019-03-19 10:23:35,910 [INFO] ---------------------------------
2019-03-19 10:23:35,911 [INFO] Summary:
2019-03-19 10:23:35,912 [INFO] Batch 117000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:23:35,912 [INFO] Regularization: 491.961334 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:23:35,913 [INFO] unfolding 0, single step 117001
2019-03-19 10:23:35,913 [INFO] Sum of grad norms of most recent batch: 0.001021
2019-03-19 10:23:35,914 [INFO] ---------------------------------
2019-03-19 10:23:57,020 [INFO] ---------------------------------
2019-03-19 10:23:57,021 [INFO] Summary:
2019-03-19 10:23:57,022 [INFO] Batch 118000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:23:57,022 [INFO] Regularization: 491.959381 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:23:57,022 [INFO] unfolding 0, single step 118001
2019-03-19 10:23:57,023 [INFO] Sum of grad norms of most recent batch: 0.007586
2019-03-19 10:23:57,023 [INFO] ---------------------------------
2019-03-19 10:24:17,989 [INFO] ---------------------------------
2019-03-19 10:24:17,990 [INFO] Summary:
2019-03-19 10:24:17,991 [INFO] Batch 119000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:24:17,992 [INFO] Regularization: 491.953918 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:24:17,992 [INFO] unfolding 0, single step 119001
2019-03-19 10:24:17,992 [INFO] Sum of grad norms of most recent batch: 0.011269
2019-03-19 10:24:17,993 [INFO] ---------------------------------
2019-03-19 10:24:38,556 [INFO] ---------------------------------
2019-03-19 10:24:38,557 [INFO] Summary:
2019-03-19 10:24:38,557 [INFO] Batch 120000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:24:38,558 [INFO] Regularization: 491.951202 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:24:38,558 [INFO] unfolding 0, single step 120001
2019-03-19 10:24:38,559 [INFO] Sum of grad norms of most recent batch: 0.010380
2019-03-19 10:24:38,560 [INFO] ---------------------------------
2019-03-19 10:24:44,360 [INFO] ---------------------------------
2019-03-19 10:24:44,361 [INFO] Evaluation:
2019-03-19 10:24:44,362 [INFO] Batch 120000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:24:44,366 [INFO] ---------------------------------
2019-03-19 10:25:05,355 [INFO] ---------------------------------
2019-03-19 10:25:05,356 [INFO] Summary:
2019-03-19 10:25:05,356 [INFO] Batch 121000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:25:05,357 [INFO] Regularization: 491.948669 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:25:05,357 [INFO] unfolding 0, single step 121001
2019-03-19 10:25:05,358 [INFO] Sum of grad norms of most recent batch: 0.001209
2019-03-19 10:25:05,358 [INFO] ---------------------------------
2019-03-19 10:25:26,198 [INFO] ---------------------------------
2019-03-19 10:25:26,199 [INFO] Summary:
2019-03-19 10:25:26,200 [INFO] Batch 122000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:25:26,200 [INFO] Regularization: 491.947845 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:25:26,200 [INFO] unfolding 0, single step 122001
2019-03-19 10:25:26,201 [INFO] Sum of grad norms of most recent batch: 0.003399
2019-03-19 10:25:26,201 [INFO] ---------------------------------
2019-03-19 10:25:47,221 [INFO] ---------------------------------
2019-03-19 10:25:47,222 [INFO] Summary:
2019-03-19 10:25:47,222 [INFO] Batch 123000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:25:47,223 [INFO] Regularization: 491.947296 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:25:47,223 [INFO] unfolding 0, single step 123001
2019-03-19 10:25:47,224 [INFO] Sum of grad norms of most recent batch: 0.002818
2019-03-19 10:25:47,225 [INFO] ---------------------------------
2019-03-19 10:26:08,186 [INFO] ---------------------------------
2019-03-19 10:26:08,187 [INFO] Summary:
2019-03-19 10:26:08,187 [INFO] Batch 124000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:26:08,188 [INFO] Regularization: 491.946106 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:26:08,188 [INFO] unfolding 0, single step 124001
2019-03-19 10:26:08,189 [INFO] Sum of grad norms of most recent batch: 0.006506
2019-03-19 10:26:08,190 [INFO] ---------------------------------
2019-03-19 10:26:29,291 [INFO] ---------------------------------
2019-03-19 10:26:29,292 [INFO] Summary:
2019-03-19 10:26:29,293 [INFO] Batch 125000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:26:29,293 [INFO] Regularization: 491.945496 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:26:29,294 [INFO] unfolding 0, single step 125001
2019-03-19 10:26:29,294 [INFO] Sum of grad norms of most recent batch: 0.002508
2019-03-19 10:26:29,295 [INFO] ---------------------------------
2019-03-19 10:26:50,480 [INFO] ---------------------------------
2019-03-19 10:26:50,481 [INFO] Summary:
2019-03-19 10:26:50,481 [INFO] Batch 126000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:26:50,482 [INFO] Regularization: 491.945068 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:26:50,482 [INFO] unfolding 0, single step 126001
2019-03-19 10:26:50,482 [INFO] Sum of grad norms of most recent batch: 0.001172
2019-03-19 10:26:50,483 [INFO] ---------------------------------
2019-03-19 10:27:11,434 [INFO] ---------------------------------
2019-03-19 10:27:11,435 [INFO] Summary:
2019-03-19 10:27:11,436 [INFO] Batch 127000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:27:11,436 [INFO] Regularization: 491.944916 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:27:11,437 [INFO] unfolding 0, single step 127001
2019-03-19 10:27:11,437 [INFO] Sum of grad norms of most recent batch: 0.009264
2019-03-19 10:27:11,438 [INFO] ---------------------------------
2019-03-19 10:27:32,169 [INFO] ---------------------------------
2019-03-19 10:27:32,170 [INFO] Summary:
2019-03-19 10:27:32,171 [INFO] Batch 128000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:27:32,171 [INFO] Regularization: 491.944733 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:27:32,172 [INFO] unfolding 0, single step 128001
2019-03-19 10:27:32,172 [INFO] Sum of grad norms of most recent batch: 0.010928
2019-03-19 10:27:32,173 [INFO] ---------------------------------
2019-03-19 10:27:53,234 [INFO] ---------------------------------
2019-03-19 10:27:53,235 [INFO] Summary:
2019-03-19 10:27:53,236 [INFO] Batch 129000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:27:53,237 [INFO] Regularization: 491.943604 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:27:53,237 [INFO] unfolding 0, single step 129001
2019-03-19 10:27:53,237 [INFO] Sum of grad norms of most recent batch: 0.011108
2019-03-19 10:27:53,238 [INFO] ---------------------------------
2019-03-19 10:28:14,107 [INFO] ---------------------------------
2019-03-19 10:28:14,108 [INFO] Summary:
2019-03-19 10:28:14,108 [INFO] Batch 130000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:28:14,109 [INFO] Regularization: 491.942993 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:28:14,109 [INFO] unfolding 0, single step 130001
2019-03-19 10:28:14,110 [INFO] Sum of grad norms of most recent batch: 0.002840
2019-03-19 10:28:14,111 [INFO] ---------------------------------
2019-03-19 10:28:19,961 [INFO] ---------------------------------
2019-03-19 10:28:19,962 [INFO] Evaluation:
2019-03-19 10:28:19,962 [INFO] Batch 130000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:28:19,963 [INFO] New best loss 0.000001, saved to file transition/1552984806/1552987699_0_transition_130000.pth
2019-03-19 10:28:19,975 [INFO] Target
2019-03-19 10:28:19,976 [INFO] [[0.8169]
 [0.2698]
 [0.3534]
 [0.7091]
 [0.2699]
 [0.2074]
 [0.6264]
 [0.6999]
 [0.0168]
 [0.4347]
 [0.9347]
 [0.0517]
 [0.0248]
 [0.2863]
 [0.7793]
 [0.3051]
 [0.3992]
 [0.6418]
 [0.337 ]
 [0.7281]
 [0.0585]
 [0.4934]
 [0.6123]
 [0.4983]
 [0.4816]
 [0.0882]
 [0.701 ]
 [0.2263]
 [0.4217]
 [0.7293]
 [0.6716]
 [0.972 ]]
2019-03-19 10:28:19,978 [INFO] Estimator output
2019-03-19 10:28:19,978 [INFO] [[0.816987]
 [0.269785]
 [0.3534  ]
 [0.709194]
 [0.2699  ]
 [0.207409]
 [0.6264  ]
 [0.6999  ]
 [0.0168  ]
 [0.4347  ]
 [0.9347  ]
 [0.0517  ]
 [0.0248  ]
 [0.2863  ]
 [0.7793  ]
 [0.3051  ]
 [0.399195]
 [0.6418  ]
 [0.337009]
 [0.7281  ]
 [0.0585  ]
 [0.4934  ]
 [0.612393]
 [0.4983  ]
 [0.481606]
 [0.088198]
 [0.701006]
 [0.2263  ]
 [0.4217  ]
 [0.7293  ]
 [0.671586]
 [0.972   ]]
2019-03-19 10:28:19,980 [INFO] ---------------------------------
2019-03-19 10:28:41,098 [INFO] ---------------------------------
2019-03-19 10:28:41,099 [INFO] Summary:
2019-03-19 10:28:41,100 [INFO] Batch 131000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:28:41,101 [INFO] Regularization: 491.941772 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:28:41,101 [INFO] unfolding 0, single step 131001
2019-03-19 10:28:41,102 [INFO] Sum of grad norms of most recent batch: 0.001096
2019-03-19 10:28:41,102 [INFO] ---------------------------------
2019-03-19 10:29:02,218 [INFO] ---------------------------------
2019-03-19 10:29:02,220 [INFO] Summary:
2019-03-19 10:29:02,220 [INFO] Batch 132000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:29:02,221 [INFO] Regularization: 491.941559 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:29:02,221 [INFO] unfolding 0, single step 132001
2019-03-19 10:29:02,222 [INFO] Sum of grad norms of most recent batch: 0.002395
2019-03-19 10:29:02,222 [INFO] ---------------------------------
2019-03-19 10:29:22,977 [INFO] ---------------------------------
2019-03-19 10:29:22,978 [INFO] Summary:
2019-03-19 10:29:22,979 [INFO] Batch 133000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:29:22,979 [INFO] Regularization: 491.941284 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:29:22,980 [INFO] unfolding 0, single step 133001
2019-03-19 10:29:22,980 [INFO] Sum of grad norms of most recent batch: 0.002694
2019-03-19 10:29:22,981 [INFO] ---------------------------------
2019-03-19 10:29:44,536 [INFO] ---------------------------------
2019-03-19 10:29:44,537 [INFO] Summary:
2019-03-19 10:29:44,538 [INFO] Batch 134000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:29:44,538 [INFO] Regularization: 491.941101 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:29:44,538 [INFO] unfolding 0, single step 134001
2019-03-19 10:29:44,539 [INFO] Sum of grad norms of most recent batch: 0.004805
2019-03-19 10:29:44,540 [INFO] ---------------------------------
2019-03-19 10:30:05,617 [INFO] ---------------------------------
2019-03-19 10:30:05,618 [INFO] Summary:
2019-03-19 10:30:05,619 [INFO] Batch 135000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:30:05,619 [INFO] Regularization: 491.940826 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:30:05,619 [INFO] unfolding 0, single step 135001
2019-03-19 10:30:05,620 [INFO] Sum of grad norms of most recent batch: 0.005139
2019-03-19 10:30:05,620 [INFO] ---------------------------------
2019-03-19 10:30:26,312 [INFO] ---------------------------------
2019-03-19 10:30:26,313 [INFO] Summary:
2019-03-19 10:30:26,313 [INFO] Batch 136000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:30:26,314 [INFO] Regularization: 491.940674 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:30:26,314 [INFO] unfolding 0, single step 136001
2019-03-19 10:30:26,315 [INFO] Sum of grad norms of most recent batch: 0.002037
2019-03-19 10:30:26,315 [INFO] ---------------------------------
2019-03-19 10:30:47,403 [INFO] ---------------------------------
2019-03-19 10:30:47,404 [INFO] Summary:
2019-03-19 10:30:47,404 [INFO] Batch 137000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:30:47,405 [INFO] Regularization: 491.940582 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:30:47,405 [INFO] unfolding 0, single step 137001
2019-03-19 10:30:47,406 [INFO] Sum of grad norms of most recent batch: 0.003533
2019-03-19 10:30:47,407 [INFO] ---------------------------------
2019-03-19 10:31:08,371 [INFO] ---------------------------------
2019-03-19 10:31:08,371 [INFO] Summary:
2019-03-19 10:31:08,372 [INFO] Batch 138000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:31:08,372 [INFO] Regularization: 491.940308 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:31:08,373 [INFO] unfolding 0, single step 138001
2019-03-19 10:31:08,374 [INFO] Sum of grad norms of most recent batch: 0.002588
2019-03-19 10:31:08,374 [INFO] ---------------------------------
2019-03-19 10:31:29,385 [INFO] ---------------------------------
2019-03-19 10:31:29,387 [INFO] Summary:
2019-03-19 10:31:29,387 [INFO] Batch 139000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:31:29,388 [INFO] Regularization: 491.940186 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:31:29,388 [INFO] unfolding 0, single step 139001
2019-03-19 10:31:29,389 [INFO] Sum of grad norms of most recent batch: 0.023468
2019-03-19 10:31:29,390 [INFO] ---------------------------------
2019-03-19 10:31:50,138 [INFO] ---------------------------------
2019-03-19 10:31:50,139 [INFO] Summary:
2019-03-19 10:31:50,140 [INFO] Batch 140000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:31:50,140 [INFO] Regularization: 491.939880 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:31:50,141 [INFO] unfolding 0, single step 140001
2019-03-19 10:31:50,141 [INFO] Sum of grad norms of most recent batch: 0.005129
2019-03-19 10:31:50,142 [INFO] ---------------------------------
2019-03-19 10:31:55,882 [INFO] ---------------------------------
2019-03-19 10:31:55,883 [INFO] Evaluation:
2019-03-19 10:31:55,884 [INFO] Batch 140000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:31:55,885 [INFO] New best loss 0.000001, saved to file transition/1552984806/1552987915_0_transition_140000.pth
2019-03-19 10:31:55,896 [INFO] Target
2019-03-19 10:31:55,896 [INFO] [[0.6105]
 [0.3581]
 [0.2573]
 [0.7564]
 [0.387 ]
 [0.1122]
 [0.9129]
 [0.9446]
 [0.3607]
 [0.6247]
 [0.8398]
 [0.3909]
 [0.1986]
 [0.6422]
 [0.1087]
 [0.6567]
 [0.3976]
 [0.3663]
 [0.0469]
 [0.2635]
 [0.837 ]
 [0.5387]
 [0.318 ]
 [0.0217]
 [0.304 ]
 [0.6443]
 [0.8882]
 [0.2033]
 [0.7657]
 [0.6135]
 [0.3532]
 [0.5372]]
2019-03-19 10:31:55,898 [INFO] Estimator output
2019-03-19 10:31:55,899 [INFO] [[0.6105  ]
 [0.3581  ]
 [0.2573  ]
 [0.756408]
 [0.386986]
 [0.1122  ]
 [0.9129  ]
 [0.9446  ]
 [0.3607  ]
 [0.6247  ]
 [0.839808]
 [0.3909  ]
 [0.1986  ]
 [0.6422  ]
 [0.1087  ]
 [0.6567  ]
 [0.397588]
 [0.3663  ]
 [0.0469  ]
 [0.2635  ]
 [0.836986]
 [0.538794]
 [0.318   ]
 [0.021792]
 [0.303991]
 [0.6443  ]
 [0.8882  ]
 [0.203299]
 [0.7657  ]
 [0.613413]
 [0.353207]
 [0.5372  ]]
2019-03-19 10:31:55,901 [INFO] ---------------------------------
2019-03-19 10:32:16,649 [INFO] ---------------------------------
2019-03-19 10:32:16,650 [INFO] Summary:
2019-03-19 10:32:16,650 [INFO] Batch 141000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:32:16,651 [INFO] Regularization: 491.939758 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:32:16,651 [INFO] unfolding 0, single step 141001
2019-03-19 10:32:16,652 [INFO] Sum of grad norms of most recent batch: 0.002474
2019-03-19 10:32:16,653 [INFO] ---------------------------------
2019-03-19 10:32:37,448 [INFO] ---------------------------------
2019-03-19 10:32:37,448 [INFO] Summary:
2019-03-19 10:32:37,449 [INFO] Batch 142000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:32:37,449 [INFO] Regularization: 491.939636 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:32:37,450 [INFO] unfolding 0, single step 142001
2019-03-19 10:32:37,450 [INFO] Sum of grad norms of most recent batch: 0.003168
2019-03-19 10:32:37,451 [INFO] ---------------------------------
2019-03-19 10:32:58,840 [INFO] ---------------------------------
2019-03-19 10:32:58,841 [INFO] Summary:
2019-03-19 10:32:58,842 [INFO] Batch 143000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:32:58,843 [INFO] Regularization: 491.939667 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:32:58,843 [INFO] unfolding 0, single step 143001
2019-03-19 10:32:58,844 [INFO] Sum of grad norms of most recent batch: 0.001127
2019-03-19 10:32:58,844 [INFO] ---------------------------------
2019-03-19 10:33:19,704 [INFO] ---------------------------------
2019-03-19 10:33:19,705 [INFO] Summary:
2019-03-19 10:33:19,706 [INFO] Batch 144000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:33:19,706 [INFO] Regularization: 491.939606 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:33:19,707 [INFO] unfolding 0, single step 144001
2019-03-19 10:33:19,707 [INFO] Sum of grad norms of most recent batch: 0.004087
2019-03-19 10:33:19,708 [INFO] ---------------------------------
2019-03-19 10:33:40,359 [INFO] ---------------------------------
2019-03-19 10:33:40,360 [INFO] Summary:
2019-03-19 10:33:40,361 [INFO] Batch 145000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:33:40,362 [INFO] Regularization: 491.939545 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:33:40,362 [INFO] unfolding 0, single step 145001
2019-03-19 10:33:40,363 [INFO] Sum of grad norms of most recent batch: 0.007636
2019-03-19 10:33:40,363 [INFO] ---------------------------------
2019-03-19 10:34:01,253 [INFO] ---------------------------------
2019-03-19 10:34:01,254 [INFO] Summary:
2019-03-19 10:34:01,254 [INFO] Batch 146000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:34:01,255 [INFO] Regularization: 491.939484 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:34:01,255 [INFO] unfolding 0, single step 146001
2019-03-19 10:34:01,256 [INFO] Sum of grad norms of most recent batch: 0.013548
2019-03-19 10:34:01,257 [INFO] ---------------------------------
2019-03-19 10:34:22,206 [INFO] ---------------------------------
2019-03-19 10:34:22,207 [INFO] Summary:
2019-03-19 10:34:22,207 [INFO] Batch 147000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:34:22,208 [INFO] Regularization: 491.939423 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:34:22,208 [INFO] unfolding 0, single step 147001
2019-03-19 10:34:22,209 [INFO] Sum of grad norms of most recent batch: 0.005943
2019-03-19 10:34:22,209 [INFO] ---------------------------------
2019-03-19 10:34:43,039 [INFO] ---------------------------------
2019-03-19 10:34:43,040 [INFO] Summary:
2019-03-19 10:34:43,041 [INFO] Batch 148000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:34:43,041 [INFO] Regularization: 491.939331 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:34:43,041 [INFO] unfolding 0, single step 148001
2019-03-19 10:34:43,042 [INFO] Sum of grad norms of most recent batch: 0.001094
2019-03-19 10:34:43,042 [INFO] ---------------------------------
2019-03-19 10:35:04,242 [INFO] ---------------------------------
2019-03-19 10:35:04,243 [INFO] Summary:
2019-03-19 10:35:04,244 [INFO] Batch 149000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:35:04,244 [INFO] Regularization: 491.939240 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:35:04,245 [INFO] unfolding 0, single step 149001
2019-03-19 10:35:04,245 [INFO] Sum of grad norms of most recent batch: 0.001223
2019-03-19 10:35:04,246 [INFO] ---------------------------------
2019-03-19 10:35:25,429 [INFO] ---------------------------------
2019-03-19 10:35:25,430 [INFO] Summary:
2019-03-19 10:35:25,431 [INFO] Batch 150000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 10:35:25,431 [INFO] Regularization: 491.939240 * 0.0000000001 = 0.0000000492 loss
2019-03-19 10:35:25,431 [INFO] unfolding 0, single step 150001
2019-03-19 10:35:25,432 [INFO] Sum of grad norms of most recent batch: 0.009498
2019-03-19 10:35:25,432 [INFO] ---------------------------------
2019-03-19 10:35:31,275 [INFO] ---------------------------------
2019-03-19 10:35:31,276 [INFO] Evaluation:
2019-03-19 10:35:31,276 [INFO] Batch 150000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:35:31,277 [INFO] New best loss 0.000001, saved to file transition/1552984806/1552988131_0_transition_150000.pth
2019-03-19 10:35:31,289 [INFO] Target
2019-03-19 10:35:31,290 [INFO] [[0.027 ]
 [0.3407]
 [0.2526]
 [0.7945]
 [0.4326]
 [0.6082]
 [0.4094]
 [0.0994]
 [0.7093]
 [0.2077]
 [0.055 ]
 [0.1635]
 [0.6804]
 [0.7651]
 [0.2839]
 [0.734 ]
 [0.6347]
 [0.4699]
 [0.7676]
 [0.6874]
 [0.7574]
 [0.0035]
 [0.4636]
 [0.0328]
 [0.2624]
 [0.1852]
 [0.6004]
 [0.276 ]
 [0.8888]
 [0.4486]
 [0.54  ]
 [0.9987]]
2019-03-19 10:35:31,291 [INFO] Estimator output
2019-03-19 10:35:31,292 [INFO] [[0.027   ]
 [0.3407  ]
 [0.25261 ]
 [0.7945  ]
 [0.43261 ]
 [0.608195]
 [0.4094  ]
 [0.0994  ]
 [0.709212]
 [0.2077  ]
 [0.055   ]
 [0.163413]
 [0.680401]
 [0.7651  ]
 [0.2839  ]
 [0.734   ]
 [0.634607]
 [0.4699  ]
 [0.767589]
 [0.6874  ]
 [0.7574  ]
 [0.0035  ]
 [0.4636  ]
 [0.0328  ]
 [0.2624  ]
 [0.185207]
 [0.600409]
 [0.276   ]
 [0.8888  ]
 [0.448594]
 [0.54    ]
 [0.9987  ]]
2019-03-19 10:35:31,294 [INFO] ---------------------------------
2019-03-19 10:35:31,294 [INFO] Finished training, saved to file transition/1552984806/1552988131_0_transition_final.pth
2019-03-19 10:35:31,507 [INFO] ---------------------------------
2019-03-19 10:35:31,508 [INFO] Training model #1: (203, 64, 1) @ 2
2019-03-19 10:35:52,967 [INFO] ---------------------------------
2019-03-19 10:35:52,968 [INFO] Summary:
2019-03-19 10:35:52,968 [INFO] Batch 1000, worst loss 13.849240 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:35:52,969 [INFO] Regularization: 9724.261719 * 0.0000000001 = 0.0000009724 loss
2019-03-19 10:35:52,969 [INFO] unfolding 0, single step 1001
2019-03-19 10:35:52,970 [INFO] Sum of grad norms of most recent batch: 5.815770
2019-03-19 10:35:52,971 [INFO] ---------------------------------
2019-03-19 10:36:14,276 [INFO] ---------------------------------
2019-03-19 10:36:14,276 [INFO] Summary:
2019-03-19 10:36:14,277 [INFO] Batch 2000, worst loss 0.171688 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:36:14,277 [INFO] Regularization: 9669.820312 * 0.0000000001 = 0.0000009670 loss
2019-03-19 10:36:14,278 [INFO] unfolding 0, single step 2001
2019-03-19 10:36:14,278 [INFO] Sum of grad norms of most recent batch: 1.459303
2019-03-19 10:36:14,279 [INFO] ---------------------------------
2019-03-19 10:36:35,583 [INFO] ---------------------------------
2019-03-19 10:36:35,584 [INFO] Summary:
2019-03-19 10:36:35,584 [INFO] Batch 3000, worst loss 0.030144 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:36:35,585 [INFO] Regularization: 9613.841797 * 0.0000000001 = 0.0000009614 loss
2019-03-19 10:36:35,585 [INFO] unfolding 0, single step 3001
2019-03-19 10:36:35,586 [INFO] Sum of grad norms of most recent batch: 2.914340
2019-03-19 10:36:35,586 [INFO] ---------------------------------
2019-03-19 10:36:57,174 [INFO] ---------------------------------
2019-03-19 10:36:57,175 [INFO] Summary:
2019-03-19 10:36:57,176 [INFO] Batch 4000, worst loss 0.047301 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:36:57,176 [INFO] Regularization: 9554.937500 * 0.0000000001 = 0.0000009555 loss
2019-03-19 10:36:57,177 [INFO] unfolding 0, single step 4001
2019-03-19 10:36:57,177 [INFO] Sum of grad norms of most recent batch: 1.233940
2019-03-19 10:36:57,178 [INFO] ---------------------------------
2019-03-19 10:37:19,105 [INFO] ---------------------------------
2019-03-19 10:37:19,106 [INFO] Summary:
2019-03-19 10:37:19,107 [INFO] Batch 5000, worst loss 0.023824 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:37:19,107 [INFO] Regularization: 9486.256836 * 0.0000000001 = 0.0000009486 loss
2019-03-19 10:37:19,108 [INFO] unfolding 0, single step 5001
2019-03-19 10:37:19,108 [INFO] Sum of grad norms of most recent batch: 3.151876
2019-03-19 10:37:19,109 [INFO] ---------------------------------
2019-03-19 10:37:40,619 [INFO] ---------------------------------
2019-03-19 10:37:40,620 [INFO] Summary:
2019-03-19 10:37:40,620 [INFO] Batch 6000, worst loss 0.024767 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:37:40,621 [INFO] Regularization: 9410.256836 * 0.0000000001 = 0.0000009410 loss
2019-03-19 10:37:40,621 [INFO] unfolding 0, single step 6001
2019-03-19 10:37:40,622 [INFO] Sum of grad norms of most recent batch: 1.550587
2019-03-19 10:37:40,623 [INFO] ---------------------------------
2019-03-19 10:38:02,016 [INFO] ---------------------------------
2019-03-19 10:38:02,017 [INFO] Summary:
2019-03-19 10:38:02,018 [INFO] Batch 7000, worst loss 0.027674 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:38:02,019 [INFO] Regularization: 9330.988281 * 0.0000000001 = 0.0000009331 loss
2019-03-19 10:38:02,019 [INFO] unfolding 0, single step 7001
2019-03-19 10:38:02,020 [INFO] Sum of grad norms of most recent batch: 2.168619
2019-03-19 10:38:02,021 [INFO] ---------------------------------
2019-03-19 10:38:23,566 [INFO] ---------------------------------
2019-03-19 10:38:23,567 [INFO] Summary:
2019-03-19 10:38:23,567 [INFO] Batch 8000, worst loss 0.018914 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:38:23,568 [INFO] Regularization: 9229.266602 * 0.0000000001 = 0.0000009229 loss
2019-03-19 10:38:23,568 [INFO] unfolding 0, single step 8001
2019-03-19 10:38:23,569 [INFO] Sum of grad norms of most recent batch: 0.649176
2019-03-19 10:38:23,569 [INFO] ---------------------------------
2019-03-19 10:38:45,431 [INFO] ---------------------------------
2019-03-19 10:38:45,431 [INFO] Summary:
2019-03-19 10:38:45,432 [INFO] Batch 9000, worst loss 0.026838 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:38:45,433 [INFO] Regularization: 9139.537109 * 0.0000000001 = 0.0000009140 loss
2019-03-19 10:38:45,433 [INFO] unfolding 0, single step 9001
2019-03-19 10:38:45,434 [INFO] Sum of grad norms of most recent batch: 7.040528
2019-03-19 10:38:45,434 [INFO] ---------------------------------
2019-03-19 10:39:07,259 [INFO] ---------------------------------
2019-03-19 10:39:07,260 [INFO] Summary:
2019-03-19 10:39:07,261 [INFO] Batch 10000, worst loss 0.019381 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:39:07,261 [INFO] Regularization: 9047.976562 * 0.0000000001 = 0.0000009048 loss
2019-03-19 10:39:07,261 [INFO] unfolding 0, single step 10001
2019-03-19 10:39:07,262 [INFO] Sum of grad norms of most recent batch: 2.226009
2019-03-19 10:39:07,262 [INFO] ---------------------------------
2019-03-19 10:39:13,029 [INFO] ---------------------------------
2019-03-19 10:39:13,031 [INFO] Evaluation:
2019-03-19 10:39:13,031 [INFO] Batch 10000, worst loss 0.004558 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:39:13,034 [INFO] ---------------------------------
2019-03-19 10:39:34,577 [INFO] ---------------------------------
2019-03-19 10:39:34,577 [INFO] Summary:
2019-03-19 10:39:34,578 [INFO] Batch 11000, worst loss 0.012630 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:39:34,579 [INFO] Regularization: 8937.235352 * 0.0000000001 = 0.0000008937 loss
2019-03-19 10:39:34,579 [INFO] unfolding 0, single step 11001
2019-03-19 10:39:34,579 [INFO] Sum of grad norms of most recent batch: 2.320872
2019-03-19 10:39:34,580 [INFO] ---------------------------------
2019-03-19 10:39:56,111 [INFO] ---------------------------------
2019-03-19 10:39:56,112 [INFO] Summary:
2019-03-19 10:39:56,113 [INFO] Batch 12000, worst loss 0.020445 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:39:56,113 [INFO] Regularization: 8825.454102 * 0.0000000001 = 0.0000008825 loss
2019-03-19 10:39:56,114 [INFO] unfolding 0, single step 12001
2019-03-19 10:39:56,114 [INFO] Sum of grad norms of most recent batch: 3.823719
2019-03-19 10:39:56,115 [INFO] ---------------------------------
2019-03-19 10:40:17,571 [INFO] ---------------------------------
2019-03-19 10:40:17,572 [INFO] Summary:
2019-03-19 10:40:17,573 [INFO] Batch 13000, worst loss 0.018449 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:40:17,573 [INFO] Regularization: 8706.117188 * 0.0000000001 = 0.0000008706 loss
2019-03-19 10:40:17,574 [INFO] unfolding 0, single step 13001
2019-03-19 10:40:17,574 [INFO] Sum of grad norms of most recent batch: 1.840163
2019-03-19 10:40:17,575 [INFO] ---------------------------------
2019-03-19 10:40:39,004 [INFO] ---------------------------------
2019-03-19 10:40:39,005 [INFO] Summary:
2019-03-19 10:40:39,006 [INFO] Batch 14000, worst loss 0.011452 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:40:39,006 [INFO] Regularization: 8574.256836 * 0.0000000001 = 0.0000008574 loss
2019-03-19 10:40:39,007 [INFO] unfolding 0, single step 14001
2019-03-19 10:40:39,007 [INFO] Sum of grad norms of most recent batch: 2.481248
2019-03-19 10:40:39,008 [INFO] ---------------------------------
2019-03-19 10:41:00,829 [INFO] ---------------------------------
2019-03-19 10:41:00,830 [INFO] Summary:
2019-03-19 10:41:00,831 [INFO] Batch 15000, worst loss 0.023317 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:41:00,831 [INFO] Regularization: 8436.671875 * 0.0000000001 = 0.0000008437 loss
2019-03-19 10:41:00,832 [INFO] unfolding 0, single step 15001
2019-03-19 10:41:00,832 [INFO] Sum of grad norms of most recent batch: 4.258870
2019-03-19 10:41:00,833 [INFO] ---------------------------------
2019-03-19 10:41:22,189 [INFO] ---------------------------------
2019-03-19 10:41:22,190 [INFO] Summary:
2019-03-19 10:41:22,191 [INFO] Batch 16000, worst loss 0.022874 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:41:22,192 [INFO] Regularization: 8356.099609 * 0.0000000001 = 0.0000008356 loss
2019-03-19 10:41:22,192 [INFO] unfolding 0, single step 16001
2019-03-19 10:41:22,193 [INFO] Sum of grad norms of most recent batch: 1.515591
2019-03-19 10:41:22,193 [INFO] ---------------------------------
2019-03-19 10:41:43,674 [INFO] ---------------------------------
2019-03-19 10:41:43,675 [INFO] Summary:
2019-03-19 10:41:43,675 [INFO] Batch 17000, worst loss 0.030356 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:41:43,676 [INFO] Regularization: 8237.629883 * 0.0000000001 = 0.0000008238 loss
2019-03-19 10:41:43,676 [INFO] unfolding 0, single step 17001
2019-03-19 10:41:43,677 [INFO] Sum of grad norms of most recent batch: 2.656538
2019-03-19 10:41:43,678 [INFO] ---------------------------------
2019-03-19 10:42:04,852 [INFO] ---------------------------------
2019-03-19 10:42:04,853 [INFO] Summary:
2019-03-19 10:42:04,854 [INFO] Batch 18000, worst loss 0.019571 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:42:04,855 [INFO] Regularization: 8125.808594 * 0.0000000001 = 0.0000008126 loss
2019-03-19 10:42:04,855 [INFO] unfolding 0, single step 18001
2019-03-19 10:42:04,856 [INFO] Sum of grad norms of most recent batch: 2.101595
2019-03-19 10:42:04,856 [INFO] ---------------------------------
2019-03-19 10:42:26,406 [INFO] ---------------------------------
2019-03-19 10:42:26,407 [INFO] Summary:
2019-03-19 10:42:26,408 [INFO] Batch 19000, worst loss 0.019017 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:42:26,408 [INFO] Regularization: 8031.171387 * 0.0000000001 = 0.0000008031 loss
2019-03-19 10:42:26,408 [INFO] unfolding 0, single step 19001
2019-03-19 10:42:26,409 [INFO] Sum of grad norms of most recent batch: 2.271594
2019-03-19 10:42:26,410 [INFO] ---------------------------------
2019-03-19 10:42:47,743 [INFO] ---------------------------------
2019-03-19 10:42:47,744 [INFO] Summary:
2019-03-19 10:42:47,745 [INFO] Batch 20000, worst loss 0.018510 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:42:47,746 [INFO] Regularization: 7938.097168 * 0.0000000001 = 0.0000007938 loss
2019-03-19 10:42:47,746 [INFO] unfolding 0, single step 20001
2019-03-19 10:42:47,747 [INFO] Sum of grad norms of most recent batch: 2.136734
2019-03-19 10:42:47,747 [INFO] ---------------------------------
2019-03-19 10:42:53,653 [INFO] ---------------------------------
2019-03-19 10:42:53,654 [INFO] Evaluation:
2019-03-19 10:42:53,656 [INFO] Batch 20000, worst loss 0.020156 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:42:53,657 [INFO] ---------------------------------
2019-03-19 10:43:15,317 [INFO] ---------------------------------
2019-03-19 10:43:15,318 [INFO] Summary:
2019-03-19 10:43:15,318 [INFO] Batch 21000, worst loss 0.023508 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:43:15,319 [INFO] Regularization: 7785.014648 * 0.0000000001 = 0.0000007785 loss
2019-03-19 10:43:15,320 [INFO] unfolding 0, single step 21001
2019-03-19 10:43:15,321 [INFO] Sum of grad norms of most recent batch: 2.432433
2019-03-19 10:43:15,322 [INFO] ---------------------------------
2019-03-19 10:43:36,664 [INFO] ---------------------------------
2019-03-19 10:43:36,665 [INFO] Summary:
2019-03-19 10:43:36,665 [INFO] Batch 22000, worst loss 0.018374 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:43:36,666 [INFO] Regularization: 7646.528809 * 0.0000000001 = 0.0000007647 loss
2019-03-19 10:43:36,666 [INFO] unfolding 0, single step 22001
2019-03-19 10:43:36,667 [INFO] Sum of grad norms of most recent batch: 3.336402
2019-03-19 10:43:36,667 [INFO] ---------------------------------
2019-03-19 10:43:58,103 [INFO] ---------------------------------
2019-03-19 10:43:58,103 [INFO] Summary:
2019-03-19 10:43:58,104 [INFO] Batch 23000, worst loss 0.014730 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:43:58,105 [INFO] Regularization: 7524.699219 * 0.0000000001 = 0.0000007525 loss
2019-03-19 10:43:58,105 [INFO] unfolding 0, single step 23001
2019-03-19 10:43:58,105 [INFO] Sum of grad norms of most recent batch: 2.918386
2019-03-19 10:43:58,106 [INFO] ---------------------------------
2019-03-19 10:44:19,845 [INFO] ---------------------------------
2019-03-19 10:44:19,846 [INFO] Summary:
2019-03-19 10:44:19,847 [INFO] Batch 24000, worst loss 0.010024 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:44:19,847 [INFO] Regularization: 7410.883301 * 0.0000000001 = 0.0000007411 loss
2019-03-19 10:44:19,848 [INFO] unfolding 0, single step 24001
2019-03-19 10:44:19,848 [INFO] Sum of grad norms of most recent batch: 2.395358
2019-03-19 10:44:19,849 [INFO] ---------------------------------
2019-03-19 10:44:41,458 [INFO] ---------------------------------
2019-03-19 10:44:41,459 [INFO] Summary:
2019-03-19 10:44:41,459 [INFO] Batch 25000, worst loss 0.010859 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:44:41,460 [INFO] Regularization: 7287.590332 * 0.0000000001 = 0.0000007288 loss
2019-03-19 10:44:41,460 [INFO] unfolding 0, single step 25001
2019-03-19 10:44:41,461 [INFO] Sum of grad norms of most recent batch: 3.783694
2019-03-19 10:44:41,461 [INFO] ---------------------------------
2019-03-19 10:45:03,282 [INFO] ---------------------------------
2019-03-19 10:45:03,283 [INFO] Summary:
2019-03-19 10:45:03,283 [INFO] Batch 26000, worst loss 0.010274 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:45:03,284 [INFO] Regularization: 7177.374512 * 0.0000000001 = 0.0000007177 loss
2019-03-19 10:45:03,284 [INFO] unfolding 0, single step 26001
2019-03-19 10:45:03,285 [INFO] Sum of grad norms of most recent batch: 1.792660
2019-03-19 10:45:03,285 [INFO] ---------------------------------
2019-03-19 10:45:25,024 [INFO] ---------------------------------
2019-03-19 10:45:25,025 [INFO] Summary:
2019-03-19 10:45:25,026 [INFO] Batch 27000, worst loss 0.008900 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:45:25,026 [INFO] Regularization: 7055.591797 * 0.0000000001 = 0.0000007056 loss
2019-03-19 10:45:25,027 [INFO] unfolding 0, single step 27001
2019-03-19 10:45:25,027 [INFO] Sum of grad norms of most recent batch: 0.774639
2019-03-19 10:45:25,028 [INFO] ---------------------------------
2019-03-19 10:45:46,507 [INFO] ---------------------------------
2019-03-19 10:45:46,507 [INFO] Summary:
2019-03-19 10:45:46,508 [INFO] Batch 28000, worst loss 0.015575 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:45:46,508 [INFO] Regularization: 6934.129883 * 0.0000000001 = 0.0000006934 loss
2019-03-19 10:45:46,509 [INFO] unfolding 0, single step 28001
2019-03-19 10:45:46,509 [INFO] Sum of grad norms of most recent batch: 1.921386
2019-03-19 10:45:46,510 [INFO] ---------------------------------
2019-03-19 10:46:08,062 [INFO] ---------------------------------
2019-03-19 10:46:08,063 [INFO] Summary:
2019-03-19 10:46:08,063 [INFO] Batch 29000, worst loss 0.013855 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:46:08,064 [INFO] Regularization: 6810.171875 * 0.0000000001 = 0.0000006810 loss
2019-03-19 10:46:08,064 [INFO] unfolding 0, single step 29001
2019-03-19 10:46:08,065 [INFO] Sum of grad norms of most recent batch: 1.381754
2019-03-19 10:46:08,065 [INFO] ---------------------------------
2019-03-19 10:46:29,799 [INFO] ---------------------------------
2019-03-19 10:46:29,800 [INFO] Summary:
2019-03-19 10:46:29,801 [INFO] Batch 30000, worst loss 0.011566 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:46:29,801 [INFO] Regularization: 6702.129395 * 0.0000000001 = 0.0000006702 loss
2019-03-19 10:46:29,802 [INFO] unfolding 0, single step 30001
2019-03-19 10:46:29,802 [INFO] Sum of grad norms of most recent batch: 2.887377
2019-03-19 10:46:29,803 [INFO] ---------------------------------
2019-03-19 10:46:35,655 [INFO] ---------------------------------
2019-03-19 10:46:35,656 [INFO] Evaluation:
2019-03-19 10:46:35,659 [INFO] Batch 30000, worst loss 0.003245 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:46:35,659 [INFO] ---------------------------------
2019-03-19 10:46:57,454 [INFO] ---------------------------------
2019-03-19 10:46:57,455 [INFO] Summary:
2019-03-19 10:46:57,455 [INFO] Batch 31000, worst loss 0.011032 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:46:57,456 [INFO] Regularization: 6565.006836 * 0.0000000001 = 0.0000006565 loss
2019-03-19 10:46:57,456 [INFO] unfolding 0, single step 31001
2019-03-19 10:46:57,457 [INFO] Sum of grad norms of most recent batch: 2.608611
2019-03-19 10:46:57,457 [INFO] ---------------------------------
2019-03-19 10:47:18,856 [INFO] ---------------------------------
2019-03-19 10:47:18,857 [INFO] Summary:
2019-03-19 10:47:18,857 [INFO] Batch 32000, worst loss 0.009990 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:47:18,858 [INFO] Regularization: 6453.445801 * 0.0000000001 = 0.0000006453 loss
2019-03-19 10:47:18,858 [INFO] unfolding 0, single step 32001
2019-03-19 10:47:18,859 [INFO] Sum of grad norms of most recent batch: 0.915174
2019-03-19 10:47:18,859 [INFO] ---------------------------------
2019-03-19 10:47:40,688 [INFO] ---------------------------------
2019-03-19 10:47:40,689 [INFO] Summary:
2019-03-19 10:47:40,690 [INFO] Batch 33000, worst loss 0.012764 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:47:40,691 [INFO] Regularization: 6384.196777 * 0.0000000001 = 0.0000006384 loss
2019-03-19 10:47:40,691 [INFO] unfolding 0, single step 33001
2019-03-19 10:47:40,692 [INFO] Sum of grad norms of most recent batch: 3.048980
2019-03-19 10:47:40,692 [INFO] ---------------------------------
2019-03-19 10:48:02,274 [INFO] ---------------------------------
2019-03-19 10:48:02,275 [INFO] Summary:
2019-03-19 10:48:02,275 [INFO] Batch 34000, worst loss 0.015573 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:48:02,276 [INFO] Regularization: 6247.284668 * 0.0000000001 = 0.0000006247 loss
2019-03-19 10:48:02,276 [INFO] unfolding 0, single step 34001
2019-03-19 10:48:02,276 [INFO] Sum of grad norms of most recent batch: 3.226729
2019-03-19 10:48:02,277 [INFO] ---------------------------------
2019-03-19 10:48:24,111 [INFO] ---------------------------------
2019-03-19 10:48:24,112 [INFO] Summary:
2019-03-19 10:48:24,112 [INFO] Batch 35000, worst loss 0.009815 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:48:24,113 [INFO] Regularization: 6108.234375 * 0.0000000001 = 0.0000006108 loss
2019-03-19 10:48:24,113 [INFO] unfolding 0, single step 35001
2019-03-19 10:48:24,114 [INFO] Sum of grad norms of most recent batch: 0.358182
2019-03-19 10:48:24,115 [INFO] ---------------------------------
2019-03-19 10:48:45,750 [INFO] ---------------------------------
2019-03-19 10:48:45,750 [INFO] Summary:
2019-03-19 10:48:45,751 [INFO] Batch 36000, worst loss 0.006627 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:48:45,751 [INFO] Regularization: 5968.877930 * 0.0000000001 = 0.0000005969 loss
2019-03-19 10:48:45,752 [INFO] unfolding 0, single step 36001
2019-03-19 10:48:45,752 [INFO] Sum of grad norms of most recent batch: 1.441015
2019-03-19 10:48:45,753 [INFO] ---------------------------------
2019-03-19 10:49:07,378 [INFO] ---------------------------------
2019-03-19 10:49:07,379 [INFO] Summary:
2019-03-19 10:49:07,379 [INFO] Batch 37000, worst loss 0.011831 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:49:07,380 [INFO] Regularization: 5884.943848 * 0.0000000001 = 0.0000005885 loss
2019-03-19 10:49:07,380 [INFO] unfolding 0, single step 37001
2019-03-19 10:49:07,381 [INFO] Sum of grad norms of most recent batch: 2.240013
2019-03-19 10:49:07,381 [INFO] ---------------------------------
2019-03-19 10:49:28,973 [INFO] ---------------------------------
2019-03-19 10:49:28,974 [INFO] Summary:
2019-03-19 10:49:28,974 [INFO] Batch 38000, worst loss 0.008794 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:49:28,975 [INFO] Regularization: 5748.215332 * 0.0000000001 = 0.0000005748 loss
2019-03-19 10:49:28,975 [INFO] unfolding 0, single step 38001
2019-03-19 10:49:28,976 [INFO] Sum of grad norms of most recent batch: 2.133176
2019-03-19 10:49:28,976 [INFO] ---------------------------------
2019-03-19 10:49:50,225 [INFO] ---------------------------------
2019-03-19 10:49:50,226 [INFO] Summary:
2019-03-19 10:49:50,227 [INFO] Batch 39000, worst loss 0.008789 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:49:50,227 [INFO] Regularization: 5652.917969 * 0.0000000001 = 0.0000005653 loss
2019-03-19 10:49:50,228 [INFO] unfolding 0, single step 39001
2019-03-19 10:49:50,228 [INFO] Sum of grad norms of most recent batch: 1.620252
2019-03-19 10:49:50,229 [INFO] ---------------------------------
2019-03-19 10:50:11,570 [INFO] ---------------------------------
2019-03-19 10:50:11,571 [INFO] Summary:
2019-03-19 10:50:11,572 [INFO] Batch 40000, worst loss 0.012476 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:50:11,572 [INFO] Regularization: 5539.278809 * 0.0000000001 = 0.0000005539 loss
2019-03-19 10:50:11,572 [INFO] unfolding 0, single step 40001
2019-03-19 10:50:11,573 [INFO] Sum of grad norms of most recent batch: 0.676019
2019-03-19 10:50:11,574 [INFO] ---------------------------------
2019-03-19 10:50:17,365 [INFO] ---------------------------------
2019-03-19 10:50:17,365 [INFO] Evaluation:
2019-03-19 10:50:17,366 [INFO] Batch 40000, worst loss 0.004671 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:50:17,367 [INFO] ---------------------------------
2019-03-19 10:50:38,713 [INFO] ---------------------------------
2019-03-19 10:50:38,714 [INFO] Summary:
2019-03-19 10:50:38,715 [INFO] Batch 41000, worst loss 0.007714 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:50:38,716 [INFO] Regularization: 5389.138672 * 0.0000000001 = 0.0000005389 loss
2019-03-19 10:50:38,716 [INFO] unfolding 0, single step 41001
2019-03-19 10:50:38,717 [INFO] Sum of grad norms of most recent batch: 1.200256
2019-03-19 10:50:38,718 [INFO] ---------------------------------
2019-03-19 10:51:00,027 [INFO] ---------------------------------
2019-03-19 10:51:00,028 [INFO] Summary:
2019-03-19 10:51:00,028 [INFO] Batch 42000, worst loss 0.006847 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:51:00,029 [INFO] Regularization: 5249.100586 * 0.0000000001 = 0.0000005249 loss
2019-03-19 10:51:00,029 [INFO] unfolding 0, single step 42001
2019-03-19 10:51:00,030 [INFO] Sum of grad norms of most recent batch: 1.002600
2019-03-19 10:51:00,030 [INFO] ---------------------------------
2019-03-19 10:51:21,397 [INFO] ---------------------------------
2019-03-19 10:51:21,398 [INFO] Summary:
2019-03-19 10:51:21,398 [INFO] Batch 43000, worst loss 0.005013 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:51:21,399 [INFO] Regularization: 5098.404297 * 0.0000000001 = 0.0000005098 loss
2019-03-19 10:51:21,399 [INFO] unfolding 0, single step 43001
2019-03-19 10:51:21,400 [INFO] Sum of grad norms of most recent batch: 1.408759
2019-03-19 10:51:21,401 [INFO] ---------------------------------
2019-03-19 10:51:43,138 [INFO] ---------------------------------
2019-03-19 10:51:43,139 [INFO] Summary:
2019-03-19 10:51:43,139 [INFO] Batch 44000, worst loss 0.007076 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:51:43,140 [INFO] Regularization: 4965.202637 * 0.0000000001 = 0.0000004965 loss
2019-03-19 10:51:43,140 [INFO] unfolding 0, single step 44001
2019-03-19 10:51:43,141 [INFO] Sum of grad norms of most recent batch: 0.501859
2019-03-19 10:51:43,141 [INFO] ---------------------------------
2019-03-19 10:52:04,451 [INFO] ---------------------------------
2019-03-19 10:52:04,452 [INFO] Summary:
2019-03-19 10:52:04,453 [INFO] Batch 45000, worst loss 0.005944 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:52:04,453 [INFO] Regularization: 4817.231934 * 0.0000000001 = 0.0000004817 loss
2019-03-19 10:52:04,454 [INFO] unfolding 0, single step 45001
2019-03-19 10:52:04,455 [INFO] Sum of grad norms of most recent batch: 0.817776
2019-03-19 10:52:04,455 [INFO] ---------------------------------
2019-03-19 10:52:25,868 [INFO] ---------------------------------
2019-03-19 10:52:25,869 [INFO] Summary:
2019-03-19 10:52:25,870 [INFO] Batch 46000, worst loss 0.005626 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:52:25,870 [INFO] Regularization: 4683.018066 * 0.0000000001 = 0.0000004683 loss
2019-03-19 10:52:25,871 [INFO] unfolding 0, single step 46001
2019-03-19 10:52:25,871 [INFO] Sum of grad norms of most recent batch: 0.642855
2019-03-19 10:52:25,872 [INFO] ---------------------------------
2019-03-19 10:52:47,284 [INFO] ---------------------------------
2019-03-19 10:52:47,285 [INFO] Summary:
2019-03-19 10:52:47,285 [INFO] Batch 47000, worst loss 0.006494 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:52:47,286 [INFO] Regularization: 4559.734375 * 0.0000000001 = 0.0000004560 loss
2019-03-19 10:52:47,286 [INFO] unfolding 0, single step 47001
2019-03-19 10:52:47,287 [INFO] Sum of grad norms of most recent batch: 1.639730
2019-03-19 10:52:47,287 [INFO] ---------------------------------
2019-03-19 10:53:08,861 [INFO] ---------------------------------
2019-03-19 10:53:08,862 [INFO] Summary:
2019-03-19 10:53:08,862 [INFO] Batch 48000, worst loss 0.006034 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:53:08,863 [INFO] Regularization: 4418.765625 * 0.0000000001 = 0.0000004419 loss
2019-03-19 10:53:08,863 [INFO] unfolding 0, single step 48001
2019-03-19 10:53:08,863 [INFO] Sum of grad norms of most recent batch: 0.571624
2019-03-19 10:53:08,864 [INFO] ---------------------------------
2019-03-19 10:53:30,484 [INFO] ---------------------------------
2019-03-19 10:53:30,485 [INFO] Summary:
2019-03-19 10:53:30,486 [INFO] Batch 49000, worst loss 0.006605 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:53:30,486 [INFO] Regularization: 4289.755371 * 0.0000000001 = 0.0000004290 loss
2019-03-19 10:53:30,487 [INFO] unfolding 0, single step 49001
2019-03-19 10:53:30,487 [INFO] Sum of grad norms of most recent batch: 0.572456
2019-03-19 10:53:30,488 [INFO] ---------------------------------
2019-03-19 10:53:52,049 [INFO] ---------------------------------
2019-03-19 10:53:52,050 [INFO] Summary:
2019-03-19 10:53:52,051 [INFO] Batch 50000, worst loss 0.005667 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:53:52,051 [INFO] Regularization: 4158.459961 * 0.0000000001 = 0.0000004158 loss
2019-03-19 10:53:52,052 [INFO] unfolding 0, single step 50001
2019-03-19 10:53:52,052 [INFO] Sum of grad norms of most recent batch: 1.701935
2019-03-19 10:53:52,053 [INFO] ---------------------------------
2019-03-19 10:53:57,935 [INFO] ---------------------------------
2019-03-19 10:53:57,937 [INFO] Evaluation:
2019-03-19 10:53:57,939 [INFO] Batch 50000, worst loss 0.001589 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:53:57,940 [INFO] ---------------------------------
2019-03-19 10:54:19,360 [INFO] ---------------------------------
2019-03-19 10:54:19,361 [INFO] Summary:
2019-03-19 10:54:19,361 [INFO] Batch 51000, worst loss 0.009163 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:54:19,362 [INFO] Regularization: 4024.021729 * 0.0000000001 = 0.0000004024 loss
2019-03-19 10:54:19,362 [INFO] unfolding 0, single step 51001
2019-03-19 10:54:19,363 [INFO] Sum of grad norms of most recent batch: 0.581888
2019-03-19 10:54:19,363 [INFO] ---------------------------------
2019-03-19 10:54:40,811 [INFO] ---------------------------------
2019-03-19 10:54:40,812 [INFO] Summary:
2019-03-19 10:54:40,812 [INFO] Batch 52000, worst loss 0.003931 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:54:40,813 [INFO] Regularization: 3881.786865 * 0.0000000001 = 0.0000003882 loss
2019-03-19 10:54:40,813 [INFO] unfolding 0, single step 52001
2019-03-19 10:54:40,814 [INFO] Sum of grad norms of most recent batch: 0.976495
2019-03-19 10:54:40,814 [INFO] ---------------------------------
2019-03-19 10:55:02,528 [INFO] ---------------------------------
2019-03-19 10:55:02,529 [INFO] Summary:
2019-03-19 10:55:02,530 [INFO] Batch 53000, worst loss 0.005970 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:55:02,531 [INFO] Regularization: 3750.710693 * 0.0000000001 = 0.0000003751 loss
2019-03-19 10:55:02,531 [INFO] unfolding 0, single step 53001
2019-03-19 10:55:02,532 [INFO] Sum of grad norms of most recent batch: 0.754047
2019-03-19 10:55:02,532 [INFO] ---------------------------------
2019-03-19 10:55:24,381 [INFO] ---------------------------------
2019-03-19 10:55:24,382 [INFO] Summary:
2019-03-19 10:55:24,382 [INFO] Batch 54000, worst loss 0.004636 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:55:24,383 [INFO] Regularization: 3610.329590 * 0.0000000001 = 0.0000003610 loss
2019-03-19 10:55:24,383 [INFO] unfolding 0, single step 54001
2019-03-19 10:55:24,384 [INFO] Sum of grad norms of most recent batch: 0.826720
2019-03-19 10:55:24,384 [INFO] ---------------------------------
2019-03-19 10:55:45,750 [INFO] ---------------------------------
2019-03-19 10:55:45,751 [INFO] Summary:
2019-03-19 10:55:45,751 [INFO] Batch 55000, worst loss 0.004490 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:55:45,752 [INFO] Regularization: 3465.597900 * 0.0000000001 = 0.0000003466 loss
2019-03-19 10:55:45,752 [INFO] unfolding 0, single step 55001
2019-03-19 10:55:45,753 [INFO] Sum of grad norms of most recent batch: 1.405868
2019-03-19 10:55:45,754 [INFO] ---------------------------------
2019-03-19 10:56:07,400 [INFO] ---------------------------------
2019-03-19 10:56:07,401 [INFO] Summary:
2019-03-19 10:56:07,402 [INFO] Batch 56000, worst loss 0.004626 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:56:07,402 [INFO] Regularization: 3337.966797 * 0.0000000001 = 0.0000003338 loss
2019-03-19 10:56:07,402 [INFO] unfolding 0, single step 56001
2019-03-19 10:56:07,403 [INFO] Sum of grad norms of most recent batch: 1.100200
2019-03-19 10:56:07,404 [INFO] ---------------------------------
2019-03-19 10:56:28,626 [INFO] ---------------------------------
2019-03-19 10:56:28,627 [INFO] Summary:
2019-03-19 10:56:28,628 [INFO] Batch 57000, worst loss 0.005420 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:56:28,628 [INFO] Regularization: 3223.601318 * 0.0000000001 = 0.0000003224 loss
2019-03-19 10:56:28,628 [INFO] unfolding 0, single step 57001
2019-03-19 10:56:28,629 [INFO] Sum of grad norms of most recent batch: 0.903770
2019-03-19 10:56:28,629 [INFO] ---------------------------------
2019-03-19 10:56:49,971 [INFO] ---------------------------------
2019-03-19 10:56:49,971 [INFO] Summary:
2019-03-19 10:56:49,972 [INFO] Batch 58000, worst loss 0.005931 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:56:49,972 [INFO] Regularization: 3095.890137 * 0.0000000001 = 0.0000003096 loss
2019-03-19 10:56:49,973 [INFO] unfolding 0, single step 58001
2019-03-19 10:56:49,974 [INFO] Sum of grad norms of most recent batch: 3.203105
2019-03-19 10:56:49,974 [INFO] ---------------------------------
2019-03-19 10:57:11,734 [INFO] ---------------------------------
2019-03-19 10:57:11,735 [INFO] Summary:
2019-03-19 10:57:11,736 [INFO] Batch 59000, worst loss 0.007978 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:57:11,736 [INFO] Regularization: 3001.521729 * 0.0000000001 = 0.0000003002 loss
2019-03-19 10:57:11,736 [INFO] unfolding 0, single step 59001
2019-03-19 10:57:11,737 [INFO] Sum of grad norms of most recent batch: 1.601359
2019-03-19 10:57:11,738 [INFO] ---------------------------------
2019-03-19 10:57:33,000 [INFO] ---------------------------------
2019-03-19 10:57:33,001 [INFO] Summary:
2019-03-19 10:57:33,002 [INFO] Batch 60000, worst loss 0.007845 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 10:57:33,002 [INFO] Regularization: 2879.811768 * 0.0000000001 = 0.0000002880 loss
2019-03-19 10:57:33,003 [INFO] unfolding 0, single step 60001
2019-03-19 10:57:33,003 [INFO] Sum of grad norms of most recent batch: 1.664455
2019-03-19 10:57:33,004 [INFO] ---------------------------------
2019-03-19 10:57:38,714 [INFO] ---------------------------------
2019-03-19 10:57:38,715 [INFO] Evaluation:
2019-03-19 10:57:38,715 [INFO] Batch 60000, worst loss 0.005695 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 10:57:38,716 [INFO] ---------------------------------
2019-03-19 10:58:00,299 [INFO] ---------------------------------
2019-03-19 10:58:00,300 [INFO] Summary:
2019-03-19 10:58:00,301 [INFO] Batch 61000, worst loss 0.003955 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:58:00,301 [INFO] Regularization: 2778.524658 * 0.0000000001 = 0.0000002779 loss
2019-03-19 10:58:00,302 [INFO] unfolding 0, single step 61001
2019-03-19 10:58:00,302 [INFO] Sum of grad norms of most recent batch: 0.365339
2019-03-19 10:58:00,303 [INFO] ---------------------------------
2019-03-19 10:58:21,883 [INFO] ---------------------------------
2019-03-19 10:58:21,884 [INFO] Summary:
2019-03-19 10:58:21,884 [INFO] Batch 62000, worst loss 0.001361 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:58:21,885 [INFO] Regularization: 2658.987305 * 0.0000000001 = 0.0000002659 loss
2019-03-19 10:58:21,885 [INFO] unfolding 0, single step 62001
2019-03-19 10:58:21,886 [INFO] Sum of grad norms of most recent batch: 0.269500
2019-03-19 10:58:21,886 [INFO] ---------------------------------
2019-03-19 10:58:43,558 [INFO] ---------------------------------
2019-03-19 10:58:43,559 [INFO] Summary:
2019-03-19 10:58:43,560 [INFO] Batch 63000, worst loss 0.001193 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:58:43,561 [INFO] Regularization: 2543.922119 * 0.0000000001 = 0.0000002544 loss
2019-03-19 10:58:43,561 [INFO] unfolding 0, single step 63001
2019-03-19 10:58:43,562 [INFO] Sum of grad norms of most recent batch: 1.277246
2019-03-19 10:58:43,562 [INFO] ---------------------------------
2019-03-19 10:59:05,140 [INFO] ---------------------------------
2019-03-19 10:59:05,141 [INFO] Summary:
2019-03-19 10:59:05,142 [INFO] Batch 64000, worst loss 0.001042 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:59:05,142 [INFO] Regularization: 2447.626465 * 0.0000000001 = 0.0000002448 loss
2019-03-19 10:59:05,143 [INFO] unfolding 0, single step 64001
2019-03-19 10:59:05,143 [INFO] Sum of grad norms of most recent batch: 0.880745
2019-03-19 10:59:05,144 [INFO] ---------------------------------
2019-03-19 10:59:26,601 [INFO] ---------------------------------
2019-03-19 10:59:26,602 [INFO] Summary:
2019-03-19 10:59:26,603 [INFO] Batch 65000, worst loss 0.002259 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:59:26,603 [INFO] Regularization: 2402.719482 * 0.0000000001 = 0.0000002403 loss
2019-03-19 10:59:26,603 [INFO] unfolding 0, single step 65001
2019-03-19 10:59:26,604 [INFO] Sum of grad norms of most recent batch: 1.621160
2019-03-19 10:59:26,605 [INFO] ---------------------------------
2019-03-19 10:59:48,118 [INFO] ---------------------------------
2019-03-19 10:59:48,119 [INFO] Summary:
2019-03-19 10:59:48,120 [INFO] Batch 66000, worst loss 0.001551 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 10:59:48,121 [INFO] Regularization: 2307.492188 * 0.0000000001 = 0.0000002307 loss
2019-03-19 10:59:48,121 [INFO] unfolding 0, single step 66001
2019-03-19 10:59:48,122 [INFO] Sum of grad norms of most recent batch: 0.522945
2019-03-19 10:59:48,122 [INFO] ---------------------------------
2019-03-19 11:00:09,778 [INFO] ---------------------------------
2019-03-19 11:00:09,779 [INFO] Summary:
2019-03-19 11:00:09,780 [INFO] Batch 67000, worst loss 0.001677 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:00:09,780 [INFO] Regularization: 2235.995117 * 0.0000000001 = 0.0000002236 loss
2019-03-19 11:00:09,781 [INFO] unfolding 0, single step 67001
2019-03-19 11:00:09,781 [INFO] Sum of grad norms of most recent batch: 0.642760
2019-03-19 11:00:09,782 [INFO] ---------------------------------
2019-03-19 11:00:31,017 [INFO] ---------------------------------
2019-03-19 11:00:31,018 [INFO] Summary:
2019-03-19 11:00:31,019 [INFO] Batch 68000, worst loss 0.001452 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:00:31,020 [INFO] Regularization: 2156.469727 * 0.0000000001 = 0.0000002156 loss
2019-03-19 11:00:31,020 [INFO] unfolding 0, single step 68001
2019-03-19 11:00:31,020 [INFO] Sum of grad norms of most recent batch: 0.719275
2019-03-19 11:00:31,021 [INFO] ---------------------------------
2019-03-19 11:00:52,693 [INFO] ---------------------------------
2019-03-19 11:00:52,694 [INFO] Summary:
2019-03-19 11:00:52,694 [INFO] Batch 69000, worst loss 0.001432 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:00:52,695 [INFO] Regularization: 2089.196533 * 0.0000000001 = 0.0000002089 loss
2019-03-19 11:00:52,695 [INFO] unfolding 0, single step 69001
2019-03-19 11:00:52,696 [INFO] Sum of grad norms of most recent batch: 0.359557
2019-03-19 11:00:52,697 [INFO] ---------------------------------
2019-03-19 11:01:14,446 [INFO] ---------------------------------
2019-03-19 11:01:14,447 [INFO] Summary:
2019-03-19 11:01:14,448 [INFO] Batch 70000, worst loss 0.001102 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:01:14,448 [INFO] Regularization: 2009.696655 * 0.0000000001 = 0.0000002010 loss
2019-03-19 11:01:14,449 [INFO] unfolding 0, single step 70001
2019-03-19 11:01:14,449 [INFO] Sum of grad norms of most recent batch: 0.924095
2019-03-19 11:01:14,450 [INFO] ---------------------------------
2019-03-19 11:01:20,266 [INFO] ---------------------------------
2019-03-19 11:01:20,267 [INFO] Evaluation:
2019-03-19 11:01:20,267 [INFO] Batch 70000, worst loss 0.000548 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:01:20,268 [INFO] ---------------------------------
2019-03-19 11:01:41,781 [INFO] ---------------------------------
2019-03-19 11:01:41,782 [INFO] Summary:
2019-03-19 11:01:41,782 [INFO] Batch 71000, worst loss 0.000868 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 11:01:41,783 [INFO] Regularization: 1921.917847 * 0.0000000001 = 0.0000001922 loss
2019-03-19 11:01:41,783 [INFO] unfolding 0, single step 71001
2019-03-19 11:01:41,784 [INFO] Sum of grad norms of most recent batch: 0.287429
2019-03-19 11:01:41,784 [INFO] ---------------------------------
2019-03-19 11:02:03,370 [INFO] ---------------------------------
2019-03-19 11:02:03,371 [INFO] Summary:
2019-03-19 11:02:03,372 [INFO] Batch 72000, worst loss 0.000380 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 11:02:03,372 [INFO] Regularization: 1854.129028 * 0.0000000001 = 0.0000001854 loss
2019-03-19 11:02:03,372 [INFO] unfolding 0, single step 72001
2019-03-19 11:02:03,373 [INFO] Sum of grad norms of most recent batch: 0.286114
2019-03-19 11:02:03,374 [INFO] ---------------------------------
2019-03-19 11:02:24,683 [INFO] ---------------------------------
2019-03-19 11:02:24,684 [INFO] Summary:
2019-03-19 11:02:24,685 [INFO] Batch 73000, worst loss 0.000214 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 11:02:24,685 [INFO] Regularization: 1790.125854 * 0.0000000001 = 0.0000001790 loss
2019-03-19 11:02:24,685 [INFO] unfolding 0, single step 73001
2019-03-19 11:02:24,686 [INFO] Sum of grad norms of most recent batch: 0.209136
2019-03-19 11:02:24,687 [INFO] ---------------------------------
2019-03-19 11:02:46,098 [INFO] ---------------------------------
2019-03-19 11:02:46,099 [INFO] Summary:
2019-03-19 11:02:46,100 [INFO] Batch 74000, worst loss 0.000111 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000214 @est.-depth 2
2019-03-19 11:02:46,100 [INFO] Regularization: 1732.261475 * 0.0000000001 = 0.0000001732 loss
2019-03-19 11:02:46,101 [INFO] unfolding 0, single step 74001
2019-03-19 11:02:46,101 [INFO] Sum of grad norms of most recent batch: 0.145472
2019-03-19 11:02:46,102 [INFO] ---------------------------------
2019-03-19 11:03:07,010 [INFO] ---------------------------------
2019-03-19 11:03:07,011 [INFO] Summary:
2019-03-19 11:03:07,012 [INFO] Batch 75000, worst loss 0.000039 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000111 @est.-depth 2
2019-03-19 11:03:07,012 [INFO] Regularization: 1704.345825 * 0.0000000001 = 0.0000001704 loss
2019-03-19 11:03:07,012 [INFO] unfolding 0, single step 75001
2019-03-19 11:03:07,013 [INFO] Sum of grad norms of most recent batch: 0.045856
2019-03-19 11:03:07,014 [INFO] ---------------------------------
2019-03-19 11:03:27,835 [INFO] ---------------------------------
2019-03-19 11:03:27,836 [INFO] Summary:
2019-03-19 11:03:27,837 [INFO] Batch 76000, worst loss 0.000006 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000039 @est.-depth 2
2019-03-19 11:03:27,837 [INFO] Regularization: 1694.714722 * 0.0000000001 = 0.0000001695 loss
2019-03-19 11:03:27,837 [INFO] unfolding 0, single step 76001
2019-03-19 11:03:27,838 [INFO] Sum of grad norms of most recent batch: 0.009056
2019-03-19 11:03:27,838 [INFO] ---------------------------------
2019-03-19 11:03:48,553 [INFO] ---------------------------------
2019-03-19 11:03:48,554 [INFO] Summary:
2019-03-19 11:03:48,554 [INFO] Batch 77000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000006 @est.-depth 2
2019-03-19 11:03:48,555 [INFO] Regularization: 1693.293945 * 0.0000000001 = 0.0000001693 loss
2019-03-19 11:03:48,555 [INFO] unfolding 0, single step 77001
2019-03-19 11:03:48,556 [INFO] reducing reg_loss_factor
2019-03-19 11:03:48,556 [INFO] Sum of grad norms of most recent batch: 0.002490
2019-03-19 11:03:48,557 [INFO] ---------------------------------
2019-03-19 11:04:09,723 [INFO] ---------------------------------
2019-03-19 11:04:09,724 [INFO] Summary:
2019-03-19 11:04:09,724 [INFO] Batch 78000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 11:04:09,725 [INFO] Regularization: 1693.275391 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:04:09,725 [INFO] unfolding 0, single step 78001
2019-03-19 11:04:09,726 [INFO] Sum of grad norms of most recent batch: 0.001602
2019-03-19 11:04:09,726 [INFO] ---------------------------------
2019-03-19 11:04:30,840 [INFO] ---------------------------------
2019-03-19 11:04:30,841 [INFO] Summary:
2019-03-19 11:04:30,842 [INFO] Batch 79000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 11:04:30,842 [INFO] Regularization: 1693.262207 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:04:30,843 [INFO] unfolding 0, single step 79001
2019-03-19 11:04:30,844 [INFO] Sum of grad norms of most recent batch: 0.008498
2019-03-19 11:04:30,844 [INFO] ---------------------------------
2019-03-19 11:04:52,400 [INFO] ---------------------------------
2019-03-19 11:04:52,401 [INFO] Summary:
2019-03-19 11:04:52,401 [INFO] Batch 80000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 11:04:52,402 [INFO] Regularization: 1693.248535 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:04:52,402 [INFO] unfolding 0, single step 80001
2019-03-19 11:04:52,403 [INFO] Sum of grad norms of most recent batch: 0.006064
2019-03-19 11:04:52,403 [INFO] ---------------------------------
2019-03-19 11:04:58,314 [INFO] ---------------------------------
2019-03-19 11:04:58,315 [INFO] Evaluation:
2019-03-19 11:04:58,316 [INFO] Batch 80000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:04:58,319 [INFO] New best loss 0.000001, saved to file transition/1552984806/1552989898_1_transition_80000.pth
2019-03-19 11:04:58,331 [INFO] Target
2019-03-19 11:04:58,332 [INFO] [[0.8109]
 [0.547 ]
 [0.5594]
 [0.5598]
 [0.2253]
 [0.4452]
 [0.6566]
 [0.8911]
 [0.5159]
 [0.0466]
 [0.6211]
 [0.7626]
 [0.2761]
 [0.478 ]
 [0.1382]
 [0.2621]
 [0.4129]
 [0.7917]
 [0.7788]
 [0.9395]
 [0.2493]
 [0.6032]
 [0.6432]
 [0.0951]
 [0.284 ]
 [0.2028]
 [0.3303]
 [0.9292]
 [0.8856]
 [0.5991]
 [0.3197]
 [0.317 ]]
2019-03-19 11:04:58,335 [INFO] Estimator output
2019-03-19 11:04:58,335 [INFO] [[0.811002]
 [0.547001]
 [0.559401]
 [0.559803]
 [0.225298]
 [0.445202]
 [0.6566  ]
 [0.891102]
 [0.515899]
 [0.046599]
 [0.621102]
 [0.7626  ]
 [0.2761  ]
 [0.477978]
 [0.1382  ]
 [0.262102]
 [0.412901]
 [0.7917  ]
 [0.778792]
 [0.939501]
 [0.249301]
 [0.603222]
 [0.643222]
 [0.095099]
 [0.284001]
 [0.20281 ]
 [0.330402]
 [0.929203]
 [0.885604]
 [0.599099]
 [0.3197  ]
 [0.316999]]
2019-03-19 11:04:58,337 [INFO] ---------------------------------
2019-03-19 11:05:19,121 [INFO] ---------------------------------
2019-03-19 11:05:19,122 [INFO] Summary:
2019-03-19 11:05:19,122 [INFO] Batch 81000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:05:19,123 [INFO] Regularization: 1693.233765 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:05:19,123 [INFO] unfolding 0, single step 81001
2019-03-19 11:05:19,124 [INFO] Sum of grad norms of most recent batch: 0.001467
2019-03-19 11:05:19,125 [INFO] ---------------------------------
2019-03-19 11:05:40,392 [INFO] ---------------------------------
2019-03-19 11:05:40,393 [INFO] Summary:
2019-03-19 11:05:40,394 [INFO] Batch 82000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:05:40,395 [INFO] Regularization: 1693.230713 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:05:40,395 [INFO] unfolding 0, single step 82001
2019-03-19 11:05:40,396 [INFO] Sum of grad norms of most recent batch: 0.006195
2019-03-19 11:05:40,396 [INFO] ---------------------------------
2019-03-19 11:06:01,573 [INFO] ---------------------------------
2019-03-19 11:06:01,574 [INFO] Summary:
2019-03-19 11:06:01,574 [INFO] Batch 83000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:06:01,575 [INFO] Regularization: 1693.225098 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:06:01,575 [INFO] unfolding 0, single step 83001
2019-03-19 11:06:01,576 [INFO] Sum of grad norms of most recent batch: 0.008950
2019-03-19 11:06:01,577 [INFO] ---------------------------------
2019-03-19 11:06:22,490 [INFO] ---------------------------------
2019-03-19 11:06:22,491 [INFO] Summary:
2019-03-19 11:06:22,491 [INFO] Batch 84000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:06:22,492 [INFO] Regularization: 1693.220459 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:06:22,492 [INFO] unfolding 0, single step 84001
2019-03-19 11:06:22,493 [INFO] Sum of grad norms of most recent batch: 0.006086
2019-03-19 11:06:22,493 [INFO] ---------------------------------
2019-03-19 11:06:43,245 [INFO] ---------------------------------
2019-03-19 11:06:43,246 [INFO] Summary:
2019-03-19 11:06:43,246 [INFO] Batch 85000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:06:43,247 [INFO] Regularization: 1693.213257 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:06:43,247 [INFO] unfolding 0, single step 85001
2019-03-19 11:06:43,248 [INFO] Sum of grad norms of most recent batch: 0.002002
2019-03-19 11:06:43,249 [INFO] ---------------------------------
2019-03-19 11:07:04,000 [INFO] ---------------------------------
2019-03-19 11:07:04,001 [INFO] Summary:
2019-03-19 11:07:04,002 [INFO] Batch 86000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:07:04,003 [INFO] Regularization: 1693.207275 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:07:04,003 [INFO] unfolding 0, single step 86001
2019-03-19 11:07:04,004 [INFO] Sum of grad norms of most recent batch: 0.001185
2019-03-19 11:07:04,005 [INFO] ---------------------------------
2019-03-19 11:07:25,228 [INFO] ---------------------------------
2019-03-19 11:07:25,229 [INFO] Summary:
2019-03-19 11:07:25,229 [INFO] Batch 87000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:07:25,230 [INFO] Regularization: 1693.202148 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:07:25,230 [INFO] unfolding 0, single step 87001
2019-03-19 11:07:25,231 [INFO] Sum of grad norms of most recent batch: 0.003631
2019-03-19 11:07:25,231 [INFO] ---------------------------------
2019-03-19 11:07:31,070 [INFO] ---------------------------------
2019-03-19 11:07:31,071 [INFO] Evaluation:
2019-03-19 11:07:31,072 [INFO] Batch 87000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:07:31,073 [INFO] ---------------------------------
2019-03-19 11:07:52,179 [INFO] ---------------------------------
2019-03-19 11:07:52,180 [INFO] Summary:
2019-03-19 11:07:52,181 [INFO] Batch 88000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:07:52,181 [INFO] Regularization: 1693.193848 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:07:52,181 [INFO] unfolding 0, single step 88001
2019-03-19 11:07:52,182 [INFO] Sum of grad norms of most recent batch: 0.002163
2019-03-19 11:07:52,182 [INFO] ---------------------------------
2019-03-19 11:08:13,362 [INFO] ---------------------------------
2019-03-19 11:08:13,363 [INFO] Summary:
2019-03-19 11:08:13,364 [INFO] Batch 89000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:08:13,364 [INFO] Regularization: 1693.190186 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:08:13,365 [INFO] unfolding 0, single step 89001
2019-03-19 11:08:13,365 [INFO] Sum of grad norms of most recent batch: 0.000685
2019-03-19 11:08:13,366 [INFO] ---------------------------------
2019-03-19 11:08:34,294 [INFO] ---------------------------------
2019-03-19 11:08:34,295 [INFO] Summary:
2019-03-19 11:08:34,295 [INFO] Batch 90000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:08:34,296 [INFO] Regularization: 1693.181152 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:08:34,296 [INFO] unfolding 0, single step 90001
2019-03-19 11:08:34,297 [INFO] Sum of grad norms of most recent batch: 0.001337
2019-03-19 11:08:34,297 [INFO] ---------------------------------
2019-03-19 11:08:40,109 [INFO] ---------------------------------
2019-03-19 11:08:40,110 [INFO] Evaluation:
2019-03-19 11:08:40,113 [INFO] Batch 90000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:08:40,115 [INFO] ---------------------------------
2019-03-19 11:09:01,572 [INFO] ---------------------------------
2019-03-19 11:09:01,573 [INFO] Summary:
2019-03-19 11:09:01,573 [INFO] Batch 91000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:09:01,574 [INFO] Regularization: 1693.174805 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:09:01,574 [INFO] unfolding 0, single step 91001
2019-03-19 11:09:01,575 [INFO] Sum of grad norms of most recent batch: 0.009928
2019-03-19 11:09:01,576 [INFO] ---------------------------------
2019-03-19 11:09:22,513 [INFO] ---------------------------------
2019-03-19 11:09:22,514 [INFO] Summary:
2019-03-19 11:09:22,514 [INFO] Batch 92000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:09:22,515 [INFO] Regularization: 1693.171997 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:09:22,515 [INFO] unfolding 0, single step 92001
2019-03-19 11:09:22,516 [INFO] Sum of grad norms of most recent batch: 0.000848
2019-03-19 11:09:22,516 [INFO] ---------------------------------
2019-03-19 11:09:43,464 [INFO] ---------------------------------
2019-03-19 11:09:43,465 [INFO] Summary:
2019-03-19 11:09:43,466 [INFO] Batch 93000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:09:43,466 [INFO] Regularization: 1693.169922 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:09:43,467 [INFO] unfolding 0, single step 93001
2019-03-19 11:09:43,468 [INFO] Sum of grad norms of most recent batch: 0.007875
2019-03-19 11:09:43,468 [INFO] ---------------------------------
2019-03-19 11:10:04,511 [INFO] ---------------------------------
2019-03-19 11:10:04,512 [INFO] Summary:
2019-03-19 11:10:04,512 [INFO] Batch 94000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:10:04,513 [INFO] Regularization: 1693.167603 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:10:04,513 [INFO] unfolding 0, single step 94001
2019-03-19 11:10:04,514 [INFO] Sum of grad norms of most recent batch: 0.001427
2019-03-19 11:10:04,514 [INFO] ---------------------------------
2019-03-19 11:10:25,479 [INFO] ---------------------------------
2019-03-19 11:10:25,480 [INFO] Summary:
2019-03-19 11:10:25,480 [INFO] Batch 95000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:10:25,481 [INFO] Regularization: 1693.166138 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:10:25,481 [INFO] unfolding 0, single step 95001
2019-03-19 11:10:25,482 [INFO] Sum of grad norms of most recent batch: 0.000657
2019-03-19 11:10:25,482 [INFO] ---------------------------------
2019-03-19 11:10:46,851 [INFO] ---------------------------------
2019-03-19 11:10:46,852 [INFO] Summary:
2019-03-19 11:10:46,852 [INFO] Batch 96000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:10:46,853 [INFO] Regularization: 1693.163574 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:10:46,853 [INFO] unfolding 0, single step 96001
2019-03-19 11:10:46,854 [INFO] Sum of grad norms of most recent batch: 0.008984
2019-03-19 11:10:46,855 [INFO] ---------------------------------
2019-03-19 11:11:08,166 [INFO] ---------------------------------
2019-03-19 11:11:08,167 [INFO] Summary:
2019-03-19 11:11:08,167 [INFO] Batch 97000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:11:08,168 [INFO] Regularization: 1693.160645 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:11:08,168 [INFO] unfolding 0, single step 97001
2019-03-19 11:11:08,169 [INFO] Sum of grad norms of most recent batch: 0.024590
2019-03-19 11:11:08,170 [INFO] ---------------------------------
2019-03-19 11:11:29,398 [INFO] ---------------------------------
2019-03-19 11:11:29,399 [INFO] Summary:
2019-03-19 11:11:29,399 [INFO] Batch 98000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:11:29,400 [INFO] Regularization: 1693.159302 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:11:29,400 [INFO] unfolding 0, single step 98001
2019-03-19 11:11:29,400 [INFO] Sum of grad norms of most recent batch: 0.018339
2019-03-19 11:11:29,401 [INFO] ---------------------------------
2019-03-19 11:11:50,586 [INFO] ---------------------------------
2019-03-19 11:11:50,587 [INFO] Summary:
2019-03-19 11:11:50,588 [INFO] Batch 99000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:11:50,588 [INFO] Regularization: 1693.156128 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:11:50,589 [INFO] unfolding 0, single step 99001
2019-03-19 11:11:50,589 [INFO] Sum of grad norms of most recent batch: 0.006785
2019-03-19 11:11:50,590 [INFO] ---------------------------------
2019-03-19 11:12:11,521 [INFO] ---------------------------------
2019-03-19 11:12:11,522 [INFO] Summary:
2019-03-19 11:12:11,522 [INFO] Batch 100000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:12:11,523 [INFO] Regularization: 1693.153687 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:12:11,523 [INFO] unfolding 0, single step 100001
2019-03-19 11:12:11,524 [INFO] Sum of grad norms of most recent batch: 0.010293
2019-03-19 11:12:11,524 [INFO] ---------------------------------
2019-03-19 11:12:17,346 [INFO] ---------------------------------
2019-03-19 11:12:17,347 [INFO] Evaluation:
2019-03-19 11:12:17,349 [INFO] Batch 100000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:12:17,351 [INFO] ---------------------------------
2019-03-19 11:12:38,842 [INFO] ---------------------------------
2019-03-19 11:12:38,843 [INFO] Summary:
2019-03-19 11:12:38,844 [INFO] Batch 101000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:12:38,844 [INFO] Regularization: 1693.153442 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:12:38,845 [INFO] unfolding 0, single step 101001
2019-03-19 11:12:38,846 [INFO] Sum of grad norms of most recent batch: 0.001945
2019-03-19 11:12:38,846 [INFO] ---------------------------------
2019-03-19 11:12:59,744 [INFO] ---------------------------------
2019-03-19 11:12:59,745 [INFO] Summary:
2019-03-19 11:12:59,746 [INFO] Batch 102000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:12:59,746 [INFO] Regularization: 1693.151489 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:12:59,747 [INFO] unfolding 0, single step 102001
2019-03-19 11:12:59,748 [INFO] Sum of grad norms of most recent batch: 0.010968
2019-03-19 11:12:59,748 [INFO] ---------------------------------
2019-03-19 11:13:20,898 [INFO] ---------------------------------
2019-03-19 11:13:20,899 [INFO] Summary:
2019-03-19 11:13:20,900 [INFO] Batch 103000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:13:20,901 [INFO] Regularization: 1693.151733 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:13:20,901 [INFO] unfolding 0, single step 103001
2019-03-19 11:13:20,902 [INFO] Sum of grad norms of most recent batch: 0.001626
2019-03-19 11:13:20,903 [INFO] ---------------------------------
2019-03-19 11:13:41,804 [INFO] ---------------------------------
2019-03-19 11:13:41,804 [INFO] Summary:
2019-03-19 11:13:41,805 [INFO] Batch 104000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:13:41,805 [INFO] Regularization: 1693.149658 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:13:41,805 [INFO] unfolding 0, single step 104001
2019-03-19 11:13:41,806 [INFO] Sum of grad norms of most recent batch: 0.000725
2019-03-19 11:13:41,807 [INFO] ---------------------------------
2019-03-19 11:14:02,644 [INFO] ---------------------------------
2019-03-19 11:14:02,645 [INFO] Summary:
2019-03-19 11:14:02,645 [INFO] Batch 105000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:14:02,646 [INFO] Regularization: 1693.152344 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:14:02,646 [INFO] unfolding 0, single step 105001
2019-03-19 11:14:02,647 [INFO] Sum of grad norms of most recent batch: 0.004610
2019-03-19 11:14:02,647 [INFO] ---------------------------------
2019-03-19 11:14:23,734 [INFO] ---------------------------------
2019-03-19 11:14:23,735 [INFO] Summary:
2019-03-19 11:14:23,736 [INFO] Batch 106000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:14:23,736 [INFO] Regularization: 1693.149658 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:14:23,737 [INFO] unfolding 0, single step 106001
2019-03-19 11:14:23,737 [INFO] Sum of grad norms of most recent batch: 0.013006
2019-03-19 11:14:23,738 [INFO] ---------------------------------
2019-03-19 11:14:44,747 [INFO] ---------------------------------
2019-03-19 11:14:44,748 [INFO] Summary:
2019-03-19 11:14:44,748 [INFO] Batch 107000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:14:44,749 [INFO] Regularization: 1693.149658 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:14:44,749 [INFO] unfolding 0, single step 107001
2019-03-19 11:14:44,750 [INFO] Sum of grad norms of most recent batch: 0.002737
2019-03-19 11:14:44,750 [INFO] ---------------------------------
2019-03-19 11:15:05,682 [INFO] ---------------------------------
2019-03-19 11:15:05,682 [INFO] Summary:
2019-03-19 11:15:05,683 [INFO] Batch 108000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:15:05,684 [INFO] Regularization: 1693.148682 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:15:05,684 [INFO] unfolding 0, single step 108001
2019-03-19 11:15:05,685 [INFO] Sum of grad norms of most recent batch: 0.001101
2019-03-19 11:15:05,685 [INFO] ---------------------------------
2019-03-19 11:15:26,669 [INFO] ---------------------------------
2019-03-19 11:15:26,670 [INFO] Summary:
2019-03-19 11:15:26,670 [INFO] Batch 109000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:15:26,671 [INFO] Regularization: 1693.147949 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:15:26,671 [INFO] unfolding 0, single step 109001
2019-03-19 11:15:26,672 [INFO] Sum of grad norms of most recent batch: 0.002065
2019-03-19 11:15:26,672 [INFO] ---------------------------------
2019-03-19 11:15:47,341 [INFO] ---------------------------------
2019-03-19 11:15:47,342 [INFO] Summary:
2019-03-19 11:15:47,342 [INFO] Batch 110000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:15:47,344 [INFO] Regularization: 1693.148438 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:15:47,344 [INFO] unfolding 0, single step 110001
2019-03-19 11:15:47,347 [INFO] Sum of grad norms of most recent batch: 0.001307
2019-03-19 11:15:47,348 [INFO] ---------------------------------
2019-03-19 11:15:53,150 [INFO] ---------------------------------
2019-03-19 11:15:53,151 [INFO] Evaluation:
2019-03-19 11:15:53,152 [INFO] Batch 110000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:15:53,156 [INFO] ---------------------------------
2019-03-19 11:16:14,308 [INFO] ---------------------------------
2019-03-19 11:16:14,309 [INFO] Summary:
2019-03-19 11:16:14,309 [INFO] Batch 111000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:16:14,310 [INFO] Regularization: 1693.147339 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:16:14,310 [INFO] unfolding 0, single step 111001
2019-03-19 11:16:14,311 [INFO] Sum of grad norms of most recent batch: 0.012613
2019-03-19 11:16:14,311 [INFO] ---------------------------------
2019-03-19 11:16:35,171 [INFO] ---------------------------------
2019-03-19 11:16:35,172 [INFO] Summary:
2019-03-19 11:16:35,173 [INFO] Batch 112000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:16:35,174 [INFO] Regularization: 1693.146729 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:16:35,174 [INFO] unfolding 0, single step 112001
2019-03-19 11:16:35,175 [INFO] Sum of grad norms of most recent batch: 0.008100
2019-03-19 11:16:35,176 [INFO] ---------------------------------
2019-03-19 11:16:56,271 [INFO] ---------------------------------
2019-03-19 11:16:56,272 [INFO] Summary:
2019-03-19 11:16:56,273 [INFO] Batch 113000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:16:56,273 [INFO] Regularization: 1693.145264 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:16:56,274 [INFO] unfolding 0, single step 113001
2019-03-19 11:16:56,274 [INFO] Sum of grad norms of most recent batch: 0.001746
2019-03-19 11:16:56,275 [INFO] ---------------------------------
2019-03-19 11:17:16,975 [INFO] ---------------------------------
2019-03-19 11:17:16,976 [INFO] Summary:
2019-03-19 11:17:16,976 [INFO] Batch 114000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:17:16,977 [INFO] Regularization: 1693.145630 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:17:16,977 [INFO] unfolding 0, single step 114001
2019-03-19 11:17:16,978 [INFO] Sum of grad norms of most recent batch: 0.008272
2019-03-19 11:17:16,978 [INFO] ---------------------------------
2019-03-19 11:17:38,205 [INFO] ---------------------------------
2019-03-19 11:17:38,206 [INFO] Summary:
2019-03-19 11:17:38,207 [INFO] Batch 115000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:17:38,207 [INFO] Regularization: 1693.145874 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:17:38,207 [INFO] unfolding 0, single step 115001
2019-03-19 11:17:38,208 [INFO] Sum of grad norms of most recent batch: 0.013897
2019-03-19 11:17:38,208 [INFO] ---------------------------------
2019-03-19 11:17:58,968 [INFO] ---------------------------------
2019-03-19 11:17:58,969 [INFO] Summary:
2019-03-19 11:17:58,969 [INFO] Batch 116000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:17:58,970 [INFO] Regularization: 1693.145630 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:17:58,970 [INFO] unfolding 0, single step 116001
2019-03-19 11:17:58,971 [INFO] Sum of grad norms of most recent batch: 0.000815
2019-03-19 11:17:58,971 [INFO] ---------------------------------
2019-03-19 11:18:20,224 [INFO] ---------------------------------
2019-03-19 11:18:20,224 [INFO] Summary:
2019-03-19 11:18:20,225 [INFO] Batch 117000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:18:20,226 [INFO] Regularization: 1693.145996 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:18:20,226 [INFO] unfolding 0, single step 117001
2019-03-19 11:18:20,227 [INFO] Sum of grad norms of most recent batch: 0.003676
2019-03-19 11:18:20,227 [INFO] ---------------------------------
2019-03-19 11:18:41,301 [INFO] ---------------------------------
2019-03-19 11:18:41,301 [INFO] Summary:
2019-03-19 11:18:41,302 [INFO] Batch 118000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:18:41,303 [INFO] Regularization: 1693.147461 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:18:41,303 [INFO] unfolding 0, single step 118001
2019-03-19 11:18:41,303 [INFO] Sum of grad norms of most recent batch: 0.001263
2019-03-19 11:18:41,304 [INFO] ---------------------------------
2019-03-19 11:19:02,402 [INFO] ---------------------------------
2019-03-19 11:19:02,403 [INFO] Summary:
2019-03-19 11:19:02,404 [INFO] Batch 119000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:19:02,404 [INFO] Regularization: 1693.146118 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:19:02,404 [INFO] unfolding 0, single step 119001
2019-03-19 11:19:02,405 [INFO] Sum of grad norms of most recent batch: 0.011866
2019-03-19 11:19:02,405 [INFO] ---------------------------------
2019-03-19 11:19:23,186 [INFO] ---------------------------------
2019-03-19 11:19:23,187 [INFO] Summary:
2019-03-19 11:19:23,187 [INFO] Batch 120000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:19:23,188 [INFO] Regularization: 1693.146484 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:19:23,188 [INFO] unfolding 0, single step 120001
2019-03-19 11:19:23,189 [INFO] Sum of grad norms of most recent batch: 0.002811
2019-03-19 11:19:23,189 [INFO] ---------------------------------
2019-03-19 11:19:29,122 [INFO] ---------------------------------
2019-03-19 11:19:29,123 [INFO] Evaluation:
2019-03-19 11:19:29,124 [INFO] Batch 120000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:19:29,125 [INFO] ---------------------------------
2019-03-19 11:19:50,341 [INFO] ---------------------------------
2019-03-19 11:19:50,342 [INFO] Summary:
2019-03-19 11:19:50,343 [INFO] Batch 121000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:19:50,343 [INFO] Regularization: 1693.147095 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:19:50,344 [INFO] unfolding 0, single step 121001
2019-03-19 11:19:50,344 [INFO] Sum of grad norms of most recent batch: 0.001580
2019-03-19 11:19:50,345 [INFO] ---------------------------------
2019-03-19 11:20:11,024 [INFO] ---------------------------------
2019-03-19 11:20:11,025 [INFO] Summary:
2019-03-19 11:20:11,026 [INFO] Batch 122000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:20:11,026 [INFO] Regularization: 1693.147339 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:20:11,026 [INFO] unfolding 0, single step 122001
2019-03-19 11:20:11,027 [INFO] Sum of grad norms of most recent batch: 0.007349
2019-03-19 11:20:11,027 [INFO] ---------------------------------
2019-03-19 11:20:32,117 [INFO] ---------------------------------
2019-03-19 11:20:32,118 [INFO] Summary:
2019-03-19 11:20:32,119 [INFO] Batch 123000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:20:32,119 [INFO] Regularization: 1693.147095 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:20:32,120 [INFO] unfolding 0, single step 123001
2019-03-19 11:20:32,121 [INFO] Sum of grad norms of most recent batch: 0.009476
2019-03-19 11:20:32,121 [INFO] ---------------------------------
2019-03-19 11:20:53,032 [INFO] ---------------------------------
2019-03-19 11:20:53,033 [INFO] Summary:
2019-03-19 11:20:53,034 [INFO] Batch 124000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:20:53,034 [INFO] Regularization: 1693.147217 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:20:53,035 [INFO] unfolding 0, single step 124001
2019-03-19 11:20:53,035 [INFO] Sum of grad norms of most recent batch: 0.001751
2019-03-19 11:20:53,036 [INFO] ---------------------------------
2019-03-19 11:21:14,084 [INFO] ---------------------------------
2019-03-19 11:21:14,085 [INFO] Summary:
2019-03-19 11:21:14,085 [INFO] Batch 125000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:21:14,086 [INFO] Regularization: 1693.146851 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:21:14,086 [INFO] unfolding 0, single step 125001
2019-03-19 11:21:14,087 [INFO] Sum of grad norms of most recent batch: 0.010744
2019-03-19 11:21:14,088 [INFO] ---------------------------------
2019-03-19 11:21:34,982 [INFO] ---------------------------------
2019-03-19 11:21:34,983 [INFO] Summary:
2019-03-19 11:21:34,983 [INFO] Batch 126000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:21:34,984 [INFO] Regularization: 1693.146973 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:21:34,984 [INFO] unfolding 0, single step 126001
2019-03-19 11:21:34,985 [INFO] Sum of grad norms of most recent batch: 0.008225
2019-03-19 11:21:34,985 [INFO] ---------------------------------
2019-03-19 11:21:55,889 [INFO] ---------------------------------
2019-03-19 11:21:55,890 [INFO] Summary:
2019-03-19 11:21:55,890 [INFO] Batch 127000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:21:55,891 [INFO] Regularization: 1693.146729 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:21:55,891 [INFO] unfolding 0, single step 127001
2019-03-19 11:21:55,892 [INFO] Sum of grad norms of most recent batch: 0.001217
2019-03-19 11:21:55,892 [INFO] ---------------------------------
2019-03-19 11:22:17,039 [INFO] ---------------------------------
2019-03-19 11:22:17,040 [INFO] Summary:
2019-03-19 11:22:17,041 [INFO] Batch 128000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:22:17,041 [INFO] Regularization: 1693.147095 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:22:17,042 [INFO] unfolding 0, single step 128001
2019-03-19 11:22:17,042 [INFO] Sum of grad norms of most recent batch: 0.005358
2019-03-19 11:22:17,043 [INFO] ---------------------------------
2019-03-19 11:22:38,066 [INFO] ---------------------------------
2019-03-19 11:22:38,067 [INFO] Summary:
2019-03-19 11:22:38,068 [INFO] Batch 129000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:22:38,068 [INFO] Regularization: 1693.147095 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:22:38,068 [INFO] unfolding 0, single step 129001
2019-03-19 11:22:38,069 [INFO] Sum of grad norms of most recent batch: 0.006300
2019-03-19 11:22:38,069 [INFO] ---------------------------------
2019-03-19 11:22:59,165 [INFO] ---------------------------------
2019-03-19 11:22:59,166 [INFO] Summary:
2019-03-19 11:22:59,167 [INFO] Batch 130000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:22:59,167 [INFO] Regularization: 1693.147217 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:22:59,168 [INFO] unfolding 0, single step 130001
2019-03-19 11:22:59,168 [INFO] Sum of grad norms of most recent batch: 0.001256
2019-03-19 11:22:59,169 [INFO] ---------------------------------
2019-03-19 11:23:04,975 [INFO] ---------------------------------
2019-03-19 11:23:04,976 [INFO] Evaluation:
2019-03-19 11:23:04,976 [INFO] Batch 130000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:23:04,977 [INFO] ---------------------------------
2019-03-19 11:23:25,893 [INFO] ---------------------------------
2019-03-19 11:23:25,894 [INFO] Summary:
2019-03-19 11:23:25,894 [INFO] Batch 131000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:23:25,895 [INFO] Regularization: 1693.147583 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:23:25,895 [INFO] unfolding 0, single step 131001
2019-03-19 11:23:25,896 [INFO] Sum of grad norms of most recent batch: 0.002735
2019-03-19 11:23:25,896 [INFO] ---------------------------------
2019-03-19 11:23:46,852 [INFO] ---------------------------------
2019-03-19 11:23:46,853 [INFO] Summary:
2019-03-19 11:23:46,853 [INFO] Batch 132000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:23:46,854 [INFO] Regularization: 1693.147583 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:23:46,854 [INFO] unfolding 0, single step 132001
2019-03-19 11:23:46,855 [INFO] Sum of grad norms of most recent batch: 0.003179
2019-03-19 11:23:46,856 [INFO] ---------------------------------
2019-03-19 11:24:07,745 [INFO] ---------------------------------
2019-03-19 11:24:07,746 [INFO] Summary:
2019-03-19 11:24:07,747 [INFO] Batch 133000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:24:07,747 [INFO] Regularization: 1693.147583 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:24:07,748 [INFO] unfolding 0, single step 133001
2019-03-19 11:24:07,749 [INFO] Sum of grad norms of most recent batch: 0.002289
2019-03-19 11:24:07,749 [INFO] ---------------------------------
2019-03-19 11:24:28,540 [INFO] ---------------------------------
2019-03-19 11:24:28,541 [INFO] Summary:
2019-03-19 11:24:28,541 [INFO] Batch 134000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:24:28,542 [INFO] Regularization: 1693.147949 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:24:28,543 [INFO] unfolding 0, single step 134001
2019-03-19 11:24:28,543 [INFO] Sum of grad norms of most recent batch: 0.003205
2019-03-19 11:24:28,544 [INFO] ---------------------------------
2019-03-19 11:24:49,411 [INFO] ---------------------------------
2019-03-19 11:24:49,412 [INFO] Summary:
2019-03-19 11:24:49,412 [INFO] Batch 135000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:24:49,413 [INFO] Regularization: 1693.147949 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:24:49,414 [INFO] unfolding 0, single step 135001
2019-03-19 11:24:49,415 [INFO] Sum of grad norms of most recent batch: 0.001238
2019-03-19 11:24:49,415 [INFO] ---------------------------------
2019-03-19 11:25:10,780 [INFO] ---------------------------------
2019-03-19 11:25:10,781 [INFO] Summary:
2019-03-19 11:25:10,781 [INFO] Batch 136000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:25:10,782 [INFO] Regularization: 1693.147949 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:25:10,782 [INFO] unfolding 0, single step 136001
2019-03-19 11:25:10,783 [INFO] Sum of grad norms of most recent batch: 0.009075
2019-03-19 11:25:10,783 [INFO] ---------------------------------
2019-03-19 11:25:32,041 [INFO] ---------------------------------
2019-03-19 11:25:32,042 [INFO] Summary:
2019-03-19 11:25:32,043 [INFO] Batch 137000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:25:32,043 [INFO] Regularization: 1693.147827 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:25:32,043 [INFO] unfolding 0, single step 137001
2019-03-19 11:25:32,044 [INFO] Sum of grad norms of most recent batch: 0.008462
2019-03-19 11:25:32,044 [INFO] ---------------------------------
2019-03-19 11:25:53,692 [INFO] ---------------------------------
2019-03-19 11:25:53,693 [INFO] Summary:
2019-03-19 11:25:53,694 [INFO] Batch 138000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:25:53,694 [INFO] Regularization: 1693.148071 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:25:53,695 [INFO] unfolding 0, single step 138001
2019-03-19 11:25:53,696 [INFO] Sum of grad norms of most recent batch: 0.001847
2019-03-19 11:25:53,696 [INFO] ---------------------------------
2019-03-19 11:26:14,629 [INFO] ---------------------------------
2019-03-19 11:26:14,630 [INFO] Summary:
2019-03-19 11:26:14,630 [INFO] Batch 139000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:26:14,631 [INFO] Regularization: 1693.148315 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:26:14,631 [INFO] unfolding 0, single step 139001
2019-03-19 11:26:14,632 [INFO] Sum of grad norms of most recent batch: 0.007297
2019-03-19 11:26:14,632 [INFO] ---------------------------------
2019-03-19 11:26:35,528 [INFO] ---------------------------------
2019-03-19 11:26:35,529 [INFO] Summary:
2019-03-19 11:26:35,530 [INFO] Batch 140000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:26:35,530 [INFO] Regularization: 1693.148315 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:26:35,531 [INFO] unfolding 0, single step 140001
2019-03-19 11:26:35,531 [INFO] Sum of grad norms of most recent batch: 0.001459
2019-03-19 11:26:35,532 [INFO] ---------------------------------
2019-03-19 11:26:41,386 [INFO] ---------------------------------
2019-03-19 11:26:41,388 [INFO] Evaluation:
2019-03-19 11:26:41,389 [INFO] Batch 140000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:26:41,390 [INFO] ---------------------------------
2019-03-19 11:27:02,240 [INFO] ---------------------------------
2019-03-19 11:27:02,241 [INFO] Summary:
2019-03-19 11:27:02,242 [INFO] Batch 141000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:27:02,242 [INFO] Regularization: 1693.148438 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:27:02,242 [INFO] unfolding 0, single step 141001
2019-03-19 11:27:02,243 [INFO] Sum of grad norms of most recent batch: 0.007432
2019-03-19 11:27:02,243 [INFO] ---------------------------------
2019-03-19 11:27:22,880 [INFO] ---------------------------------
2019-03-19 11:27:22,881 [INFO] Summary:
2019-03-19 11:27:22,882 [INFO] Batch 142000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:27:22,883 [INFO] Regularization: 1693.148560 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:27:22,883 [INFO] unfolding 0, single step 142001
2019-03-19 11:27:22,884 [INFO] Sum of grad norms of most recent batch: 0.003296
2019-03-19 11:27:22,885 [INFO] ---------------------------------
2019-03-19 11:27:44,083 [INFO] ---------------------------------
2019-03-19 11:27:44,084 [INFO] Summary:
2019-03-19 11:27:44,084 [INFO] Batch 143000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:27:44,085 [INFO] Regularization: 1693.148560 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:27:44,085 [INFO] unfolding 0, single step 143001
2019-03-19 11:27:44,086 [INFO] Sum of grad norms of most recent batch: 0.003328
2019-03-19 11:27:44,087 [INFO] ---------------------------------
2019-03-19 11:28:05,294 [INFO] ---------------------------------
2019-03-19 11:28:05,295 [INFO] Summary:
2019-03-19 11:28:05,295 [INFO] Batch 144000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:28:05,296 [INFO] Regularization: 1693.148560 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:28:05,296 [INFO] unfolding 0, single step 144001
2019-03-19 11:28:05,297 [INFO] Sum of grad norms of most recent batch: 0.002745
2019-03-19 11:28:05,298 [INFO] ---------------------------------
2019-03-19 11:28:26,592 [INFO] ---------------------------------
2019-03-19 11:28:26,593 [INFO] Summary:
2019-03-19 11:28:26,593 [INFO] Batch 145000, worst loss 0.000000 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:28:26,594 [INFO] Regularization: 1693.148560 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:28:26,594 [INFO] unfolding 0, single step 145001
2019-03-19 11:28:26,595 [INFO] Sum of grad norms of most recent batch: 0.010884
2019-03-19 11:28:26,596 [INFO] ---------------------------------
2019-03-19 11:28:32,323 [INFO] ---------------------------------
2019-03-19 11:28:32,324 [INFO] Evaluation:
2019-03-19 11:28:32,326 [INFO] Batch 145000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:28:32,327 [INFO] ---------------------------------
2019-03-19 11:28:53,189 [INFO] ---------------------------------
2019-03-19 11:28:53,190 [INFO] Summary:
2019-03-19 11:28:53,190 [INFO] Batch 146000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:28:53,191 [INFO] Regularization: 1693.148682 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:28:53,192 [INFO] unfolding 0, single step 146001
2019-03-19 11:28:53,193 [INFO] Sum of grad norms of most recent batch: 0.001500
2019-03-19 11:28:53,194 [INFO] ---------------------------------
2019-03-19 11:29:14,147 [INFO] ---------------------------------
2019-03-19 11:29:14,148 [INFO] Summary:
2019-03-19 11:29:14,149 [INFO] Batch 147000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:29:14,149 [INFO] Regularization: 1693.148560 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:29:14,150 [INFO] unfolding 0, single step 147001
2019-03-19 11:29:14,150 [INFO] Sum of grad norms of most recent batch: 0.001287
2019-03-19 11:29:14,151 [INFO] ---------------------------------
2019-03-19 11:29:35,505 [INFO] ---------------------------------
2019-03-19 11:29:35,506 [INFO] Summary:
2019-03-19 11:29:35,507 [INFO] Batch 148000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:29:35,507 [INFO] Regularization: 1693.148560 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:29:35,508 [INFO] unfolding 0, single step 148001
2019-03-19 11:29:35,508 [INFO] Sum of grad norms of most recent batch: 0.006040
2019-03-19 11:29:35,509 [INFO] ---------------------------------
2019-03-19 11:29:56,842 [INFO] ---------------------------------
2019-03-19 11:29:56,843 [INFO] Summary:
2019-03-19 11:29:56,843 [INFO] Batch 149000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:29:56,844 [INFO] Regularization: 1693.148682 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:29:56,844 [INFO] unfolding 0, single step 149001
2019-03-19 11:29:56,845 [INFO] Sum of grad norms of most recent batch: 0.002356
2019-03-19 11:29:56,845 [INFO] ---------------------------------
2019-03-19 11:30:18,028 [INFO] ---------------------------------
2019-03-19 11:30:18,029 [INFO] Summary:
2019-03-19 11:30:18,030 [INFO] Batch 150000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 11:30:18,030 [INFO] Regularization: 1693.148682 * 0.0000000000 = 0.0000000169 loss
2019-03-19 11:30:18,030 [INFO] unfolding 0, single step 150001
2019-03-19 11:30:18,031 [INFO] Sum of grad norms of most recent batch: 0.003873
2019-03-19 11:30:18,032 [INFO] ---------------------------------
2019-03-19 11:30:23,944 [INFO] ---------------------------------
2019-03-19 11:30:23,945 [INFO] Evaluation:
2019-03-19 11:30:23,946 [INFO] Batch 150000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:30:23,946 [INFO] ---------------------------------
2019-03-19 11:30:23,947 [INFO] Finished training, saved to file transition/1552984806/1552991423_1_transition_final.pth
2019-03-19 11:30:24,113 [INFO] ---------------------------------
2019-03-19 11:30:24,114 [INFO] Training model #2: (203, 64, 1) @ 2
2019-03-19 11:30:45,390 [INFO] ---------------------------------
2019-03-19 11:30:45,391 [INFO] Summary:
2019-03-19 11:30:45,391 [INFO] Batch 1000, worst loss 19.030546 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:30:45,392 [INFO] Regularization: 9479.462891 * 0.0000000000 = 0.0000000948 loss
2019-03-19 11:30:45,392 [INFO] unfolding 0, single step 1001
2019-03-19 11:30:45,393 [INFO] Sum of grad norms of most recent batch: 13.229774
2019-03-19 11:30:45,394 [INFO] ---------------------------------
2019-03-19 11:31:06,642 [INFO] ---------------------------------
2019-03-19 11:31:06,643 [INFO] Summary:
2019-03-19 11:31:06,644 [INFO] Batch 2000, worst loss 0.061928 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:31:06,645 [INFO] Regularization: 9479.538086 * 0.0000000000 = 0.0000000948 loss
2019-03-19 11:31:06,645 [INFO] unfolding 0, single step 2001
2019-03-19 11:31:06,646 [INFO] Sum of grad norms of most recent batch: 7.329130
2019-03-19 11:31:06,647 [INFO] ---------------------------------
2019-03-19 11:31:28,274 [INFO] ---------------------------------
2019-03-19 11:31:28,275 [INFO] Summary:
2019-03-19 11:31:28,276 [INFO] Batch 3000, worst loss 0.033459 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:31:28,277 [INFO] Regularization: 9484.639648 * 0.0000000000 = 0.0000000948 loss
2019-03-19 11:31:28,277 [INFO] unfolding 0, single step 3001
2019-03-19 11:31:28,278 [INFO] Sum of grad norms of most recent batch: 7.508124
2019-03-19 11:31:28,279 [INFO] ---------------------------------
2019-03-19 11:31:49,376 [INFO] ---------------------------------
2019-03-19 11:31:49,377 [INFO] Summary:
2019-03-19 11:31:49,378 [INFO] Batch 4000, worst loss 0.053270 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:31:49,378 [INFO] Regularization: 9493.444336 * 0.0000000000 = 0.0000000949 loss
2019-03-19 11:31:49,378 [INFO] unfolding 0, single step 4001
2019-03-19 11:31:49,379 [INFO] Sum of grad norms of most recent batch: 5.296314
2019-03-19 11:31:49,379 [INFO] ---------------------------------
2019-03-19 11:32:10,813 [INFO] ---------------------------------
2019-03-19 11:32:10,814 [INFO] Summary:
2019-03-19 11:32:10,814 [INFO] Batch 5000, worst loss 0.072654 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:32:10,815 [INFO] Regularization: 9504.865234 * 0.0000000000 = 0.0000000950 loss
2019-03-19 11:32:10,815 [INFO] unfolding 0, single step 5001
2019-03-19 11:32:10,816 [INFO] Sum of grad norms of most recent batch: 3.741362
2019-03-19 11:32:10,816 [INFO] ---------------------------------
2019-03-19 11:32:31,734 [INFO] ---------------------------------
2019-03-19 11:32:31,735 [INFO] Summary:
2019-03-19 11:32:31,735 [INFO] Batch 6000, worst loss 0.076276 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:32:31,736 [INFO] Regularization: 9502.304688 * 0.0000000000 = 0.0000000950 loss
2019-03-19 11:32:31,736 [INFO] unfolding 0, single step 6001
2019-03-19 11:32:31,737 [INFO] Sum of grad norms of most recent batch: 2.507928
2019-03-19 11:32:31,737 [INFO] ---------------------------------
2019-03-19 11:32:53,323 [INFO] ---------------------------------
2019-03-19 11:32:53,324 [INFO] Summary:
2019-03-19 11:32:53,325 [INFO] Batch 7000, worst loss 0.028848 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:32:53,325 [INFO] Regularization: 9502.355469 * 0.0000000000 = 0.0000000950 loss
2019-03-19 11:32:53,326 [INFO] unfolding 0, single step 7001
2019-03-19 11:32:53,326 [INFO] Sum of grad norms of most recent batch: 2.433353
2019-03-19 11:32:53,327 [INFO] ---------------------------------
2019-03-19 11:33:14,929 [INFO] ---------------------------------
2019-03-19 11:33:14,930 [INFO] Summary:
2019-03-19 11:33:14,931 [INFO] Batch 8000, worst loss 0.041360 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:33:14,932 [INFO] Regularization: 9493.840820 * 0.0000000000 = 0.0000000949 loss
2019-03-19 11:33:14,932 [INFO] unfolding 0, single step 8001
2019-03-19 11:33:14,934 [INFO] Sum of grad norms of most recent batch: 0.684508
2019-03-19 11:33:14,935 [INFO] ---------------------------------
2019-03-19 11:33:36,420 [INFO] ---------------------------------
2019-03-19 11:33:36,421 [INFO] Summary:
2019-03-19 11:33:36,421 [INFO] Batch 9000, worst loss 0.018808 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:33:36,422 [INFO] Regularization: 9478.671875 * 0.0000000000 = 0.0000000948 loss
2019-03-19 11:33:36,422 [INFO] unfolding 0, single step 9001
2019-03-19 11:33:36,423 [INFO] Sum of grad norms of most recent batch: 1.831567
2019-03-19 11:33:36,423 [INFO] ---------------------------------
2019-03-19 11:33:57,692 [INFO] ---------------------------------
2019-03-19 11:33:57,693 [INFO] Summary:
2019-03-19 11:33:57,694 [INFO] Batch 10000, worst loss 0.021943 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:33:57,694 [INFO] Regularization: 9477.298828 * 0.0000000000 = 0.0000000948 loss
2019-03-19 11:33:57,695 [INFO] unfolding 0, single step 10001
2019-03-19 11:33:57,695 [INFO] Sum of grad norms of most recent batch: 1.012576
2019-03-19 11:33:57,696 [INFO] ---------------------------------
2019-03-19 11:34:03,515 [INFO] ---------------------------------
2019-03-19 11:34:03,516 [INFO] Evaluation:
2019-03-19 11:34:03,517 [INFO] Batch 10000, worst loss 0.009540 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:34:03,517 [INFO] ---------------------------------
2019-03-19 11:34:24,736 [INFO] ---------------------------------
2019-03-19 11:34:24,737 [INFO] Summary:
2019-03-19 11:34:24,737 [INFO] Batch 11000, worst loss 0.062189 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:34:24,738 [INFO] Regularization: 9483.713867 * 0.0000000000 = 0.0000000948 loss
2019-03-19 11:34:24,738 [INFO] unfolding 0, single step 11001
2019-03-19 11:34:24,739 [INFO] Sum of grad norms of most recent batch: 2.057818
2019-03-19 11:34:24,739 [INFO] ---------------------------------
2019-03-19 11:34:46,158 [INFO] ---------------------------------
2019-03-19 11:34:46,159 [INFO] Summary:
2019-03-19 11:34:46,159 [INFO] Batch 12000, worst loss 0.017587 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:34:46,160 [INFO] Regularization: 9448.192383 * 0.0000000000 = 0.0000000945 loss
2019-03-19 11:34:46,160 [INFO] unfolding 0, single step 12001
2019-03-19 11:34:46,161 [INFO] Sum of grad norms of most recent batch: 3.365075
2019-03-19 11:34:46,161 [INFO] ---------------------------------
2019-03-19 11:35:07,697 [INFO] ---------------------------------
2019-03-19 11:35:07,698 [INFO] Summary:
2019-03-19 11:35:07,699 [INFO] Batch 13000, worst loss 0.014660 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:35:07,699 [INFO] Regularization: 9415.026367 * 0.0000000000 = 0.0000000942 loss
2019-03-19 11:35:07,700 [INFO] unfolding 0, single step 13001
2019-03-19 11:35:07,700 [INFO] Sum of grad norms of most recent batch: 4.502002
2019-03-19 11:35:07,701 [INFO] ---------------------------------
2019-03-19 11:35:29,425 [INFO] ---------------------------------
2019-03-19 11:35:29,426 [INFO] Summary:
2019-03-19 11:35:29,426 [INFO] Batch 14000, worst loss 0.045877 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:35:29,427 [INFO] Regularization: 9367.329102 * 0.0000000000 = 0.0000000937 loss
2019-03-19 11:35:29,427 [INFO] unfolding 0, single step 14001
2019-03-19 11:35:29,428 [INFO] Sum of grad norms of most recent batch: 3.052916
2019-03-19 11:35:29,428 [INFO] ---------------------------------
2019-03-19 11:35:50,769 [INFO] ---------------------------------
2019-03-19 11:35:50,770 [INFO] Summary:
2019-03-19 11:35:50,770 [INFO] Batch 15000, worst loss 0.086896 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:35:50,771 [INFO] Regularization: 9359.965820 * 0.0000000000 = 0.0000000936 loss
2019-03-19 11:35:50,771 [INFO] unfolding 0, single step 15001
2019-03-19 11:35:50,772 [INFO] Sum of grad norms of most recent batch: 1.117373
2019-03-19 11:35:50,773 [INFO] ---------------------------------
2019-03-19 11:36:12,243 [INFO] ---------------------------------
2019-03-19 11:36:12,244 [INFO] Summary:
2019-03-19 11:36:12,245 [INFO] Batch 16000, worst loss 0.028741 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:36:12,246 [INFO] Regularization: 9369.318359 * 0.0000000000 = 0.0000000937 loss
2019-03-19 11:36:12,246 [INFO] unfolding 0, single step 16001
2019-03-19 11:36:12,246 [INFO] Sum of grad norms of most recent batch: 3.104167
2019-03-19 11:36:12,247 [INFO] ---------------------------------
2019-03-19 11:36:33,400 [INFO] ---------------------------------
2019-03-19 11:36:33,401 [INFO] Summary:
2019-03-19 11:36:33,402 [INFO] Batch 17000, worst loss 0.107782 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:36:33,403 [INFO] Regularization: 9361.962891 * 0.0000000000 = 0.0000000936 loss
2019-03-19 11:36:33,403 [INFO] unfolding 0, single step 17001
2019-03-19 11:36:33,403 [INFO] Sum of grad norms of most recent batch: 1.555375
2019-03-19 11:36:33,404 [INFO] ---------------------------------
2019-03-19 11:36:55,040 [INFO] ---------------------------------
2019-03-19 11:36:55,041 [INFO] Summary:
2019-03-19 11:36:55,042 [INFO] Batch 18000, worst loss 0.090827 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:36:55,042 [INFO] Regularization: 9369.779297 * 0.0000000000 = 0.0000000937 loss
2019-03-19 11:36:55,043 [INFO] unfolding 0, single step 18001
2019-03-19 11:36:55,043 [INFO] Sum of grad norms of most recent batch: 2.074599
2019-03-19 11:36:55,044 [INFO] ---------------------------------
2019-03-19 11:37:17,146 [INFO] ---------------------------------
2019-03-19 11:37:17,147 [INFO] Summary:
2019-03-19 11:37:17,148 [INFO] Batch 19000, worst loss 0.012101 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:37:17,148 [INFO] Regularization: 9329.280273 * 0.0000000000 = 0.0000000933 loss
2019-03-19 11:37:17,149 [INFO] unfolding 0, single step 19001
2019-03-19 11:37:17,149 [INFO] Sum of grad norms of most recent batch: 1.768735
2019-03-19 11:37:17,150 [INFO] ---------------------------------
2019-03-19 11:37:38,510 [INFO] ---------------------------------
2019-03-19 11:37:38,511 [INFO] Summary:
2019-03-19 11:37:38,512 [INFO] Batch 20000, worst loss 0.015044 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:37:38,512 [INFO] Regularization: 9308.529297 * 0.0000000000 = 0.0000000931 loss
2019-03-19 11:37:38,513 [INFO] unfolding 0, single step 20001
2019-03-19 11:37:38,513 [INFO] Sum of grad norms of most recent batch: 1.999385
2019-03-19 11:37:38,514 [INFO] ---------------------------------
2019-03-19 11:37:44,200 [INFO] ---------------------------------
2019-03-19 11:37:44,200 [INFO] Evaluation:
2019-03-19 11:37:44,201 [INFO] Batch 20000, worst loss 0.008028 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:37:44,202 [INFO] ---------------------------------
2019-03-19 11:38:05,817 [INFO] ---------------------------------
2019-03-19 11:38:05,818 [INFO] Summary:
2019-03-19 11:38:05,818 [INFO] Batch 21000, worst loss 0.027485 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:38:05,819 [INFO] Regularization: 9284.326172 * 0.0000000000 = 0.0000000928 loss
2019-03-19 11:38:05,819 [INFO] unfolding 0, single step 21001
2019-03-19 11:38:05,820 [INFO] Sum of grad norms of most recent batch: 1.454628
2019-03-19 11:38:05,821 [INFO] ---------------------------------
2019-03-19 11:38:27,139 [INFO] ---------------------------------
2019-03-19 11:38:27,140 [INFO] Summary:
2019-03-19 11:38:27,141 [INFO] Batch 22000, worst loss 0.014279 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:38:27,142 [INFO] Regularization: 9258.858398 * 0.0000000000 = 0.0000000926 loss
2019-03-19 11:38:27,142 [INFO] unfolding 0, single step 22001
2019-03-19 11:38:27,143 [INFO] Sum of grad norms of most recent batch: 1.776994
2019-03-19 11:38:27,143 [INFO] ---------------------------------
2019-03-19 11:38:48,583 [INFO] ---------------------------------
2019-03-19 11:38:48,584 [INFO] Summary:
2019-03-19 11:38:48,585 [INFO] Batch 23000, worst loss 0.098683 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:38:48,585 [INFO] Regularization: 9260.002930 * 0.0000000000 = 0.0000000926 loss
2019-03-19 11:38:48,586 [INFO] unfolding 0, single step 23001
2019-03-19 11:38:48,587 [INFO] Sum of grad norms of most recent batch: 4.901850
2019-03-19 11:38:48,588 [INFO] ---------------------------------
2019-03-19 11:39:10,278 [INFO] ---------------------------------
2019-03-19 11:39:10,279 [INFO] Summary:
2019-03-19 11:39:10,280 [INFO] Batch 24000, worst loss 0.015134 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:39:10,280 [INFO] Regularization: 9229.434570 * 0.0000000000 = 0.0000000923 loss
2019-03-19 11:39:10,280 [INFO] unfolding 0, single step 24001
2019-03-19 11:39:10,281 [INFO] Sum of grad norms of most recent batch: 4.320104
2019-03-19 11:39:10,281 [INFO] ---------------------------------
2019-03-19 11:39:32,066 [INFO] ---------------------------------
2019-03-19 11:39:32,068 [INFO] Summary:
2019-03-19 11:39:32,069 [INFO] Batch 25000, worst loss 0.086034 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:39:32,070 [INFO] Regularization: 9215.696289 * 0.0000000000 = 0.0000000922 loss
2019-03-19 11:39:32,071 [INFO] unfolding 0, single step 25001
2019-03-19 11:39:32,073 [INFO] Sum of grad norms of most recent batch: 4.210243
2019-03-19 11:39:32,074 [INFO] ---------------------------------
2019-03-19 11:39:53,602 [INFO] ---------------------------------
2019-03-19 11:39:53,603 [INFO] Summary:
2019-03-19 11:39:53,603 [INFO] Batch 26000, worst loss 0.148622 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:39:53,604 [INFO] Regularization: 9250.616211 * 0.0000000000 = 0.0000000925 loss
2019-03-19 11:39:53,604 [INFO] unfolding 0, single step 26001
2019-03-19 11:39:53,605 [INFO] Sum of grad norms of most recent batch: 2.781847
2019-03-19 11:39:53,605 [INFO] ---------------------------------
2019-03-19 11:40:15,139 [INFO] ---------------------------------
2019-03-19 11:40:15,140 [INFO] Summary:
2019-03-19 11:40:15,141 [INFO] Batch 27000, worst loss 0.031039 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:40:15,141 [INFO] Regularization: 9227.949219 * 0.0000000000 = 0.0000000923 loss
2019-03-19 11:40:15,142 [INFO] unfolding 0, single step 27001
2019-03-19 11:40:15,142 [INFO] Sum of grad norms of most recent batch: 2.531414
2019-03-19 11:40:15,143 [INFO] ---------------------------------
2019-03-19 11:40:36,670 [INFO] ---------------------------------
2019-03-19 11:40:36,672 [INFO] Summary:
2019-03-19 11:40:36,673 [INFO] Batch 28000, worst loss 0.007371 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:40:36,674 [INFO] Regularization: 9198.307617 * 0.0000000000 = 0.0000000920 loss
2019-03-19 11:40:36,674 [INFO] unfolding 0, single step 28001
2019-03-19 11:40:36,675 [INFO] Sum of grad norms of most recent batch: 0.815023
2019-03-19 11:40:36,676 [INFO] ---------------------------------
2019-03-19 11:40:58,266 [INFO] ---------------------------------
2019-03-19 11:40:58,267 [INFO] Summary:
2019-03-19 11:40:58,268 [INFO] Batch 29000, worst loss 0.009020 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:40:58,269 [INFO] Regularization: 9181.684570 * 0.0000000000 = 0.0000000918 loss
2019-03-19 11:40:58,269 [INFO] unfolding 0, single step 29001
2019-03-19 11:40:58,270 [INFO] Sum of grad norms of most recent batch: 0.392547
2019-03-19 11:40:58,270 [INFO] ---------------------------------
2019-03-19 11:41:20,166 [INFO] ---------------------------------
2019-03-19 11:41:20,167 [INFO] Summary:
2019-03-19 11:41:20,168 [INFO] Batch 30000, worst loss 0.009489 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:41:20,168 [INFO] Regularization: 9181.661133 * 0.0000000000 = 0.0000000918 loss
2019-03-19 11:41:20,169 [INFO] unfolding 0, single step 30001
2019-03-19 11:41:20,169 [INFO] Sum of grad norms of most recent batch: 1.767787
2019-03-19 11:41:20,170 [INFO] ---------------------------------
2019-03-19 11:41:26,013 [INFO] ---------------------------------
2019-03-19 11:41:26,014 [INFO] Evaluation:
2019-03-19 11:41:26,015 [INFO] Batch 30000, worst loss 0.003142 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:41:26,015 [INFO] ---------------------------------
2019-03-19 11:41:47,901 [INFO] ---------------------------------
2019-03-19 11:41:47,901 [INFO] Summary:
2019-03-19 11:41:47,902 [INFO] Batch 31000, worst loss 0.188462 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:41:47,902 [INFO] Regularization: 9180.086914 * 0.0000000000 = 0.0000000918 loss
2019-03-19 11:41:47,903 [INFO] unfolding 0, single step 31001
2019-03-19 11:41:47,903 [INFO] Sum of grad norms of most recent batch: 3.711097
2019-03-19 11:41:47,904 [INFO] ---------------------------------
2019-03-19 11:42:09,548 [INFO] ---------------------------------
2019-03-19 11:42:09,549 [INFO] Summary:
2019-03-19 11:42:09,550 [INFO] Batch 32000, worst loss 0.009222 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:42:09,550 [INFO] Regularization: 9149.831055 * 0.0000000000 = 0.0000000915 loss
2019-03-19 11:42:09,550 [INFO] unfolding 0, single step 32001
2019-03-19 11:42:09,551 [INFO] Sum of grad norms of most recent batch: 2.050723
2019-03-19 11:42:09,551 [INFO] ---------------------------------
2019-03-19 11:42:31,195 [INFO] ---------------------------------
2019-03-19 11:42:31,196 [INFO] Summary:
2019-03-19 11:42:31,197 [INFO] Batch 33000, worst loss 0.012420 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:42:31,197 [INFO] Regularization: 9127.580078 * 0.0000000000 = 0.0000000913 loss
2019-03-19 11:42:31,198 [INFO] unfolding 0, single step 33001
2019-03-19 11:42:31,199 [INFO] Sum of grad norms of most recent batch: 1.353602
2019-03-19 11:42:31,199 [INFO] ---------------------------------
2019-03-19 11:42:52,660 [INFO] ---------------------------------
2019-03-19 11:42:52,661 [INFO] Summary:
2019-03-19 11:42:52,662 [INFO] Batch 34000, worst loss 0.316277 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:42:52,663 [INFO] Regularization: 9129.288086 * 0.0000000000 = 0.0000000913 loss
2019-03-19 11:42:52,663 [INFO] unfolding 0, single step 34001
2019-03-19 11:42:52,664 [INFO] Sum of grad norms of most recent batch: 1.890756
2019-03-19 11:42:52,664 [INFO] ---------------------------------
2019-03-19 11:43:14,669 [INFO] ---------------------------------
2019-03-19 11:43:14,670 [INFO] Summary:
2019-03-19 11:43:14,670 [INFO] Batch 35000, worst loss 0.011241 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:43:14,671 [INFO] Regularization: 9114.625000 * 0.0000000000 = 0.0000000911 loss
2019-03-19 11:43:14,671 [INFO] unfolding 0, single step 35001
2019-03-19 11:43:14,672 [INFO] Sum of grad norms of most recent batch: 3.597758
2019-03-19 11:43:14,672 [INFO] ---------------------------------
2019-03-19 11:43:36,225 [INFO] ---------------------------------
2019-03-19 11:43:36,226 [INFO] Summary:
2019-03-19 11:43:36,227 [INFO] Batch 36000, worst loss 0.115806 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:43:36,228 [INFO] Regularization: 9088.666992 * 0.0000000000 = 0.0000000909 loss
2019-03-19 11:43:36,229 [INFO] unfolding 0, single step 36001
2019-03-19 11:43:36,230 [INFO] Sum of grad norms of most recent batch: 1.010295
2019-03-19 11:43:36,231 [INFO] ---------------------------------
2019-03-19 11:43:57,540 [INFO] ---------------------------------
2019-03-19 11:43:57,541 [INFO] Summary:
2019-03-19 11:43:57,542 [INFO] Batch 37000, worst loss 0.186843 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:43:57,542 [INFO] Regularization: 9063.282227 * 0.0000000000 = 0.0000000906 loss
2019-03-19 11:43:57,542 [INFO] unfolding 0, single step 37001
2019-03-19 11:43:57,543 [INFO] Sum of grad norms of most recent batch: 5.158827
2019-03-19 11:43:57,544 [INFO] ---------------------------------
2019-03-19 11:44:19,190 [INFO] ---------------------------------
2019-03-19 11:44:19,191 [INFO] Summary:
2019-03-19 11:44:19,191 [INFO] Batch 38000, worst loss 0.068790 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:44:19,192 [INFO] Regularization: 9057.121094 * 0.0000000000 = 0.0000000906 loss
2019-03-19 11:44:19,192 [INFO] unfolding 0, single step 38001
2019-03-19 11:44:19,193 [INFO] Sum of grad norms of most recent batch: 1.355047
2019-03-19 11:44:19,193 [INFO] ---------------------------------
2019-03-19 11:44:40,514 [INFO] ---------------------------------
2019-03-19 11:44:40,515 [INFO] Summary:
2019-03-19 11:44:40,515 [INFO] Batch 39000, worst loss 0.233976 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:44:40,516 [INFO] Regularization: 9040.866211 * 0.0000000000 = 0.0000000904 loss
2019-03-19 11:44:40,516 [INFO] unfolding 0, single step 39001
2019-03-19 11:44:40,517 [INFO] Sum of grad norms of most recent batch: 3.050478
2019-03-19 11:44:40,517 [INFO] ---------------------------------
2019-03-19 11:45:02,329 [INFO] ---------------------------------
2019-03-19 11:45:02,330 [INFO] Summary:
2019-03-19 11:45:02,330 [INFO] Batch 40000, worst loss 0.119932 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:45:02,331 [INFO] Regularization: 9031.670898 * 0.0000000000 = 0.0000000903 loss
2019-03-19 11:45:02,331 [INFO] unfolding 0, single step 40001
2019-03-19 11:45:02,332 [INFO] Sum of grad norms of most recent batch: 0.907222
2019-03-19 11:45:02,332 [INFO] ---------------------------------
2019-03-19 11:45:08,139 [INFO] ---------------------------------
2019-03-19 11:45:08,140 [INFO] Evaluation:
2019-03-19 11:45:08,141 [INFO] Batch 40000, worst loss 0.002265 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:45:08,141 [INFO] ---------------------------------
2019-03-19 11:45:29,651 [INFO] ---------------------------------
2019-03-19 11:45:29,652 [INFO] Summary:
2019-03-19 11:45:29,652 [INFO] Batch 41000, worst loss 0.007742 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:45:29,653 [INFO] Regularization: 8995.189453 * 0.0000000000 = 0.0000000900 loss
2019-03-19 11:45:29,653 [INFO] unfolding 0, single step 41001
2019-03-19 11:45:29,654 [INFO] Sum of grad norms of most recent batch: 2.718316
2019-03-19 11:45:29,655 [INFO] ---------------------------------
2019-03-19 11:45:51,154 [INFO] ---------------------------------
2019-03-19 11:45:51,155 [INFO] Summary:
2019-03-19 11:45:51,156 [INFO] Batch 42000, worst loss 0.010731 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:45:51,156 [INFO] Regularization: 8985.053711 * 0.0000000000 = 0.0000000899 loss
2019-03-19 11:45:51,157 [INFO] unfolding 0, single step 42001
2019-03-19 11:45:51,157 [INFO] Sum of grad norms of most recent batch: 1.155759
2019-03-19 11:45:51,158 [INFO] ---------------------------------
2019-03-19 11:46:12,311 [INFO] ---------------------------------
2019-03-19 11:46:12,312 [INFO] Summary:
2019-03-19 11:46:12,313 [INFO] Batch 43000, worst loss 0.010403 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:46:12,313 [INFO] Regularization: 8964.258789 * 0.0000000000 = 0.0000000896 loss
2019-03-19 11:46:12,314 [INFO] unfolding 0, single step 43001
2019-03-19 11:46:12,315 [INFO] Sum of grad norms of most recent batch: 0.654017
2019-03-19 11:46:12,315 [INFO] ---------------------------------
2019-03-19 11:46:33,930 [INFO] ---------------------------------
2019-03-19 11:46:33,931 [INFO] Summary:
2019-03-19 11:46:33,932 [INFO] Batch 44000, worst loss 0.007815 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:46:33,932 [INFO] Regularization: 8943.431641 * 0.0000000000 = 0.0000000894 loss
2019-03-19 11:46:33,933 [INFO] unfolding 0, single step 44001
2019-03-19 11:46:33,933 [INFO] Sum of grad norms of most recent batch: 0.956021
2019-03-19 11:46:33,934 [INFO] ---------------------------------
2019-03-19 11:46:55,892 [INFO] ---------------------------------
2019-03-19 11:46:55,893 [INFO] Summary:
2019-03-19 11:46:55,894 [INFO] Batch 45000, worst loss 0.457668 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:46:55,894 [INFO] Regularization: 8956.382812 * 0.0000000000 = 0.0000000896 loss
2019-03-19 11:46:55,894 [INFO] unfolding 0, single step 45001
2019-03-19 11:46:55,895 [INFO] Sum of grad norms of most recent batch: 1.504431
2019-03-19 11:46:55,896 [INFO] ---------------------------------
2019-03-19 11:47:17,467 [INFO] ---------------------------------
2019-03-19 11:47:17,468 [INFO] Summary:
2019-03-19 11:47:17,469 [INFO] Batch 46000, worst loss 0.007499 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:47:17,469 [INFO] Regularization: 8940.206055 * 0.0000000000 = 0.0000000894 loss
2019-03-19 11:47:17,470 [INFO] unfolding 0, single step 46001
2019-03-19 11:47:17,470 [INFO] Sum of grad norms of most recent batch: 0.813926
2019-03-19 11:47:17,471 [INFO] ---------------------------------
2019-03-19 11:47:39,295 [INFO] ---------------------------------
2019-03-19 11:47:39,296 [INFO] Summary:
2019-03-19 11:47:39,297 [INFO] Batch 47000, worst loss 0.011187 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:47:39,297 [INFO] Regularization: 8923.620117 * 0.0000000000 = 0.0000000892 loss
2019-03-19 11:47:39,298 [INFO] unfolding 0, single step 47001
2019-03-19 11:47:39,299 [INFO] Sum of grad norms of most recent batch: 0.452393
2019-03-19 11:47:39,300 [INFO] ---------------------------------
2019-03-19 11:48:00,815 [INFO] ---------------------------------
2019-03-19 11:48:00,816 [INFO] Summary:
2019-03-19 11:48:00,817 [INFO] Batch 48000, worst loss 0.008576 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:48:00,817 [INFO] Regularization: 8894.084961 * 0.0000000000 = 0.0000000889 loss
2019-03-19 11:48:00,818 [INFO] unfolding 0, single step 48001
2019-03-19 11:48:00,818 [INFO] Sum of grad norms of most recent batch: 1.755366
2019-03-19 11:48:00,819 [INFO] ---------------------------------
2019-03-19 11:48:22,343 [INFO] ---------------------------------
2019-03-19 11:48:22,344 [INFO] Summary:
2019-03-19 11:48:22,344 [INFO] Batch 49000, worst loss 0.008675 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:48:22,345 [INFO] Regularization: 8858.671875 * 0.0000000000 = 0.0000000886 loss
2019-03-19 11:48:22,345 [INFO] unfolding 0, single step 49001
2019-03-19 11:48:22,346 [INFO] Sum of grad norms of most recent batch: 1.400838
2019-03-19 11:48:22,347 [INFO] ---------------------------------
2019-03-19 11:48:43,540 [INFO] ---------------------------------
2019-03-19 11:48:43,541 [INFO] Summary:
2019-03-19 11:48:43,542 [INFO] Batch 50000, worst loss 0.344207 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:48:43,543 [INFO] Regularization: 8875.900391 * 0.0000000000 = 0.0000000888 loss
2019-03-19 11:48:43,543 [INFO] unfolding 0, single step 50001
2019-03-19 11:48:43,544 [INFO] Sum of grad norms of most recent batch: 0.890063
2019-03-19 11:48:43,545 [INFO] ---------------------------------
2019-03-19 11:48:49,394 [INFO] ---------------------------------
2019-03-19 11:48:49,395 [INFO] Evaluation:
2019-03-19 11:48:49,398 [INFO] Batch 50000, worst loss 0.002094 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:48:49,399 [INFO] ---------------------------------
2019-03-19 11:49:11,037 [INFO] ---------------------------------
2019-03-19 11:49:11,038 [INFO] Summary:
2019-03-19 11:49:11,039 [INFO] Batch 51000, worst loss 0.093944 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:49:11,040 [INFO] Regularization: 8839.779297 * 0.0000000000 = 0.0000000884 loss
2019-03-19 11:49:11,040 [INFO] unfolding 0, single step 51001
2019-03-19 11:49:11,041 [INFO] Sum of grad norms of most recent batch: 1.947198
2019-03-19 11:49:11,041 [INFO] ---------------------------------
2019-03-19 11:49:32,503 [INFO] ---------------------------------
2019-03-19 11:49:32,504 [INFO] Summary:
2019-03-19 11:49:32,505 [INFO] Batch 52000, worst loss 0.009909 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:49:32,505 [INFO] Regularization: 8819.181641 * 0.0000000000 = 0.0000000882 loss
2019-03-19 11:49:32,506 [INFO] unfolding 0, single step 52001
2019-03-19 11:49:32,507 [INFO] Sum of grad norms of most recent batch: 1.218680
2019-03-19 11:49:32,507 [INFO] ---------------------------------
2019-03-19 11:49:54,216 [INFO] ---------------------------------
2019-03-19 11:49:54,217 [INFO] Summary:
2019-03-19 11:49:54,217 [INFO] Batch 53000, worst loss 0.011943 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:49:54,218 [INFO] Regularization: 8792.451172 * 0.0000000000 = 0.0000000879 loss
2019-03-19 11:49:54,218 [INFO] unfolding 0, single step 53001
2019-03-19 11:49:54,219 [INFO] Sum of grad norms of most recent batch: 3.955668
2019-03-19 11:49:54,220 [INFO] ---------------------------------
2019-03-19 11:50:15,560 [INFO] ---------------------------------
2019-03-19 11:50:15,560 [INFO] Summary:
2019-03-19 11:50:15,561 [INFO] Batch 54000, worst loss 0.006185 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:50:15,562 [INFO] Regularization: 8767.330078 * 0.0000000000 = 0.0000000877 loss
2019-03-19 11:50:15,562 [INFO] unfolding 0, single step 54001
2019-03-19 11:50:15,563 [INFO] Sum of grad norms of most recent batch: 0.552716
2019-03-19 11:50:15,563 [INFO] ---------------------------------
2019-03-19 11:50:36,967 [INFO] ---------------------------------
2019-03-19 11:50:36,968 [INFO] Summary:
2019-03-19 11:50:36,968 [INFO] Batch 55000, worst loss 0.278558 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:50:36,969 [INFO] Regularization: 8785.053711 * 0.0000000000 = 0.0000000879 loss
2019-03-19 11:50:36,969 [INFO] unfolding 0, single step 55001
2019-03-19 11:50:36,970 [INFO] Sum of grad norms of most recent batch: 1.493019
2019-03-19 11:50:36,970 [INFO] ---------------------------------
2019-03-19 11:50:58,663 [INFO] ---------------------------------
2019-03-19 11:50:58,664 [INFO] Summary:
2019-03-19 11:50:58,664 [INFO] Batch 56000, worst loss 0.007271 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:50:58,665 [INFO] Regularization: 8757.239258 * 0.0000000000 = 0.0000000876 loss
2019-03-19 11:50:58,665 [INFO] unfolding 0, single step 56001
2019-03-19 11:50:58,666 [INFO] Sum of grad norms of most recent batch: 1.508606
2019-03-19 11:50:58,666 [INFO] ---------------------------------
2019-03-19 11:51:20,140 [INFO] ---------------------------------
2019-03-19 11:51:20,141 [INFO] Summary:
2019-03-19 11:51:20,142 [INFO] Batch 57000, worst loss 0.005569 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:51:20,143 [INFO] Regularization: 8724.233398 * 0.0000000000 = 0.0000000872 loss
2019-03-19 11:51:20,143 [INFO] unfolding 0, single step 57001
2019-03-19 11:51:20,144 [INFO] Sum of grad norms of most recent batch: 1.579680
2019-03-19 11:51:20,144 [INFO] ---------------------------------
2019-03-19 11:51:41,745 [INFO] ---------------------------------
2019-03-19 11:51:41,746 [INFO] Summary:
2019-03-19 11:51:41,746 [INFO] Batch 58000, worst loss 0.006549 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:51:41,747 [INFO] Regularization: 8693.466797 * 0.0000000000 = 0.0000000869 loss
2019-03-19 11:51:41,747 [INFO] unfolding 0, single step 58001
2019-03-19 11:51:41,748 [INFO] Sum of grad norms of most recent batch: 1.475148
2019-03-19 11:51:41,749 [INFO] ---------------------------------
2019-03-19 11:52:03,638 [INFO] ---------------------------------
2019-03-19 11:52:03,639 [INFO] Summary:
2019-03-19 11:52:03,640 [INFO] Batch 59000, worst loss 0.013876 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:52:03,641 [INFO] Regularization: 8686.099609 * 0.0000000000 = 0.0000000869 loss
2019-03-19 11:52:03,641 [INFO] unfolding 0, single step 59001
2019-03-19 11:52:03,643 [INFO] Sum of grad norms of most recent batch: 0.486871
2019-03-19 11:52:03,643 [INFO] ---------------------------------
2019-03-19 11:52:25,362 [INFO] ---------------------------------
2019-03-19 11:52:25,363 [INFO] Summary:
2019-03-19 11:52:25,363 [INFO] Batch 60000, worst loss 0.018106 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 11:52:25,364 [INFO] Regularization: 8674.167969 * 0.0000000000 = 0.0000000867 loss
2019-03-19 11:52:25,364 [INFO] unfolding 0, single step 60001
2019-03-19 11:52:25,365 [INFO] Sum of grad norms of most recent batch: 2.359691
2019-03-19 11:52:25,365 [INFO] ---------------------------------
2019-03-19 11:52:31,194 [INFO] ---------------------------------
2019-03-19 11:52:31,195 [INFO] Evaluation:
2019-03-19 11:52:31,196 [INFO] Batch 60000, worst loss 0.002367 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:52:31,197 [INFO] ---------------------------------
2019-03-19 11:52:52,800 [INFO] ---------------------------------
2019-03-19 11:52:52,801 [INFO] Summary:
2019-03-19 11:52:52,802 [INFO] Batch 61000, worst loss 0.004201 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:52:52,803 [INFO] Regularization: 8644.803711 * 0.0000000000 = 0.0000000864 loss
2019-03-19 11:52:52,803 [INFO] unfolding 0, single step 61001
2019-03-19 11:52:52,804 [INFO] Sum of grad norms of most recent batch: 1.245311
2019-03-19 11:52:52,805 [INFO] ---------------------------------
2019-03-19 11:53:14,075 [INFO] ---------------------------------
2019-03-19 11:53:14,076 [INFO] Summary:
2019-03-19 11:53:14,077 [INFO] Batch 62000, worst loss 0.001810 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:53:14,077 [INFO] Regularization: 8606.010742 * 0.0000000000 = 0.0000000861 loss
2019-03-19 11:53:14,078 [INFO] unfolding 0, single step 62001
2019-03-19 11:53:14,078 [INFO] Sum of grad norms of most recent batch: 0.651949
2019-03-19 11:53:14,079 [INFO] ---------------------------------
2019-03-19 11:53:35,389 [INFO] ---------------------------------
2019-03-19 11:53:35,390 [INFO] Summary:
2019-03-19 11:53:35,391 [INFO] Batch 63000, worst loss 0.001451 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:53:35,391 [INFO] Regularization: 8565.710938 * 0.0000000000 = 0.0000000857 loss
2019-03-19 11:53:35,392 [INFO] unfolding 0, single step 63001
2019-03-19 11:53:35,392 [INFO] Sum of grad norms of most recent batch: 1.859795
2019-03-19 11:53:35,393 [INFO] ---------------------------------
2019-03-19 11:53:57,377 [INFO] ---------------------------------
2019-03-19 11:53:57,378 [INFO] Summary:
2019-03-19 11:53:57,379 [INFO] Batch 64000, worst loss 0.001364 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:53:57,379 [INFO] Regularization: 8537.266602 * 0.0000000000 = 0.0000000854 loss
2019-03-19 11:53:57,379 [INFO] unfolding 0, single step 64001
2019-03-19 11:53:57,380 [INFO] Sum of grad norms of most recent batch: 0.457886
2019-03-19 11:53:57,380 [INFO] ---------------------------------
2019-03-19 11:54:18,839 [INFO] ---------------------------------
2019-03-19 11:54:18,840 [INFO] Summary:
2019-03-19 11:54:18,841 [INFO] Batch 65000, worst loss 0.001123 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:54:18,841 [INFO] Regularization: 8507.802734 * 0.0000000000 = 0.0000000851 loss
2019-03-19 11:54:18,841 [INFO] unfolding 0, single step 65001
2019-03-19 11:54:18,842 [INFO] Sum of grad norms of most recent batch: 1.488747
2019-03-19 11:54:18,843 [INFO] ---------------------------------
2019-03-19 11:54:40,754 [INFO] ---------------------------------
2019-03-19 11:54:40,754 [INFO] Summary:
2019-03-19 11:54:40,755 [INFO] Batch 66000, worst loss 0.028860 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:54:40,756 [INFO] Regularization: 8483.574219 * 0.0000000000 = 0.0000000848 loss
2019-03-19 11:54:40,756 [INFO] unfolding 0, single step 66001
2019-03-19 11:54:40,757 [INFO] Sum of grad norms of most recent batch: 0.595709
2019-03-19 11:54:40,757 [INFO] ---------------------------------
2019-03-19 11:55:02,048 [INFO] ---------------------------------
2019-03-19 11:55:02,049 [INFO] Summary:
2019-03-19 11:55:02,050 [INFO] Batch 67000, worst loss 0.001237 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:55:02,051 [INFO] Regularization: 8466.635742 * 0.0000000000 = 0.0000000847 loss
2019-03-19 11:55:02,051 [INFO] unfolding 0, single step 67001
2019-03-19 11:55:02,051 [INFO] Sum of grad norms of most recent batch: 1.820711
2019-03-19 11:55:02,052 [INFO] ---------------------------------
2019-03-19 11:55:23,778 [INFO] ---------------------------------
2019-03-19 11:55:23,779 [INFO] Summary:
2019-03-19 11:55:23,779 [INFO] Batch 68000, worst loss 0.000934 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:55:23,780 [INFO] Regularization: 8441.165039 * 0.0000000000 = 0.0000000844 loss
2019-03-19 11:55:23,780 [INFO] unfolding 0, single step 68001
2019-03-19 11:55:23,781 [INFO] Sum of grad norms of most recent batch: 0.965962
2019-03-19 11:55:23,781 [INFO] ---------------------------------
2019-03-19 11:55:45,359 [INFO] ---------------------------------
2019-03-19 11:55:45,360 [INFO] Summary:
2019-03-19 11:55:45,361 [INFO] Batch 69000, worst loss 0.038029 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:55:45,361 [INFO] Regularization: 8424.368164 * 0.0000000000 = 0.0000000842 loss
2019-03-19 11:55:45,361 [INFO] unfolding 0, single step 69001
2019-03-19 11:55:45,362 [INFO] Sum of grad norms of most recent batch: 1.403716
2019-03-19 11:55:45,363 [INFO] ---------------------------------
2019-03-19 11:56:06,325 [INFO] ---------------------------------
2019-03-19 11:56:06,326 [INFO] Summary:
2019-03-19 11:56:06,326 [INFO] Batch 70000, worst loss 0.000741 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 11:56:06,327 [INFO] Regularization: 8399.265625 * 0.0000000000 = 0.0000000840 loss
2019-03-19 11:56:06,327 [INFO] unfolding 0, single step 70001
2019-03-19 11:56:06,328 [INFO] Sum of grad norms of most recent batch: 0.305271
2019-03-19 11:56:06,328 [INFO] ---------------------------------
2019-03-19 11:56:12,088 [INFO] ---------------------------------
2019-03-19 11:56:12,089 [INFO] Evaluation:
2019-03-19 11:56:12,090 [INFO] Batch 70000, worst loss 0.000549 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:56:12,094 [INFO] ---------------------------------
2019-03-19 11:56:33,427 [INFO] ---------------------------------
2019-03-19 11:56:33,428 [INFO] Summary:
2019-03-19 11:56:33,429 [INFO] Batch 71000, worst loss 0.001543 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 11:56:33,429 [INFO] Regularization: 8381.128906 * 0.0000000000 = 0.0000000838 loss
2019-03-19 11:56:33,430 [INFO] unfolding 0, single step 71001
2019-03-19 11:56:33,430 [INFO] Sum of grad norms of most recent batch: 0.514959
2019-03-19 11:56:33,431 [INFO] ---------------------------------
2019-03-19 11:56:54,684 [INFO] ---------------------------------
2019-03-19 11:56:54,685 [INFO] Summary:
2019-03-19 11:56:54,686 [INFO] Batch 72000, worst loss 0.001014 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 11:56:54,686 [INFO] Regularization: 8358.958008 * 0.0000000000 = 0.0000000836 loss
2019-03-19 11:56:54,686 [INFO] unfolding 0, single step 72001
2019-03-19 11:56:54,687 [INFO] Sum of grad norms of most recent batch: 1.149964
2019-03-19 11:56:54,688 [INFO] ---------------------------------
2019-03-19 11:57:16,025 [INFO] ---------------------------------
2019-03-19 11:57:16,026 [INFO] Summary:
2019-03-19 11:57:16,026 [INFO] Batch 73000, worst loss 0.000229 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 11:57:16,027 [INFO] Regularization: 8335.858398 * 0.0000000000 = 0.0000000834 loss
2019-03-19 11:57:16,027 [INFO] unfolding 0, single step 73001
2019-03-19 11:57:16,028 [INFO] Sum of grad norms of most recent batch: 0.068473
2019-03-19 11:57:16,028 [INFO] ---------------------------------
2019-03-19 11:57:36,973 [INFO] ---------------------------------
2019-03-19 11:57:36,974 [INFO] Summary:
2019-03-19 11:57:36,974 [INFO] Batch 74000, worst loss 0.000200 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000229 @est.-depth 2
2019-03-19 11:57:36,975 [INFO] Regularization: 8311.648438 * 0.0000000000 = 0.0000000831 loss
2019-03-19 11:57:36,975 [INFO] unfolding 0, single step 74001
2019-03-19 11:57:36,976 [INFO] Sum of grad norms of most recent batch: 0.668982
2019-03-19 11:57:36,977 [INFO] ---------------------------------
2019-03-19 11:57:58,722 [INFO] ---------------------------------
2019-03-19 11:57:58,723 [INFO] Summary:
2019-03-19 11:57:58,723 [INFO] Batch 75000, worst loss 0.000109 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000200 @est.-depth 2
2019-03-19 11:57:58,724 [INFO] Regularization: 8289.931641 * 0.0000000000 = 0.0000000829 loss
2019-03-19 11:57:58,724 [INFO] unfolding 0, single step 75001
2019-03-19 11:57:58,725 [INFO] Sum of grad norms of most recent batch: 0.611359
2019-03-19 11:57:58,725 [INFO] ---------------------------------
2019-03-19 11:58:19,630 [INFO] ---------------------------------
2019-03-19 11:58:19,631 [INFO] Summary:
2019-03-19 11:58:19,632 [INFO] Batch 76000, worst loss 0.000025 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000109 @est.-depth 2
2019-03-19 11:58:19,632 [INFO] Regularization: 8276.097656 * 0.0000000000 = 0.0000000828 loss
2019-03-19 11:58:19,632 [INFO] unfolding 0, single step 76001
2019-03-19 11:58:19,633 [INFO] Sum of grad norms of most recent batch: 0.128123
2019-03-19 11:58:19,634 [INFO] ---------------------------------
2019-03-19 11:58:40,447 [INFO] ---------------------------------
2019-03-19 11:58:40,448 [INFO] Summary:
2019-03-19 11:58:40,449 [INFO] Batch 77000, worst loss 0.000012 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000025 @est.-depth 2
2019-03-19 11:58:40,449 [INFO] Regularization: 8273.250000 * 0.0000000000 = 0.0000000827 loss
2019-03-19 11:58:40,450 [INFO] unfolding 0, single step 77001
2019-03-19 11:58:40,451 [INFO] Sum of grad norms of most recent batch: 0.018294
2019-03-19 11:58:40,451 [INFO] ---------------------------------
2019-03-19 11:59:01,476 [INFO] ---------------------------------
2019-03-19 11:59:01,477 [INFO] Summary:
2019-03-19 11:59:01,477 [INFO] Batch 78000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000012 @est.-depth 2
2019-03-19 11:59:01,478 [INFO] Regularization: 8271.759766 * 0.0000000000 = 0.0000000827 loss
2019-03-19 11:59:01,478 [INFO] unfolding 0, single step 78001
2019-03-19 11:59:01,479 [INFO] Sum of grad norms of most recent batch: 0.007887
2019-03-19 11:59:01,479 [INFO] ---------------------------------
2019-03-19 11:59:22,617 [INFO] ---------------------------------
2019-03-19 11:59:22,618 [INFO] Summary:
2019-03-19 11:59:22,618 [INFO] Batch 79000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 11:59:22,619 [INFO] Regularization: 8271.744141 * 0.0000000000 = 0.0000000827 loss
2019-03-19 11:59:22,619 [INFO] unfolding 0, single step 79001
2019-03-19 11:59:22,620 [INFO] Sum of grad norms of most recent batch: 0.001298
2019-03-19 11:59:22,621 [INFO] ---------------------------------
2019-03-19 11:59:43,876 [INFO] ---------------------------------
2019-03-19 11:59:43,877 [INFO] Summary:
2019-03-19 11:59:43,878 [INFO] Batch 80000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 11:59:43,878 [INFO] Regularization: 8271.730469 * 0.0000000000 = 0.0000000827 loss
2019-03-19 11:59:43,879 [INFO] unfolding 0, single step 80001
2019-03-19 11:59:43,879 [INFO] reducing reg_loss_factor
2019-03-19 11:59:43,880 [INFO] Sum of grad norms of most recent batch: 0.012408
2019-03-19 11:59:43,880 [INFO] ---------------------------------
2019-03-19 11:59:49,750 [INFO] ---------------------------------
2019-03-19 11:59:49,751 [INFO] Evaluation:
2019-03-19 11:59:49,755 [INFO] Batch 80000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 11:59:49,755 [INFO] ---------------------------------
2019-03-19 12:00:11,291 [INFO] ---------------------------------
2019-03-19 12:00:11,292 [INFO] Summary:
2019-03-19 12:00:11,293 [INFO] Batch 81000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:00:11,293 [INFO] Regularization: 8271.732422 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:00:11,294 [INFO] unfolding 0, single step 81001
2019-03-19 12:00:11,295 [INFO] Sum of grad norms of most recent batch: 0.005884
2019-03-19 12:00:11,295 [INFO] ---------------------------------
2019-03-19 12:00:32,954 [INFO] ---------------------------------
2019-03-19 12:00:32,955 [INFO] Summary:
2019-03-19 12:00:32,956 [INFO] Batch 82000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:00:32,956 [INFO] Regularization: 8271.733398 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:00:32,956 [INFO] unfolding 0, single step 82001
2019-03-19 12:00:32,957 [INFO] Sum of grad norms of most recent batch: 0.001263
2019-03-19 12:00:32,958 [INFO] ---------------------------------
2019-03-19 12:00:54,141 [INFO] ---------------------------------
2019-03-19 12:00:54,142 [INFO] Summary:
2019-03-19 12:00:54,143 [INFO] Batch 83000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:00:54,143 [INFO] Regularization: 8271.735352 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:00:54,144 [INFO] unfolding 0, single step 83001
2019-03-19 12:00:54,144 [INFO] Sum of grad norms of most recent batch: 0.012505
2019-03-19 12:00:54,145 [INFO] ---------------------------------
2019-03-19 12:01:14,865 [INFO] ---------------------------------
2019-03-19 12:01:14,866 [INFO] Summary:
2019-03-19 12:01:14,866 [INFO] Batch 84000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:01:14,867 [INFO] Regularization: 8271.734375 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:01:14,867 [INFO] unfolding 0, single step 84001
2019-03-19 12:01:14,868 [INFO] Sum of grad norms of most recent batch: 0.018681
2019-03-19 12:01:14,868 [INFO] ---------------------------------
2019-03-19 12:01:35,944 [INFO] ---------------------------------
2019-03-19 12:01:35,945 [INFO] Summary:
2019-03-19 12:01:35,945 [INFO] Batch 85000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:01:35,946 [INFO] Regularization: 8271.738281 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:01:35,946 [INFO] unfolding 0, single step 85001
2019-03-19 12:01:35,947 [INFO] Sum of grad norms of most recent batch: 0.002563
2019-03-19 12:01:35,947 [INFO] ---------------------------------
2019-03-19 12:01:56,586 [INFO] ---------------------------------
2019-03-19 12:01:56,587 [INFO] Summary:
2019-03-19 12:01:56,588 [INFO] Batch 86000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:01:56,588 [INFO] Regularization: 8271.741211 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:01:56,589 [INFO] unfolding 0, single step 86001
2019-03-19 12:01:56,589 [INFO] Sum of grad norms of most recent batch: 0.002239
2019-03-19 12:01:56,590 [INFO] ---------------------------------
2019-03-19 12:02:17,604 [INFO] ---------------------------------
2019-03-19 12:02:17,604 [INFO] Summary:
2019-03-19 12:02:17,605 [INFO] Batch 87000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:02:17,606 [INFO] Regularization: 8271.743164 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:02:17,606 [INFO] unfolding 0, single step 87001
2019-03-19 12:02:17,607 [INFO] Sum of grad norms of most recent batch: 0.003874
2019-03-19 12:02:17,607 [INFO] ---------------------------------
2019-03-19 12:02:38,116 [INFO] ---------------------------------
2019-03-19 12:02:38,117 [INFO] Summary:
2019-03-19 12:02:38,118 [INFO] Batch 88000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:02:38,118 [INFO] Regularization: 8271.743164 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:02:38,118 [INFO] unfolding 0, single step 88001
2019-03-19 12:02:38,119 [INFO] Sum of grad norms of most recent batch: 0.002114
2019-03-19 12:02:38,120 [INFO] ---------------------------------
2019-03-19 12:02:59,513 [INFO] ---------------------------------
2019-03-19 12:02:59,514 [INFO] Summary:
2019-03-19 12:02:59,514 [INFO] Batch 89000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:02:59,515 [INFO] Regularization: 8271.746094 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:02:59,515 [INFO] unfolding 0, single step 89001
2019-03-19 12:02:59,516 [INFO] Sum of grad norms of most recent batch: 0.001597
2019-03-19 12:02:59,516 [INFO] ---------------------------------
2019-03-19 12:03:20,933 [INFO] ---------------------------------
2019-03-19 12:03:20,934 [INFO] Summary:
2019-03-19 12:03:20,935 [INFO] Batch 90000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:03:20,936 [INFO] Regularization: 8271.747070 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:03:20,936 [INFO] unfolding 0, single step 90001
2019-03-19 12:03:20,937 [INFO] Sum of grad norms of most recent batch: 0.004734
2019-03-19 12:03:20,937 [INFO] ---------------------------------
2019-03-19 12:03:26,692 [INFO] ---------------------------------
2019-03-19 12:03:26,692 [INFO] Evaluation:
2019-03-19 12:03:26,693 [INFO] Batch 90000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:03:26,694 [INFO] ---------------------------------
2019-03-19 12:03:47,271 [INFO] ---------------------------------
2019-03-19 12:03:47,272 [INFO] Summary:
2019-03-19 12:03:47,273 [INFO] Batch 91000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:03:47,273 [INFO] Regularization: 8271.750000 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:03:47,274 [INFO] unfolding 0, single step 91001
2019-03-19 12:03:47,274 [INFO] Sum of grad norms of most recent batch: 0.019706
2019-03-19 12:03:47,275 [INFO] ---------------------------------
2019-03-19 12:04:08,396 [INFO] ---------------------------------
2019-03-19 12:04:08,397 [INFO] Summary:
2019-03-19 12:04:08,398 [INFO] Batch 92000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:04:08,398 [INFO] Regularization: 8271.749023 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:04:08,399 [INFO] unfolding 0, single step 92001
2019-03-19 12:04:08,399 [INFO] Sum of grad norms of most recent batch: 0.001033
2019-03-19 12:04:08,400 [INFO] ---------------------------------
2019-03-19 12:04:28,966 [INFO] ---------------------------------
2019-03-19 12:04:28,967 [INFO] Summary:
2019-03-19 12:04:28,967 [INFO] Batch 93000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:04:28,968 [INFO] Regularization: 8271.753906 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:04:28,968 [INFO] unfolding 0, single step 93001
2019-03-19 12:04:28,969 [INFO] Sum of grad norms of most recent batch: 0.006501
2019-03-19 12:04:28,969 [INFO] ---------------------------------
2019-03-19 12:04:50,088 [INFO] ---------------------------------
2019-03-19 12:04:50,089 [INFO] Summary:
2019-03-19 12:04:50,090 [INFO] Batch 94000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:04:50,090 [INFO] Regularization: 8271.752930 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:04:50,091 [INFO] unfolding 0, single step 94001
2019-03-19 12:04:50,091 [INFO] Sum of grad norms of most recent batch: 0.004386
2019-03-19 12:04:50,092 [INFO] ---------------------------------
2019-03-19 12:05:10,884 [INFO] ---------------------------------
2019-03-19 12:05:10,885 [INFO] Summary:
2019-03-19 12:05:10,885 [INFO] Batch 95000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:05:10,886 [INFO] Regularization: 8271.752930 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:05:10,886 [INFO] unfolding 0, single step 95001
2019-03-19 12:05:10,887 [INFO] Sum of grad norms of most recent batch: 0.001845
2019-03-19 12:05:10,887 [INFO] ---------------------------------
2019-03-19 12:05:32,076 [INFO] ---------------------------------
2019-03-19 12:05:32,077 [INFO] Summary:
2019-03-19 12:05:32,078 [INFO] Batch 96000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:05:32,078 [INFO] Regularization: 8271.753906 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:05:32,079 [INFO] unfolding 0, single step 96001
2019-03-19 12:05:32,079 [INFO] Sum of grad norms of most recent batch: 0.003881
2019-03-19 12:05:32,080 [INFO] ---------------------------------
2019-03-19 12:05:53,063 [INFO] ---------------------------------
2019-03-19 12:05:53,064 [INFO] Summary:
2019-03-19 12:05:53,065 [INFO] Batch 97000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:05:53,065 [INFO] Regularization: 8271.751953 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:05:53,065 [INFO] unfolding 0, single step 97001
2019-03-19 12:05:53,066 [INFO] Sum of grad norms of most recent batch: 0.003002
2019-03-19 12:05:53,066 [INFO] ---------------------------------
2019-03-19 12:06:14,344 [INFO] ---------------------------------
2019-03-19 12:06:14,345 [INFO] Summary:
2019-03-19 12:06:14,346 [INFO] Batch 98000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:06:14,347 [INFO] Regularization: 8271.758789 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:06:14,347 [INFO] unfolding 0, single step 98001
2019-03-19 12:06:14,347 [INFO] Sum of grad norms of most recent batch: 0.009996
2019-03-19 12:06:14,348 [INFO] ---------------------------------
2019-03-19 12:06:35,290 [INFO] ---------------------------------
2019-03-19 12:06:35,290 [INFO] Summary:
2019-03-19 12:06:35,291 [INFO] Batch 99000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:06:35,291 [INFO] Regularization: 8271.756836 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:06:35,291 [INFO] unfolding 0, single step 99001
2019-03-19 12:06:35,292 [INFO] Sum of grad norms of most recent batch: 0.007563
2019-03-19 12:06:35,293 [INFO] ---------------------------------
2019-03-19 12:06:56,518 [INFO] ---------------------------------
2019-03-19 12:06:56,519 [INFO] Summary:
2019-03-19 12:06:56,519 [INFO] Batch 100000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:06:56,520 [INFO] Regularization: 8271.759766 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:06:56,520 [INFO] unfolding 0, single step 100001
2019-03-19 12:06:56,521 [INFO] Sum of grad norms of most recent batch: 0.011774
2019-03-19 12:06:56,521 [INFO] ---------------------------------
2019-03-19 12:07:02,367 [INFO] ---------------------------------
2019-03-19 12:07:02,369 [INFO] Evaluation:
2019-03-19 12:07:02,370 [INFO] Batch 100000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:07:02,370 [INFO] ---------------------------------
2019-03-19 12:07:23,597 [INFO] ---------------------------------
2019-03-19 12:07:23,598 [INFO] Summary:
2019-03-19 12:07:23,599 [INFO] Batch 101000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:07:23,600 [INFO] Regularization: 8271.763672 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:07:23,600 [INFO] unfolding 0, single step 101001
2019-03-19 12:07:23,601 [INFO] Sum of grad norms of most recent batch: 0.008650
2019-03-19 12:07:23,601 [INFO] ---------------------------------
2019-03-19 12:07:44,384 [INFO] ---------------------------------
2019-03-19 12:07:44,384 [INFO] Summary:
2019-03-19 12:07:44,385 [INFO] Batch 102000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:07:44,386 [INFO] Regularization: 8271.761719 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:07:44,386 [INFO] unfolding 0, single step 102001
2019-03-19 12:07:44,386 [INFO] Sum of grad norms of most recent batch: 0.000840
2019-03-19 12:07:44,387 [INFO] ---------------------------------
2019-03-19 12:08:05,079 [INFO] ---------------------------------
2019-03-19 12:08:05,080 [INFO] Summary:
2019-03-19 12:08:05,081 [INFO] Batch 103000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:08:05,081 [INFO] Regularization: 8271.760742 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:08:05,081 [INFO] unfolding 0, single step 103001
2019-03-19 12:08:05,082 [INFO] Sum of grad norms of most recent batch: 0.001309
2019-03-19 12:08:05,083 [INFO] ---------------------------------
2019-03-19 12:08:26,234 [INFO] ---------------------------------
2019-03-19 12:08:26,235 [INFO] Summary:
2019-03-19 12:08:26,235 [INFO] Batch 104000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:08:26,236 [INFO] Regularization: 8271.760742 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:08:26,236 [INFO] unfolding 0, single step 104001
2019-03-19 12:08:26,237 [INFO] Sum of grad norms of most recent batch: 0.000870
2019-03-19 12:08:26,237 [INFO] ---------------------------------
2019-03-19 12:08:47,014 [INFO] ---------------------------------
2019-03-19 12:08:47,015 [INFO] Summary:
2019-03-19 12:08:47,015 [INFO] Batch 105000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:08:47,016 [INFO] Regularization: 8271.765625 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:08:47,016 [INFO] unfolding 0, single step 105001
2019-03-19 12:08:47,017 [INFO] Sum of grad norms of most recent batch: 0.005885
2019-03-19 12:08:47,018 [INFO] ---------------------------------
2019-03-19 12:09:07,914 [INFO] ---------------------------------
2019-03-19 12:09:07,915 [INFO] Summary:
2019-03-19 12:09:07,915 [INFO] Batch 106000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:09:07,916 [INFO] Regularization: 8271.765625 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:09:07,916 [INFO] unfolding 0, single step 106001
2019-03-19 12:09:07,917 [INFO] Sum of grad norms of most recent batch: 0.018013
2019-03-19 12:09:07,917 [INFO] ---------------------------------
2019-03-19 12:09:28,731 [INFO] ---------------------------------
2019-03-19 12:09:28,731 [INFO] Summary:
2019-03-19 12:09:28,732 [INFO] Batch 107000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:09:28,732 [INFO] Regularization: 8271.765625 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:09:28,733 [INFO] unfolding 0, single step 107001
2019-03-19 12:09:28,733 [INFO] Sum of grad norms of most recent batch: 0.002318
2019-03-19 12:09:28,734 [INFO] ---------------------------------
2019-03-19 12:09:49,477 [INFO] ---------------------------------
2019-03-19 12:09:49,478 [INFO] Summary:
2019-03-19 12:09:49,478 [INFO] Batch 108000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:09:49,479 [INFO] Regularization: 8271.766602 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:09:49,479 [INFO] unfolding 0, single step 108001
2019-03-19 12:09:49,480 [INFO] Sum of grad norms of most recent batch: 0.008459
2019-03-19 12:09:49,480 [INFO] ---------------------------------
2019-03-19 12:10:09,887 [INFO] ---------------------------------
2019-03-19 12:10:09,888 [INFO] Summary:
2019-03-19 12:10:09,889 [INFO] Batch 109000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:10:09,889 [INFO] Regularization: 8271.768555 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:10:09,889 [INFO] unfolding 0, single step 109001
2019-03-19 12:10:09,890 [INFO] Sum of grad norms of most recent batch: 0.013472
2019-03-19 12:10:09,890 [INFO] ---------------------------------
2019-03-19 12:10:30,825 [INFO] ---------------------------------
2019-03-19 12:10:30,826 [INFO] Summary:
2019-03-19 12:10:30,826 [INFO] Batch 110000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:10:30,827 [INFO] Regularization: 8271.769531 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:10:30,827 [INFO] unfolding 0, single step 110001
2019-03-19 12:10:30,828 [INFO] Sum of grad norms of most recent batch: 0.018977
2019-03-19 12:10:30,828 [INFO] ---------------------------------
2019-03-19 12:10:36,579 [INFO] ---------------------------------
2019-03-19 12:10:36,580 [INFO] Evaluation:
2019-03-19 12:10:36,581 [INFO] Batch 110000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:10:36,582 [INFO] ---------------------------------
2019-03-19 12:10:57,515 [INFO] ---------------------------------
2019-03-19 12:10:57,516 [INFO] Summary:
2019-03-19 12:10:57,516 [INFO] Batch 111000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:10:57,517 [INFO] Regularization: 8271.769531 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:10:57,517 [INFO] unfolding 0, single step 111001
2019-03-19 12:10:57,518 [INFO] Sum of grad norms of most recent batch: 0.001836
2019-03-19 12:10:57,518 [INFO] ---------------------------------
2019-03-19 12:11:18,313 [INFO] ---------------------------------
2019-03-19 12:11:18,314 [INFO] Summary:
2019-03-19 12:11:18,315 [INFO] Batch 112000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:11:18,315 [INFO] Regularization: 8271.768555 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:11:18,316 [INFO] unfolding 0, single step 112001
2019-03-19 12:11:18,316 [INFO] Sum of grad norms of most recent batch: 0.001451
2019-03-19 12:11:18,317 [INFO] ---------------------------------
2019-03-19 12:11:39,551 [INFO] ---------------------------------
2019-03-19 12:11:39,552 [INFO] Summary:
2019-03-19 12:11:39,552 [INFO] Batch 113000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:11:39,553 [INFO] Regularization: 8271.767578 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:11:39,553 [INFO] unfolding 0, single step 113001
2019-03-19 12:11:39,553 [INFO] Sum of grad norms of most recent batch: 0.001867
2019-03-19 12:11:39,554 [INFO] ---------------------------------
2019-03-19 12:12:00,505 [INFO] ---------------------------------
2019-03-19 12:12:00,506 [INFO] Summary:
2019-03-19 12:12:00,507 [INFO] Batch 114000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:12:00,508 [INFO] Regularization: 8271.768555 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:12:00,508 [INFO] unfolding 0, single step 114001
2019-03-19 12:12:00,509 [INFO] Sum of grad norms of most recent batch: 0.001712
2019-03-19 12:12:00,510 [INFO] ---------------------------------
2019-03-19 12:12:21,495 [INFO] ---------------------------------
2019-03-19 12:12:21,497 [INFO] Summary:
2019-03-19 12:12:21,497 [INFO] Batch 115000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:12:21,498 [INFO] Regularization: 8271.770508 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:12:21,498 [INFO] unfolding 0, single step 115001
2019-03-19 12:12:21,499 [INFO] Sum of grad norms of most recent batch: 0.003394
2019-03-19 12:12:21,499 [INFO] ---------------------------------
2019-03-19 12:12:42,515 [INFO] ---------------------------------
2019-03-19 12:12:42,516 [INFO] Summary:
2019-03-19 12:12:42,516 [INFO] Batch 116000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:12:42,517 [INFO] Regularization: 8271.768555 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:12:42,517 [INFO] unfolding 0, single step 116001
2019-03-19 12:12:42,518 [INFO] Sum of grad norms of most recent batch: 0.004838
2019-03-19 12:12:42,518 [INFO] ---------------------------------
2019-03-19 12:13:03,551 [INFO] ---------------------------------
2019-03-19 12:13:03,552 [INFO] Summary:
2019-03-19 12:13:03,552 [INFO] Batch 117000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:13:03,553 [INFO] Regularization: 8271.770508 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:13:03,553 [INFO] unfolding 0, single step 117001
2019-03-19 12:13:03,554 [INFO] Sum of grad norms of most recent batch: 0.008466
2019-03-19 12:13:03,554 [INFO] ---------------------------------
2019-03-19 12:13:24,755 [INFO] ---------------------------------
2019-03-19 12:13:24,756 [INFO] Summary:
2019-03-19 12:13:24,756 [INFO] Batch 118000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:13:24,757 [INFO] Regularization: 8271.771484 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:13:24,757 [INFO] unfolding 0, single step 118001
2019-03-19 12:13:24,758 [INFO] Sum of grad norms of most recent batch: 0.003616
2019-03-19 12:13:24,758 [INFO] ---------------------------------
2019-03-19 12:13:45,451 [INFO] ---------------------------------
2019-03-19 12:13:45,452 [INFO] Summary:
2019-03-19 12:13:45,453 [INFO] Batch 119000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:13:45,453 [INFO] Regularization: 8271.772461 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:13:45,453 [INFO] unfolding 0, single step 119001
2019-03-19 12:13:45,454 [INFO] Sum of grad norms of most recent batch: 0.015140
2019-03-19 12:13:45,455 [INFO] ---------------------------------
2019-03-19 12:14:06,688 [INFO] ---------------------------------
2019-03-19 12:14:06,689 [INFO] Summary:
2019-03-19 12:14:06,689 [INFO] Batch 120000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:14:06,690 [INFO] Regularization: 8271.774414 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:14:06,691 [INFO] unfolding 0, single step 120001
2019-03-19 12:14:06,691 [INFO] Sum of grad norms of most recent batch: 0.003471
2019-03-19 12:14:06,692 [INFO] ---------------------------------
2019-03-19 12:14:12,481 [INFO] ---------------------------------
2019-03-19 12:14:12,482 [INFO] Evaluation:
2019-03-19 12:14:12,483 [INFO] Batch 120000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:14:12,484 [INFO] ---------------------------------
2019-03-19 12:14:33,557 [INFO] ---------------------------------
2019-03-19 12:14:33,558 [INFO] Summary:
2019-03-19 12:14:33,559 [INFO] Batch 121000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:14:33,559 [INFO] Regularization: 8271.773438 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:14:33,560 [INFO] unfolding 0, single step 121001
2019-03-19 12:14:33,560 [INFO] Sum of grad norms of most recent batch: 0.016596
2019-03-19 12:14:33,561 [INFO] ---------------------------------
2019-03-19 12:14:54,558 [INFO] ---------------------------------
2019-03-19 12:14:54,559 [INFO] Summary:
2019-03-19 12:14:54,559 [INFO] Batch 122000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:14:54,560 [INFO] Regularization: 8271.773438 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:14:54,560 [INFO] unfolding 0, single step 122001
2019-03-19 12:14:54,561 [INFO] Sum of grad norms of most recent batch: 0.010902
2019-03-19 12:14:54,561 [INFO] ---------------------------------
2019-03-19 12:15:15,482 [INFO] ---------------------------------
2019-03-19 12:15:15,483 [INFO] Summary:
2019-03-19 12:15:15,484 [INFO] Batch 123000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:15:15,484 [INFO] Regularization: 8271.774414 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:15:15,485 [INFO] unfolding 0, single step 123001
2019-03-19 12:15:15,485 [INFO] Sum of grad norms of most recent batch: 0.005326
2019-03-19 12:15:15,486 [INFO] ---------------------------------
2019-03-19 12:15:36,695 [INFO] ---------------------------------
2019-03-19 12:15:36,697 [INFO] Summary:
2019-03-19 12:15:36,697 [INFO] Batch 124000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:15:36,698 [INFO] Regularization: 8271.774414 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:15:36,698 [INFO] unfolding 0, single step 124001
2019-03-19 12:15:36,699 [INFO] Sum of grad norms of most recent batch: 0.006078
2019-03-19 12:15:36,699 [INFO] ---------------------------------
2019-03-19 12:15:57,713 [INFO] ---------------------------------
2019-03-19 12:15:57,714 [INFO] Summary:
2019-03-19 12:15:57,714 [INFO] Batch 125000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:15:57,715 [INFO] Regularization: 8271.774414 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:15:57,715 [INFO] unfolding 0, single step 125001
2019-03-19 12:15:57,716 [INFO] Sum of grad norms of most recent batch: 0.006357
2019-03-19 12:15:57,716 [INFO] ---------------------------------
2019-03-19 12:16:18,631 [INFO] ---------------------------------
2019-03-19 12:16:18,632 [INFO] Summary:
2019-03-19 12:16:18,633 [INFO] Batch 126000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:16:18,633 [INFO] Regularization: 8271.775391 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:16:18,634 [INFO] unfolding 0, single step 126001
2019-03-19 12:16:18,635 [INFO] Sum of grad norms of most recent batch: 0.017365
2019-03-19 12:16:18,635 [INFO] ---------------------------------
2019-03-19 12:16:39,507 [INFO] ---------------------------------
2019-03-19 12:16:39,508 [INFO] Summary:
2019-03-19 12:16:39,508 [INFO] Batch 127000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:16:39,509 [INFO] Regularization: 8271.776367 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:16:39,509 [INFO] unfolding 0, single step 127001
2019-03-19 12:16:39,510 [INFO] Sum of grad norms of most recent batch: 0.002515
2019-03-19 12:16:39,511 [INFO] ---------------------------------
2019-03-19 12:17:00,185 [INFO] ---------------------------------
2019-03-19 12:17:00,186 [INFO] Summary:
2019-03-19 12:17:00,187 [INFO] Batch 128000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:17:00,187 [INFO] Regularization: 8271.776367 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:17:00,187 [INFO] unfolding 0, single step 128001
2019-03-19 12:17:00,188 [INFO] Sum of grad norms of most recent batch: 0.001839
2019-03-19 12:17:00,189 [INFO] ---------------------------------
2019-03-19 12:17:20,794 [INFO] ---------------------------------
2019-03-19 12:17:20,795 [INFO] Summary:
2019-03-19 12:17:20,796 [INFO] Batch 129000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:17:20,796 [INFO] Regularization: 8271.776367 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:17:20,796 [INFO] unfolding 0, single step 129001
2019-03-19 12:17:20,797 [INFO] Sum of grad norms of most recent batch: 0.008951
2019-03-19 12:17:20,797 [INFO] ---------------------------------
2019-03-19 12:17:41,497 [INFO] ---------------------------------
2019-03-19 12:17:41,498 [INFO] Summary:
2019-03-19 12:17:41,499 [INFO] Batch 130000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:17:41,499 [INFO] Regularization: 8271.776367 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:17:41,500 [INFO] unfolding 0, single step 130001
2019-03-19 12:17:41,500 [INFO] Sum of grad norms of most recent batch: 0.002100
2019-03-19 12:17:41,501 [INFO] ---------------------------------
2019-03-19 12:17:47,287 [INFO] ---------------------------------
2019-03-19 12:17:47,288 [INFO] Evaluation:
2019-03-19 12:17:47,289 [INFO] Batch 130000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:17:47,290 [INFO] ---------------------------------
2019-03-19 12:18:08,086 [INFO] ---------------------------------
2019-03-19 12:18:08,087 [INFO] Summary:
2019-03-19 12:18:08,088 [INFO] Batch 131000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:18:08,088 [INFO] Regularization: 8271.777344 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:18:08,088 [INFO] unfolding 0, single step 131001
2019-03-19 12:18:08,089 [INFO] Sum of grad norms of most recent batch: 0.011415
2019-03-19 12:18:08,089 [INFO] ---------------------------------
2019-03-19 12:18:28,988 [INFO] ---------------------------------
2019-03-19 12:18:28,989 [INFO] Summary:
2019-03-19 12:18:28,991 [INFO] Batch 132000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:18:28,991 [INFO] Regularization: 8271.777344 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:18:28,991 [INFO] unfolding 0, single step 132001
2019-03-19 12:18:28,992 [INFO] Sum of grad norms of most recent batch: 0.001971
2019-03-19 12:18:28,993 [INFO] ---------------------------------
2019-03-19 12:18:49,756 [INFO] ---------------------------------
2019-03-19 12:18:49,757 [INFO] Summary:
2019-03-19 12:18:49,758 [INFO] Batch 133000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:18:49,758 [INFO] Regularization: 8271.777344 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:18:49,759 [INFO] unfolding 0, single step 133001
2019-03-19 12:18:49,759 [INFO] Sum of grad norms of most recent batch: 0.006777
2019-03-19 12:18:49,760 [INFO] ---------------------------------
2019-03-19 12:19:10,914 [INFO] ---------------------------------
2019-03-19 12:19:10,915 [INFO] Summary:
2019-03-19 12:19:10,915 [INFO] Batch 134000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:19:10,916 [INFO] Regularization: 8271.777344 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:19:10,916 [INFO] unfolding 0, single step 134001
2019-03-19 12:19:10,917 [INFO] Sum of grad norms of most recent batch: 0.012914
2019-03-19 12:19:10,917 [INFO] ---------------------------------
2019-03-19 12:19:31,874 [INFO] ---------------------------------
2019-03-19 12:19:31,875 [INFO] Summary:
2019-03-19 12:19:31,875 [INFO] Batch 135000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:19:31,876 [INFO] Regularization: 8271.777344 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:19:31,876 [INFO] unfolding 0, single step 135001
2019-03-19 12:19:31,877 [INFO] Sum of grad norms of most recent batch: 0.001701
2019-03-19 12:19:31,877 [INFO] ---------------------------------
2019-03-19 12:19:53,074 [INFO] ---------------------------------
2019-03-19 12:19:53,075 [INFO] Summary:
2019-03-19 12:19:53,076 [INFO] Batch 136000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:19:53,076 [INFO] Regularization: 8271.777344 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:19:53,076 [INFO] unfolding 0, single step 136001
2019-03-19 12:19:53,077 [INFO] Sum of grad norms of most recent batch: 0.001023
2019-03-19 12:19:53,078 [INFO] ---------------------------------
2019-03-19 12:20:14,473 [INFO] ---------------------------------
2019-03-19 12:20:14,474 [INFO] Summary:
2019-03-19 12:20:14,475 [INFO] Batch 137000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:20:14,475 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:20:14,476 [INFO] unfolding 0, single step 137001
2019-03-19 12:20:14,476 [INFO] Sum of grad norms of most recent batch: 0.010869
2019-03-19 12:20:14,477 [INFO] ---------------------------------
2019-03-19 12:20:35,640 [INFO] ---------------------------------
2019-03-19 12:20:35,641 [INFO] Summary:
2019-03-19 12:20:35,642 [INFO] Batch 138000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:20:35,643 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:20:35,643 [INFO] unfolding 0, single step 138001
2019-03-19 12:20:35,644 [INFO] Sum of grad norms of most recent batch: 0.007963
2019-03-19 12:20:35,645 [INFO] ---------------------------------
2019-03-19 12:20:56,633 [INFO] ---------------------------------
2019-03-19 12:20:56,634 [INFO] Summary:
2019-03-19 12:20:56,635 [INFO] Batch 139000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:20:56,636 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:20:56,636 [INFO] unfolding 0, single step 139001
2019-03-19 12:20:56,637 [INFO] Sum of grad norms of most recent batch: 0.008405
2019-03-19 12:20:56,637 [INFO] ---------------------------------
2019-03-19 12:21:17,610 [INFO] ---------------------------------
2019-03-19 12:21:17,610 [INFO] Summary:
2019-03-19 12:21:17,611 [INFO] Batch 140000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:21:17,612 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:21:17,612 [INFO] unfolding 0, single step 140001
2019-03-19 12:21:17,613 [INFO] Sum of grad norms of most recent batch: 0.012196
2019-03-19 12:21:17,613 [INFO] ---------------------------------
2019-03-19 12:21:23,343 [INFO] ---------------------------------
2019-03-19 12:21:23,344 [INFO] Evaluation:
2019-03-19 12:21:23,345 [INFO] Batch 140000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:21:23,346 [INFO] ---------------------------------
2019-03-19 12:21:43,986 [INFO] ---------------------------------
2019-03-19 12:21:43,987 [INFO] Summary:
2019-03-19 12:21:43,988 [INFO] Batch 141000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:21:43,988 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:21:43,989 [INFO] unfolding 0, single step 141001
2019-03-19 12:21:43,990 [INFO] Sum of grad norms of most recent batch: 0.013856
2019-03-19 12:21:43,990 [INFO] ---------------------------------
2019-03-19 12:22:05,173 [INFO] ---------------------------------
2019-03-19 12:22:05,174 [INFO] Summary:
2019-03-19 12:22:05,175 [INFO] Batch 142000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:22:05,175 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:22:05,176 [INFO] unfolding 0, single step 142001
2019-03-19 12:22:05,176 [INFO] Sum of grad norms of most recent batch: 0.007759
2019-03-19 12:22:05,177 [INFO] ---------------------------------
2019-03-19 12:22:26,109 [INFO] ---------------------------------
2019-03-19 12:22:26,110 [INFO] Summary:
2019-03-19 12:22:26,110 [INFO] Batch 143000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:22:26,111 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:22:26,111 [INFO] unfolding 0, single step 143001
2019-03-19 12:22:26,112 [INFO] Sum of grad norms of most recent batch: 0.003299
2019-03-19 12:22:26,112 [INFO] ---------------------------------
2019-03-19 12:22:47,028 [INFO] ---------------------------------
2019-03-19 12:22:47,029 [INFO] Summary:
2019-03-19 12:22:47,030 [INFO] Batch 144000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:22:47,031 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:22:47,031 [INFO] unfolding 0, single step 144001
2019-03-19 12:22:47,031 [INFO] Sum of grad norms of most recent batch: 0.002903
2019-03-19 12:22:47,032 [INFO] ---------------------------------
2019-03-19 12:23:08,288 [INFO] ---------------------------------
2019-03-19 12:23:08,288 [INFO] Summary:
2019-03-19 12:23:08,289 [INFO] Batch 145000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:23:08,289 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:23:08,290 [INFO] unfolding 0, single step 145001
2019-03-19 12:23:08,291 [INFO] Sum of grad norms of most recent batch: 0.004817
2019-03-19 12:23:08,292 [INFO] ---------------------------------
2019-03-19 12:23:29,073 [INFO] ---------------------------------
2019-03-19 12:23:29,074 [INFO] Summary:
2019-03-19 12:23:29,075 [INFO] Batch 146000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:23:29,076 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:23:29,077 [INFO] unfolding 0, single step 146001
2019-03-19 12:23:29,078 [INFO] Sum of grad norms of most recent batch: 0.014831
2019-03-19 12:23:29,079 [INFO] ---------------------------------
2019-03-19 12:23:50,069 [INFO] ---------------------------------
2019-03-19 12:23:50,070 [INFO] Summary:
2019-03-19 12:23:50,071 [INFO] Batch 147000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:23:50,071 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:23:50,072 [INFO] unfolding 0, single step 147001
2019-03-19 12:23:50,072 [INFO] Sum of grad norms of most recent batch: 0.007559
2019-03-19 12:23:50,073 [INFO] ---------------------------------
2019-03-19 12:24:11,120 [INFO] ---------------------------------
2019-03-19 12:24:11,121 [INFO] Summary:
2019-03-19 12:24:11,122 [INFO] Batch 148000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:24:11,123 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:24:11,123 [INFO] unfolding 0, single step 148001
2019-03-19 12:24:11,124 [INFO] Sum of grad norms of most recent batch: 0.003388
2019-03-19 12:24:11,125 [INFO] ---------------------------------
2019-03-19 12:24:32,081 [INFO] ---------------------------------
2019-03-19 12:24:32,081 [INFO] Summary:
2019-03-19 12:24:32,082 [INFO] Batch 149000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:24:32,083 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:24:32,083 [INFO] unfolding 0, single step 149001
2019-03-19 12:24:32,084 [INFO] Sum of grad norms of most recent batch: 0.019950
2019-03-19 12:24:32,084 [INFO] ---------------------------------
2019-03-19 12:24:53,107 [INFO] ---------------------------------
2019-03-19 12:24:53,108 [INFO] Summary:
2019-03-19 12:24:53,109 [INFO] Batch 150000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:24:53,109 [INFO] Regularization: 8271.778320 * 0.0000000000 = 0.0000000083 loss
2019-03-19 12:24:53,110 [INFO] unfolding 0, single step 150001
2019-03-19 12:24:53,110 [INFO] Sum of grad norms of most recent batch: 0.005431
2019-03-19 12:24:53,111 [INFO] ---------------------------------
2019-03-19 12:24:58,983 [INFO] ---------------------------------
2019-03-19 12:24:58,984 [INFO] Evaluation:
2019-03-19 12:24:58,985 [INFO] Batch 150000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:24:58,986 [INFO] ---------------------------------
2019-03-19 12:24:58,986 [INFO] Finished training, saved to file transition/1552984806/1552994698_2_transition_final.pth
2019-03-19 12:24:59,178 [INFO] ---------------------------------
2019-03-19 12:24:59,179 [INFO] Training model #3: (203, 64, 1) @ 2
2019-03-19 12:25:21,005 [INFO] ---------------------------------
2019-03-19 12:25:21,006 [INFO] Summary:
2019-03-19 12:25:21,007 [INFO] Batch 1000, worst loss 19.484846 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:25:21,007 [INFO] Regularization: 9800.520508 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:25:21,007 [INFO] unfolding 0, single step 1001
2019-03-19 12:25:21,008 [INFO] Sum of grad norms of most recent batch: 13.780344
2019-03-19 12:25:21,009 [INFO] ---------------------------------
2019-03-19 12:25:42,460 [INFO] ---------------------------------
2019-03-19 12:25:42,461 [INFO] Summary:
2019-03-19 12:25:42,462 [INFO] Batch 2000, worst loss 0.212051 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:25:42,462 [INFO] Regularization: 9810.223633 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:25:42,463 [INFO] unfolding 0, single step 2001
2019-03-19 12:25:42,463 [INFO] Sum of grad norms of most recent batch: 0.810672
2019-03-19 12:25:42,464 [INFO] ---------------------------------
2019-03-19 12:26:03,892 [INFO] ---------------------------------
2019-03-19 12:26:03,893 [INFO] Summary:
2019-03-19 12:26:03,893 [INFO] Batch 3000, worst loss 0.028011 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:26:03,894 [INFO] Regularization: 9807.263672 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:26:03,894 [INFO] unfolding 0, single step 3001
2019-03-19 12:26:03,895 [INFO] Sum of grad norms of most recent batch: 2.543741
2019-03-19 12:26:03,895 [INFO] ---------------------------------
2019-03-19 12:26:25,495 [INFO] ---------------------------------
2019-03-19 12:26:25,496 [INFO] Summary:
2019-03-19 12:26:25,497 [INFO] Batch 4000, worst loss 0.022404 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:26:25,497 [INFO] Regularization: 9811.918945 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:26:25,498 [INFO] unfolding 0, single step 4001
2019-03-19 12:26:25,499 [INFO] Sum of grad norms of most recent batch: 1.365938
2019-03-19 12:26:25,500 [INFO] ---------------------------------
2019-03-19 12:26:46,988 [INFO] ---------------------------------
2019-03-19 12:26:46,989 [INFO] Summary:
2019-03-19 12:26:46,990 [INFO] Batch 5000, worst loss 0.019414 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:26:46,990 [INFO] Regularization: 9813.510742 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:26:46,990 [INFO] unfolding 0, single step 5001
2019-03-19 12:26:46,991 [INFO] Sum of grad norms of most recent batch: 7.823182
2019-03-19 12:26:46,991 [INFO] ---------------------------------
2019-03-19 12:27:08,705 [INFO] ---------------------------------
2019-03-19 12:27:08,706 [INFO] Summary:
2019-03-19 12:27:08,707 [INFO] Batch 6000, worst loss 0.057170 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:27:08,707 [INFO] Regularization: 9815.920898 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:27:08,708 [INFO] unfolding 0, single step 6001
2019-03-19 12:27:08,708 [INFO] Sum of grad norms of most recent batch: 3.297631
2019-03-19 12:27:08,709 [INFO] ---------------------------------
2019-03-19 12:27:30,648 [INFO] ---------------------------------
2019-03-19 12:27:30,649 [INFO] Summary:
2019-03-19 12:27:30,650 [INFO] Batch 7000, worst loss 0.050511 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:27:30,650 [INFO] Regularization: 9827.399414 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:27:30,651 [INFO] unfolding 0, single step 7001
2019-03-19 12:27:30,651 [INFO] Sum of grad norms of most recent batch: 7.144303
2019-03-19 12:27:30,652 [INFO] ---------------------------------
2019-03-19 12:27:52,392 [INFO] ---------------------------------
2019-03-19 12:27:52,393 [INFO] Summary:
2019-03-19 12:27:52,393 [INFO] Batch 8000, worst loss 0.026990 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:27:52,394 [INFO] Regularization: 9812.013672 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:27:52,394 [INFO] unfolding 0, single step 8001
2019-03-19 12:27:52,395 [INFO] Sum of grad norms of most recent batch: 4.647346
2019-03-19 12:27:52,395 [INFO] ---------------------------------
2019-03-19 12:28:13,763 [INFO] ---------------------------------
2019-03-19 12:28:13,764 [INFO] Summary:
2019-03-19 12:28:13,764 [INFO] Batch 9000, worst loss 0.031449 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:28:13,765 [INFO] Regularization: 9794.142578 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:28:13,765 [INFO] unfolding 0, single step 9001
2019-03-19 12:28:13,766 [INFO] Sum of grad norms of most recent batch: 1.797531
2019-03-19 12:28:13,766 [INFO] ---------------------------------
2019-03-19 12:28:34,827 [INFO] ---------------------------------
2019-03-19 12:28:34,828 [INFO] Summary:
2019-03-19 12:28:34,828 [INFO] Batch 10000, worst loss 0.036138 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:28:34,829 [INFO] Regularization: 9788.175781 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:28:34,829 [INFO] unfolding 0, single step 10001
2019-03-19 12:28:34,829 [INFO] Sum of grad norms of most recent batch: 1.680611
2019-03-19 12:28:34,830 [INFO] ---------------------------------
2019-03-19 12:28:40,732 [INFO] ---------------------------------
2019-03-19 12:28:40,733 [INFO] Evaluation:
2019-03-19 12:28:40,734 [INFO] Batch 10000, worst loss 0.006077 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:28:40,734 [INFO] ---------------------------------
2019-03-19 12:29:02,175 [INFO] ---------------------------------
2019-03-19 12:29:02,176 [INFO] Summary:
2019-03-19 12:29:02,176 [INFO] Batch 11000, worst loss 0.016793 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:29:02,177 [INFO] Regularization: 9769.889648 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:29:02,177 [INFO] unfolding 0, single step 11001
2019-03-19 12:29:02,178 [INFO] Sum of grad norms of most recent batch: 3.414086
2019-03-19 12:29:02,179 [INFO] ---------------------------------
2019-03-19 12:29:23,598 [INFO] ---------------------------------
2019-03-19 12:29:23,599 [INFO] Summary:
2019-03-19 12:29:23,600 [INFO] Batch 12000, worst loss 0.021778 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:29:23,600 [INFO] Regularization: 9755.662109 * 0.0000000000 = 0.0000000098 loss
2019-03-19 12:29:23,601 [INFO] unfolding 0, single step 12001
2019-03-19 12:29:23,601 [INFO] Sum of grad norms of most recent batch: 2.237126
2019-03-19 12:29:23,602 [INFO] ---------------------------------
2019-03-19 12:29:44,917 [INFO] ---------------------------------
2019-03-19 12:29:44,918 [INFO] Summary:
2019-03-19 12:29:44,919 [INFO] Batch 13000, worst loss 0.016085 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:29:44,919 [INFO] Regularization: 9727.862305 * 0.0000000000 = 0.0000000097 loss
2019-03-19 12:29:44,920 [INFO] unfolding 0, single step 13001
2019-03-19 12:29:44,920 [INFO] Sum of grad norms of most recent batch: 3.640343
2019-03-19 12:29:44,921 [INFO] ---------------------------------
2019-03-19 12:30:06,972 [INFO] ---------------------------------
2019-03-19 12:30:06,973 [INFO] Summary:
2019-03-19 12:30:06,973 [INFO] Batch 14000, worst loss 0.027727 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:30:06,974 [INFO] Regularization: 9697.199219 * 0.0000000000 = 0.0000000097 loss
2019-03-19 12:30:06,974 [INFO] unfolding 0, single step 14001
2019-03-19 12:30:06,975 [INFO] Sum of grad norms of most recent batch: 0.480700
2019-03-19 12:30:06,976 [INFO] ---------------------------------
2019-03-19 12:30:28,565 [INFO] ---------------------------------
2019-03-19 12:30:28,566 [INFO] Summary:
2019-03-19 12:30:28,567 [INFO] Batch 15000, worst loss 0.020864 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:30:28,567 [INFO] Regularization: 9680.244141 * 0.0000000000 = 0.0000000097 loss
2019-03-19 12:30:28,567 [INFO] unfolding 0, single step 15001
2019-03-19 12:30:28,568 [INFO] Sum of grad norms of most recent batch: 3.891296
2019-03-19 12:30:28,569 [INFO] ---------------------------------
2019-03-19 12:30:50,104 [INFO] ---------------------------------
2019-03-19 12:30:50,105 [INFO] Summary:
2019-03-19 12:30:50,106 [INFO] Batch 16000, worst loss 0.015671 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:30:50,107 [INFO] Regularization: 9639.814453 * 0.0000000000 = 0.0000000096 loss
2019-03-19 12:30:50,107 [INFO] unfolding 0, single step 16001
2019-03-19 12:30:50,107 [INFO] Sum of grad norms of most recent batch: 2.586908
2019-03-19 12:30:50,108 [INFO] ---------------------------------
2019-03-19 12:31:11,782 [INFO] ---------------------------------
2019-03-19 12:31:11,782 [INFO] Summary:
2019-03-19 12:31:11,783 [INFO] Batch 17000, worst loss 0.017551 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:31:11,784 [INFO] Regularization: 9612.124023 * 0.0000000000 = 0.0000000096 loss
2019-03-19 12:31:11,784 [INFO] unfolding 0, single step 17001
2019-03-19 12:31:11,784 [INFO] Sum of grad norms of most recent batch: 2.370386
2019-03-19 12:31:11,785 [INFO] ---------------------------------
2019-03-19 12:31:33,495 [INFO] ---------------------------------
2019-03-19 12:31:33,495 [INFO] Summary:
2019-03-19 12:31:33,496 [INFO] Batch 18000, worst loss 0.020985 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:31:33,497 [INFO] Regularization: 9571.835938 * 0.0000000000 = 0.0000000096 loss
2019-03-19 12:31:33,497 [INFO] unfolding 0, single step 18001
2019-03-19 12:31:33,498 [INFO] Sum of grad norms of most recent batch: 1.931733
2019-03-19 12:31:33,498 [INFO] ---------------------------------
2019-03-19 12:31:54,937 [INFO] ---------------------------------
2019-03-19 12:31:54,938 [INFO] Summary:
2019-03-19 12:31:54,939 [INFO] Batch 19000, worst loss 0.011799 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:31:54,939 [INFO] Regularization: 9554.190430 * 0.0000000000 = 0.0000000096 loss
2019-03-19 12:31:54,940 [INFO] unfolding 0, single step 19001
2019-03-19 12:31:54,940 [INFO] Sum of grad norms of most recent batch: 2.170319
2019-03-19 12:31:54,941 [INFO] ---------------------------------
2019-03-19 12:32:16,583 [INFO] ---------------------------------
2019-03-19 12:32:16,584 [INFO] Summary:
2019-03-19 12:32:16,585 [INFO] Batch 20000, worst loss 0.021358 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:32:16,585 [INFO] Regularization: 9523.210938 * 0.0000000000 = 0.0000000095 loss
2019-03-19 12:32:16,586 [INFO] unfolding 0, single step 20001
2019-03-19 12:32:16,586 [INFO] Sum of grad norms of most recent batch: 2.858079
2019-03-19 12:32:16,587 [INFO] ---------------------------------
2019-03-19 12:32:22,379 [INFO] ---------------------------------
2019-03-19 12:32:22,380 [INFO] Evaluation:
2019-03-19 12:32:22,380 [INFO] Batch 20000, worst loss 0.004359 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:32:22,381 [INFO] ---------------------------------
2019-03-19 12:32:44,011 [INFO] ---------------------------------
2019-03-19 12:32:44,012 [INFO] Summary:
2019-03-19 12:32:44,014 [INFO] Batch 21000, worst loss 0.017693 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:32:44,014 [INFO] Regularization: 9495.303711 * 0.0000000000 = 0.0000000095 loss
2019-03-19 12:32:44,015 [INFO] unfolding 0, single step 21001
2019-03-19 12:32:44,015 [INFO] Sum of grad norms of most recent batch: 1.068173
2019-03-19 12:32:44,016 [INFO] ---------------------------------
2019-03-19 12:33:05,499 [INFO] ---------------------------------
2019-03-19 12:33:05,500 [INFO] Summary:
2019-03-19 12:33:05,500 [INFO] Batch 22000, worst loss 0.008292 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:33:05,501 [INFO] Regularization: 9462.000000 * 0.0000000000 = 0.0000000095 loss
2019-03-19 12:33:05,501 [INFO] unfolding 0, single step 22001
2019-03-19 12:33:05,502 [INFO] Sum of grad norms of most recent batch: 0.892497
2019-03-19 12:33:05,502 [INFO] ---------------------------------
2019-03-19 12:33:26,642 [INFO] ---------------------------------
2019-03-19 12:33:26,644 [INFO] Summary:
2019-03-19 12:33:26,644 [INFO] Batch 23000, worst loss 0.011667 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:33:26,645 [INFO] Regularization: 9455.458008 * 0.0000000000 = 0.0000000095 loss
2019-03-19 12:33:26,645 [INFO] unfolding 0, single step 23001
2019-03-19 12:33:26,646 [INFO] Sum of grad norms of most recent batch: 2.739365
2019-03-19 12:33:26,646 [INFO] ---------------------------------
2019-03-19 12:33:48,565 [INFO] ---------------------------------
2019-03-19 12:33:48,566 [INFO] Summary:
2019-03-19 12:33:48,566 [INFO] Batch 24000, worst loss 0.030530 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:33:48,567 [INFO] Regularization: 9448.067383 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:33:48,567 [INFO] unfolding 0, single step 24001
2019-03-19 12:33:48,568 [INFO] Sum of grad norms of most recent batch: 2.916933
2019-03-19 12:33:48,568 [INFO] ---------------------------------
2019-03-19 12:34:10,051 [INFO] ---------------------------------
2019-03-19 12:34:10,052 [INFO] Summary:
2019-03-19 12:34:10,053 [INFO] Batch 25000, worst loss 0.013456 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:34:10,053 [INFO] Regularization: 9446.796875 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:34:10,054 [INFO] unfolding 0, single step 25001
2019-03-19 12:34:10,054 [INFO] Sum of grad norms of most recent batch: 2.396847
2019-03-19 12:34:10,055 [INFO] ---------------------------------
2019-03-19 12:34:31,583 [INFO] ---------------------------------
2019-03-19 12:34:31,584 [INFO] Summary:
2019-03-19 12:34:31,585 [INFO] Batch 26000, worst loss 0.010099 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:34:31,585 [INFO] Regularization: 9424.438477 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:34:31,586 [INFO] unfolding 0, single step 26001
2019-03-19 12:34:31,587 [INFO] Sum of grad norms of most recent batch: 1.281353
2019-03-19 12:34:31,587 [INFO] ---------------------------------
2019-03-19 12:34:52,678 [INFO] ---------------------------------
2019-03-19 12:34:52,679 [INFO] Summary:
2019-03-19 12:34:52,680 [INFO] Batch 27000, worst loss 0.014702 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:34:52,680 [INFO] Regularization: 9421.316406 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:34:52,680 [INFO] unfolding 0, single step 27001
2019-03-19 12:34:52,681 [INFO] Sum of grad norms of most recent batch: 2.832535
2019-03-19 12:34:52,682 [INFO] ---------------------------------
2019-03-19 12:35:14,083 [INFO] ---------------------------------
2019-03-19 12:35:14,084 [INFO] Summary:
2019-03-19 12:35:14,085 [INFO] Batch 28000, worst loss 0.009632 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:35:14,086 [INFO] Regularization: 9413.200195 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:35:14,086 [INFO] unfolding 0, single step 28001
2019-03-19 12:35:14,087 [INFO] Sum of grad norms of most recent batch: 2.779859
2019-03-19 12:35:14,087 [INFO] ---------------------------------
2019-03-19 12:35:35,786 [INFO] ---------------------------------
2019-03-19 12:35:35,787 [INFO] Summary:
2019-03-19 12:35:35,788 [INFO] Batch 29000, worst loss 0.007218 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:35:35,788 [INFO] Regularization: 9388.184570 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:35:35,788 [INFO] unfolding 0, single step 29001
2019-03-19 12:35:35,789 [INFO] Sum of grad norms of most recent batch: 2.544592
2019-03-19 12:35:35,789 [INFO] ---------------------------------
2019-03-19 12:35:57,352 [INFO] ---------------------------------
2019-03-19 12:35:57,353 [INFO] Summary:
2019-03-19 12:35:57,353 [INFO] Batch 30000, worst loss 0.010972 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:35:57,354 [INFO] Regularization: 9404.777344 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:35:57,354 [INFO] unfolding 0, single step 30001
2019-03-19 12:35:57,355 [INFO] Sum of grad norms of most recent batch: 0.704151
2019-03-19 12:35:57,355 [INFO] ---------------------------------
2019-03-19 12:36:03,112 [INFO] ---------------------------------
2019-03-19 12:36:03,113 [INFO] Evaluation:
2019-03-19 12:36:03,113 [INFO] Batch 30000, worst loss 0.002656 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:36:03,116 [INFO] ---------------------------------
2019-03-19 12:36:24,683 [INFO] ---------------------------------
2019-03-19 12:36:24,684 [INFO] Summary:
2019-03-19 12:36:24,685 [INFO] Batch 31000, worst loss 0.010502 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:36:24,685 [INFO] Regularization: 9429.371094 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:36:24,686 [INFO] unfolding 0, single step 31001
2019-03-19 12:36:24,686 [INFO] Sum of grad norms of most recent batch: 2.230846
2019-03-19 12:36:24,687 [INFO] ---------------------------------
2019-03-19 12:36:46,093 [INFO] ---------------------------------
2019-03-19 12:36:46,094 [INFO] Summary:
2019-03-19 12:36:46,095 [INFO] Batch 32000, worst loss 0.011262 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:36:46,095 [INFO] Regularization: 9415.776367 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:36:46,095 [INFO] unfolding 0, single step 32001
2019-03-19 12:36:46,096 [INFO] Sum of grad norms of most recent batch: 0.726235
2019-03-19 12:36:46,096 [INFO] ---------------------------------
2019-03-19 12:37:07,624 [INFO] ---------------------------------
2019-03-19 12:37:07,625 [INFO] Summary:
2019-03-19 12:37:07,626 [INFO] Batch 33000, worst loss 0.014610 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:37:07,626 [INFO] Regularization: 9406.884766 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:37:07,627 [INFO] unfolding 0, single step 33001
2019-03-19 12:37:07,628 [INFO] Sum of grad norms of most recent batch: 1.606755
2019-03-19 12:37:07,628 [INFO] ---------------------------------
2019-03-19 12:37:29,037 [INFO] ---------------------------------
2019-03-19 12:37:29,038 [INFO] Summary:
2019-03-19 12:37:29,039 [INFO] Batch 34000, worst loss 0.008956 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:37:29,039 [INFO] Regularization: 9374.069336 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:37:29,039 [INFO] unfolding 0, single step 34001
2019-03-19 12:37:29,040 [INFO] Sum of grad norms of most recent batch: 3.069276
2019-03-19 12:37:29,040 [INFO] ---------------------------------
2019-03-19 12:37:50,133 [INFO] ---------------------------------
2019-03-19 12:37:50,134 [INFO] Summary:
2019-03-19 12:37:50,135 [INFO] Batch 35000, worst loss 0.015800 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:37:50,135 [INFO] Regularization: 9375.441406 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:37:50,135 [INFO] unfolding 0, single step 35001
2019-03-19 12:37:50,136 [INFO] Sum of grad norms of most recent batch: 1.803007
2019-03-19 12:37:50,137 [INFO] ---------------------------------
2019-03-19 12:38:11,656 [INFO] ---------------------------------
2019-03-19 12:38:11,657 [INFO] Summary:
2019-03-19 12:38:11,658 [INFO] Batch 36000, worst loss 0.006382 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:38:11,658 [INFO] Regularization: 9367.714844 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:38:11,659 [INFO] unfolding 0, single step 36001
2019-03-19 12:38:11,659 [INFO] Sum of grad norms of most recent batch: 2.130024
2019-03-19 12:38:11,660 [INFO] ---------------------------------
2019-03-19 12:38:33,239 [INFO] ---------------------------------
2019-03-19 12:38:33,240 [INFO] Summary:
2019-03-19 12:38:33,241 [INFO] Batch 37000, worst loss 0.007429 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:38:33,242 [INFO] Regularization: 9351.880859 * 0.0000000000 = 0.0000000094 loss
2019-03-19 12:38:33,242 [INFO] unfolding 0, single step 37001
2019-03-19 12:38:33,243 [INFO] Sum of grad norms of most recent batch: 1.393812
2019-03-19 12:38:33,243 [INFO] ---------------------------------
2019-03-19 12:38:54,592 [INFO] ---------------------------------
2019-03-19 12:38:54,593 [INFO] Summary:
2019-03-19 12:38:54,594 [INFO] Batch 38000, worst loss 0.006269 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:38:54,594 [INFO] Regularization: 9336.587891 * 0.0000000000 = 0.0000000093 loss
2019-03-19 12:38:54,595 [INFO] unfolding 0, single step 38001
2019-03-19 12:38:54,595 [INFO] Sum of grad norms of most recent batch: 1.424117
2019-03-19 12:38:54,596 [INFO] ---------------------------------
2019-03-19 12:39:16,478 [INFO] ---------------------------------
2019-03-19 12:39:16,479 [INFO] Summary:
2019-03-19 12:39:16,479 [INFO] Batch 39000, worst loss 0.007943 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:39:16,480 [INFO] Regularization: 9323.120117 * 0.0000000000 = 0.0000000093 loss
2019-03-19 12:39:16,480 [INFO] unfolding 0, single step 39001
2019-03-19 12:39:16,481 [INFO] Sum of grad norms of most recent batch: 0.983805
2019-03-19 12:39:16,481 [INFO] ---------------------------------
2019-03-19 12:39:37,961 [INFO] ---------------------------------
2019-03-19 12:39:37,962 [INFO] Summary:
2019-03-19 12:39:37,963 [INFO] Batch 40000, worst loss 0.008421 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:39:37,964 [INFO] Regularization: 9307.950195 * 0.0000000000 = 0.0000000093 loss
2019-03-19 12:39:37,964 [INFO] unfolding 0, single step 40001
2019-03-19 12:39:37,965 [INFO] Sum of grad norms of most recent batch: 1.017432
2019-03-19 12:39:37,966 [INFO] ---------------------------------
2019-03-19 12:39:43,856 [INFO] ---------------------------------
2019-03-19 12:39:43,856 [INFO] Evaluation:
2019-03-19 12:39:43,857 [INFO] Batch 40000, worst loss 0.002286 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:39:43,858 [INFO] ---------------------------------
2019-03-19 12:40:05,449 [INFO] ---------------------------------
2019-03-19 12:40:05,450 [INFO] Summary:
2019-03-19 12:40:05,450 [INFO] Batch 41000, worst loss 0.007656 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:40:05,451 [INFO] Regularization: 9292.554688 * 0.0000000000 = 0.0000000093 loss
2019-03-19 12:40:05,451 [INFO] unfolding 0, single step 41001
2019-03-19 12:40:05,452 [INFO] Sum of grad norms of most recent batch: 0.502150
2019-03-19 12:40:05,453 [INFO] ---------------------------------
2019-03-19 12:40:26,850 [INFO] ---------------------------------
2019-03-19 12:40:26,851 [INFO] Summary:
2019-03-19 12:40:26,851 [INFO] Batch 42000, worst loss 0.008370 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:40:26,852 [INFO] Regularization: 9281.041992 * 0.0000000000 = 0.0000000093 loss
2019-03-19 12:40:26,852 [INFO] unfolding 0, single step 42001
2019-03-19 12:40:26,853 [INFO] Sum of grad norms of most recent batch: 1.273327
2019-03-19 12:40:26,853 [INFO] ---------------------------------
2019-03-19 12:40:48,684 [INFO] ---------------------------------
2019-03-19 12:40:48,685 [INFO] Summary:
2019-03-19 12:40:48,686 [INFO] Batch 43000, worst loss 0.005971 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:40:48,686 [INFO] Regularization: 9261.285156 * 0.0000000000 = 0.0000000093 loss
2019-03-19 12:40:48,686 [INFO] unfolding 0, single step 43001
2019-03-19 12:40:48,687 [INFO] Sum of grad norms of most recent batch: 1.151918
2019-03-19 12:40:48,687 [INFO] ---------------------------------
2019-03-19 12:41:10,178 [INFO] ---------------------------------
2019-03-19 12:41:10,179 [INFO] Summary:
2019-03-19 12:41:10,180 [INFO] Batch 44000, worst loss 0.009119 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:41:10,180 [INFO] Regularization: 9239.091797 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:41:10,181 [INFO] unfolding 0, single step 44001
2019-03-19 12:41:10,181 [INFO] Sum of grad norms of most recent batch: 1.495543
2019-03-19 12:41:10,182 [INFO] ---------------------------------
2019-03-19 12:41:31,870 [INFO] ---------------------------------
2019-03-19 12:41:31,871 [INFO] Summary:
2019-03-19 12:41:31,871 [INFO] Batch 45000, worst loss 0.009895 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:41:31,872 [INFO] Regularization: 9218.445312 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:41:31,872 [INFO] unfolding 0, single step 45001
2019-03-19 12:41:31,872 [INFO] Sum of grad norms of most recent batch: 0.697660
2019-03-19 12:41:31,873 [INFO] ---------------------------------
2019-03-19 12:41:53,499 [INFO] ---------------------------------
2019-03-19 12:41:53,500 [INFO] Summary:
2019-03-19 12:41:53,501 [INFO] Batch 46000, worst loss 0.006431 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:41:53,501 [INFO] Regularization: 9209.565430 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:41:53,501 [INFO] unfolding 0, single step 46001
2019-03-19 12:41:53,502 [INFO] Sum of grad norms of most recent batch: 3.316228
2019-03-19 12:41:53,502 [INFO] ---------------------------------
2019-03-19 12:42:15,418 [INFO] ---------------------------------
2019-03-19 12:42:15,420 [INFO] Summary:
2019-03-19 12:42:15,420 [INFO] Batch 47000, worst loss 0.005113 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:42:15,421 [INFO] Regularization: 9214.278320 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:42:15,421 [INFO] unfolding 0, single step 47001
2019-03-19 12:42:15,421 [INFO] Sum of grad norms of most recent batch: 1.039348
2019-03-19 12:42:15,422 [INFO] ---------------------------------
2019-03-19 12:42:36,962 [INFO] ---------------------------------
2019-03-19 12:42:36,963 [INFO] Summary:
2019-03-19 12:42:36,964 [INFO] Batch 48000, worst loss 0.006529 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:42:36,964 [INFO] Regularization: 9200.086914 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:42:36,965 [INFO] unfolding 0, single step 48001
2019-03-19 12:42:36,965 [INFO] Sum of grad norms of most recent batch: 0.841604
2019-03-19 12:42:36,966 [INFO] ---------------------------------
2019-03-19 12:42:58,364 [INFO] ---------------------------------
2019-03-19 12:42:58,365 [INFO] Summary:
2019-03-19 12:42:58,366 [INFO] Batch 49000, worst loss 0.010516 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:42:58,366 [INFO] Regularization: 9193.013672 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:42:58,367 [INFO] unfolding 0, single step 49001
2019-03-19 12:42:58,367 [INFO] Sum of grad norms of most recent batch: 0.618117
2019-03-19 12:42:58,368 [INFO] ---------------------------------
2019-03-19 12:43:20,013 [INFO] ---------------------------------
2019-03-19 12:43:20,014 [INFO] Summary:
2019-03-19 12:43:20,014 [INFO] Batch 50000, worst loss 0.006939 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:43:20,015 [INFO] Regularization: 9183.891602 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:43:20,015 [INFO] unfolding 0, single step 50001
2019-03-19 12:43:20,016 [INFO] Sum of grad norms of most recent batch: 0.508384
2019-03-19 12:43:20,016 [INFO] ---------------------------------
2019-03-19 12:43:25,747 [INFO] ---------------------------------
2019-03-19 12:43:25,748 [INFO] Evaluation:
2019-03-19 12:43:25,749 [INFO] Batch 50000, worst loss 0.003100 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:43:25,749 [INFO] ---------------------------------
2019-03-19 12:43:47,234 [INFO] ---------------------------------
2019-03-19 12:43:47,235 [INFO] Summary:
2019-03-19 12:43:47,236 [INFO] Batch 51000, worst loss 0.004446 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:43:47,236 [INFO] Regularization: 9172.965820 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:43:47,236 [INFO] unfolding 0, single step 51001
2019-03-19 12:43:47,237 [INFO] Sum of grad norms of most recent batch: 2.092403
2019-03-19 12:43:47,237 [INFO] ---------------------------------
2019-03-19 12:44:09,193 [INFO] ---------------------------------
2019-03-19 12:44:09,195 [INFO] Summary:
2019-03-19 12:44:09,195 [INFO] Batch 52000, worst loss 0.005433 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:44:09,196 [INFO] Regularization: 9163.147461 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:44:09,196 [INFO] unfolding 0, single step 52001
2019-03-19 12:44:09,197 [INFO] Sum of grad norms of most recent batch: 0.410001
2019-03-19 12:44:09,197 [INFO] ---------------------------------
2019-03-19 12:44:30,864 [INFO] ---------------------------------
2019-03-19 12:44:30,865 [INFO] Summary:
2019-03-19 12:44:30,867 [INFO] Batch 53000, worst loss 0.005375 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:44:30,867 [INFO] Regularization: 9155.283203 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:44:30,868 [INFO] unfolding 0, single step 53001
2019-03-19 12:44:30,868 [INFO] Sum of grad norms of most recent batch: 2.227814
2019-03-19 12:44:30,869 [INFO] ---------------------------------
2019-03-19 12:44:52,677 [INFO] ---------------------------------
2019-03-19 12:44:52,678 [INFO] Summary:
2019-03-19 12:44:52,678 [INFO] Batch 54000, worst loss 0.009016 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:44:52,679 [INFO] Regularization: 9155.614258 * 0.0000000000 = 0.0000000092 loss
2019-03-19 12:44:52,679 [INFO] unfolding 0, single step 54001
2019-03-19 12:44:52,680 [INFO] Sum of grad norms of most recent batch: 0.612458
2019-03-19 12:44:52,680 [INFO] ---------------------------------
2019-03-19 12:45:13,832 [INFO] ---------------------------------
2019-03-19 12:45:13,833 [INFO] Summary:
2019-03-19 12:45:13,833 [INFO] Batch 55000, worst loss 0.006076 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:45:13,834 [INFO] Regularization: 9144.491211 * 0.0000000000 = 0.0000000091 loss
2019-03-19 12:45:13,834 [INFO] unfolding 0, single step 55001
2019-03-19 12:45:13,835 [INFO] Sum of grad norms of most recent batch: 2.982673
2019-03-19 12:45:13,835 [INFO] ---------------------------------
2019-03-19 12:45:35,076 [INFO] ---------------------------------
2019-03-19 12:45:35,077 [INFO] Summary:
2019-03-19 12:45:35,078 [INFO] Batch 56000, worst loss 0.005289 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:45:35,079 [INFO] Regularization: 9134.393555 * 0.0000000000 = 0.0000000091 loss
2019-03-19 12:45:35,079 [INFO] unfolding 0, single step 56001
2019-03-19 12:45:35,080 [INFO] Sum of grad norms of most recent batch: 1.633344
2019-03-19 12:45:35,081 [INFO] ---------------------------------
2019-03-19 12:45:56,678 [INFO] ---------------------------------
2019-03-19 12:45:56,679 [INFO] Summary:
2019-03-19 12:45:56,680 [INFO] Batch 57000, worst loss 0.006243 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:45:56,681 [INFO] Regularization: 9137.915039 * 0.0000000000 = 0.0000000091 loss
2019-03-19 12:45:56,682 [INFO] unfolding 0, single step 57001
2019-03-19 12:45:56,683 [INFO] Sum of grad norms of most recent batch: 0.219081
2019-03-19 12:45:56,684 [INFO] ---------------------------------
2019-03-19 12:46:18,755 [INFO] ---------------------------------
2019-03-19 12:46:18,756 [INFO] Summary:
2019-03-19 12:46:18,756 [INFO] Batch 58000, worst loss 0.005735 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:46:18,757 [INFO] Regularization: 9138.925781 * 0.0000000000 = 0.0000000091 loss
2019-03-19 12:46:18,757 [INFO] unfolding 0, single step 58001
2019-03-19 12:46:18,758 [INFO] Sum of grad norms of most recent batch: 1.971462
2019-03-19 12:46:18,758 [INFO] ---------------------------------
2019-03-19 12:46:40,016 [INFO] ---------------------------------
2019-03-19 12:46:40,017 [INFO] Summary:
2019-03-19 12:46:40,018 [INFO] Batch 59000, worst loss 0.004070 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:46:40,018 [INFO] Regularization: 9128.092773 * 0.0000000000 = 0.0000000091 loss
2019-03-19 12:46:40,019 [INFO] unfolding 0, single step 59001
2019-03-19 12:46:40,019 [INFO] Sum of grad norms of most recent batch: 2.091359
2019-03-19 12:46:40,020 [INFO] ---------------------------------
2019-03-19 12:47:01,406 [INFO] ---------------------------------
2019-03-19 12:47:01,407 [INFO] Summary:
2019-03-19 12:47:01,407 [INFO] Batch 60000, worst loss 0.006629 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 12:47:01,408 [INFO] Regularization: 9132.668945 * 0.0000000000 = 0.0000000091 loss
2019-03-19 12:47:01,408 [INFO] unfolding 0, single step 60001
2019-03-19 12:47:01,409 [INFO] Sum of grad norms of most recent batch: 0.821900
2019-03-19 12:47:01,410 [INFO] ---------------------------------
2019-03-19 12:47:07,334 [INFO] ---------------------------------
2019-03-19 12:47:07,335 [INFO] Evaluation:
2019-03-19 12:47:07,336 [INFO] Batch 60000, worst loss 0.001771 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:47:07,337 [INFO] ---------------------------------
2019-03-19 12:47:29,022 [INFO] ---------------------------------
2019-03-19 12:47:29,023 [INFO] Summary:
2019-03-19 12:47:29,023 [INFO] Batch 61000, worst loss 0.006104 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 12:47:29,024 [INFO] Regularization: 9124.987305 * 0.0000000000 = 0.0000000091 loss
2019-03-19 12:47:29,024 [INFO] unfolding 0, single step 61001
2019-03-19 12:47:29,025 [INFO] Sum of grad norms of most recent batch: 0.930880
2019-03-19 12:47:29,025 [INFO] ---------------------------------
2019-03-19 12:47:50,508 [INFO] ---------------------------------
2019-03-19 12:47:50,509 [INFO] Summary:
2019-03-19 12:47:50,509 [INFO] Batch 62000, worst loss 0.003112 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 12:47:50,510 [INFO] Regularization: 9093.069336 * 0.0000000000 = 0.0000000091 loss
2019-03-19 12:47:50,510 [INFO] unfolding 0, single step 62001
2019-03-19 12:47:50,511 [INFO] Sum of grad norms of most recent batch: 0.176539
2019-03-19 12:47:50,511 [INFO] ---------------------------------
2019-03-19 12:48:11,964 [INFO] ---------------------------------
2019-03-19 12:48:11,965 [INFO] Summary:
2019-03-19 12:48:11,966 [INFO] Batch 63000, worst loss 0.001115 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 12:48:11,966 [INFO] Regularization: 9062.591797 * 0.0000000000 = 0.0000000091 loss
2019-03-19 12:48:11,966 [INFO] unfolding 0, single step 63001
2019-03-19 12:48:11,967 [INFO] Sum of grad norms of most recent batch: 0.181238
2019-03-19 12:48:11,968 [INFO] ---------------------------------
2019-03-19 12:48:33,486 [INFO] ---------------------------------
2019-03-19 12:48:33,487 [INFO] Summary:
2019-03-19 12:48:33,488 [INFO] Batch 64000, worst loss 0.000748 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 12:48:33,488 [INFO] Regularization: 9031.532227 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:48:33,488 [INFO] unfolding 0, single step 64001
2019-03-19 12:48:33,489 [INFO] Sum of grad norms of most recent batch: 0.931870
2019-03-19 12:48:33,490 [INFO] ---------------------------------
2019-03-19 12:48:55,235 [INFO] ---------------------------------
2019-03-19 12:48:55,236 [INFO] Summary:
2019-03-19 12:48:55,236 [INFO] Batch 65000, worst loss 0.000934 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 12:48:55,237 [INFO] Regularization: 9015.171875 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:48:55,237 [INFO] unfolding 0, single step 65001
2019-03-19 12:48:55,238 [INFO] Sum of grad norms of most recent batch: 0.155180
2019-03-19 12:48:55,238 [INFO] ---------------------------------
2019-03-19 12:49:16,912 [INFO] ---------------------------------
2019-03-19 12:49:16,913 [INFO] Summary:
2019-03-19 12:49:16,914 [INFO] Batch 66000, worst loss 0.001531 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 12:49:16,915 [INFO] Regularization: 9008.825195 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:49:16,915 [INFO] unfolding 0, single step 66001
2019-03-19 12:49:16,915 [INFO] Sum of grad norms of most recent batch: 0.170867
2019-03-19 12:49:16,916 [INFO] ---------------------------------
2019-03-19 12:49:38,378 [INFO] ---------------------------------
2019-03-19 12:49:38,379 [INFO] Summary:
2019-03-19 12:49:38,380 [INFO] Batch 67000, worst loss 0.000935 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 12:49:38,380 [INFO] Regularization: 8998.184570 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:49:38,380 [INFO] unfolding 0, single step 67001
2019-03-19 12:49:38,381 [INFO] Sum of grad norms of most recent batch: 0.269174
2019-03-19 12:49:38,381 [INFO] ---------------------------------
2019-03-19 12:49:59,956 [INFO] ---------------------------------
2019-03-19 12:49:59,957 [INFO] Summary:
2019-03-19 12:49:59,958 [INFO] Batch 68000, worst loss 0.001969 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 12:49:59,958 [INFO] Regularization: 8997.217773 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:49:59,958 [INFO] unfolding 0, single step 68001
2019-03-19 12:49:59,959 [INFO] Sum of grad norms of most recent batch: 1.328875
2019-03-19 12:49:59,959 [INFO] ---------------------------------
2019-03-19 12:50:21,518 [INFO] ---------------------------------
2019-03-19 12:50:21,519 [INFO] Summary:
2019-03-19 12:50:21,519 [INFO] Batch 69000, worst loss 0.001504 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 12:50:21,520 [INFO] Regularization: 8989.823242 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:50:21,520 [INFO] unfolding 0, single step 69001
2019-03-19 12:50:21,521 [INFO] Sum of grad norms of most recent batch: 0.323609
2019-03-19 12:50:21,521 [INFO] ---------------------------------
2019-03-19 12:50:42,818 [INFO] ---------------------------------
2019-03-19 12:50:42,819 [INFO] Summary:
2019-03-19 12:50:42,820 [INFO] Batch 70000, worst loss 0.000872 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000500 @est.-depth 2
2019-03-19 12:50:42,820 [INFO] Regularization: 8989.370117 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:50:42,821 [INFO] unfolding 0, single step 70001
2019-03-19 12:50:42,821 [INFO] Sum of grad norms of most recent batch: 0.520014
2019-03-19 12:50:42,822 [INFO] ---------------------------------
2019-03-19 12:50:48,604 [INFO] ---------------------------------
2019-03-19 12:50:48,604 [INFO] Evaluation:
2019-03-19 12:50:48,605 [INFO] Batch 70000, worst loss 0.000553 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:50:48,606 [INFO] ---------------------------------
2019-03-19 12:51:10,021 [INFO] ---------------------------------
2019-03-19 12:51:10,022 [INFO] Summary:
2019-03-19 12:51:10,023 [INFO] Batch 71000, worst loss 0.002300 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 12:51:10,023 [INFO] Regularization: 9017.130859 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:51:10,024 [INFO] unfolding 0, single step 71001
2019-03-19 12:51:10,024 [INFO] Sum of grad norms of most recent batch: 0.553721
2019-03-19 12:51:10,025 [INFO] ---------------------------------
2019-03-19 12:51:31,538 [INFO] ---------------------------------
2019-03-19 12:51:31,539 [INFO] Summary:
2019-03-19 12:51:31,539 [INFO] Batch 72000, worst loss 0.000582 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 12:51:31,540 [INFO] Regularization: 9005.827148 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:51:31,540 [INFO] unfolding 0, single step 72001
2019-03-19 12:51:31,541 [INFO] Sum of grad norms of most recent batch: 0.376196
2019-03-19 12:51:31,541 [INFO] ---------------------------------
2019-03-19 12:51:52,933 [INFO] ---------------------------------
2019-03-19 12:51:52,934 [INFO] Summary:
2019-03-19 12:51:52,935 [INFO] Batch 73000, worst loss 0.000319 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 12:51:52,936 [INFO] Regularization: 8991.220703 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:51:52,936 [INFO] unfolding 0, single step 73001
2019-03-19 12:51:52,937 [INFO] Sum of grad norms of most recent batch: 0.119062
2019-03-19 12:51:52,937 [INFO] ---------------------------------
2019-03-19 12:52:14,376 [INFO] ---------------------------------
2019-03-19 12:52:14,377 [INFO] Summary:
2019-03-19 12:52:14,378 [INFO] Batch 74000, worst loss 0.000443 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 12:52:14,378 [INFO] Regularization: 8977.912109 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:52:14,379 [INFO] unfolding 0, single step 74001
2019-03-19 12:52:14,380 [INFO] Sum of grad norms of most recent batch: 0.109959
2019-03-19 12:52:14,381 [INFO] ---------------------------------
2019-03-19 12:52:35,635 [INFO] ---------------------------------
2019-03-19 12:52:35,636 [INFO] Summary:
2019-03-19 12:52:35,637 [INFO] Batch 75000, worst loss 0.000348 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 12:52:35,637 [INFO] Regularization: 8962.041016 * 0.0000000000 = 0.0000000090 loss
2019-03-19 12:52:35,638 [INFO] unfolding 0, single step 75001
2019-03-19 12:52:35,638 [INFO] Sum of grad norms of most recent batch: 0.352696
2019-03-19 12:52:35,639 [INFO] ---------------------------------
2019-03-19 12:52:57,226 [INFO] ---------------------------------
2019-03-19 12:52:57,227 [INFO] Summary:
2019-03-19 12:52:57,228 [INFO] Batch 76000, worst loss 0.000202 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000250 @est.-depth 2
2019-03-19 12:52:57,228 [INFO] Regularization: 8948.876953 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:52:57,229 [INFO] unfolding 0, single step 76001
2019-03-19 12:52:57,229 [INFO] Sum of grad norms of most recent batch: 0.360048
2019-03-19 12:52:57,230 [INFO] ---------------------------------
2019-03-19 12:53:18,653 [INFO] ---------------------------------
2019-03-19 12:53:18,654 [INFO] Summary:
2019-03-19 12:53:18,655 [INFO] Batch 77000, worst loss 0.000212 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000202 @est.-depth 2
2019-03-19 12:53:18,655 [INFO] Regularization: 8940.527344 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:53:18,655 [INFO] unfolding 0, single step 77001
2019-03-19 12:53:18,656 [INFO] Sum of grad norms of most recent batch: 0.287693
2019-03-19 12:53:18,657 [INFO] ---------------------------------
2019-03-19 12:53:39,571 [INFO] ---------------------------------
2019-03-19 12:53:39,572 [INFO] Summary:
2019-03-19 12:53:39,573 [INFO] Batch 78000, worst loss 0.000204 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000202 @est.-depth 2
2019-03-19 12:53:39,574 [INFO] Regularization: 8931.965820 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:53:39,574 [INFO] unfolding 0, single step 78001
2019-03-19 12:53:39,574 [INFO] Sum of grad norms of most recent batch: 0.177726
2019-03-19 12:53:39,575 [INFO] ---------------------------------
2019-03-19 12:54:01,301 [INFO] ---------------------------------
2019-03-19 12:54:01,302 [INFO] Summary:
2019-03-19 12:54:01,303 [INFO] Batch 79000, worst loss 0.000189 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000202 @est.-depth 2
2019-03-19 12:54:01,304 [INFO] Regularization: 8923.002930 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:54:01,304 [INFO] unfolding 0, single step 79001
2019-03-19 12:54:01,304 [INFO] Sum of grad norms of most recent batch: 0.168489
2019-03-19 12:54:01,305 [INFO] ---------------------------------
2019-03-19 12:54:22,855 [INFO] ---------------------------------
2019-03-19 12:54:22,855 [INFO] Summary:
2019-03-19 12:54:22,856 [INFO] Batch 80000, worst loss 0.000071 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000189 @est.-depth 2
2019-03-19 12:54:22,856 [INFO] Regularization: 8914.277344 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:54:22,857 [INFO] unfolding 0, single step 80001
2019-03-19 12:54:22,857 [INFO] Sum of grad norms of most recent batch: 0.178074
2019-03-19 12:54:22,858 [INFO] ---------------------------------
2019-03-19 12:54:28,670 [INFO] ---------------------------------
2019-03-19 12:54:28,671 [INFO] Evaluation:
2019-03-19 12:54:28,672 [INFO] Batch 80000, worst loss 0.000030 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:54:28,672 [INFO] ---------------------------------
2019-03-19 12:54:49,602 [INFO] ---------------------------------
2019-03-19 12:54:49,603 [INFO] Summary:
2019-03-19 12:54:49,604 [INFO] Batch 81000, worst loss 0.000015 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000035 @est.-depth 2
2019-03-19 12:54:49,604 [INFO] Regularization: 8913.148438 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:54:49,605 [INFO] unfolding 0, single step 81001
2019-03-19 12:54:49,605 [INFO] Sum of grad norms of most recent batch: 0.053974
2019-03-19 12:54:49,606 [INFO] ---------------------------------
2019-03-19 12:55:10,879 [INFO] ---------------------------------
2019-03-19 12:55:10,880 [INFO] Summary:
2019-03-19 12:55:10,881 [INFO] Batch 82000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000015 @est.-depth 2
2019-03-19 12:55:10,881 [INFO] Regularization: 8913.081055 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:55:10,882 [INFO] unfolding 0, single step 82001
2019-03-19 12:55:10,882 [INFO] Sum of grad norms of most recent batch: 0.005269
2019-03-19 12:55:10,883 [INFO] ---------------------------------
2019-03-19 12:55:31,752 [INFO] ---------------------------------
2019-03-19 12:55:31,753 [INFO] Summary:
2019-03-19 12:55:31,754 [INFO] Batch 83000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 12:55:31,754 [INFO] Regularization: 8913.081055 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:55:31,754 [INFO] unfolding 0, single step 83001
2019-03-19 12:55:31,755 [INFO] Sum of grad norms of most recent batch: 0.012980
2019-03-19 12:55:31,755 [INFO] ---------------------------------
2019-03-19 12:55:52,497 [INFO] ---------------------------------
2019-03-19 12:55:52,497 [INFO] Summary:
2019-03-19 12:55:52,498 [INFO] Batch 84000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 12:55:52,499 [INFO] Regularization: 8913.081055 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:55:52,499 [INFO] unfolding 0, single step 84001
2019-03-19 12:55:52,499 [INFO] Sum of grad norms of most recent batch: 0.011769
2019-03-19 12:55:52,500 [INFO] ---------------------------------
2019-03-19 12:56:13,314 [INFO] ---------------------------------
2019-03-19 12:56:13,315 [INFO] Summary:
2019-03-19 12:56:13,315 [INFO] Batch 85000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 12:56:13,316 [INFO] Regularization: 8913.080078 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:56:13,316 [INFO] unfolding 0, single step 85001
2019-03-19 12:56:13,317 [INFO] Sum of grad norms of most recent batch: 0.006219
2019-03-19 12:56:13,317 [INFO] ---------------------------------
2019-03-19 12:56:34,132 [INFO] ---------------------------------
2019-03-19 12:56:34,133 [INFO] Summary:
2019-03-19 12:56:34,134 [INFO] Batch 86000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 12:56:34,135 [INFO] Regularization: 8913.080078 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:56:34,136 [INFO] unfolding 0, single step 86001
2019-03-19 12:56:34,137 [INFO] Sum of grad norms of most recent batch: 0.002116
2019-03-19 12:56:34,138 [INFO] ---------------------------------
2019-03-19 12:56:55,098 [INFO] ---------------------------------
2019-03-19 12:56:55,099 [INFO] Summary:
2019-03-19 12:56:55,100 [INFO] Batch 87000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 12:56:55,100 [INFO] Regularization: 8913.078125 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:56:55,101 [INFO] unfolding 0, single step 87001
2019-03-19 12:56:55,102 [INFO] Sum of grad norms of most recent batch: 0.004653
2019-03-19 12:56:55,103 [INFO] ---------------------------------
2019-03-19 12:57:16,085 [INFO] ---------------------------------
2019-03-19 12:57:16,086 [INFO] Summary:
2019-03-19 12:57:16,086 [INFO] Batch 88000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 12:57:16,087 [INFO] Regularization: 8913.075195 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:57:16,087 [INFO] unfolding 0, single step 88001
2019-03-19 12:57:16,088 [INFO] Sum of grad norms of most recent batch: 0.005689
2019-03-19 12:57:16,089 [INFO] ---------------------------------
2019-03-19 12:57:36,857 [INFO] ---------------------------------
2019-03-19 12:57:36,858 [INFO] Summary:
2019-03-19 12:57:36,859 [INFO] Batch 89000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 12:57:36,859 [INFO] Regularization: 8913.074219 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:57:36,859 [INFO] unfolding 0, single step 89001
2019-03-19 12:57:36,860 [INFO] Sum of grad norms of most recent batch: 0.004412
2019-03-19 12:57:36,861 [INFO] ---------------------------------
2019-03-19 12:57:57,858 [INFO] ---------------------------------
2019-03-19 12:57:57,858 [INFO] Summary:
2019-03-19 12:57:57,859 [INFO] Batch 90000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000001 @est.-depth 2
2019-03-19 12:57:57,860 [INFO] Regularization: 8913.078125 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:57:57,860 [INFO] unfolding 0, single step 90001
2019-03-19 12:57:57,861 [INFO] Sum of grad norms of most recent batch: 0.013057
2019-03-19 12:57:57,861 [INFO] ---------------------------------
2019-03-19 12:58:03,708 [INFO] ---------------------------------
2019-03-19 12:58:03,709 [INFO] Evaluation:
2019-03-19 12:58:03,710 [INFO] Batch 90000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 12:58:03,714 [INFO] ---------------------------------
2019-03-19 12:58:24,641 [INFO] ---------------------------------
2019-03-19 12:58:24,641 [INFO] Summary:
2019-03-19 12:58:24,642 [INFO] Batch 91000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:58:24,643 [INFO] Regularization: 8913.070312 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:58:24,643 [INFO] unfolding 0, single step 91001
2019-03-19 12:58:24,644 [INFO] Sum of grad norms of most recent batch: 0.002083
2019-03-19 12:58:24,645 [INFO] ---------------------------------
2019-03-19 12:58:45,289 [INFO] ---------------------------------
2019-03-19 12:58:45,290 [INFO] Summary:
2019-03-19 12:58:45,291 [INFO] Batch 92000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:58:45,291 [INFO] Regularization: 8913.074219 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:58:45,292 [INFO] unfolding 0, single step 92001
2019-03-19 12:58:45,292 [INFO] Sum of grad norms of most recent batch: 0.004091
2019-03-19 12:58:45,293 [INFO] ---------------------------------
2019-03-19 12:59:05,941 [INFO] ---------------------------------
2019-03-19 12:59:05,942 [INFO] Summary:
2019-03-19 12:59:05,943 [INFO] Batch 93000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:59:05,944 [INFO] Regularization: 8913.068359 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:59:05,944 [INFO] unfolding 0, single step 93001
2019-03-19 12:59:05,945 [INFO] Sum of grad norms of most recent batch: 0.012054
2019-03-19 12:59:05,945 [INFO] ---------------------------------
2019-03-19 12:59:26,878 [INFO] ---------------------------------
2019-03-19 12:59:26,879 [INFO] Summary:
2019-03-19 12:59:26,879 [INFO] Batch 94000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:59:26,880 [INFO] Regularization: 8913.070312 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:59:26,880 [INFO] unfolding 0, single step 94001
2019-03-19 12:59:26,881 [INFO] Sum of grad norms of most recent batch: 0.014143
2019-03-19 12:59:26,881 [INFO] ---------------------------------
2019-03-19 12:59:47,932 [INFO] ---------------------------------
2019-03-19 12:59:47,933 [INFO] Summary:
2019-03-19 12:59:47,934 [INFO] Batch 95000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 12:59:47,935 [INFO] Regularization: 8913.068359 * 0.0000000000 = 0.0000000089 loss
2019-03-19 12:59:47,936 [INFO] unfolding 0, single step 95001
2019-03-19 12:59:47,937 [INFO] Sum of grad norms of most recent batch: 0.008731
2019-03-19 12:59:47,937 [INFO] ---------------------------------
2019-03-19 13:00:08,883 [INFO] ---------------------------------
2019-03-19 13:00:08,884 [INFO] Summary:
2019-03-19 13:00:08,885 [INFO] Batch 96000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:00:08,886 [INFO] Regularization: 8913.068359 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:00:08,886 [INFO] unfolding 0, single step 96001
2019-03-19 13:00:08,887 [INFO] Sum of grad norms of most recent batch: 0.003897
2019-03-19 13:00:08,887 [INFO] ---------------------------------
2019-03-19 13:00:29,908 [INFO] ---------------------------------
2019-03-19 13:00:29,909 [INFO] Summary:
2019-03-19 13:00:29,910 [INFO] Batch 97000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:00:29,910 [INFO] Regularization: 8913.069336 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:00:29,911 [INFO] unfolding 0, single step 97001
2019-03-19 13:00:29,911 [INFO] Sum of grad norms of most recent batch: 0.012128
2019-03-19 13:00:29,912 [INFO] ---------------------------------
2019-03-19 13:00:51,110 [INFO] ---------------------------------
2019-03-19 13:00:51,111 [INFO] Summary:
2019-03-19 13:00:51,111 [INFO] Batch 98000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:00:51,112 [INFO] Regularization: 8913.066406 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:00:51,112 [INFO] unfolding 0, single step 98001
2019-03-19 13:00:51,113 [INFO] Sum of grad norms of most recent batch: 0.000459
2019-03-19 13:00:51,113 [INFO] ---------------------------------
2019-03-19 13:01:12,091 [INFO] ---------------------------------
2019-03-19 13:01:12,092 [INFO] Summary:
2019-03-19 13:01:12,093 [INFO] Batch 99000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:01:12,093 [INFO] Regularization: 8913.066406 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:01:12,094 [INFO] unfolding 0, single step 99001
2019-03-19 13:01:12,094 [INFO] Sum of grad norms of most recent batch: 0.004185
2019-03-19 13:01:12,095 [INFO] ---------------------------------
2019-03-19 13:01:33,104 [INFO] ---------------------------------
2019-03-19 13:01:33,105 [INFO] Summary:
2019-03-19 13:01:33,106 [INFO] Batch 100000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:01:33,106 [INFO] Regularization: 8913.062500 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:01:33,107 [INFO] unfolding 0, single step 100001
2019-03-19 13:01:33,107 [INFO] Sum of grad norms of most recent batch: 0.003841
2019-03-19 13:01:33,108 [INFO] ---------------------------------
2019-03-19 13:01:39,028 [INFO] ---------------------------------
2019-03-19 13:01:39,030 [INFO] Evaluation:
2019-03-19 13:01:39,030 [INFO] Batch 100000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 13:01:39,031 [INFO] ---------------------------------
2019-03-19 13:02:00,370 [INFO] ---------------------------------
2019-03-19 13:02:00,371 [INFO] Summary:
2019-03-19 13:02:00,371 [INFO] Batch 101000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:02:00,372 [INFO] Regularization: 8913.061523 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:02:00,372 [INFO] unfolding 0, single step 101001
2019-03-19 13:02:00,373 [INFO] Sum of grad norms of most recent batch: 0.007619
2019-03-19 13:02:00,373 [INFO] ---------------------------------
2019-03-19 13:02:20,910 [INFO] ---------------------------------
2019-03-19 13:02:20,911 [INFO] Summary:
2019-03-19 13:02:20,911 [INFO] Batch 102000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:02:20,912 [INFO] Regularization: 8913.061523 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:02:20,912 [INFO] unfolding 0, single step 102001
2019-03-19 13:02:20,913 [INFO] Sum of grad norms of most recent batch: 0.003385
2019-03-19 13:02:20,913 [INFO] ---------------------------------
2019-03-19 13:02:42,248 [INFO] ---------------------------------
2019-03-19 13:02:42,249 [INFO] Summary:
2019-03-19 13:02:42,250 [INFO] Batch 103000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:02:42,250 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:02:42,251 [INFO] unfolding 0, single step 103001
2019-03-19 13:02:42,251 [INFO] Sum of grad norms of most recent batch: 0.000936
2019-03-19 13:02:42,252 [INFO] ---------------------------------
2019-03-19 13:03:03,349 [INFO] ---------------------------------
2019-03-19 13:03:03,350 [INFO] Summary:
2019-03-19 13:03:03,351 [INFO] Batch 104000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:03:03,351 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:03:03,352 [INFO] unfolding 0, single step 104001
2019-03-19 13:03:03,352 [INFO] Sum of grad norms of most recent batch: 0.020558
2019-03-19 13:03:03,353 [INFO] ---------------------------------
2019-03-19 13:03:23,987 [INFO] ---------------------------------
2019-03-19 13:03:23,987 [INFO] Summary:
2019-03-19 13:03:23,988 [INFO] Batch 105000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:03:23,988 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:03:23,989 [INFO] unfolding 0, single step 105001
2019-03-19 13:03:23,989 [INFO] Sum of grad norms of most recent batch: 0.002098
2019-03-19 13:03:23,990 [INFO] ---------------------------------
2019-03-19 13:03:44,994 [INFO] ---------------------------------
2019-03-19 13:03:44,995 [INFO] Summary:
2019-03-19 13:03:44,996 [INFO] Batch 106000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:03:44,996 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:03:44,997 [INFO] unfolding 0, single step 106001
2019-03-19 13:03:44,998 [INFO] Sum of grad norms of most recent batch: 0.004903
2019-03-19 13:03:44,998 [INFO] ---------------------------------
2019-03-19 13:04:05,876 [INFO] ---------------------------------
2019-03-19 13:04:05,877 [INFO] Summary:
2019-03-19 13:04:05,877 [INFO] Batch 107000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:04:05,878 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:04:05,878 [INFO] unfolding 0, single step 107001
2019-03-19 13:04:05,879 [INFO] Sum of grad norms of most recent batch: 0.020388
2019-03-19 13:04:05,879 [INFO] ---------------------------------
2019-03-19 13:04:27,471 [INFO] ---------------------------------
2019-03-19 13:04:27,472 [INFO] Summary:
2019-03-19 13:04:27,472 [INFO] Batch 108000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:04:27,473 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:04:27,473 [INFO] unfolding 0, single step 108001
2019-03-19 13:04:27,474 [INFO] Sum of grad norms of most recent batch: 0.007401
2019-03-19 13:04:27,474 [INFO] ---------------------------------
2019-03-19 13:04:48,422 [INFO] ---------------------------------
2019-03-19 13:04:48,423 [INFO] Summary:
2019-03-19 13:04:48,423 [INFO] Batch 109000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:04:48,424 [INFO] Regularization: 8913.056641 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:04:48,424 [INFO] unfolding 0, single step 109001
2019-03-19 13:04:48,425 [INFO] Sum of grad norms of most recent batch: 0.001276
2019-03-19 13:04:48,425 [INFO] ---------------------------------
2019-03-19 13:05:09,409 [INFO] ---------------------------------
2019-03-19 13:05:09,410 [INFO] Summary:
2019-03-19 13:05:09,411 [INFO] Batch 110000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:05:09,411 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:05:09,411 [INFO] unfolding 0, single step 110001
2019-03-19 13:05:09,412 [INFO] Sum of grad norms of most recent batch: 0.012938
2019-03-19 13:05:09,413 [INFO] ---------------------------------
2019-03-19 13:05:15,219 [INFO] ---------------------------------
2019-03-19 13:05:15,221 [INFO] Evaluation:
2019-03-19 13:05:15,221 [INFO] Batch 110000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 13:05:15,222 [INFO] ---------------------------------
2019-03-19 13:05:36,329 [INFO] ---------------------------------
2019-03-19 13:05:36,330 [INFO] Summary:
2019-03-19 13:05:36,330 [INFO] Batch 111000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:05:36,331 [INFO] Regularization: 8913.058594 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:05:36,331 [INFO] unfolding 0, single step 111001
2019-03-19 13:05:36,332 [INFO] Sum of grad norms of most recent batch: 0.001368
2019-03-19 13:05:36,332 [INFO] ---------------------------------
2019-03-19 13:05:57,296 [INFO] ---------------------------------
2019-03-19 13:05:57,297 [INFO] Summary:
2019-03-19 13:05:57,297 [INFO] Batch 112000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:05:57,298 [INFO] Regularization: 8913.058594 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:05:57,298 [INFO] unfolding 0, single step 112001
2019-03-19 13:05:57,299 [INFO] Sum of grad norms of most recent batch: 0.001600
2019-03-19 13:05:57,299 [INFO] ---------------------------------
2019-03-19 13:06:18,056 [INFO] ---------------------------------
2019-03-19 13:06:18,057 [INFO] Summary:
2019-03-19 13:06:18,058 [INFO] Batch 113000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:06:18,058 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:06:18,058 [INFO] unfolding 0, single step 113001
2019-03-19 13:06:18,059 [INFO] Sum of grad norms of most recent batch: 0.021736
2019-03-19 13:06:18,059 [INFO] ---------------------------------
2019-03-19 13:06:39,183 [INFO] ---------------------------------
2019-03-19 13:06:39,184 [INFO] Summary:
2019-03-19 13:06:39,184 [INFO] Batch 114000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:06:39,185 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:06:39,185 [INFO] unfolding 0, single step 114001
2019-03-19 13:06:39,186 [INFO] Sum of grad norms of most recent batch: 0.003866
2019-03-19 13:06:39,186 [INFO] ---------------------------------
2019-03-19 13:06:59,927 [INFO] ---------------------------------
2019-03-19 13:06:59,928 [INFO] Summary:
2019-03-19 13:06:59,928 [INFO] Batch 115000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:06:59,929 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:06:59,929 [INFO] unfolding 0, single step 115001
2019-03-19 13:06:59,930 [INFO] Sum of grad norms of most recent batch: 0.003873
2019-03-19 13:06:59,930 [INFO] ---------------------------------
2019-03-19 13:07:21,129 [INFO] ---------------------------------
2019-03-19 13:07:21,130 [INFO] Summary:
2019-03-19 13:07:21,131 [INFO] Batch 116000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:07:21,132 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:07:21,132 [INFO] unfolding 0, single step 116001
2019-03-19 13:07:21,133 [INFO] Sum of grad norms of most recent batch: 0.016078
2019-03-19 13:07:21,133 [INFO] ---------------------------------
2019-03-19 13:07:41,911 [INFO] ---------------------------------
2019-03-19 13:07:41,912 [INFO] Summary:
2019-03-19 13:07:41,913 [INFO] Batch 117000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:07:41,913 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:07:41,914 [INFO] unfolding 0, single step 117001
2019-03-19 13:07:41,914 [INFO] Sum of grad norms of most recent batch: 0.001629
2019-03-19 13:07:41,915 [INFO] ---------------------------------
2019-03-19 13:08:02,829 [INFO] ---------------------------------
2019-03-19 13:08:02,830 [INFO] Summary:
2019-03-19 13:08:02,830 [INFO] Batch 118000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:08:02,832 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:08:02,832 [INFO] unfolding 0, single step 118001
2019-03-19 13:08:02,833 [INFO] Sum of grad norms of most recent batch: 0.006307
2019-03-19 13:08:02,834 [INFO] ---------------------------------
2019-03-19 13:08:23,980 [INFO] ---------------------------------
2019-03-19 13:08:23,981 [INFO] Summary:
2019-03-19 13:08:23,982 [INFO] Batch 119000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:08:23,982 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:08:23,983 [INFO] unfolding 0, single step 119001
2019-03-19 13:08:23,983 [INFO] Sum of grad norms of most recent batch: 0.011235
2019-03-19 13:08:23,984 [INFO] ---------------------------------
2019-03-19 13:08:44,705 [INFO] ---------------------------------
2019-03-19 13:08:44,706 [INFO] Summary:
2019-03-19 13:08:44,707 [INFO] Batch 120000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:08:44,707 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:08:44,707 [INFO] unfolding 0, single step 120001
2019-03-19 13:08:44,708 [INFO] Sum of grad norms of most recent batch: 0.000748
2019-03-19 13:08:44,708 [INFO] ---------------------------------
2019-03-19 13:08:50,440 [INFO] ---------------------------------
2019-03-19 13:08:50,441 [INFO] Evaluation:
2019-03-19 13:08:50,442 [INFO] Batch 120000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 13:08:50,443 [INFO] ---------------------------------
2019-03-19 13:09:11,809 [INFO] ---------------------------------
2019-03-19 13:09:11,810 [INFO] Summary:
2019-03-19 13:09:11,811 [INFO] Batch 121000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:09:11,811 [INFO] Regularization: 8913.058594 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:09:11,812 [INFO] unfolding 0, single step 121001
2019-03-19 13:09:11,812 [INFO] Sum of grad norms of most recent batch: 0.004912
2019-03-19 13:09:11,813 [INFO] ---------------------------------
2019-03-19 13:09:32,776 [INFO] ---------------------------------
2019-03-19 13:09:32,776 [INFO] Summary:
2019-03-19 13:09:32,777 [INFO] Batch 122000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:09:32,777 [INFO] Regularization: 8913.058594 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:09:32,778 [INFO] unfolding 0, single step 122001
2019-03-19 13:09:32,778 [INFO] Sum of grad norms of most recent batch: 0.002826
2019-03-19 13:09:32,779 [INFO] ---------------------------------
2019-03-19 13:09:53,875 [INFO] ---------------------------------
2019-03-19 13:09:53,876 [INFO] Summary:
2019-03-19 13:09:53,876 [INFO] Batch 123000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:09:53,877 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:09:53,877 [INFO] unfolding 0, single step 123001
2019-03-19 13:09:53,878 [INFO] Sum of grad norms of most recent batch: 0.003355
2019-03-19 13:09:53,878 [INFO] ---------------------------------
2019-03-19 13:10:15,038 [INFO] ---------------------------------
2019-03-19 13:10:15,039 [INFO] Summary:
2019-03-19 13:10:15,040 [INFO] Batch 124000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:10:15,040 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:10:15,041 [INFO] unfolding 0, single step 124001
2019-03-19 13:10:15,041 [INFO] Sum of grad norms of most recent batch: 0.011588
2019-03-19 13:10:15,042 [INFO] ---------------------------------
2019-03-19 13:10:36,037 [INFO] ---------------------------------
2019-03-19 13:10:36,038 [INFO] Summary:
2019-03-19 13:10:36,039 [INFO] Batch 125000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:10:36,039 [INFO] Regularization: 8913.058594 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:10:36,040 [INFO] unfolding 0, single step 125001
2019-03-19 13:10:36,040 [INFO] Sum of grad norms of most recent batch: 0.014427
2019-03-19 13:10:36,041 [INFO] ---------------------------------
2019-03-19 13:10:56,851 [INFO] ---------------------------------
2019-03-19 13:10:56,852 [INFO] Summary:
2019-03-19 13:10:56,852 [INFO] Batch 126000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:10:56,853 [INFO] Regularization: 8913.058594 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:10:56,853 [INFO] unfolding 0, single step 126001
2019-03-19 13:10:56,853 [INFO] Sum of grad norms of most recent batch: 0.001451
2019-03-19 13:10:56,854 [INFO] ---------------------------------
2019-03-19 13:11:17,891 [INFO] ---------------------------------
2019-03-19 13:11:17,892 [INFO] Summary:
2019-03-19 13:11:17,892 [INFO] Batch 127000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:11:17,893 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:11:17,893 [INFO] unfolding 0, single step 127001
2019-03-19 13:11:17,894 [INFO] Sum of grad norms of most recent batch: 0.003973
2019-03-19 13:11:17,895 [INFO] ---------------------------------
2019-03-19 13:11:39,281 [INFO] ---------------------------------
2019-03-19 13:11:39,282 [INFO] Summary:
2019-03-19 13:11:39,282 [INFO] Batch 128000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:11:39,283 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:11:39,283 [INFO] unfolding 0, single step 128001
2019-03-19 13:11:39,284 [INFO] Sum of grad norms of most recent batch: 0.002132
2019-03-19 13:11:39,284 [INFO] ---------------------------------
2019-03-19 13:12:00,478 [INFO] ---------------------------------
2019-03-19 13:12:00,479 [INFO] Summary:
2019-03-19 13:12:00,480 [INFO] Batch 129000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:12:00,481 [INFO] Regularization: 8913.058594 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:12:00,481 [INFO] unfolding 0, single step 129001
2019-03-19 13:12:00,482 [INFO] Sum of grad norms of most recent batch: 0.001046
2019-03-19 13:12:00,482 [INFO] ---------------------------------
2019-03-19 13:12:21,411 [INFO] ---------------------------------
2019-03-19 13:12:21,412 [INFO] Summary:
2019-03-19 13:12:21,413 [INFO] Batch 130000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:12:21,414 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:12:21,414 [INFO] unfolding 0, single step 130001
2019-03-19 13:12:21,414 [INFO] Sum of grad norms of most recent batch: 0.004496
2019-03-19 13:12:21,415 [INFO] ---------------------------------
2019-03-19 13:12:27,255 [INFO] ---------------------------------
2019-03-19 13:12:27,256 [INFO] Evaluation:
2019-03-19 13:12:27,257 [INFO] Batch 130000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 13:12:27,258 [INFO] ---------------------------------
2019-03-19 13:12:48,208 [INFO] ---------------------------------
2019-03-19 13:12:48,209 [INFO] Summary:
2019-03-19 13:12:48,210 [INFO] Batch 131000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:12:48,212 [INFO] Regularization: 8913.056641 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:12:48,212 [INFO] unfolding 0, single step 131001
2019-03-19 13:12:48,214 [INFO] Sum of grad norms of most recent batch: 0.000816
2019-03-19 13:12:48,214 [INFO] ---------------------------------
2019-03-19 13:13:09,254 [INFO] ---------------------------------
2019-03-19 13:13:09,255 [INFO] Summary:
2019-03-19 13:13:09,255 [INFO] Batch 132000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:13:09,256 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:13:09,256 [INFO] unfolding 0, single step 132001
2019-03-19 13:13:09,257 [INFO] Sum of grad norms of most recent batch: 0.006424
2019-03-19 13:13:09,257 [INFO] ---------------------------------
2019-03-19 13:13:29,944 [INFO] ---------------------------------
2019-03-19 13:13:29,944 [INFO] Summary:
2019-03-19 13:13:29,945 [INFO] Batch 133000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:13:29,945 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:13:29,946 [INFO] unfolding 0, single step 133001
2019-03-19 13:13:29,947 [INFO] Sum of grad norms of most recent batch: 0.010440
2019-03-19 13:13:29,947 [INFO] ---------------------------------
2019-03-19 13:13:51,035 [INFO] ---------------------------------
2019-03-19 13:13:51,036 [INFO] Summary:
2019-03-19 13:13:51,036 [INFO] Batch 134000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:13:51,037 [INFO] Regularization: 8913.057617 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:13:51,037 [INFO] unfolding 0, single step 134001
2019-03-19 13:13:51,038 [INFO] Sum of grad norms of most recent batch: 0.004540
2019-03-19 13:13:51,039 [INFO] ---------------------------------
2019-03-19 13:14:11,998 [INFO] ---------------------------------
2019-03-19 13:14:11,999 [INFO] Summary:
2019-03-19 13:14:11,999 [INFO] Batch 135000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:14:12,000 [INFO] Regularization: 8913.058594 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:14:12,000 [INFO] unfolding 0, single step 135001
2019-03-19 13:14:12,001 [INFO] Sum of grad norms of most recent batch: 0.005051
2019-03-19 13:14:12,001 [INFO] ---------------------------------
2019-03-19 13:14:32,949 [INFO] ---------------------------------
2019-03-19 13:14:32,950 [INFO] Summary:
2019-03-19 13:14:32,951 [INFO] Batch 136000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:14:32,951 [INFO] Regularization: 8913.058594 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:14:32,951 [INFO] unfolding 0, single step 136001
2019-03-19 13:14:32,952 [INFO] Sum of grad norms of most recent batch: 0.000469
2019-03-19 13:14:32,952 [INFO] ---------------------------------
2019-03-19 13:14:54,014 [INFO] ---------------------------------
2019-03-19 13:14:54,015 [INFO] Summary:
2019-03-19 13:14:54,016 [INFO] Batch 137000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:14:54,016 [INFO] Regularization: 8913.058594 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:14:54,016 [INFO] unfolding 0, single step 137001
2019-03-19 13:14:54,017 [INFO] Sum of grad norms of most recent batch: 0.002331
2019-03-19 13:14:54,018 [INFO] ---------------------------------
2019-03-19 13:15:15,003 [INFO] ---------------------------------
2019-03-19 13:15:15,004 [INFO] Summary:
2019-03-19 13:15:15,005 [INFO] Batch 138000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:15:15,005 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:15:15,006 [INFO] unfolding 0, single step 138001
2019-03-19 13:15:15,006 [INFO] Sum of grad norms of most recent batch: 0.001184
2019-03-19 13:15:15,007 [INFO] ---------------------------------
2019-03-19 13:15:36,353 [INFO] ---------------------------------
2019-03-19 13:15:36,354 [INFO] Summary:
2019-03-19 13:15:36,354 [INFO] Batch 139000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:15:36,355 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:15:36,355 [INFO] unfolding 0, single step 139001
2019-03-19 13:15:36,356 [INFO] Sum of grad norms of most recent batch: 0.021861
2019-03-19 13:15:36,356 [INFO] ---------------------------------
2019-03-19 13:15:57,800 [INFO] ---------------------------------
2019-03-19 13:15:57,801 [INFO] Summary:
2019-03-19 13:15:57,802 [INFO] Batch 140000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:15:57,802 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:15:57,803 [INFO] unfolding 0, single step 140001
2019-03-19 13:15:57,804 [INFO] Sum of grad norms of most recent batch: 0.003902
2019-03-19 13:15:57,804 [INFO] ---------------------------------
2019-03-19 13:16:03,639 [INFO] ---------------------------------
2019-03-19 13:16:03,640 [INFO] Evaluation:
2019-03-19 13:16:03,641 [INFO] Batch 140000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 13:16:03,642 [INFO] ---------------------------------
2019-03-19 13:16:24,911 [INFO] ---------------------------------
2019-03-19 13:16:24,912 [INFO] Summary:
2019-03-19 13:16:24,912 [INFO] Batch 141000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:16:24,913 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:16:24,913 [INFO] unfolding 0, single step 141001
2019-03-19 13:16:24,914 [INFO] Sum of grad norms of most recent batch: 0.013196
2019-03-19 13:16:24,914 [INFO] ---------------------------------
2019-03-19 13:16:45,843 [INFO] ---------------------------------
2019-03-19 13:16:45,844 [INFO] Summary:
2019-03-19 13:16:45,844 [INFO] Batch 142000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:16:45,845 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:16:45,845 [INFO] unfolding 0, single step 142001
2019-03-19 13:16:45,846 [INFO] Sum of grad norms of most recent batch: 0.003185
2019-03-19 13:16:45,846 [INFO] ---------------------------------
2019-03-19 13:17:06,702 [INFO] ---------------------------------
2019-03-19 13:17:06,703 [INFO] Summary:
2019-03-19 13:17:06,703 [INFO] Batch 143000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:17:06,704 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:17:06,704 [INFO] unfolding 0, single step 143001
2019-03-19 13:17:06,705 [INFO] Sum of grad norms of most recent batch: 0.009206
2019-03-19 13:17:06,705 [INFO] ---------------------------------
2019-03-19 13:17:27,740 [INFO] ---------------------------------
2019-03-19 13:17:27,741 [INFO] Summary:
2019-03-19 13:17:27,742 [INFO] Batch 144000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:17:27,742 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:17:27,743 [INFO] unfolding 0, single step 144001
2019-03-19 13:17:27,743 [INFO] Sum of grad norms of most recent batch: 0.001352
2019-03-19 13:17:27,744 [INFO] ---------------------------------
2019-03-19 13:17:48,753 [INFO] ---------------------------------
2019-03-19 13:17:48,754 [INFO] Summary:
2019-03-19 13:17:48,754 [INFO] Batch 145000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:17:48,755 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:17:48,755 [INFO] unfolding 0, single step 145001
2019-03-19 13:17:48,756 [INFO] Sum of grad norms of most recent batch: 0.000868
2019-03-19 13:17:48,756 [INFO] ---------------------------------
2019-03-19 13:18:09,871 [INFO] ---------------------------------
2019-03-19 13:18:09,872 [INFO] Summary:
2019-03-19 13:18:09,873 [INFO] Batch 146000, worst loss 0.000000 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:18:09,874 [INFO] Regularization: 8913.059570 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:18:09,874 [INFO] unfolding 0, single step 146001
2019-03-19 13:18:09,875 [INFO] Sum of grad norms of most recent batch: 0.002742
2019-03-19 13:18:09,875 [INFO] ---------------------------------
2019-03-19 13:18:15,694 [INFO] ---------------------------------
2019-03-19 13:18:15,695 [INFO] Evaluation:
2019-03-19 13:18:15,696 [INFO] Batch 146000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 13:18:15,697 [INFO] ---------------------------------
2019-03-19 13:18:36,461 [INFO] ---------------------------------
2019-03-19 13:18:36,462 [INFO] Summary:
2019-03-19 13:18:36,463 [INFO] Batch 147000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:18:36,463 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:18:36,464 [INFO] unfolding 0, single step 147001
2019-03-19 13:18:36,464 [INFO] Sum of grad norms of most recent batch: 0.001579
2019-03-19 13:18:36,465 [INFO] ---------------------------------
2019-03-19 13:18:57,374 [INFO] ---------------------------------
2019-03-19 13:18:57,374 [INFO] Summary:
2019-03-19 13:18:57,375 [INFO] Batch 148000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:18:57,375 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:18:57,376 [INFO] unfolding 0, single step 148001
2019-03-19 13:18:57,376 [INFO] Sum of grad norms of most recent batch: 0.007719
2019-03-19 13:18:57,377 [INFO] ---------------------------------
2019-03-19 13:19:18,481 [INFO] ---------------------------------
2019-03-19 13:19:18,482 [INFO] Summary:
2019-03-19 13:19:18,483 [INFO] Batch 149000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:19:18,483 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:19:18,483 [INFO] unfolding 0, single step 149001
2019-03-19 13:19:18,484 [INFO] Sum of grad norms of most recent batch: 0.010843
2019-03-19 13:19:18,484 [INFO] ---------------------------------
2019-03-19 13:19:39,681 [INFO] ---------------------------------
2019-03-19 13:19:39,682 [INFO] Summary:
2019-03-19 13:19:39,682 [INFO] Batch 150000, worst loss 0.000001 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.000000 @est.-depth 2
2019-03-19 13:19:39,683 [INFO] Regularization: 8913.060547 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:19:39,683 [INFO] unfolding 0, single step 150001
2019-03-19 13:19:39,684 [INFO] Sum of grad norms of most recent batch: 0.005692
2019-03-19 13:19:39,684 [INFO] ---------------------------------
2019-03-19 13:19:45,505 [INFO] ---------------------------------
2019-03-19 13:19:45,508 [INFO] Evaluation:
2019-03-19 13:19:45,509 [INFO] Batch 150000, worst loss 0.000001 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 13:19:45,511 [INFO] ---------------------------------
2019-03-19 13:19:45,512 [INFO] Finished training, saved to file transition/1552984806/1552997985_3_transition_final.pth
2019-03-19 13:19:45,699 [INFO] ---------------------------------
2019-03-19 13:19:45,701 [INFO] Training model #4: (203, 64, 1) @ 2
2019-03-19 13:20:07,090 [INFO] ---------------------------------
2019-03-19 13:20:07,091 [INFO] Summary:
2019-03-19 13:20:07,091 [INFO] Batch 1000, worst loss 17.115896 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:20:07,092 [INFO] Regularization: 9256.607422 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:20:07,092 [INFO] unfolding 0, single step 1001
2019-03-19 13:20:07,093 [INFO] Sum of grad norms of most recent batch: 15.145786
2019-03-19 13:20:07,093 [INFO] ---------------------------------
2019-03-19 13:20:28,109 [INFO] ---------------------------------
2019-03-19 13:20:28,110 [INFO] Summary:
2019-03-19 13:20:28,110 [INFO] Batch 2000, worst loss 0.168807 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:20:28,111 [INFO] Regularization: 9284.940430 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:20:28,111 [INFO] unfolding 0, single step 2001
2019-03-19 13:20:28,112 [INFO] Sum of grad norms of most recent batch: 3.576879
2019-03-19 13:20:28,113 [INFO] ---------------------------------
2019-03-19 13:20:49,611 [INFO] ---------------------------------
2019-03-19 13:20:49,612 [INFO] Summary:
2019-03-19 13:20:49,613 [INFO] Batch 3000, worst loss 0.028154 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:20:49,613 [INFO] Regularization: 9296.216797 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:20:49,613 [INFO] unfolding 0, single step 3001
2019-03-19 13:20:49,614 [INFO] Sum of grad norms of most recent batch: 3.492936
2019-03-19 13:20:49,614 [INFO] ---------------------------------
2019-03-19 13:21:10,886 [INFO] ---------------------------------
2019-03-19 13:21:10,887 [INFO] Summary:
2019-03-19 13:21:10,888 [INFO] Batch 4000, worst loss 0.028248 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:21:10,888 [INFO] Regularization: 9313.983398 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:21:10,888 [INFO] unfolding 0, single step 4001
2019-03-19 13:21:10,889 [INFO] Sum of grad norms of most recent batch: 3.616064
2019-03-19 13:21:10,889 [INFO] ---------------------------------
2019-03-19 13:21:33,142 [INFO] ---------------------------------
2019-03-19 13:21:33,143 [INFO] Summary:
2019-03-19 13:21:33,143 [INFO] Batch 5000, worst loss 0.016144 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:21:33,144 [INFO] Regularization: 9319.385742 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:21:33,144 [INFO] unfolding 0, single step 5001
2019-03-19 13:21:33,145 [INFO] Sum of grad norms of most recent batch: 2.795099
2019-03-19 13:21:33,146 [INFO] ---------------------------------
2019-03-19 13:21:54,791 [INFO] ---------------------------------
2019-03-19 13:21:54,792 [INFO] Summary:
2019-03-19 13:21:54,793 [INFO] Batch 6000, worst loss 0.022275 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:21:54,793 [INFO] Regularization: 9322.945312 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:21:54,794 [INFO] unfolding 0, single step 6001
2019-03-19 13:21:54,794 [INFO] Sum of grad norms of most recent batch: 0.704595
2019-03-19 13:21:54,795 [INFO] ---------------------------------
2019-03-19 13:22:16,668 [INFO] ---------------------------------
2019-03-19 13:22:16,668 [INFO] Summary:
2019-03-19 13:22:16,669 [INFO] Batch 7000, worst loss 0.041343 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:22:16,669 [INFO] Regularization: 9343.843750 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:22:16,670 [INFO] unfolding 0, single step 7001
2019-03-19 13:22:16,670 [INFO] Sum of grad norms of most recent batch: 2.055211
2019-03-19 13:22:16,671 [INFO] ---------------------------------
2019-03-19 13:22:38,420 [INFO] ---------------------------------
2019-03-19 13:22:38,421 [INFO] Summary:
2019-03-19 13:22:38,421 [INFO] Batch 8000, worst loss 0.040219 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:22:38,422 [INFO] Regularization: 9361.792969 * 0.0000000000 = 0.0000000094 loss
2019-03-19 13:22:38,422 [INFO] unfolding 0, single step 8001
2019-03-19 13:22:38,423 [INFO] Sum of grad norms of most recent batch: 1.250918
2019-03-19 13:22:38,423 [INFO] ---------------------------------
2019-03-19 13:22:59,769 [INFO] ---------------------------------
2019-03-19 13:22:59,770 [INFO] Summary:
2019-03-19 13:22:59,770 [INFO] Batch 9000, worst loss 0.048743 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:22:59,771 [INFO] Regularization: 9335.261719 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:22:59,771 [INFO] unfolding 0, single step 9001
2019-03-19 13:22:59,772 [INFO] Sum of grad norms of most recent batch: 3.614220
2019-03-19 13:22:59,772 [INFO] ---------------------------------
2019-03-19 13:23:21,137 [INFO] ---------------------------------
2019-03-19 13:23:21,138 [INFO] Summary:
2019-03-19 13:23:21,139 [INFO] Batch 10000, worst loss 0.072713 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:23:21,139 [INFO] Regularization: 9287.184570 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:23:21,140 [INFO] unfolding 0, single step 10001
2019-03-19 13:23:21,140 [INFO] Sum of grad norms of most recent batch: 3.416665
2019-03-19 13:23:21,141 [INFO] ---------------------------------
2019-03-19 13:23:27,003 [INFO] ---------------------------------
2019-03-19 13:23:27,003 [INFO] Evaluation:
2019-03-19 13:23:27,004 [INFO] Batch 10000, worst loss 0.009877 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 13:23:27,006 [INFO] ---------------------------------
2019-03-19 13:23:48,786 [INFO] ---------------------------------
2019-03-19 13:23:48,787 [INFO] Summary:
2019-03-19 13:23:48,787 [INFO] Batch 11000, worst loss 0.041868 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:23:48,788 [INFO] Regularization: 9299.797852 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:23:48,788 [INFO] unfolding 0, single step 11001
2019-03-19 13:23:48,789 [INFO] Sum of grad norms of most recent batch: 3.542730
2019-03-19 13:23:48,789 [INFO] ---------------------------------
2019-03-19 13:24:10,230 [INFO] ---------------------------------
2019-03-19 13:24:10,231 [INFO] Summary:
2019-03-19 13:24:10,232 [INFO] Batch 12000, worst loss 0.023857 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:24:10,232 [INFO] Regularization: 9269.465820 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:24:10,233 [INFO] unfolding 0, single step 12001
2019-03-19 13:24:10,233 [INFO] Sum of grad norms of most recent batch: 2.546577
2019-03-19 13:24:10,234 [INFO] ---------------------------------
2019-03-19 13:24:31,717 [INFO] ---------------------------------
2019-03-19 13:24:31,718 [INFO] Summary:
2019-03-19 13:24:31,719 [INFO] Batch 13000, worst loss 0.019232 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:24:31,720 [INFO] Regularization: 9265.784180 * 0.0000000000 = 0.0000000093 loss
2019-03-19 13:24:31,720 [INFO] unfolding 0, single step 13001
2019-03-19 13:24:31,720 [INFO] Sum of grad norms of most recent batch: 1.402001
2019-03-19 13:24:31,721 [INFO] ---------------------------------
2019-03-19 13:24:53,431 [INFO] ---------------------------------
2019-03-19 13:24:53,432 [INFO] Summary:
2019-03-19 13:24:53,432 [INFO] Batch 14000, worst loss 0.023049 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:24:53,433 [INFO] Regularization: 9247.959961 * 0.0000000000 = 0.0000000092 loss
2019-03-19 13:24:53,433 [INFO] unfolding 0, single step 14001
2019-03-19 13:24:53,434 [INFO] Sum of grad norms of most recent batch: 1.367085
2019-03-19 13:24:53,434 [INFO] ---------------------------------
2019-03-19 13:25:15,622 [INFO] ---------------------------------
2019-03-19 13:25:15,623 [INFO] Summary:
2019-03-19 13:25:15,624 [INFO] Batch 15000, worst loss 0.019518 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:25:15,624 [INFO] Regularization: 9213.892578 * 0.0000000000 = 0.0000000092 loss
2019-03-19 13:25:15,625 [INFO] unfolding 0, single step 15001
2019-03-19 13:25:15,625 [INFO] Sum of grad norms of most recent batch: 2.730890
2019-03-19 13:25:15,626 [INFO] ---------------------------------
2019-03-19 13:25:37,582 [INFO] ---------------------------------
2019-03-19 13:25:37,582 [INFO] Summary:
2019-03-19 13:25:37,583 [INFO] Batch 16000, worst loss 0.018581 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:25:37,584 [INFO] Regularization: 9168.782227 * 0.0000000000 = 0.0000000092 loss
2019-03-19 13:25:37,584 [INFO] unfolding 0, single step 16001
2019-03-19 13:25:37,584 [INFO] Sum of grad norms of most recent batch: 3.064811
2019-03-19 13:25:37,585 [INFO] ---------------------------------
2019-03-19 13:25:59,327 [INFO] ---------------------------------
2019-03-19 13:25:59,328 [INFO] Summary:
2019-03-19 13:25:59,329 [INFO] Batch 17000, worst loss 0.018649 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:25:59,329 [INFO] Regularization: 9116.355469 * 0.0000000000 = 0.0000000091 loss
2019-03-19 13:25:59,330 [INFO] unfolding 0, single step 17001
2019-03-19 13:25:59,330 [INFO] Sum of grad norms of most recent batch: 1.288544
2019-03-19 13:25:59,331 [INFO] ---------------------------------
2019-03-19 13:26:21,030 [INFO] ---------------------------------
2019-03-19 13:26:21,031 [INFO] Summary:
2019-03-19 13:26:21,032 [INFO] Batch 18000, worst loss 0.016941 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:26:21,032 [INFO] Regularization: 9079.800781 * 0.0000000000 = 0.0000000091 loss
2019-03-19 13:26:21,033 [INFO] unfolding 0, single step 18001
2019-03-19 13:26:21,033 [INFO] Sum of grad norms of most recent batch: 1.703602
2019-03-19 13:26:21,034 [INFO] ---------------------------------
2019-03-19 13:26:42,302 [INFO] ---------------------------------
2019-03-19 13:26:42,303 [INFO] Summary:
2019-03-19 13:26:42,304 [INFO] Batch 19000, worst loss 0.021333 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:26:42,304 [INFO] Regularization: 9035.741211 * 0.0000000000 = 0.0000000090 loss
2019-03-19 13:26:42,304 [INFO] unfolding 0, single step 19001
2019-03-19 13:26:42,305 [INFO] Sum of grad norms of most recent batch: 1.392172
2019-03-19 13:26:42,306 [INFO] ---------------------------------
2019-03-19 13:27:03,552 [INFO] ---------------------------------
2019-03-19 13:27:03,553 [INFO] Summary:
2019-03-19 13:27:03,554 [INFO] Batch 20000, worst loss 0.015145 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:27:03,554 [INFO] Regularization: 9032.811523 * 0.0000000000 = 0.0000000090 loss
2019-03-19 13:27:03,555 [INFO] unfolding 0, single step 20001
2019-03-19 13:27:03,555 [INFO] Sum of grad norms of most recent batch: 0.252132
2019-03-19 13:27:03,556 [INFO] ---------------------------------
2019-03-19 13:27:09,394 [INFO] ---------------------------------
2019-03-19 13:27:09,395 [INFO] Evaluation:
2019-03-19 13:27:09,396 [INFO] Batch 20000, worst loss 0.005744 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 13:27:09,397 [INFO] ---------------------------------
2019-03-19 13:27:31,210 [INFO] ---------------------------------
2019-03-19 13:27:31,211 [INFO] Summary:
2019-03-19 13:27:31,212 [INFO] Batch 21000, worst loss 0.017784 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:27:31,212 [INFO] Regularization: 9005.616211 * 0.0000000000 = 0.0000000090 loss
2019-03-19 13:27:31,212 [INFO] unfolding 0, single step 21001
2019-03-19 13:27:31,213 [INFO] Sum of grad norms of most recent batch: 0.631651
2019-03-19 13:27:31,214 [INFO] ---------------------------------
2019-03-19 13:27:52,833 [INFO] ---------------------------------
2019-03-19 13:27:52,834 [INFO] Summary:
2019-03-19 13:27:52,835 [INFO] Batch 22000, worst loss 0.017330 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:27:52,836 [INFO] Regularization: 8999.487305 * 0.0000000000 = 0.0000000090 loss
2019-03-19 13:27:52,836 [INFO] unfolding 0, single step 22001
2019-03-19 13:27:52,837 [INFO] Sum of grad norms of most recent batch: 1.043145
2019-03-19 13:27:52,837 [INFO] ---------------------------------
2019-03-19 13:28:14,387 [INFO] ---------------------------------
2019-03-19 13:28:14,388 [INFO] Summary:
2019-03-19 13:28:14,389 [INFO] Batch 23000, worst loss 0.017878 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:28:14,389 [INFO] Regularization: 8972.853516 * 0.0000000000 = 0.0000000090 loss
2019-03-19 13:28:14,390 [INFO] unfolding 0, single step 23001
2019-03-19 13:28:14,390 [INFO] Sum of grad norms of most recent batch: 2.623915
2019-03-19 13:28:14,391 [INFO] ---------------------------------
2019-03-19 13:28:36,041 [INFO] ---------------------------------
2019-03-19 13:28:36,041 [INFO] Summary:
2019-03-19 13:28:36,042 [INFO] Batch 24000, worst loss 0.010361 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:28:36,043 [INFO] Regularization: 8929.688477 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:28:36,043 [INFO] unfolding 0, single step 24001
2019-03-19 13:28:36,043 [INFO] Sum of grad norms of most recent batch: 4.553942
2019-03-19 13:28:36,044 [INFO] ---------------------------------
2019-03-19 13:28:57,934 [INFO] ---------------------------------
2019-03-19 13:28:57,935 [INFO] Summary:
2019-03-19 13:28:57,936 [INFO] Batch 25000, worst loss 0.011210 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:28:57,936 [INFO] Regularization: 8957.327148 * 0.0000000000 = 0.0000000090 loss
2019-03-19 13:28:57,937 [INFO] unfolding 0, single step 25001
2019-03-19 13:28:57,937 [INFO] Sum of grad norms of most recent batch: 0.502989
2019-03-19 13:28:57,938 [INFO] ---------------------------------
2019-03-19 13:29:19,548 [INFO] ---------------------------------
2019-03-19 13:29:19,549 [INFO] Summary:
2019-03-19 13:29:19,550 [INFO] Batch 26000, worst loss 0.012928 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:29:19,550 [INFO] Regularization: 8939.481445 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:29:19,551 [INFO] unfolding 0, single step 26001
2019-03-19 13:29:19,551 [INFO] Sum of grad norms of most recent batch: 1.482218
2019-03-19 13:29:19,552 [INFO] ---------------------------------
2019-03-19 13:29:41,393 [INFO] ---------------------------------
2019-03-19 13:29:41,394 [INFO] Summary:
2019-03-19 13:29:41,395 [INFO] Batch 27000, worst loss 0.012626 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:29:41,395 [INFO] Regularization: 8911.211914 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:29:41,396 [INFO] unfolding 0, single step 27001
2019-03-19 13:29:41,396 [INFO] Sum of grad norms of most recent batch: 1.589370
2019-03-19 13:29:41,397 [INFO] ---------------------------------
2019-03-19 13:30:03,097 [INFO] ---------------------------------
2019-03-19 13:30:03,097 [INFO] Summary:
2019-03-19 13:30:03,098 [INFO] Batch 28000, worst loss 0.007227 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:30:03,099 [INFO] Regularization: 8866.622070 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:30:03,099 [INFO] unfolding 0, single step 28001
2019-03-19 13:30:03,100 [INFO] Sum of grad norms of most recent batch: 1.402302
2019-03-19 13:30:03,100 [INFO] ---------------------------------
2019-03-19 13:30:25,017 [INFO] ---------------------------------
2019-03-19 13:30:25,018 [INFO] Summary:
2019-03-19 13:30:25,019 [INFO] Batch 29000, worst loss 0.008226 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:30:25,019 [INFO] Regularization: 8864.491211 * 0.0000000000 = 0.0000000089 loss
2019-03-19 13:30:25,020 [INFO] unfolding 0, single step 29001
2019-03-19 13:30:25,020 [INFO] Sum of grad norms of most recent batch: 1.302855
2019-03-19 13:30:25,021 [INFO] ---------------------------------
2019-03-19 13:30:46,912 [INFO] ---------------------------------
2019-03-19 13:30:46,913 [INFO] Summary:
2019-03-19 13:30:46,913 [INFO] Batch 30000, worst loss 0.009398 of 1000 batches (incl. reg.) (unfolding: 0), learning rate 0.001000 @est.-depth 2
2019-03-19 13:30:46,914 [INFO] Regularization: 8840.645508 * 0.0000000000 = 0.0000000088 loss
2019-03-19 13:30:46,914 [INFO] unfolding 0, single step 30001
2019-03-19 13:30:46,915 [INFO] Sum of grad norms of most recent batch: 1.568656
2019-03-19 13:30:46,915 [INFO] ---------------------------------
2019-03-19 13:30:52,757 [INFO] ---------------------------------
2019-03-19 13:30:52,758 [INFO] Evaluation:
2019-03-19 13:30:52,759 [INFO] Batch 30000, worst loss 0.007234 of 1000 batches (without reg.) @est.-depth 2
2019-03-19 13:30:52,760 [INFO] ---------------------------------
